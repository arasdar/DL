{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "      <th>msd_track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41.08</td>\n",
       "      <td>6.579</td>\n",
       "      <td>4.307</td>\n",
       "      <td>3.421</td>\n",
       "      <td>3.192</td>\n",
       "      <td>2.076</td>\n",
       "      <td>2.179</td>\n",
       "      <td>2.052</td>\n",
       "      <td>1.794</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3470</td>\n",
       "      <td>-0.2463</td>\n",
       "      <td>-1.5470</td>\n",
       "      <td>0.17920</td>\n",
       "      <td>-1.1530</td>\n",
       "      <td>-0.7370</td>\n",
       "      <td>0.40750</td>\n",
       "      <td>-0.67190</td>\n",
       "      <td>-0.05147</td>\n",
       "      <td>TRPLTEM128F92E1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>60.80</td>\n",
       "      <td>5.973</td>\n",
       "      <td>4.344</td>\n",
       "      <td>3.261</td>\n",
       "      <td>2.835</td>\n",
       "      <td>2.725</td>\n",
       "      <td>2.446</td>\n",
       "      <td>1.884</td>\n",
       "      <td>1.962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3316</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>-1.4760</td>\n",
       "      <td>0.52700</td>\n",
       "      <td>-2.1960</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>-1.39000</td>\n",
       "      <td>0.22560</td>\n",
       "      <td>-0.72080</td>\n",
       "      <td>TRJWMBQ128F424155E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51.47</td>\n",
       "      <td>4.971</td>\n",
       "      <td>4.316</td>\n",
       "      <td>2.916</td>\n",
       "      <td>3.112</td>\n",
       "      <td>2.290</td>\n",
       "      <td>2.053</td>\n",
       "      <td>1.934</td>\n",
       "      <td>1.878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2803</td>\n",
       "      <td>-0.1603</td>\n",
       "      <td>-0.1355</td>\n",
       "      <td>1.03500</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>1.4890</td>\n",
       "      <td>0.02959</td>\n",
       "      <td>-0.13670</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>TRRZWMO12903CCFCC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>41.28</td>\n",
       "      <td>6.610</td>\n",
       "      <td>4.411</td>\n",
       "      <td>2.602</td>\n",
       "      <td>2.822</td>\n",
       "      <td>2.126</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1.973</td>\n",
       "      <td>1.945</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6930</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>-0.3953</td>\n",
       "      <td>0.26710</td>\n",
       "      <td>-1.0450</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.03724</td>\n",
       "      <td>1.04500</td>\n",
       "      <td>-0.20000</td>\n",
       "      <td>TRBZRUT12903CE6C04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54.17</td>\n",
       "      <td>8.945</td>\n",
       "      <td>4.685</td>\n",
       "      <td>4.208</td>\n",
       "      <td>3.154</td>\n",
       "      <td>3.527</td>\n",
       "      <td>2.733</td>\n",
       "      <td>2.202</td>\n",
       "      <td>2.686</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4690</td>\n",
       "      <td>-0.5449</td>\n",
       "      <td>-0.5622</td>\n",
       "      <td>-0.08968</td>\n",
       "      <td>-0.9823</td>\n",
       "      <td>-0.2445</td>\n",
       "      <td>-1.65800</td>\n",
       "      <td>-0.04825</td>\n",
       "      <td>-0.70950</td>\n",
       "      <td>TRLUJQF128F42AF5BF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9  \\\n",
       "0   1  41.08  6.579  4.307  3.421  3.192  2.076  2.179  2.052  1.794   \n",
       "1   2  60.80  5.973  4.344  3.261  2.835  2.725  2.446  1.884  1.962   \n",
       "2   3  51.47  4.971  4.316  2.916  3.112  2.290  2.053  1.934  1.878   \n",
       "3   4  41.28  6.610  4.411  2.602  2.822  2.126  1.984  1.973  1.945   \n",
       "4   5  54.17  8.945  4.685  4.208  3.154  3.527  2.733  2.202  2.686   \n",
       "\n",
       "          ...           att18   att19   att20    att21   att22   att23  \\\n",
       "0         ...          1.3470 -0.2463 -1.5470  0.17920 -1.1530 -0.7370   \n",
       "1         ...         -0.3316  0.3519 -1.4760  0.52700 -2.1960  1.5990   \n",
       "2         ...         -0.2803 -0.1603 -0.1355  1.03500  0.2370  1.4890   \n",
       "3         ...         -1.6930  1.0040 -0.3953  0.26710 -1.0450  0.4974   \n",
       "4         ...          2.4690 -0.5449 -0.5622 -0.08968 -0.9823 -0.2445   \n",
       "\n",
       "     att24    att25    att26        msd_track_id  \n",
       "0  0.40750 -0.67190 -0.05147  TRPLTEM128F92E1389  \n",
       "1 -1.39000  0.22560 -0.72080  TRJWMBQ128F424155E  \n",
       "2  0.02959 -0.13670  0.10820  TRRZWMO12903CCFCC2  \n",
       "3  0.03724  1.04500 -0.20000  TRBZRUT12903CE6C04  \n",
       "4 -1.65800 -0.04825 -0.70950  TRLUJQF128F42AF5BF  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att17</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.22</td>\n",
       "      <td>8.076</td>\n",
       "      <td>6.935</td>\n",
       "      <td>4.696</td>\n",
       "      <td>3.856</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.922</td>\n",
       "      <td>2.568</td>\n",
       "      <td>2.070</td>\n",
       "      <td>...</td>\n",
       "      <td>3.988</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>-2.2210</td>\n",
       "      <td>0.6453</td>\n",
       "      <td>-0.2923</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>-0.09179</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36.42</td>\n",
       "      <td>6.131</td>\n",
       "      <td>5.364</td>\n",
       "      <td>4.292</td>\n",
       "      <td>3.968</td>\n",
       "      <td>2.937</td>\n",
       "      <td>2.872</td>\n",
       "      <td>2.142</td>\n",
       "      <td>2.050</td>\n",
       "      <td>...</td>\n",
       "      <td>7.098</td>\n",
       "      <td>1.2290</td>\n",
       "      <td>0.5971</td>\n",
       "      <td>-1.0670</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>-1.8240</td>\n",
       "      <td>2.3130</td>\n",
       "      <td>-0.80890</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>-0.6225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70.01</td>\n",
       "      <td>5.496</td>\n",
       "      <td>4.698</td>\n",
       "      <td>3.699</td>\n",
       "      <td>3.258</td>\n",
       "      <td>2.293</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.034</td>\n",
       "      <td>...</td>\n",
       "      <td>4.449</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>1.6370</td>\n",
       "      <td>-1.0690</td>\n",
       "      <td>2.4160</td>\n",
       "      <td>-0.6299</td>\n",
       "      <td>1.4190</td>\n",
       "      <td>-0.81960</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>-0.5948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40.64</td>\n",
       "      <td>7.281</td>\n",
       "      <td>6.702</td>\n",
       "      <td>4.043</td>\n",
       "      <td>3.729</td>\n",
       "      <td>3.043</td>\n",
       "      <td>2.644</td>\n",
       "      <td>2.366</td>\n",
       "      <td>1.940</td>\n",
       "      <td>...</td>\n",
       "      <td>2.785</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>-1.1370</td>\n",
       "      <td>1.2750</td>\n",
       "      <td>1.7920</td>\n",
       "      <td>-2.1250</td>\n",
       "      <td>1.6090</td>\n",
       "      <td>-0.83230</td>\n",
       "      <td>-0.1998</td>\n",
       "      <td>-0.1218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38.85</td>\n",
       "      <td>7.118</td>\n",
       "      <td>5.703</td>\n",
       "      <td>4.825</td>\n",
       "      <td>4.088</td>\n",
       "      <td>3.823</td>\n",
       "      <td>3.254</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.193</td>\n",
       "      <td>...</td>\n",
       "      <td>4.536</td>\n",
       "      <td>2.1470</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>2.8050</td>\n",
       "      <td>0.2762</td>\n",
       "      <td>0.2504</td>\n",
       "      <td>1.04900</td>\n",
       "      <td>0.3447</td>\n",
       "      <td>-0.7689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9   ...    \\\n",
       "0   1  38.22  8.076  6.935  4.696  3.856  3.465  2.922  2.568  2.070   ...     \n",
       "1   2  36.42  6.131  5.364  4.292  3.968  2.937  2.872  2.142  2.050   ...     \n",
       "2   3  70.01  5.496  4.698  3.699  3.258  2.293  2.680  2.226  2.034   ...     \n",
       "3   4  40.64  7.281  6.702  4.043  3.729  3.043  2.644  2.366  1.940   ...     \n",
       "4   5  38.85  7.118  5.703  4.825  4.088  3.823  3.254  2.551  2.193   ...     \n",
       "\n",
       "   att17   att18   att19   att20   att21   att22   att23    att24   att25  \\\n",
       "0  3.988  0.4957  0.1836 -2.2210  0.6453 -0.2923  1.2000 -0.09179  0.4674   \n",
       "1  7.098  1.2290  0.5971 -1.0670  0.9569 -1.8240  2.3130 -0.80890  0.5612   \n",
       "2  4.449  0.4773  1.6370 -1.0690  2.4160 -0.6299  1.4190 -0.81960  0.9151   \n",
       "3  2.785  1.9000 -1.1370  1.2750  1.7920 -2.1250  1.6090 -0.83230 -0.1998   \n",
       "4  4.536  2.1470  1.0200 -0.2656  2.8050  0.2762  0.2504  1.04900  0.3447   \n",
       "\n",
       "    att26  \n",
       "0  0.2158  \n",
       "1 -0.6225  \n",
       "2 -0.5948  \n",
       "3 -0.1218  \n",
       "4 -0.7689  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/submission-random.csv')\n",
    "test_y_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Blues, Country, Electronic, Folk, International, Jazz, Latin, New_Age, Pop_Rock, Rap, Reggae, RnB, Vocal]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "test_X = np.array(test_x)\n",
    "\n",
    "# Getting rid of the first and the last column: Id and msd_track_id\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)\n",
    "\n",
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Vocal', 'New_Age', 'Rap', 'Folk', 'International', 'Blues', 'Country', 'Latin', 'Reggae', 'Electronic', 'Pop_Rock', 'RnB', 'Jazz'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the freq of the keys in the training labels\n",
    "counted_labels = Counter(train_Y)\n",
    "labels_keys = counted_labels.keys()\n",
    "labels_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blues',\n",
       " 'Country',\n",
       " 'Electronic',\n",
       " 'Folk',\n",
       " 'International',\n",
       " 'Jazz',\n",
       " 'Latin',\n",
       " 'New_Age',\n",
       " 'Pop_Rock',\n",
       " 'Rap',\n",
       " 'Reggae',\n",
       " 'RnB',\n",
       " 'Vocal']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_keys_sorted = sorted(labels_keys)\n",
    "labels_keys_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Blues': 0,\n",
       " 'Country': 1,\n",
       " 'Electronic': 2,\n",
       " 'Folk': 3,\n",
       " 'International': 4,\n",
       " 'Jazz': 5,\n",
       " 'Latin': 6,\n",
       " 'New_Age': 7,\n",
       " 'Pop_Rock': 8,\n",
       " 'Rap': 9,\n",
       " 'Reggae': 10,\n",
       " 'RnB': 11,\n",
       " 'Vocal': 12}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This for loop for creating a dictionary/ vocab\n",
    "key_to_val = {key: val for val, key in enumerate(labels_keys_sorted)}\n",
    "key_to_val['Country']\n",
    "key_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Blues',\n",
       " 1: 'Country',\n",
       " 2: 'Electronic',\n",
       " 3: 'Folk',\n",
       " 4: 'International',\n",
       " 5: 'Jazz',\n",
       " 6: 'Latin',\n",
       " 7: 'New_Age',\n",
       " 8: 'Pop_Rock',\n",
       " 9: 'Rap',\n",
       " 10: 'Reggae',\n",
       " 11: 'RnB',\n",
       " 12: 'Vocal'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_to_key = {val: key for val, key in enumerate(labels_keys_sorted)}\n",
    "val_to_key[1]\n",
    "val_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_val[each])\n",
    "    Y_train_vec.append(key_to_val[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13000, 26), (10400, 26), dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std\n",
    "X_train_val.shape, X_test.shape, X_train_val.dtype, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 26), (1300, 26), (10400, 26), (1300,), (11700,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, \n",
    "X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape \n",
    "# X_train.dtype, X_val.dtype\n",
    "# Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # Backprop\n",
    "#         dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "#         # Test the final model\n",
    "#         y_pred, y_logit = nn.test(X_test)\n",
    "#         loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "#         acc = np.mean(y_pred == y_test)\n",
    "#         print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#             acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700,), (11700, 26), (1300, 26), (1300,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1000 train loss: 2.4874 valid loss: 2.5332, valid accuracy: 0.1123\n",
      "Iter-2000 train loss: 2.4457 valid loss: 2.4914, valid accuracy: 0.1408\n",
      "Iter-3000 train loss: 2.4227 valid loss: 2.4514, valid accuracy: 0.1746\n",
      "Iter-4000 train loss: 2.4397 valid loss: 2.4113, valid accuracy: 0.1938\n",
      "Iter-5000 train loss: 2.3727 valid loss: 2.3737, valid accuracy: 0.1908\n",
      "Iter-6000 train loss: 2.2824 valid loss: 2.3396, valid accuracy: 0.2000\n",
      "Iter-7000 train loss: 2.3048 valid loss: 2.3097, valid accuracy: 0.2108\n",
      "Iter-8000 train loss: 2.3064 valid loss: 2.2838, valid accuracy: 0.2069\n",
      "Iter-9000 train loss: 2.2956 valid loss: 2.2622, valid accuracy: 0.2154\n",
      "Iter-10000 train loss: 2.1951 valid loss: 2.2435, valid accuracy: 0.2223\n",
      "Iter-11000 train loss: 2.1807 valid loss: 2.2271, valid accuracy: 0.2331\n",
      "Iter-12000 train loss: 2.1597 valid loss: 2.2124, valid accuracy: 0.2408\n",
      "Iter-13000 train loss: 2.1369 valid loss: 2.1991, valid accuracy: 0.2454\n",
      "Iter-14000 train loss: 2.3268 valid loss: 2.1867, valid accuracy: 0.2500\n",
      "Iter-15000 train loss: 2.1739 valid loss: 2.1753, valid accuracy: 0.2585\n",
      "Iter-16000 train loss: 2.2704 valid loss: 2.1638, valid accuracy: 0.2608\n",
      "Iter-17000 train loss: 2.1662 valid loss: 2.1541, valid accuracy: 0.2646\n",
      "Iter-18000 train loss: 2.2879 valid loss: 2.1451, valid accuracy: 0.2708\n",
      "Iter-19000 train loss: 2.1616 valid loss: 2.1368, valid accuracy: 0.2754\n",
      "Iter-20000 train loss: 2.1621 valid loss: 2.1287, valid accuracy: 0.2808\n",
      "Iter-21000 train loss: 2.1568 valid loss: 2.1207, valid accuracy: 0.2838\n",
      "Iter-22000 train loss: 2.0634 valid loss: 2.1147, valid accuracy: 0.2792\n",
      "Iter-23000 train loss: 2.2445 valid loss: 2.1081, valid accuracy: 0.2869\n",
      "Iter-24000 train loss: 1.9489 valid loss: 2.1033, valid accuracy: 0.2869\n",
      "Iter-25000 train loss: 1.9094 valid loss: 2.0987, valid accuracy: 0.2915\n",
      "Iter-26000 train loss: 1.9778 valid loss: 2.0940, valid accuracy: 0.2915\n",
      "Iter-27000 train loss: 2.0984 valid loss: 2.0900, valid accuracy: 0.2962\n",
      "Iter-28000 train loss: 2.1610 valid loss: 2.0861, valid accuracy: 0.2969\n",
      "Iter-29000 train loss: 2.1150 valid loss: 2.0825, valid accuracy: 0.3046\n",
      "Iter-30000 train loss: 2.0761 valid loss: 2.0792, valid accuracy: 0.3100\n",
      "Iter-31000 train loss: 2.1694 valid loss: 2.0764, valid accuracy: 0.3138\n",
      "Iter-32000 train loss: 1.9361 valid loss: 2.0734, valid accuracy: 0.3154\n",
      "Iter-33000 train loss: 1.9075 valid loss: 2.0707, valid accuracy: 0.3192\n",
      "Iter-34000 train loss: 2.0422 valid loss: 2.0686, valid accuracy: 0.3185\n",
      "Iter-35000 train loss: 1.7397 valid loss: 2.0667, valid accuracy: 0.3169\n",
      "Iter-36000 train loss: 1.9311 valid loss: 2.0641, valid accuracy: 0.3138\n",
      "Iter-37000 train loss: 1.9618 valid loss: 2.0612, valid accuracy: 0.3162\n",
      "Iter-38000 train loss: 2.0394 valid loss: 2.0592, valid accuracy: 0.3200\n",
      "Iter-39000 train loss: 2.2750 valid loss: 2.0580, valid accuracy: 0.3200\n",
      "Iter-40000 train loss: 2.0055 valid loss: 2.0564, valid accuracy: 0.3154\n",
      "Iter-41000 train loss: 1.8964 valid loss: 2.0543, valid accuracy: 0.3169\n",
      "Iter-42000 train loss: 1.8393 valid loss: 2.0523, valid accuracy: 0.3162\n",
      "Iter-43000 train loss: 1.9970 valid loss: 2.0514, valid accuracy: 0.3162\n",
      "Iter-44000 train loss: 1.9649 valid loss: 2.0495, valid accuracy: 0.3177\n",
      "Iter-45000 train loss: 1.9899 valid loss: 2.0472, valid accuracy: 0.3169\n",
      "Iter-46000 train loss: 2.1119 valid loss: 2.0463, valid accuracy: 0.3223\n",
      "Iter-47000 train loss: 2.0064 valid loss: 2.0450, valid accuracy: 0.3185\n",
      "Iter-48000 train loss: 2.0789 valid loss: 2.0444, valid accuracy: 0.3177\n",
      "Iter-49000 train loss: 1.9025 valid loss: 2.0430, valid accuracy: 0.3192\n",
      "Iter-50000 train loss: 1.9137 valid loss: 2.0420, valid accuracy: 0.3238\n",
      "Iter-51000 train loss: 1.7851 valid loss: 2.0414, valid accuracy: 0.3246\n",
      "Iter-52000 train loss: 1.9235 valid loss: 2.0396, valid accuracy: 0.3238\n",
      "Iter-53000 train loss: 2.0958 valid loss: 2.0387, valid accuracy: 0.3215\n",
      "Iter-54000 train loss: 1.9145 valid loss: 2.0365, valid accuracy: 0.3246\n",
      "Iter-55000 train loss: 1.6929 valid loss: 2.0350, valid accuracy: 0.3238\n",
      "Iter-56000 train loss: 1.8507 valid loss: 2.0342, valid accuracy: 0.3262\n",
      "Iter-57000 train loss: 1.8110 valid loss: 2.0324, valid accuracy: 0.3277\n",
      "Iter-58000 train loss: 2.0237 valid loss: 2.0315, valid accuracy: 0.3254\n",
      "Iter-59000 train loss: 2.0857 valid loss: 2.0305, valid accuracy: 0.3285\n",
      "Iter-60000 train loss: 2.1128 valid loss: 2.0297, valid accuracy: 0.3323\n",
      "Iter-61000 train loss: 1.9479 valid loss: 2.0288, valid accuracy: 0.3308\n",
      "Iter-62000 train loss: 2.0238 valid loss: 2.0276, valid accuracy: 0.3323\n",
      "Iter-63000 train loss: 1.9744 valid loss: 2.0263, valid accuracy: 0.3331\n",
      "Iter-64000 train loss: 1.8760 valid loss: 2.0255, valid accuracy: 0.3323\n",
      "Iter-65000 train loss: 2.1676 valid loss: 2.0253, valid accuracy: 0.3354\n",
      "Iter-66000 train loss: 2.1912 valid loss: 2.0235, valid accuracy: 0.3369\n",
      "Iter-67000 train loss: 1.8207 valid loss: 2.0230, valid accuracy: 0.3392\n",
      "Iter-68000 train loss: 2.0522 valid loss: 2.0227, valid accuracy: 0.3377\n",
      "Iter-69000 train loss: 2.0973 valid loss: 2.0224, valid accuracy: 0.3369\n",
      "Iter-70000 train loss: 2.0817 valid loss: 2.0208, valid accuracy: 0.3385\n",
      "Iter-71000 train loss: 1.8259 valid loss: 2.0194, valid accuracy: 0.3362\n",
      "Iter-72000 train loss: 1.9549 valid loss: 2.0181, valid accuracy: 0.3377\n",
      "Iter-73000 train loss: 1.8629 valid loss: 2.0175, valid accuracy: 0.3377\n",
      "Iter-74000 train loss: 2.1868 valid loss: 2.0166, valid accuracy: 0.3385\n",
      "Iter-75000 train loss: 2.1571 valid loss: 2.0153, valid accuracy: 0.3400\n",
      "Iter-76000 train loss: 1.8146 valid loss: 2.0146, valid accuracy: 0.3400\n",
      "Iter-77000 train loss: 2.1255 valid loss: 2.0141, valid accuracy: 0.3415\n",
      "Iter-78000 train loss: 1.7764 valid loss: 2.0137, valid accuracy: 0.3392\n",
      "Iter-79000 train loss: 2.0576 valid loss: 2.0124, valid accuracy: 0.3392\n",
      "Iter-80000 train loss: 2.1068 valid loss: 2.0117, valid accuracy: 0.3362\n",
      "Iter-81000 train loss: 2.1725 valid loss: 2.0110, valid accuracy: 0.3392\n",
      "Iter-82000 train loss: 1.8516 valid loss: 2.0105, valid accuracy: 0.3392\n",
      "Iter-83000 train loss: 1.7698 valid loss: 2.0090, valid accuracy: 0.3392\n",
      "Iter-84000 train loss: 2.0893 valid loss: 2.0088, valid accuracy: 0.3377\n",
      "Iter-85000 train loss: 1.9869 valid loss: 2.0078, valid accuracy: 0.3377\n",
      "Iter-86000 train loss: 1.7579 valid loss: 2.0068, valid accuracy: 0.3369\n",
      "Iter-87000 train loss: 1.8157 valid loss: 2.0062, valid accuracy: 0.3362\n",
      "Iter-88000 train loss: 1.9175 valid loss: 2.0055, valid accuracy: 0.3362\n",
      "Iter-89000 train loss: 2.2848 valid loss: 2.0051, valid accuracy: 0.3354\n",
      "Iter-90000 train loss: 2.0957 valid loss: 2.0043, valid accuracy: 0.3338\n",
      "Iter-91000 train loss: 1.8608 valid loss: 2.0034, valid accuracy: 0.3354\n",
      "Iter-92000 train loss: 1.9170 valid loss: 2.0028, valid accuracy: 0.3362\n",
      "Iter-93000 train loss: 1.7378 valid loss: 2.0023, valid accuracy: 0.3346\n",
      "Iter-94000 train loss: 1.9688 valid loss: 2.0013, valid accuracy: 0.3362\n",
      "Iter-95000 train loss: 1.8014 valid loss: 2.0011, valid accuracy: 0.3362\n",
      "Iter-96000 train loss: 1.7216 valid loss: 2.0001, valid accuracy: 0.3369\n",
      "Iter-97000 train loss: 2.0470 valid loss: 1.9990, valid accuracy: 0.3385\n",
      "Iter-98000 train loss: 2.0513 valid loss: 1.9985, valid accuracy: 0.3354\n",
      "Iter-99000 train loss: 1.8725 valid loss: 1.9983, valid accuracy: 0.3362\n",
      "Iter-100000 train loss: 1.8671 valid loss: 1.9977, valid accuracy: 0.3338\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 1000 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "# num_output_units = Y_train.shape[1] # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ddd603198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6db6186908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'Blues',\n",
       " 'Country',\n",
       " 'Electronic',\n",
       " 'Folk',\n",
       " 'International',\n",
       " 'Jazz',\n",
       " 'Latin',\n",
       " 'New_Age',\n",
       " 'Pop_Rock',\n",
       " 'Rap',\n",
       " 'Reggae',\n",
       " 'RnB',\n",
       " 'Vocal']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = labels_keys_sorted.copy()\n",
    "heading.insert(0, 'Id')\n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10400, 13),\n",
       " (10400, 26),\n",
       " (10400, 13),\n",
       " (10400, 14),\n",
       "    Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       " 0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       " \n",
       "    New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       " 0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  )"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, y_logits = nn.test(X_test)\n",
    "y_prob = l.softmax(y_logits)\n",
    "y_prob.shape, X_test.shape, y_logits.shape, test_y_sample.shape, test_y_sample[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for Id, pred in enumerate(y_prob):\n",
    "#     print(Id+1, *pred)\n",
    "    pred_list.append([Id+1, *pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_file = open(file='prediction.csv', mode='w')\n",
    "pred_file.write('\\n') # because of the previous line        \n",
    "\n",
    "for idx in range(len(heading)):\n",
    "    if idx < len(heading) - 1:\n",
    "        pred_file.write(heading[idx] + ',')\n",
    "    else:\n",
    "        pred_file.write(heading[idx] + '\\n')        \n",
    "\n",
    "# len(test), test[0]\n",
    "# for key in test:\n",
    "for i in range(len(pred_list)): # rows\n",
    "    for j in range(len(pred_list[i])): # cols\n",
    "        if j < (len(pred_list[i]) - 1):\n",
    "            pred_file.write(str(pred_list[i][j]))\n",
    "            pred_file.write(',')\n",
    "        else: # last item before starting a new line\n",
    "            pred_file.write(str(pred_list[i][j]) + '\\n')        \n",
    "\n",
    "# pred_file.write(-',')\n",
    "pred_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.014351</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.027349</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.021185</td>\n",
       "      <td>0.028903</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.357759</td>\n",
       "      <td>0.466736</td>\n",
       "      <td>0.045330</td>\n",
       "      <td>0.011533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.033985</td>\n",
       "      <td>0.021606</td>\n",
       "      <td>0.006480</td>\n",
       "      <td>0.021184</td>\n",
       "      <td>0.035349</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.104205</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.177233</td>\n",
       "      <td>0.476158</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.057869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.015306</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.047801</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.055136</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.257526</td>\n",
       "      <td>0.402544</td>\n",
       "      <td>0.117527</td>\n",
       "      <td>0.011267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.037962</td>\n",
       "      <td>0.059091</td>\n",
       "      <td>0.079912</td>\n",
       "      <td>0.071817</td>\n",
       "      <td>0.046805</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>0.045886</td>\n",
       "      <td>0.005654</td>\n",
       "      <td>0.019982</td>\n",
       "      <td>0.070013</td>\n",
       "      <td>0.295195</td>\n",
       "      <td>0.121141</td>\n",
       "      <td>0.134472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.017175</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>0.035481</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.010198</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.020936</td>\n",
       "      <td>0.517281</td>\n",
       "      <td>0.342062</td>\n",
       "      <td>0.021382</td>\n",
       "      <td>0.002367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id     Blues   Country  Electronic      Folk  International      Jazz  \\\n",
       "0   1  0.014351  0.005998    0.027349  0.003593       0.011859  0.021185   \n",
       "1   2  0.033985  0.021606    0.006480  0.021184       0.035349  0.004296   \n",
       "2   3  0.015306  0.016725    0.047801  0.010999       0.029815  0.003330   \n",
       "3   4  0.037962  0.059091    0.079912  0.071817       0.046805  0.012071   \n",
       "4   5  0.017175  0.006762    0.035481  0.002944       0.010198  0.001259   \n",
       "\n",
       "      Latin   New_Age  Pop_Rock       Rap    Reggae       RnB     Vocal  \n",
       "0  0.028903  0.001260  0.004145  0.357759  0.466736  0.045330  0.011533  \n",
       "1  0.104205  0.001535  0.005822  0.177233  0.476158  0.054277  0.057869  \n",
       "2  0.055136  0.001317  0.030708  0.257526  0.402544  0.117527  0.011267  \n",
       "3  0.045886  0.005654  0.019982  0.070013  0.295195  0.121141  0.134472  \n",
       "4  0.021930  0.000224  0.020936  0.517281  0.342062  0.021382  0.002367  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10400, 14), (10400, 14))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').shape, test_y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

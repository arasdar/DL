{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        layer = 0 # self.L = 1\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "            caches[layer].append(cache)\n",
    "            ys.append(y)\n",
    "            \n",
    "        for layer in range(1, self.L):\n",
    "            X_train = ys.copy()\n",
    "            ys = []\n",
    "            for X in X_train:\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                ys.append(y)\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for layer in reversed(range(self.L)):\n",
    "            if layer < (self.L - 1): dys = dXs.copy()\n",
    "            dXs = []\n",
    "            for t in reversed(range(len(dys))):\n",
    "                dX = dys[t]\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    #     for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    eps = 1e-8\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.0\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches/ full batches/ batch files\n",
    "        # Minibacthes\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_train=y_mini, ys=ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "        \n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 35.6258\n",
      "in Fivinalacinarly ghan\"kodsidad gatiny in the world's femperorldâ€“Taty world's Stitidionn, The of be,\n",
      "Iter-20 loss: 28.0942\n",
      "initest in michina a meleturint and and Stuman wsid cinty leceand ghan soneesd Nipto \"2Ashast upan ex\n",
      "Iter-30 loss: 29.7614\n",
      "ing powert of inal ald offligunary hom-lewery cntomobared 20th lardest menturan ecCmitest indoWsuren.\n",
      "Iter-40 loss: 27.9330\n",
      "ihas mament. In Nopplowdstireabesedef lagion. Japan orea an itht ecobal thing world-sanes. Japan, was\n",
      "Iter-50 loss: 26.4714\n",
      "inat minty id d was rconary IndeI inory, noleowiog silchylconter ea d first eckomo, infrurnowionar an\n",
      "Iter-60 loss: 19.0425\n",
      "iri, stumain and in love of Japan's loud limeded inlarlo1s, thicx and ferea in the GEosthond pirth la\n",
      "Iter-70 loss: 16.8299\n",
      "iSto-s from led's hixcopon in the worldetheled it of unhe kix, the by rurest I devinese veirlo-darea \n",
      "Iter-80 loss: 15.8789\n",
      "ith-largest of Japan, the callest atiotidevided insureation enjoys the worlowivigishif-dalgest-meines\n",
      "Iter-90 loss: 17.7103\n",
      "in-kaky. Areation, part of interid-dal a UnemsHuntar Digitareed const coufthe a7ghond and stuten the \n",
      "Iter-100 loss: 16.2417\n",
      "ing the hfudest. Ielion iv tomed bout thir canthy wipina in Earasi and is sevena, \"han make up a rolf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GRU at 0x1109f8550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVBLTJKCUUklMk4BSSiUxTQJKKZXE/h9KRnCfcH4siwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10680da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Smooth train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fNIRS data for Human Activity Recognition (HAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../../../datasets/fNIRs_data/P11-4-17-2018/': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P11-4-17-2018/\n",
    "# % find ../../datasets/fNIRs_data/ | grep fNIR_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mP12-4-17-2018\u001b[0m/  \u001b[01;34mP14-4-18-2018\u001b[0m/  \u001b[01;34mP16-4-18-2018\u001b[0m/\r\n",
      "\u001b[01;34mP13-4-17-2018\u001b[0m/  \u001b[01;34mP15-4-18-2018\u001b[0m/  \u001b[01;34mP17-4-18-2018\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(name, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpaths = find_all(name='fNIR_data.txt', path='/home/arasdar/datasets/fNIRs_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/5. Left Leg/fNIR_data.txt'],\n",
       " 30)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 folders and subjects\n",
    "# 5 folders \n",
    "# 7*5=35 but there are 34 so one is missing.\n",
    "# This is why I got rid of the first which was subject P11.\n",
    "# 6 subjects and 5 activities = 30 files\n",
    "# 1, 2, 3, 4, 5\n",
    "# 0, 1, 2, 3, 4 these are all the classes\n",
    "((sorted(allpaths, reverse=False)), len(allpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       " '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       " '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/3. Left Hand/fNIR_data.txt')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allpaths[0], allpaths[-1], allpaths[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/pandas/io/parsers.py:709: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    }
   ],
   "source": [
    "# df: data frame object\n",
    "df = []\n",
    "for each_idx in range(len(allpaths)):\n",
    "    df.append(pd.read_csv(filepath_or_buffer=allpaths[each_idx], names=['time', 'sample', \n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df), df[0].shape, df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for each in range(len(df)):\n",
    "#     print(df[each]['sample'][2])\n",
    "    df[each]['sample'][1:] = df[each]['sample'][1:].astype(str).str[2:]\n",
    "    df[each]['channel.39'][1:] = df[each]['channel.39'][1:].astype(str).str[1:-1]    \n",
    "# # df[23], allpaths[23], \n",
    "# #  '/home/arasdar/datasets/fNIRs_data/P11-4-17-2018/Right Leg/fNIR_data.txt')\n",
    "# pd.read_csv(filepath_or_buffer=allpaths[23], names=['time', 'sample', \n",
    "#                        'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "#                        'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "#                        'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "#                        'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "#                        'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "#                        'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "#                        'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "#                        'channel', 'channel', 'channel', 'channel', 'channel']).head(), allpaths[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each in range(len(df)):\n",
    "#     print(df[each].head())\n",
    "# # df['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = []\n",
    "for each in range(len(df)):\n",
    "    matrices.append(df[each][1:].as_matrix().astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For every single data or txt file here we have a label\n",
    "# # In total we 'll have five labels: 1, 2, 3, 4, 5\n",
    "# # These labels converted to a number will be: 0, 1, 2, 3, 4\n",
    "# # this would be 5 dimensional array\n",
    "# # starting like this:\n",
    "# np.array([1, 0, 0, 0, 0])\n",
    "# np.array([0, 1, 0, 0, 0])\n",
    "# # 0 1 0 0 0 \n",
    "# # 0 0 1 0 0\n",
    "# # 0 0 0 1 0\n",
    "# # 0 0 0 0 1\n",
    "# # ----------server/datasets/Project-HAR/subject-date/activity-class/input-data\n",
    "# # (['/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
    "# #   '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
    "# #   '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
    "# #   '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
    "# #   '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
    "# # len(data)\n",
    "# # For every single row in the matrix, \n",
    "# # there is gonna be a correspondence/ label/ supervision.\n",
    "# # That s how we can label this fNIRs signal.\n",
    "# # This is very much like the txt file and the folder\n",
    "# # it is subject independent, gender independent.\n",
    "# # fNIRs data meaning data dependent\n",
    "# # labeled/ certain activity\n",
    "# # subject and date independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = [], []\n",
    "for row in range(0, len(matrices), 1):\n",
    "    data.append(matrices[row][:, 2:])\n",
    "    mat = np.zeros([data[row].shape[0], 5])\n",
    "    # for each in range(mat.shape[0]):\n",
    "    mat[:, row%5] = 1\n",
    "    labels.append(mat)\n",
    "#     print('mat, len(labels): ', mat, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.25568253, 0.13550095, 0.22802679, 0.48467571, 0.23755921,\n",
       "        0.16856721, 0.29574981, 0.7120409 , 0.83408046, 0.24134316,\n",
       "        0.4348596 , 0.10550072, 0.12523653, 0.29109681, 0.22010829,\n",
       "        0.16065139, 0.39467266, 0.59167653, 0.17369114, 0.5237844 ,\n",
       "        0.3942053 , 0.26517305, 0.32810646, 0.93380678, 0.4306449 ,\n",
       "        0.23833884, 0.52017915, 1.09951508, 1.2449733 , 0.3827343 ,\n",
       "        0.4514387 , 0.22096752, 0.22596563, 0.50641578, 0.34252322,\n",
       "        0.21044534, 0.52494335, 0.75623262, 0.2755928 , 0.778355  ]),\n",
       " array([1., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape, labels[0].shape\n",
    "data[0][1], labels[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDO\n",
    "# Standardizing the dataset or normalizing the dataset\n",
    "# The question here is: if we should normalize/standardize every single batch alone or\n",
    "# normalize them all together?\n",
    "# X = np.array(data)\n",
    "# X.shape, X.dtype\n",
    "# X = np.zeros_like(data[0])\n",
    "# X.shape, X.dtype\n",
    "X, Y = data[0], labels[0] # initialize the stack\n",
    "X.shape, X.dtype, Y.shape, Y.dtype\n",
    "for each in range(1, len(data), 1): # start, stop, step\n",
    "    X = np.vstack(tup=(X, data[each]))\n",
    "    Y = np.vstack(tup=(Y, labels[each]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50405, 5), dtype('float64'), 0.0, 0.0)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, X[300, 10], data[0][300, 10],\n",
    "Y.shape, Y.dtype, Y[300, 2], labels[0][300, 2],\n",
    "# t, n = 8387+1195+2383 -1, 40-1\n",
    "# X[t, n], data[3-1][2383 -1][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((133168, 40),\n",
       " dtype('float64'),\n",
       " array([ 3.24409756e-17, -1.56228909e-16, -1.70741977e-17,  1.70741977e-17,\n",
       "        -8.19561488e-17,  1.05860026e-16,  6.82967907e-17, -7.51264697e-17,\n",
       "        -5.80522721e-17, -5.20763029e-17,  8.19561488e-17,  2.73187163e-17,\n",
       "        -1.53667779e-17,  8.70784081e-17, -1.62204878e-16, -1.66473427e-16,\n",
       "        -8.53709883e-17, -1.17811964e-16,  5.80522721e-17, -1.09274865e-16,\n",
       "         1.51960359e-16, -2.11293196e-17, -3.04134146e-17,  6.82967907e-18,\n",
       "         1.22934223e-16,  1.28056482e-17, -2.39038767e-17, -5.46374325e-17,\n",
       "         4.69540436e-17, -3.58558151e-17, -7.85413093e-17, -3.41483953e-17,\n",
       "         4.95151732e-17,  6.14671116e-17, -2.73187163e-17, -2.04890372e-17,\n",
       "         9.90303464e-17, -2.56112965e-17,  7.25653401e-17,  8.87858278e-17]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xnorm = (X - X.mean(axis=0))/ X.std(axis=0)\n",
    "Xnorm.shape, Xnorm.dtype, Xnorm.mean(axis=0), Xnorm.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(485.0, 233.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr = 0.129 # 111933.573-111933.504 is the difference between each two samples or sampling rate\n",
    "each_trial=30 # 30seconds=20sec+10sec\n",
    "# num_samples_per_trial\n",
    "# each trial or epoch based on BCI course\n",
    "# height or window size\n",
    "30/0.129, np.floor(30/0.129), np.ceil(30/0.129)\n",
    "# The size of each image for convnet will be 233x40x1 which is equal to the format NHWC, txn or NWC\n",
    "# NWC is for signals since all the channels are projected the same as all channels in an image as well.\n",
    "# In each convolutionn for image or signals the channels are all included for filtering\n",
    "# Image or video might be part of this\n",
    "# Image is NHWC or NCHW\n",
    "# Signal is NWC or NCW\n",
    "# N is the number of trials/epochs/windows\n",
    "# C is the number of channels is 40 in this experiment: 20 for hemoglobin with o2 and 20 for hemoglobin with co2\n",
    "# W is the window width size or signal window size which is 233\n",
    "width = np.ceil(30/0.129)\n",
    "# This is the number of minibatches per file\n",
    "num_mb = mat.shape[0] - width +1\n",
    "# num_mb # number of minibatches\n",
    "# totla number of all the windows with the overlapping windows of 1 sample\n",
    "num_mb, width\n",
    "# len(allpaths), allpaths[44], num_mb*len(allpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8137, 40) (250, 40) (8387, 40) 250\n",
      "float64 float64 float64 250\n",
      "(945, 40) (250, 40) (1195, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2133, 40) (250, 40) (2383, 40) 250\n",
      "float64 float64 float64 250\n",
      "(979, 40) (250, 40) (1229, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2124, 40) (250, 40) (2374, 40) 250\n",
      "float64 float64 float64 250\n",
      "(946, 40) (250, 40) (1196, 40) 250\n",
      "float64 float64 float64 250\n",
      "(952, 40) (250, 40) (1202, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2154, 40) (250, 40) (2404, 40) 250\n",
      "float64 float64 float64 250\n",
      "(945, 40) (250, 40) (1195, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2129, 40) (250, 40) (2379, 40) 250\n",
      "float64 float64 float64 250\n",
      "(991, 40) (250, 40) (1241, 40) 250\n",
      "float64 float64 float64 250\n",
      "(979, 40) (250, 40) (1229, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2136, 40) (250, 40) (2386, 40) 250\n",
      "float64 float64 float64 250\n",
      "(973, 40) (250, 40) (1223, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2128, 40) (250, 40) (2378, 40) 250\n",
      "float64 float64 float64 250\n",
      "(976, 40) (250, 40) (1226, 40) 250\n",
      "float64 float64 float64 250\n",
      "(951, 40) (250, 40) (1201, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2121, 40) (250, 40) (2371, 40) 250\n",
      "float64 float64 float64 250\n",
      "(959, 40) (250, 40) (1209, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2127, 40) (250, 40) (2377, 40) 250\n",
      "float64 float64 float64 250\n",
      "(971, 40) (250, 40) (1221, 40) 250\n",
      "float64 float64 float64 250\n",
      "(8011, 40) (250, 40) (8261, 40) 250\n",
      "float64 float64 float64 250\n",
      "(950, 40) (250, 40) (1200, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2138, 40) (250, 40) (2388, 40) 250\n",
      "float64 float64 float64 250\n",
      "(993, 40) (250, 40) (1243, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2137, 40) (250, 40) (2387, 40) 250\n",
      "float64 float64 float64 250\n",
      "(8025, 40) (250, 40) (8275, 40) 250\n",
      "float64 float64 float64 250\n",
      "(8108, 40) (250, 40) (8358, 40) 250\n",
      "float64 float64 float64 250\n",
      "(945, 40) (250, 40) (1195, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2122, 40) (250, 40) (2372, 40) 250\n",
      "float64 float64 float64 250\n",
      "(951, 40) (250, 40) (1201, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2135, 40) (250, 40) (2385, 40) 250\n",
      "float64 float64 float64 250\n",
      "(978, 40) (250, 40) (1228, 40) 250\n",
      "float64 float64 float64 250\n",
      "(8026, 40) (250, 40) (8276, 40) 250\n",
      "float64 float64 float64 250\n",
      "(972, 40) (250, 40) (1222, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2122, 40) (250, 40) (2372, 40) 250\n",
      "float64 float64 float64 250\n",
      "(969, 40) (250, 40) (1219, 40) 250\n",
      "float64 float64 float64 250\n",
      "(2121, 40) (250, 40) (2371, 40) 250\n",
      "float64 float64 float64 250\n",
      "(971, 40) (250, 40) (1221, 40) 250\n",
      "float64 float64 float64 250\n",
      "(8164, 40) (250, 40) (8414, 40) 250\n",
      "float64 float64 float64 250\n",
      "(8071, 40) (250, 40) (8321, 40) 250\n",
      "float64 float64 float64 250\n",
      "(8028, 40) (250, 40) (8278, 40) 250\n",
      "float64 float64 float64 250\n",
      "(8008, 40) (250, 40) (8258, 40) 250\n",
      "float64 float64 float64 250\n",
      "(467, 40) (250, 40) (717, 40) 250\n",
      "float64 float64 float64 250\n"
     ]
    }
   ],
   "source": [
    "width, l, h, Xnt, Xnv = 250, 0, 0, [], []\n",
    "for idx in range(0, len(data), 1): # start, stop, step\n",
    "    l = h\n",
    "    h += data[idx].shape[0]\n",
    "    #     print(idx, l, h, h-width, Xnorm.shape, data[idx].shape, Xnorm[l:h].shape, width)\n",
    "    Xnt.append(Xnorm[l:h-width])\n",
    "    Xnv.append(Xnorm[h-width:h])\n",
    "    print(Xnt[idx].shape, Xnv[idx].shape, data[idx].shape, width)    \n",
    "    print(Xnt[idx].dtype, Xnv[idx].dtype, data[idx].dtype, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111212\n"
     ]
    }
   ],
   "source": [
    "# This is scanning through the signal and extracting the minibatches\n",
    "# TO be accurate and having more samples, I want to scan it with the \n",
    "# window size=250, overlap/stride=1, no padding\n",
    "# Xnt-minibatches, only test one of them\n",
    "Xnt[0].shape, width, Xnt[0].shape[0]-width+1, Xnt[1].shape[0]-width+1\n",
    "# total number of minibatches extracted from each sample\n",
    "num_mb = 0\n",
    "for each in range(len(Xnt)):\n",
    "#     print(Xnt[each].shape[0]-width+1)\n",
    "    num_mb += Xnt[each].shape[0]-width+1\n",
    "print(num_mb)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to extract batches/minibatches from the first set of training data\n",
    "# The number of minibatches are equal to\n",
    "# This is the number of minibatches in the first set\n",
    "num_mb = Xnt[0].shape[0]-width+1\n",
    "# this the python list for holding all the minibatches inside\n",
    "mb=[]\n",
    "for each in range(num_mb):\n",
    "    #it has to start from 0+step:width+step/stride and sweeping the entire signal\n",
    "#     for stride in range()\n",
    "# The number of minibatches should be equal to the length of the list\n",
    "    mb.append(Xnt[0][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7888, 7888, (250, 40))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let s double check the minibatches list length and the estimated number of minibatches\n",
    "len(mb), num_mb, mb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mb = Xnt[1].shape[0]-width+1\n",
    "for each in range(num_mb):\n",
    "    mb.append(Xnt[1][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 8584, 8584)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_mb, len(mb), 7888+696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mb = Xnt[2].shape[0]-width+1\n",
    "for each in range(num_mb):\n",
    "    mb.append(Xnt[2][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1884, 10468, 10468)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_mb, len(mb), 7888+696+1884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = []\n",
    "for eachXnt in range(len(Xnt)):\n",
    "    num_mb = Xnt[eachXnt].shape[0]-width+1\n",
    "    for each in range(num_mb):\n",
    "        mb.append(Xnt[eachXnt][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 111212, 44)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # len(mb)\n",
    "# for each in Xnt:\n",
    "#     print(each.shape, each.dtype)\n",
    "len(Xnt), len(mb), len(Xnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n"
     ]
    }
   ],
   "source": [
    "# len(mb)\n",
    "for each in Xnv:\n",
    "    print(each.shape, each.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

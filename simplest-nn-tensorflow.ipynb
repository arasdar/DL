{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import sys\n",
    "\n",
    "def feedforward_net(D, C):\n",
    "    # Input layer which is a visible for MNIST images which 28x28x1 = 28*28=784\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, D]) # num_samples/images x 784, D=784, D=Dimensions of input images\n",
    "    \n",
    "    # output classes for MNIST which is based 10 first digits starting from 0 tp 9\n",
    "    y = tf.placeholder(dtype=tf.float32, shape=[None, C]) # num_samples for output classes x 10, C=10, 10 output classes\n",
    "\n",
    "    # Connections/synapses from input to the output\n",
    "    Wxy = tf.Variable(tf.random_normal(shape=[D, C], stddev=1.0, mean=0.0, dtype=tf.float32), dtype=tf.float32)\n",
    "    \n",
    "#     # biases which is based on the number of output classes\n",
    "#     bxy = tf.Variable(tf.zeros(shape=[C]))\n",
    "    \n",
    "    # Output probability from the network\n",
    "    # We can not use relu because we have to generate the probaby for each class\n",
    "    # Relu only cuts og the negative part but doesn't generate probabilty\n",
    "    # prob means from 0-1 but the output of relu can be more than one\n",
    "    # we have to use softmax\n",
    "    logits = tf.matmul(X, Wxy);\n",
    "    \n",
    "    # Neccessary for training for generating the probability values: 0< y<1\n",
    "    prob = tf.nn.softmax(logits=logits)\n",
    "    \n",
    "    # Error between predicted output and ground truth classes/output\n",
    "    loss = -tf.reduce_mean(y * tf.log(prob))\n",
    "\n",
    "    return X, y, prob, loss\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/arasdar/datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/arasdar/datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/arasdar/datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/arasdar/datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "# from tensorflow.contrib.learn.datasets.mnist import input_data\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "mnist = input_data.read_data_sets('/home/arasdar/datasets/MNIST_data', one_hot=True)\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "\n",
    "D, C = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "#     elif net_type == 'ff':\n",
    "X, y, forward_step, loss = feedforward_net(D, C)\n",
    "\n",
    "# This is learning rate\n",
    "alpha = 1e-3\n",
    "solver = tf.train.RMSPropOptimizer(alpha)\n",
    "train_step = solver.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 Loss: 1.7755 Validation: 0.0534\n",
      "Iter: 100 Loss: 1.4038 Validation: 0.0524\n",
      "Iter: 200 Loss: 0.9487 Validation: 0.1124\n",
      "Iter: 300 Loss: 0.6379 Validation: 0.226\n",
      "Iter: 400 Loss: 0.4458 Validation: 0.3808\n",
      "Iter: 500 Loss: 0.2486 Validation: 0.493\n",
      "Iter: 600 Loss: 0.2811 Validation: 0.5696\n",
      "Iter: 700 Loss: 0.1902 Validation: 0.6244\n",
      "Iter: 800 Loss: 0.2066 Validation: 0.6708\n",
      "Iter: 900 Loss: 0.1487 Validation: 0.7036\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "#     sess.run(tf.initialize_all_variables())\n",
    "\n",
    "#     WARNING:tensorflow:From \n",
    "#     Instructions for updating:\n",
    "#     Use `tf.global_variables_initializer` instead.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_epochs = 1000\n",
    "M = 64 # minibatch\n",
    "for i in range(num_epochs):\n",
    "    X_mb, y_mb = mnist.train.next_batch(M)\n",
    "\n",
    "    _, loss_val = sess.run([train_step, loss], feed_dict={X: X_mb, y: y_mb})\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        y_pred = sess.run(forward_step, feed_dict={X: X_val})\n",
    "        acc = accuracy(y_val, y_pred)\n",
    "\n",
    "        print('Iter: {} Loss: {:.4f} Validation: {}'.format(i, loss_val, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

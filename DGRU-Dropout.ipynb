{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0] # no regularizarion or no reg_loss\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    # import impl.constant as c\n",
    "    eps = 1e-8\n",
    "    state = nn.initial_state()\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches/ full batches/ batch files\n",
    "        # Minibacthes\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "        \n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[0].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=mb_size)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 loss: 3.1921\n",
      "e an. wopocinly,eegeang rith JJaIn, Cyn A1v andgs rad, Thin in lor Jared kepoed Wopran. ting s8erd an\n",
      "Iter-26 loss: 2.4822\n",
      "e pargosins oantionpeuth %uv; Gpy woCvd an th eorhigæœ¬ roonevgio: urdtfs. Japan cind and hi pargest lc\n",
      "Iter-39 loss: 2.4643\n",
      "ed lapes, WoSdland uf-th the Eman ithailakeicev woira,\"th Chetmr roped. Tivt piries Tou, phicier pecr\n",
      "Iter-52 loss: 2.0958\n",
      "e Japp ny5d, for Cry eai aeme po9laies uomed in in a med anr. feceaerear is tatpuncyga\",.\"he,liin,y -\n",
      "Iter-65 loss: 2.1185\n",
      "eSor. Jhivng anc pf atpiran was rirtom 1m laces the woranld, h iomeed rivdg eny grt mentilo, anll, en\n",
      "Iter-78 loss: 1.6136\n",
      "er and the far Iit.  lht 2966.oTland first mined the fe5kt ind eroud.\n",
      "s5fkcoitan Imuked aeseva Awaral\n",
      "Iter-91 loss: 1.3766\n",
      "en Couceid ofucth led es tabal Mepon Beatev. maroy of JfaNre Gincrrocitinn-lic concilan the malpe tax\n",
      "Iter-104 loss: 1.5431\n",
      "egter lopld the Uesucourtd's i lirit of an wo and the. WJargstet.iUNancicid,canceclariest. NokCangest\n",
      "Iter-117 loss: 1.1737\n",
      "e pprorlioed algifia th 1 vo-Jamankegiced fges iasetitugaest of inteata. Nhh tuhing R.ijas imdp the c\n",
      "Iter-130 loss: 1.6204\n",
      "ed xarianitamaed. maswitideE7ndirgh, apon, iition fanl au iiRthan centthtly dm encld the Empert anc i\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GRU at 0x7fa38e60cb00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width\n",
    "num_layers = 2 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # keep_prob??\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "# vocab_size = len(char_to_idx)\n",
    "num_input_units = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa38e5a1cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

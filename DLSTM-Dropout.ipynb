{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_old, c_old = h\n",
    "        X_one_hot = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = hf * c_old + hi * hc\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        h_ = (h, c)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (\n",
    "                X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache,\n",
    "                ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache\n",
    "            )\n",
    "        else: # train=False\n",
    "            cache = (\n",
    "                X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache,\n",
    "                ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache\n",
    "            )\n",
    "\n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_next, dc_next = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc = dc + dc_next\n",
    "\n",
    "        dhf = c_old * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_next = dX[:, :self.H]\n",
    "        dc_next = hf * dc\n",
    "\n",
    "        dX = dX[:, self.H:]\n",
    "        dh = (dh_next, dc_next)\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches or only one\n",
    "        # Minibatches\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 loss: 22.9688\n",
      "cthes astes is Jaby lithivod58 the Eof in 194æœ¬ whor Apeorlonitye Ul O8e perias lasso fokurctase piof \n",
      "Iter-26 loss: 22.4197\n",
      "coded West. Japan and and Nilgessiac of laroerte. The Hountry the G8,h ferterdeld an omhite omela30 a\n",
      "Iter-39 loss: 12.0632\n",
      "chase foure diged of poper2 2917y peoplopn 3116 , eod Ru siviral Seapar zedat et in toe zexcrapan was\n",
      "Iter-52 loss: 13.9670\n",
      "cupan 9ag. on Japen by ex,itin. as ic d\"s a llow gedem. Ja.y mitition maicId and of isechaco;f wom Ja\n",
      "Iter-65 loss: 7.5913\n",
      "ctions and itoke byupandex, ea, doc ntre Autand country indmute the carlar. Toky, was intmy buth an i\n",
      "Iter-78 loss: 6.8217\n",
      "coulatind rea. Japan my be 1se earcy cital chcitg and Napoforiest China Sin my llenrbofinclaik op Jap\n",
      "Iter-91 loss: 5.7428\n",
      "cy, thit cinflowidod mton mbo thryy, which is il is hisid in in and runde hiteed Wised Honurled ceger\n",
      "Iter-104 loss: 5.0354\n",
      "chised ve from thiea sitst in the folurly bifest of tou c aponomily woint enad Sun mnouritr, thecdain\n",
      "Iter-117 loss: 5.2622\n",
      "canked peacen Japan eaginto mpplir Taginat tsitary the Sumportapan Pame on mund an Japand th urcclede\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD8UrAPgiPD7cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89d84922e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

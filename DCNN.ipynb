{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im2col_indices(X_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    # Input shape\n",
    "    N, C, H, W = X_shape\n",
    "    \n",
    "    # Kernel shape\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C\n",
    "    \n",
    "    # Output shape\n",
    "    assert (H + (2 * padding) - field_height) % stride == 0\n",
    "    assert (W + (2 * padding) - field_width) % stride == 0\n",
    "    out_height = int(((H + (2 * padding) - field_height) / stride) + 1)\n",
    "    out_width = int(((W + (2 * padding) - field_width) / stride) + 1)\n",
    "    out_C = 1 # the output channel/ depth\n",
    "\n",
    "    # Row, Height, i\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, field_C)\n",
    "    i1 = np.repeat(np.arange(out_height), out_width)\n",
    "    i1 = np.tile(i1, out_C)\n",
    "    i1 *= stride\n",
    "    \n",
    "    # Column, Width, j\n",
    "    j0 = np.tile(np.arange(field_width), field_height * field_C)\n",
    "    j1 = np.tile(np.arange(out_width), out_height * out_C)\n",
    "    j1 *= stride\n",
    "    \n",
    "    # Channel, Depth, K\n",
    "    k0 = np.repeat(np.arange(field_C), field_height * field_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 = np.repeat(np.arange(out_C), out_height * out_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 *= stride\n",
    "    \n",
    "    # Indices: i, j, k index\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = k0.reshape(-1, 1) + k1.reshape(1, -1)\n",
    "    \n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(X, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    X_padded = np.pad(X, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant') # X_NxCxHxW\n",
    "\n",
    "    k, i, j = get_im2col_indices(X.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    X_col = X_padded[:, k, i, j] # X_col_txkxn\n",
    "    \n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "    \n",
    "    X_col = X_col.transpose(1, 2, 0).reshape(kernel_size, -1)\n",
    "    \n",
    "    return X_col\n",
    "\n",
    "def col2im_indices(X_col, X_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = X_shape\n",
    "    H_padded, W_padded = H + (2 * padding), W + (2 * padding)\n",
    "    X_padded = np.zeros((N, C, H_padded, W_padded), dtype=X_col.dtype)\n",
    "    \n",
    "    k, i, j = get_im2col_indices(X_shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "\n",
    "    X_col = X_col.reshape(kernel_size, -1, N).transpose(2, 0, 1) # N, K, H * W\n",
    "    np.add.at(X_padded, (slice(None), k, i, j), X_col) # slice(None)== ':'\n",
    "    # # np.add.at(a, (slice(None), 0, 1, 2), cols_reshaped) # slice(None)== ':'\n",
    "    \n",
    "    if padding > 0:\n",
    "        X = X_padded[:, :, padding:-padding, padding:-padding]\n",
    "    else:\n",
    "        X = X_padded[:, :, :, :]\n",
    "        \n",
    "    return X\n",
    "        \n",
    "def conv_forward(X, W, b, stride=1, padding=1):\n",
    "    cache = W, b, stride, padding\n",
    "    \n",
    "    # Input X\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    \n",
    "    # Kernel W\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "    \n",
    "    # Output\n",
    "    h_out = ((h_x + (2 * padding) - h_filter) / stride) + 1\n",
    "    w_out = ((w_x + (2 * padding) - w_filter) / stride) + 1\n",
    "\n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filter, -1)\n",
    "\n",
    "    out = (W_col @ X_col) + b\n",
    "    out = out.reshape(n_filter, h_out, w_out, n_x).transpose(3, 0, 1, 2)\n",
    "    cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    X, W, b, stride, padding, X_col = cache\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "    db = db.reshape(n_filter, -1)\n",
    "\n",
    "    dout = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "    dW = dout @ X_col.T\n",
    "    dW = dW.reshape(W.shape)\n",
    "\n",
    "    W = W.reshape(n_filter, -1)\n",
    "    dX_col = W.T @ dout\n",
    "    dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "    return dX, dW, db\n",
    "\n",
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)\n",
    "\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "X_train = X_train.reshape(-1, *img_shape) # (-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape\n",
    "# X_train[0, :10, :10, :10]\n",
    "M, D, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout, lam):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.lam = lam\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                # h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = selu_dropout_forward(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = cross_entropy_reg(self.model[2], y, y_train, lam=self.lam)\n",
    "        dy = dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            # if train: There is no backward in testing/prediction\n",
    "            h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "            dy = selu_dropout_backward(dout=dy, cache=h_do_cache)\n",
    "            dh = selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = conv_backward(dout=dh, cache=X_conv_cache)\n",
    "        # dX: TODO: hast not been used but this basically should be 0 \n",
    "        # which means input can be perfectly recontructed!\n",
    "        # dX is the grad_input or delta for input or the calculated error or difference or delta\n",
    "        # Can be used as noise which is because it is unwanted and can be added to data to be calculated again\n",
    "        # when the data is not abundantly available!\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        # else: # self.mode == 'regression'\n",
    "        # return np.round(y_logit)\n",
    "        # y_prob for accuracy & y_logit for loss\n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        # if val_set:\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Train the model\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            # Shuffle in every iteration\n",
    "            # The dataset is static and non-sequentiol: no time-dependency or temporal pattern\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "            \n",
    "            # Update the model\n",
    "            for key in grad[0].keys():\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer].keys():\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for key in grad[2].keys():\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Test the updated model to validate the model\n",
    "            # Avoid overfitting/ memorizing and underfitting lack of model capacity\n",
    "            # if val_set:\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            # def accuracy(y_true, y_pred):\n",
    "            # return np.mean(y_pred == y_true)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "                \n",
    "        # Test the model after training and validation after all the epochs\n",
    "        # The test data has NOT been used before and has NOT been seen by the model\n",
    "        # # Kernel dead problem sometimes!\n",
    "        y_pred, _ = nn.test(X_test)\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}'.format(acc.mean(), acc.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 train loss: 2.9141 valid loss: 2.6896, valid accuracy: 0.1514\n",
      "Iter-2 train loss: 2.8301 valid loss: 2.5634, valid accuracy: 0.1674\n",
      "Iter-3 train loss: 2.2620 valid loss: 2.4477, valid accuracy: 0.1896\n",
      "Iter-4 train loss: 2.5052 valid loss: 2.3364, valid accuracy: 0.2202\n",
      "Iter-5 train loss: 2.3113 valid loss: 2.2380, valid accuracy: 0.2506\n",
      "Iter-6 train loss: 2.4693 valid loss: 2.1437, valid accuracy: 0.2802\n",
      "Iter-7 train loss: 2.4980 valid loss: 2.0526, valid accuracy: 0.3054\n",
      "Iter-8 train loss: 1.8609 valid loss: 1.9676, valid accuracy: 0.3312\n",
      "Iter-9 train loss: 1.9544 valid loss: 1.8886, valid accuracy: 0.3554\n",
      "Iter-10 train loss: 2.1580 valid loss: 1.8140, valid accuracy: 0.3782\n",
      "Iter-11 train loss: 1.8104 valid loss: 1.7443, valid accuracy: 0.4006\n",
      "Iter-12 train loss: 1.8688 valid loss: 1.6772, valid accuracy: 0.4292\n",
      "Iter-13 train loss: 1.8230 valid loss: 1.6131, valid accuracy: 0.4550\n",
      "Iter-14 train loss: 1.8752 valid loss: 1.5526, valid accuracy: 0.4756\n",
      "Iter-15 train loss: 1.7149 valid loss: 1.4944, valid accuracy: 0.4984\n",
      "Iter-16 train loss: 1.9954 valid loss: 1.4411, valid accuracy: 0.5202\n",
      "Iter-17 train loss: 1.6274 valid loss: 1.3899, valid accuracy: 0.5402\n",
      "Iter-18 train loss: 1.3613 valid loss: 1.3407, valid accuracy: 0.5594\n",
      "Iter-19 train loss: 1.7398 valid loss: 1.2915, valid accuracy: 0.5818\n",
      "Iter-20 train loss: 1.5972 valid loss: 1.2470, valid accuracy: 0.5984\n",
      "Iter-21 train loss: 1.5331 valid loss: 1.2071, valid accuracy: 0.6168\n",
      "Iter-22 train loss: 1.2467 valid loss: 1.1691, valid accuracy: 0.6308\n",
      "Iter-23 train loss: 1.2252 valid loss: 1.1339, valid accuracy: 0.6462\n",
      "Iter-24 train loss: 1.1825 valid loss: 1.1016, valid accuracy: 0.6580\n",
      "Iter-25 train loss: 1.2193 valid loss: 1.0723, valid accuracy: 0.6694\n",
      "Iter-26 train loss: 1.1591 valid loss: 1.0463, valid accuracy: 0.6776\n",
      "Iter-27 train loss: 1.2336 valid loss: 1.0235, valid accuracy: 0.6824\n",
      "Iter-28 train loss: 1.2423 valid loss: 1.0017, valid accuracy: 0.6888\n",
      "Iter-29 train loss: 1.3820 valid loss: 0.9810, valid accuracy: 0.6940\n",
      "Iter-30 train loss: 1.3102 valid loss: 0.9599, valid accuracy: 0.6998\n",
      "Iter-31 train loss: 1.3250 valid loss: 0.9394, valid accuracy: 0.7056\n",
      "Iter-32 train loss: 1.0031 valid loss: 0.9193, valid accuracy: 0.7118\n",
      "Iter-33 train loss: 1.2182 valid loss: 0.8993, valid accuracy: 0.7190\n",
      "Iter-34 train loss: 1.0315 valid loss: 0.8799, valid accuracy: 0.7254\n",
      "Iter-35 train loss: 0.9892 valid loss: 0.8622, valid accuracy: 0.7346\n",
      "Iter-36 train loss: 1.2920 valid loss: 0.8446, valid accuracy: 0.7384\n",
      "Iter-37 train loss: 0.8859 valid loss: 0.8276, valid accuracy: 0.7462\n",
      "Iter-38 train loss: 1.1426 valid loss: 0.8110, valid accuracy: 0.7504\n",
      "Iter-39 train loss: 1.0379 valid loss: 0.7946, valid accuracy: 0.7536\n",
      "Iter-40 train loss: 1.1817 valid loss: 0.7792, valid accuracy: 0.7620\n",
      "Iter-41 train loss: 0.9948 valid loss: 0.7658, valid accuracy: 0.7658\n",
      "Iter-42 train loss: 1.1068 valid loss: 0.7537, valid accuracy: 0.7694\n",
      "Iter-43 train loss: 0.8359 valid loss: 0.7431, valid accuracy: 0.7740\n",
      "Iter-44 train loss: 0.7956 valid loss: 0.7345, valid accuracy: 0.7774\n",
      "Iter-45 train loss: 0.9151 valid loss: 0.7263, valid accuracy: 0.7792\n",
      "Iter-46 train loss: 0.9148 valid loss: 0.7183, valid accuracy: 0.7808\n",
      "Iter-47 train loss: 1.0089 valid loss: 0.7110, valid accuracy: 0.7818\n",
      "Iter-48 train loss: 0.6282 valid loss: 0.7033, valid accuracy: 0.7826\n",
      "Iter-49 train loss: 0.7500 valid loss: 0.6949, valid accuracy: 0.7854\n",
      "Iter-50 train loss: 0.9183 valid loss: 0.6858, valid accuracy: 0.7892\n",
      "Iter-51 train loss: 0.8829 valid loss: 0.6772, valid accuracy: 0.7914\n",
      "Iter-52 train loss: 0.7254 valid loss: 0.6680, valid accuracy: 0.7942\n",
      "Iter-53 train loss: 0.7940 valid loss: 0.6597, valid accuracy: 0.7968\n",
      "Iter-54 train loss: 0.8920 valid loss: 0.6516, valid accuracy: 0.8002\n",
      "Iter-55 train loss: 0.8660 valid loss: 0.6438, valid accuracy: 0.8038\n",
      "Iter-56 train loss: 0.9252 valid loss: 0.6365, valid accuracy: 0.8050\n",
      "Iter-57 train loss: 0.7559 valid loss: 0.6300, valid accuracy: 0.8074\n",
      "Iter-58 train loss: 0.6115 valid loss: 0.6234, valid accuracy: 0.8088\n",
      "Iter-59 train loss: 0.6947 valid loss: 0.6172, valid accuracy: 0.8122\n",
      "Iter-60 train loss: 0.6293 valid loss: 0.6111, valid accuracy: 0.8132\n",
      "Iter-61 train loss: 0.5721 valid loss: 0.6054, valid accuracy: 0.8162\n",
      "Iter-62 train loss: 0.5907 valid loss: 0.6003, valid accuracy: 0.8172\n",
      "Iter-63 train loss: 0.8175 valid loss: 0.5954, valid accuracy: 0.8198\n",
      "Iter-64 train loss: 0.5294 valid loss: 0.5906, valid accuracy: 0.8222\n",
      "Iter-65 train loss: 0.7665 valid loss: 0.5859, valid accuracy: 0.8218\n",
      "Iter-66 train loss: 0.9149 valid loss: 0.5810, valid accuracy: 0.8238\n",
      "Iter-67 train loss: 0.8619 valid loss: 0.5756, valid accuracy: 0.8276\n",
      "Iter-68 train loss: 0.8040 valid loss: 0.5709, valid accuracy: 0.8286\n",
      "Iter-69 train loss: 0.5944 valid loss: 0.5670, valid accuracy: 0.8292\n",
      "Iter-70 train loss: 0.7809 valid loss: 0.5637, valid accuracy: 0.8302\n",
      "Iter-71 train loss: 0.6974 valid loss: 0.5606, valid accuracy: 0.8308\n",
      "Iter-72 train loss: 0.5104 valid loss: 0.5562, valid accuracy: 0.8348\n",
      "Iter-73 train loss: 0.5252 valid loss: 0.5513, valid accuracy: 0.8380\n",
      "Iter-74 train loss: 0.8732 valid loss: 0.5466, valid accuracy: 0.8398\n",
      "Iter-75 train loss: 0.7008 valid loss: 0.5422, valid accuracy: 0.8406\n",
      "Iter-76 train loss: 0.7603 valid loss: 0.5390, valid accuracy: 0.8398\n",
      "Iter-77 train loss: 0.5475 valid loss: 0.5365, valid accuracy: 0.8406\n",
      "Iter-78 train loss: 0.7242 valid loss: 0.5341, valid accuracy: 0.8408\n",
      "Iter-79 train loss: 0.6239 valid loss: 0.5303, valid accuracy: 0.8412\n",
      "Iter-80 train loss: 0.8967 valid loss: 0.5264, valid accuracy: 0.8408\n",
      "Iter-81 train loss: 0.5599 valid loss: 0.5228, valid accuracy: 0.8418\n",
      "Iter-82 train loss: 0.5364 valid loss: 0.5192, valid accuracy: 0.8444\n",
      "Iter-83 train loss: 0.8991 valid loss: 0.5156, valid accuracy: 0.8454\n",
      "Iter-84 train loss: 0.7066 valid loss: 0.5113, valid accuracy: 0.8458\n",
      "Iter-85 train loss: 0.6670 valid loss: 0.5073, valid accuracy: 0.8480\n",
      "Iter-86 train loss: 0.5049 valid loss: 0.5033, valid accuracy: 0.8492\n",
      "Iter-87 train loss: 0.6257 valid loss: 0.4998, valid accuracy: 0.8518\n",
      "Iter-88 train loss: 0.5180 valid loss: 0.4966, valid accuracy: 0.8528\n",
      "Iter-89 train loss: 0.5683 valid loss: 0.4941, valid accuracy: 0.8544\n",
      "Iter-90 train loss: 0.5889 valid loss: 0.4917, valid accuracy: 0.8552\n",
      "Iter-91 train loss: 0.6253 valid loss: 0.4896, valid accuracy: 0.8550\n",
      "Iter-92 train loss: 0.5018 valid loss: 0.4882, valid accuracy: 0.8566\n",
      "Iter-93 train loss: 0.6468 valid loss: 0.4869, valid accuracy: 0.8576\n",
      "Iter-94 train loss: 0.4033 valid loss: 0.4858, valid accuracy: 0.8568\n",
      "Iter-95 train loss: 0.4123 valid loss: 0.4849, valid accuracy: 0.8576\n",
      "Iter-96 train loss: 0.4758 valid loss: 0.4839, valid accuracy: 0.8568\n",
      "Iter-97 train loss: 0.5579 valid loss: 0.4829, valid accuracy: 0.8570\n",
      "Iter-98 train loss: 0.6680 valid loss: 0.4812, valid accuracy: 0.8576\n",
      "Iter-99 train loss: 0.5794 valid loss: 0.4799, valid accuracy: 0.8576\n",
      "Iter-100 train loss: 0.5456 valid loss: 0.4782, valid accuracy: 0.8584\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 10 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10 # number of kernels/ filters in each layer\n",
    "num_input_units = D # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = C # number of classes in this classification problem\n",
    "p_dropout = 0.95 #  layer & unit noise: keep_prob = p_dropout, q = 1-p, 0.95 or 0.90 by default, noise at the network level or layers\n",
    "lam = 1e-3 # output noise: reg at the feedback or loss function or function loss level as noise or loss_reg added.\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = CNN(C=num_output_units, D=num_input_units, H=num_hidden_units, p_dropout=p_dropout, L=num_layers, lam=lam)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:] + dX_prime[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "#     for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    eps = 1e-8\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.0\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches/ full batches/ batch files\n",
    "        # Minibacthes\n",
    "        for idx in range(len(minibatches)):\n",
    "            \n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "        \n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1000 loss: 38.2534\n",
      "e3 ale of Nrioljo hijthy wroaelorF weg romeldti negial Chica, uilar ombli gras mymkinuminkeæ—¥ t eses a\n",
      "Iter-2000 loss: 17.7267\n",
      "eJapan hase Japankse ex Areadese Wape of limaee anf Shigestiward ulcend perins  ophithise eanl Wornd \n",
      "Iter-3000 loss: 14.4039\n",
      "e firs and in thhos loffin mama: (Japanese papuran of bountrins a figeto ardatity in lyos of abof Jap\n",
      "Iter-4000 loss: 5.8806\n",
      "e the wortr loclingcesto aktery bite forry sulesolo, an outer an levercodes leunor Oher. Alicanimaji \n",
      "Iter-5000 loss: 5.3860\n",
      "ed Japan\"s loperet Aregsudity papin the worl mition in  igB thet  lonenor. The country ic poptr larea\n",
      "Iter-6000 loss: 3.6123\n",
      "ery Brans Indivenesed. rea and Wevel cinsti the world's tend riten of is the dive erdar lomymeltion e\n",
      "Iter-7000 loss: 7.0278\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the number\n",
      "Iter-8000 loss: 5.8195\n",
      "e inttitte fe menrlow boused and is ondurty onse merial remel.b.. entilatas of in the Globnu Wer. Th \n",
      "Iter-9000 loss: 7.6751\n",
      "e iollhis Kremeretio and xportal a unoped in iW etor Oletiono, Ky chifgked if the goth the Sea endtri\n",
      "Iter-10000 loss: 5.7623\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the Countr\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHARBEIQg/h+WzcrN2dkylQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106805be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 10000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # nn_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

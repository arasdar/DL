{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "      <th>msd_track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41.08</td>\n",
       "      <td>6.579</td>\n",
       "      <td>4.307</td>\n",
       "      <td>3.421</td>\n",
       "      <td>3.192</td>\n",
       "      <td>2.076</td>\n",
       "      <td>2.179</td>\n",
       "      <td>2.052</td>\n",
       "      <td>1.794</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3470</td>\n",
       "      <td>-0.2463</td>\n",
       "      <td>-1.5470</td>\n",
       "      <td>0.17920</td>\n",
       "      <td>-1.1530</td>\n",
       "      <td>-0.7370</td>\n",
       "      <td>0.40750</td>\n",
       "      <td>-0.67190</td>\n",
       "      <td>-0.05147</td>\n",
       "      <td>TRPLTEM128F92E1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>60.80</td>\n",
       "      <td>5.973</td>\n",
       "      <td>4.344</td>\n",
       "      <td>3.261</td>\n",
       "      <td>2.835</td>\n",
       "      <td>2.725</td>\n",
       "      <td>2.446</td>\n",
       "      <td>1.884</td>\n",
       "      <td>1.962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3316</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>-1.4760</td>\n",
       "      <td>0.52700</td>\n",
       "      <td>-2.1960</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>-1.39000</td>\n",
       "      <td>0.22560</td>\n",
       "      <td>-0.72080</td>\n",
       "      <td>TRJWMBQ128F424155E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51.47</td>\n",
       "      <td>4.971</td>\n",
       "      <td>4.316</td>\n",
       "      <td>2.916</td>\n",
       "      <td>3.112</td>\n",
       "      <td>2.290</td>\n",
       "      <td>2.053</td>\n",
       "      <td>1.934</td>\n",
       "      <td>1.878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2803</td>\n",
       "      <td>-0.1603</td>\n",
       "      <td>-0.1355</td>\n",
       "      <td>1.03500</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>1.4890</td>\n",
       "      <td>0.02959</td>\n",
       "      <td>-0.13670</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>TRRZWMO12903CCFCC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>41.28</td>\n",
       "      <td>6.610</td>\n",
       "      <td>4.411</td>\n",
       "      <td>2.602</td>\n",
       "      <td>2.822</td>\n",
       "      <td>2.126</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1.973</td>\n",
       "      <td>1.945</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6930</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>-0.3953</td>\n",
       "      <td>0.26710</td>\n",
       "      <td>-1.0450</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.03724</td>\n",
       "      <td>1.04500</td>\n",
       "      <td>-0.20000</td>\n",
       "      <td>TRBZRUT12903CE6C04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54.17</td>\n",
       "      <td>8.945</td>\n",
       "      <td>4.685</td>\n",
       "      <td>4.208</td>\n",
       "      <td>3.154</td>\n",
       "      <td>3.527</td>\n",
       "      <td>2.733</td>\n",
       "      <td>2.202</td>\n",
       "      <td>2.686</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4690</td>\n",
       "      <td>-0.5449</td>\n",
       "      <td>-0.5622</td>\n",
       "      <td>-0.08968</td>\n",
       "      <td>-0.9823</td>\n",
       "      <td>-0.2445</td>\n",
       "      <td>-1.65800</td>\n",
       "      <td>-0.04825</td>\n",
       "      <td>-0.70950</td>\n",
       "      <td>TRLUJQF128F42AF5BF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9  \\\n",
       "0   1  41.08  6.579  4.307  3.421  3.192  2.076  2.179  2.052  1.794   \n",
       "1   2  60.80  5.973  4.344  3.261  2.835  2.725  2.446  1.884  1.962   \n",
       "2   3  51.47  4.971  4.316  2.916  3.112  2.290  2.053  1.934  1.878   \n",
       "3   4  41.28  6.610  4.411  2.602  2.822  2.126  1.984  1.973  1.945   \n",
       "4   5  54.17  8.945  4.685  4.208  3.154  3.527  2.733  2.202  2.686   \n",
       "\n",
       "          ...           att18   att19   att20    att21   att22   att23  \\\n",
       "0         ...          1.3470 -0.2463 -1.5470  0.17920 -1.1530 -0.7370   \n",
       "1         ...         -0.3316  0.3519 -1.4760  0.52700 -2.1960  1.5990   \n",
       "2         ...         -0.2803 -0.1603 -0.1355  1.03500  0.2370  1.4890   \n",
       "3         ...         -1.6930  1.0040 -0.3953  0.26710 -1.0450  0.4974   \n",
       "4         ...          2.4690 -0.5449 -0.5622 -0.08968 -0.9823 -0.2445   \n",
       "\n",
       "     att24    att25    att26        msd_track_id  \n",
       "0  0.40750 -0.67190 -0.05147  TRPLTEM128F92E1389  \n",
       "1 -1.39000  0.22560 -0.72080  TRJWMBQ128F424155E  \n",
       "2  0.02959 -0.13670  0.10820  TRRZWMO12903CCFCC2  \n",
       "3  0.03724  1.04500 -0.20000  TRBZRUT12903CE6C04  \n",
       "4 -1.65800 -0.04825 -0.70950  TRLUJQF128F42AF5BF  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att17</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.22</td>\n",
       "      <td>8.076</td>\n",
       "      <td>6.935</td>\n",
       "      <td>4.696</td>\n",
       "      <td>3.856</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.922</td>\n",
       "      <td>2.568</td>\n",
       "      <td>2.070</td>\n",
       "      <td>...</td>\n",
       "      <td>3.988</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>-2.2210</td>\n",
       "      <td>0.6453</td>\n",
       "      <td>-0.2923</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>-0.09179</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36.42</td>\n",
       "      <td>6.131</td>\n",
       "      <td>5.364</td>\n",
       "      <td>4.292</td>\n",
       "      <td>3.968</td>\n",
       "      <td>2.937</td>\n",
       "      <td>2.872</td>\n",
       "      <td>2.142</td>\n",
       "      <td>2.050</td>\n",
       "      <td>...</td>\n",
       "      <td>7.098</td>\n",
       "      <td>1.2290</td>\n",
       "      <td>0.5971</td>\n",
       "      <td>-1.0670</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>-1.8240</td>\n",
       "      <td>2.3130</td>\n",
       "      <td>-0.80890</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>-0.6225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70.01</td>\n",
       "      <td>5.496</td>\n",
       "      <td>4.698</td>\n",
       "      <td>3.699</td>\n",
       "      <td>3.258</td>\n",
       "      <td>2.293</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.034</td>\n",
       "      <td>...</td>\n",
       "      <td>4.449</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>1.6370</td>\n",
       "      <td>-1.0690</td>\n",
       "      <td>2.4160</td>\n",
       "      <td>-0.6299</td>\n",
       "      <td>1.4190</td>\n",
       "      <td>-0.81960</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>-0.5948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40.64</td>\n",
       "      <td>7.281</td>\n",
       "      <td>6.702</td>\n",
       "      <td>4.043</td>\n",
       "      <td>3.729</td>\n",
       "      <td>3.043</td>\n",
       "      <td>2.644</td>\n",
       "      <td>2.366</td>\n",
       "      <td>1.940</td>\n",
       "      <td>...</td>\n",
       "      <td>2.785</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>-1.1370</td>\n",
       "      <td>1.2750</td>\n",
       "      <td>1.7920</td>\n",
       "      <td>-2.1250</td>\n",
       "      <td>1.6090</td>\n",
       "      <td>-0.83230</td>\n",
       "      <td>-0.1998</td>\n",
       "      <td>-0.1218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38.85</td>\n",
       "      <td>7.118</td>\n",
       "      <td>5.703</td>\n",
       "      <td>4.825</td>\n",
       "      <td>4.088</td>\n",
       "      <td>3.823</td>\n",
       "      <td>3.254</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.193</td>\n",
       "      <td>...</td>\n",
       "      <td>4.536</td>\n",
       "      <td>2.1470</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>2.8050</td>\n",
       "      <td>0.2762</td>\n",
       "      <td>0.2504</td>\n",
       "      <td>1.04900</td>\n",
       "      <td>0.3447</td>\n",
       "      <td>-0.7689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9   ...    \\\n",
       "0   1  38.22  8.076  6.935  4.696  3.856  3.465  2.922  2.568  2.070   ...     \n",
       "1   2  36.42  6.131  5.364  4.292  3.968  2.937  2.872  2.142  2.050   ...     \n",
       "2   3  70.01  5.496  4.698  3.699  3.258  2.293  2.680  2.226  2.034   ...     \n",
       "3   4  40.64  7.281  6.702  4.043  3.729  3.043  2.644  2.366  1.940   ...     \n",
       "4   5  38.85  7.118  5.703  4.825  4.088  3.823  3.254  2.551  2.193   ...     \n",
       "\n",
       "   att17   att18   att19   att20   att21   att22   att23    att24   att25  \\\n",
       "0  3.988  0.4957  0.1836 -2.2210  0.6453 -0.2923  1.2000 -0.09179  0.4674   \n",
       "1  7.098  1.2290  0.5971 -1.0670  0.9569 -1.8240  2.3130 -0.80890  0.5612   \n",
       "2  4.449  0.4773  1.6370 -1.0690  2.4160 -0.6299  1.4190 -0.81960  0.9151   \n",
       "3  2.785  1.9000 -1.1370  1.2750  1.7920 -2.1250  1.6090 -0.83230 -0.1998   \n",
       "4  4.536  2.1470  1.0200 -0.2656  2.8050  0.2762  0.2504  1.04900  0.3447   \n",
       "\n",
       "    att26  \n",
       "0  0.2158  \n",
       "1 -0.6225  \n",
       "2 -0.5948  \n",
       "3 -0.1218  \n",
       "4 -0.7689  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/submission-random.csv')\n",
    "test_y_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Blues, Country, Electronic, Folk, International, Jazz, Latin, New_Age, Pop_Rock, Rap, Reggae, RnB, Vocal]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "test_X = np.array(test_x)\n",
    "\n",
    "# Getting rid of the first and the last column: Id and msd_track_id\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)\n",
    "\n",
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Latin', 'Country', 'Jazz', 'New_Age', 'Electronic', 'RnB', 'Vocal', 'International', 'Pop_Rock', 'Rap', 'Blues', 'Reggae', 'Folk'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the freq of the keys in the training labels\n",
    "counted_labels = Counter(train_Y)\n",
    "labels_keys = counted_labels.keys()\n",
    "labels_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blues',\n",
       " 'Country',\n",
       " 'Electronic',\n",
       " 'Folk',\n",
       " 'International',\n",
       " 'Jazz',\n",
       " 'Latin',\n",
       " 'New_Age',\n",
       " 'Pop_Rock',\n",
       " 'Rap',\n",
       " 'Reggae',\n",
       " 'RnB',\n",
       " 'Vocal']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_keys_sorted = sorted(labels_keys)\n",
    "labels_keys_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Blues': 0,\n",
       " 'Country': 1,\n",
       " 'Electronic': 2,\n",
       " 'Folk': 3,\n",
       " 'International': 4,\n",
       " 'Jazz': 5,\n",
       " 'Latin': 6,\n",
       " 'New_Age': 7,\n",
       " 'Pop_Rock': 8,\n",
       " 'Rap': 9,\n",
       " 'Reggae': 10,\n",
       " 'RnB': 11,\n",
       " 'Vocal': 12}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This for loop for creating a dictionary/ vocab\n",
    "key_to_val = {key: val for val, key in enumerate(labels_keys_sorted)}\n",
    "key_to_val['Country']\n",
    "key_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Blues',\n",
       " 1: 'Country',\n",
       " 2: 'Electronic',\n",
       " 3: 'Folk',\n",
       " 4: 'International',\n",
       " 5: 'Jazz',\n",
       " 6: 'Latin',\n",
       " 7: 'New_Age',\n",
       " 8: 'Pop_Rock',\n",
       " 9: 'Rap',\n",
       " 10: 'Reggae',\n",
       " 11: 'RnB',\n",
       " 12: 'Vocal'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_to_key = {val: key for val, key in enumerate(labels_keys_sorted)}\n",
    "val_to_key[1]\n",
    "val_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_val[each])\n",
    "    Y_train_vec.append(key_to_val[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13000, 26), (10400, 26), dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std\n",
    "X_train_val.shape, X_test.shape, X_train_val.dtype, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 26), (1300, 26), (10400, 26), (1300,), (11700,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, \n",
    "X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape \n",
    "# X_train.dtype, X_val.dtype\n",
    "# Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'train_acc':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.grads = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn        \n",
    "        dX = dout @ W.T # Backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def Xsigmoid_forward(self, X):\n",
    "        out = l.sigmoid(X) * X\n",
    "        cache = l.sigmoid(X)\n",
    "        return out, cache\n",
    "\n",
    "    def Xsigmoid_backward(self, dout, cache):\n",
    "        return cache * (2. - cache) * dout\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y, nl_cache = l.tanh_forward(X=y)\n",
    "        y, nl_cache = self.Xsigmoid_forward(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y, nl_cache = l.tanh_forward(X=y)\n",
    "            y, nl_cache = self.Xsigmoid_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dy = self.Xsigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dy = self.Xsigmoid_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    " \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "                        \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700,), (11700, 26), (1300, 26), (1300,))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100, train loss-2.5516, acc-0.0938, valid loss-2.5661, acc-0.0638\n",
      "Iter-200, train loss-2.5676, acc-0.0781, valid loss-2.5594, acc-0.0846\n",
      "Iter-300, train loss-2.5395, acc-0.2031, valid loss-2.5527, acc-0.1092\n",
      "Iter-400, train loss-2.5470, acc-0.1406, valid loss-2.5447, acc-0.1315\n",
      "Iter-500, train loss-2.5223, acc-0.1406, valid loss-2.5357, acc-0.1585\n",
      "Iter-600, train loss-2.5479, acc-0.0625, valid loss-2.5245, acc-0.1715\n",
      "Iter-700, train loss-2.4829, acc-0.1562, valid loss-2.5114, acc-0.1769\n",
      "Iter-800, train loss-2.4788, acc-0.1719, valid loss-2.4955, acc-0.1615\n",
      "Iter-900, train loss-2.4705, acc-0.1250, valid loss-2.4755, acc-0.1754\n",
      "Iter-1000, train loss-2.4132, acc-0.2188, valid loss-2.4526, acc-0.1846\n",
      "Iter-1100, train loss-2.4353, acc-0.2188, valid loss-2.4269, acc-0.1877\n",
      "Iter-1200, train loss-2.2870, acc-0.2500, valid loss-2.4020, acc-0.1831\n",
      "Iter-1300, train loss-2.3366, acc-0.1719, valid loss-2.3813, acc-0.1838\n",
      "Iter-1400, train loss-2.2585, acc-0.2656, valid loss-2.3635, acc-0.1831\n",
      "Iter-1500, train loss-2.2393, acc-0.1719, valid loss-2.3478, acc-0.1908\n",
      "Iter-1600, train loss-2.1653, acc-0.2656, valid loss-2.3347, acc-0.1985\n",
      "Iter-1700, train loss-2.2385, acc-0.2188, valid loss-2.3221, acc-0.2015\n",
      "Iter-1800, train loss-2.2297, acc-0.2188, valid loss-2.3081, acc-0.2008\n",
      "Iter-1900, train loss-2.2566, acc-0.2188, valid loss-2.2998, acc-0.2015\n",
      "Iter-2000, train loss-2.1728, acc-0.1875, valid loss-2.2892, acc-0.2108\n",
      "Iter-2100, train loss-2.2529, acc-0.1250, valid loss-2.2788, acc-0.2223\n",
      "Iter-2200, train loss-2.1265, acc-0.2812, valid loss-2.2707, acc-0.2215\n",
      "Iter-2300, train loss-2.2782, acc-0.2031, valid loss-2.2604, acc-0.2238\n",
      "Iter-2400, train loss-2.0730, acc-0.2656, valid loss-2.2494, acc-0.2262\n",
      "Iter-2500, train loss-2.1184, acc-0.2969, valid loss-2.2398, acc-0.2362\n",
      "Iter-2600, train loss-2.0092, acc-0.3594, valid loss-2.2329, acc-0.2415\n",
      "Iter-2700, train loss-2.2169, acc-0.1562, valid loss-2.2233, acc-0.2392\n",
      "Iter-2800, train loss-1.9649, acc-0.3125, valid loss-2.2164, acc-0.2469\n",
      "Iter-2900, train loss-2.2030, acc-0.2656, valid loss-2.2107, acc-0.2462\n",
      "Iter-3000, train loss-2.1745, acc-0.2188, valid loss-2.2014, acc-0.2531\n",
      "Iter-3100, train loss-2.1520, acc-0.2656, valid loss-2.1978, acc-0.2562\n",
      "Iter-3200, train loss-2.1698, acc-0.3125, valid loss-2.1907, acc-0.2608\n",
      "Iter-3300, train loss-2.0419, acc-0.3281, valid loss-2.1863, acc-0.2546\n",
      "Iter-3400, train loss-2.3392, acc-0.2656, valid loss-2.1793, acc-0.2585\n",
      "Iter-3500, train loss-2.1977, acc-0.2344, valid loss-2.1746, acc-0.2638\n",
      "Iter-3600, train loss-2.0374, acc-0.3906, valid loss-2.1679, acc-0.2585\n",
      "Iter-3700, train loss-2.0769, acc-0.3125, valid loss-2.1635, acc-0.2692\n",
      "Iter-3800, train loss-2.2128, acc-0.2969, valid loss-2.1599, acc-0.2631\n",
      "Iter-3900, train loss-2.0701, acc-0.3125, valid loss-2.1545, acc-0.2685\n",
      "Iter-4000, train loss-2.0235, acc-0.3125, valid loss-2.1499, acc-0.2692\n",
      "Iter-4100, train loss-2.3828, acc-0.2188, valid loss-2.1482, acc-0.2792\n",
      "Iter-4200, train loss-2.1570, acc-0.3125, valid loss-2.1446, acc-0.2746\n",
      "Iter-4300, train loss-2.1479, acc-0.3594, valid loss-2.1432, acc-0.2762\n",
      "Iter-4400, train loss-1.9339, acc-0.4219, valid loss-2.1382, acc-0.2692\n",
      "Iter-4500, train loss-2.2129, acc-0.3125, valid loss-2.1391, acc-0.2792\n",
      "Iter-4600, train loss-2.0105, acc-0.3462, valid loss-2.1369, acc-0.2777\n",
      "Iter-4700, train loss-2.3890, acc-0.1719, valid loss-2.1369, acc-0.2892\n",
      "Iter-4800, train loss-2.1253, acc-0.2969, valid loss-2.1306, acc-0.2785\n",
      "Iter-4900, train loss-2.0817, acc-0.2969, valid loss-2.1300, acc-0.2823\n",
      "Iter-5000, train loss-2.0036, acc-0.3438, valid loss-2.1220, acc-0.2869\n",
      "Iter-5100, train loss-2.1069, acc-0.2188, valid loss-2.1184, acc-0.2785\n",
      "Iter-5200, train loss-2.0634, acc-0.3281, valid loss-2.1198, acc-0.2846\n",
      "Iter-5300, train loss-1.9440, acc-0.2969, valid loss-2.1154, acc-0.2892\n",
      "Iter-5400, train loss-2.0401, acc-0.3281, valid loss-2.1194, acc-0.2908\n",
      "Iter-5500, train loss-1.8506, acc-0.3906, valid loss-2.1157, acc-0.2900\n",
      "Iter-5600, train loss-2.1835, acc-0.2344, valid loss-2.1148, acc-0.2892\n",
      "Iter-5700, train loss-2.1362, acc-0.3438, valid loss-2.1112, acc-0.2877\n",
      "Iter-5800, train loss-2.0505, acc-0.3438, valid loss-2.1074, acc-0.2869\n",
      "Iter-5900, train loss-1.9354, acc-0.3750, valid loss-2.1079, acc-0.2977\n",
      "Iter-6000, train loss-1.9901, acc-0.2969, valid loss-2.1045, acc-0.2931\n",
      "Iter-6100, train loss-1.8798, acc-0.4062, valid loss-2.1030, acc-0.3015\n",
      "Iter-6200, train loss-2.0335, acc-0.3438, valid loss-2.1026, acc-0.3023\n",
      "Iter-6300, train loss-2.0497, acc-0.3750, valid loss-2.0999, acc-0.3031\n",
      "Iter-6400, train loss-2.0911, acc-0.2812, valid loss-2.0976, acc-0.3008\n",
      "Iter-6500, train loss-1.9232, acc-0.3125, valid loss-2.0955, acc-0.3054\n",
      "Iter-6600, train loss-2.1955, acc-0.2656, valid loss-2.0936, acc-0.3038\n",
      "Iter-6700, train loss-1.9259, acc-0.3125, valid loss-2.0942, acc-0.3069\n",
      "Iter-6800, train loss-2.3781, acc-0.2812, valid loss-2.0946, acc-0.3085\n",
      "Iter-6900, train loss-1.9534, acc-0.4062, valid loss-2.0887, acc-0.3131\n",
      "Iter-7000, train loss-2.0737, acc-0.2656, valid loss-2.0864, acc-0.3100\n",
      "Iter-7100, train loss-1.7486, acc-0.4219, valid loss-2.0848, acc-0.3100\n",
      "Iter-7200, train loss-1.8151, acc-0.4531, valid loss-2.0855, acc-0.3092\n",
      "Iter-7300, train loss-1.9413, acc-0.3281, valid loss-2.0832, acc-0.3131\n",
      "Iter-7400, train loss-1.8803, acc-0.3594, valid loss-2.0852, acc-0.3062\n",
      "Iter-7500, train loss-1.9010, acc-0.3906, valid loss-2.0781, acc-0.3131\n",
      "Iter-7600, train loss-2.0220, acc-0.4062, valid loss-2.0798, acc-0.3185\n",
      "Iter-7700, train loss-2.2591, acc-0.2188, valid loss-2.0787, acc-0.3223\n",
      "Iter-7800, train loss-1.9847, acc-0.4219, valid loss-2.0756, acc-0.3200\n",
      "Iter-7900, train loss-1.9875, acc-0.3750, valid loss-2.0716, acc-0.3138\n",
      "Iter-8000, train loss-1.7899, acc-0.4844, valid loss-2.0734, acc-0.3215\n",
      "Iter-8100, train loss-2.0774, acc-0.3125, valid loss-2.0731, acc-0.3192\n",
      "Iter-8200, train loss-1.8741, acc-0.3438, valid loss-2.0672, acc-0.3308\n",
      "Iter-8300, train loss-1.8120, acc-0.3750, valid loss-2.0676, acc-0.3262\n",
      "Iter-8400, train loss-1.9216, acc-0.3906, valid loss-2.0707, acc-0.3277\n",
      "Iter-8500, train loss-1.9570, acc-0.3438, valid loss-2.0643, acc-0.3215\n",
      "Iter-8600, train loss-2.2252, acc-0.2188, valid loss-2.0618, acc-0.3292\n",
      "Iter-8700, train loss-1.9863, acc-0.2969, valid loss-2.0620, acc-0.3231\n",
      "Iter-8800, train loss-1.9379, acc-0.2500, valid loss-2.0583, acc-0.3262\n",
      "Iter-8900, train loss-1.9766, acc-0.3438, valid loss-2.0610, acc-0.3238\n",
      "Iter-9000, train loss-1.9116, acc-0.3906, valid loss-2.0539, acc-0.3231\n",
      "Iter-9100, train loss-1.7653, acc-0.4375, valid loss-2.0567, acc-0.3246\n",
      "Iter-9200, train loss-1.8742, acc-0.3750, valid loss-2.0523, acc-0.3246\n",
      "Iter-9300, train loss-1.9064, acc-0.2812, valid loss-2.0575, acc-0.3238\n",
      "Iter-9400, train loss-1.9481, acc-0.3594, valid loss-2.0534, acc-0.3215\n",
      "Iter-9500, train loss-1.8600, acc-0.4062, valid loss-2.0527, acc-0.3277\n",
      "Iter-9600, train loss-1.9827, acc-0.3125, valid loss-2.0550, acc-0.3331\n",
      "Iter-9700, train loss-1.9661, acc-0.3750, valid loss-2.0499, acc-0.3285\n",
      "Iter-9800, train loss-1.9097, acc-0.3438, valid loss-2.0492, acc-0.3292\n",
      "Iter-9900, train loss-1.9809, acc-0.2812, valid loss-2.0469, acc-0.3277\n",
      "Iter-10000, train loss-1.9282, acc-0.3438, valid loss-2.0482, acc-0.3346\n",
      "Iter-10100, train loss-1.9979, acc-0.2500, valid loss-2.0476, acc-0.3354\n",
      "Iter-10200, train loss-2.0041, acc-0.3750, valid loss-2.0438, acc-0.3308\n",
      "Iter-10300, train loss-1.9358, acc-0.3125, valid loss-2.0421, acc-0.3354\n",
      "Iter-10400, train loss-1.9223, acc-0.2656, valid loss-2.0427, acc-0.3354\n",
      "Iter-10500, train loss-1.8784, acc-0.4375, valid loss-2.0428, acc-0.3346\n",
      "Iter-10600, train loss-1.9614, acc-0.3438, valid loss-2.0409, acc-0.3331\n",
      "Iter-10700, train loss-2.0570, acc-0.3125, valid loss-2.0378, acc-0.3331\n",
      "Iter-10800, train loss-1.6932, acc-0.4375, valid loss-2.0357, acc-0.3369\n",
      "Iter-10900, train loss-2.0015, acc-0.4219, valid loss-2.0355, acc-0.3392\n",
      "Iter-11000, train loss-1.9139, acc-0.3594, valid loss-2.0402, acc-0.3369\n",
      "Iter-11100, train loss-1.8253, acc-0.4062, valid loss-2.0350, acc-0.3331\n",
      "Iter-11200, train loss-1.8743, acc-0.4375, valid loss-2.0340, acc-0.3354\n",
      "Iter-11300, train loss-2.0068, acc-0.2656, valid loss-2.0320, acc-0.3385\n",
      "Iter-11400, train loss-2.0427, acc-0.2500, valid loss-2.0335, acc-0.3338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500, train loss-1.7213, acc-0.4219, valid loss-2.0353, acc-0.3354\n",
      "Iter-11600, train loss-2.0082, acc-0.3438, valid loss-2.0260, acc-0.3408\n",
      "Iter-11700, train loss-1.8156, acc-0.4219, valid loss-2.0281, acc-0.3408\n",
      "Iter-11800, train loss-1.9510, acc-0.3594, valid loss-2.0255, acc-0.3454\n",
      "Iter-11900, train loss-1.9966, acc-0.3438, valid loss-2.0236, acc-0.3408\n",
      "Iter-12000, train loss-2.0509, acc-0.3281, valid loss-2.0191, acc-0.3415\n",
      "Iter-12100, train loss-1.5772, acc-0.5312, valid loss-2.0195, acc-0.3423\n",
      "Iter-12200, train loss-2.0093, acc-0.4062, valid loss-2.0214, acc-0.3377\n",
      "Iter-12300, train loss-1.7927, acc-0.4219, valid loss-2.0189, acc-0.3392\n",
      "Iter-12400, train loss-1.8399, acc-0.4219, valid loss-2.0191, acc-0.3362\n",
      "Iter-12500, train loss-1.8308, acc-0.4062, valid loss-2.0207, acc-0.3392\n",
      "Iter-12600, train loss-1.8611, acc-0.4531, valid loss-2.0215, acc-0.3400\n",
      "Iter-12700, train loss-1.7487, acc-0.5000, valid loss-2.0199, acc-0.3408\n",
      "Iter-12800, train loss-1.9538, acc-0.4062, valid loss-2.0159, acc-0.3462\n",
      "Iter-12900, train loss-1.7833, acc-0.4375, valid loss-2.0139, acc-0.3415\n",
      "Iter-13000, train loss-1.7091, acc-0.4375, valid loss-2.0147, acc-0.3400\n",
      "Iter-13100, train loss-1.8752, acc-0.3750, valid loss-2.0108, acc-0.3400\n",
      "Iter-13200, train loss-1.9450, acc-0.3438, valid loss-2.0082, acc-0.3392\n",
      "Iter-13300, train loss-1.7477, acc-0.4062, valid loss-2.0083, acc-0.3392\n",
      "Iter-13400, train loss-1.7623, acc-0.4688, valid loss-2.0068, acc-0.3515\n",
      "Iter-13500, train loss-1.7768, acc-0.4844, valid loss-2.0055, acc-0.3438\n",
      "Iter-13600, train loss-1.6517, acc-0.3906, valid loss-2.0053, acc-0.3485\n",
      "Iter-13700, train loss-1.7428, acc-0.3906, valid loss-2.0072, acc-0.3431\n",
      "Iter-13800, train loss-1.9522, acc-0.3438, valid loss-2.0068, acc-0.3408\n",
      "Iter-13900, train loss-1.8083, acc-0.4531, valid loss-2.0095, acc-0.3392\n",
      "Iter-14000, train loss-1.7170, acc-0.4219, valid loss-2.0057, acc-0.3446\n",
      "Iter-14100, train loss-2.1164, acc-0.2500, valid loss-2.0045, acc-0.3431\n",
      "Iter-14200, train loss-1.8533, acc-0.3750, valid loss-2.0068, acc-0.3454\n",
      "Iter-14300, train loss-2.0004, acc-0.3125, valid loss-2.0025, acc-0.3446\n",
      "Iter-14400, train loss-1.9409, acc-0.3906, valid loss-2.0022, acc-0.3454\n",
      "Iter-14500, train loss-1.7005, acc-0.4375, valid loss-2.0019, acc-0.3415\n",
      "Iter-14600, train loss-2.0378, acc-0.4062, valid loss-1.9977, acc-0.3454\n",
      "Iter-14700, train loss-1.8599, acc-0.3594, valid loss-2.0005, acc-0.3492\n",
      "Iter-14800, train loss-1.7995, acc-0.4375, valid loss-1.9976, acc-0.3538\n",
      "Iter-14900, train loss-1.8891, acc-0.4038, valid loss-1.9988, acc-0.3485\n",
      "Iter-15000, train loss-2.0490, acc-0.2500, valid loss-1.9942, acc-0.3492\n",
      "Iter-15100, train loss-1.9241, acc-0.3125, valid loss-1.9968, acc-0.3515\n",
      "Iter-15200, train loss-1.9549, acc-0.3438, valid loss-1.9997, acc-0.3531\n",
      "Iter-15300, train loss-1.7115, acc-0.4844, valid loss-2.0001, acc-0.3469\n",
      "Iter-15400, train loss-1.7733, acc-0.4219, valid loss-2.0007, acc-0.3469\n",
      "Iter-15500, train loss-1.7696, acc-0.3750, valid loss-1.9981, acc-0.3515\n",
      "Iter-15600, train loss-1.9817, acc-0.3750, valid loss-1.9997, acc-0.3508\n",
      "Iter-15700, train loss-1.9085, acc-0.3438, valid loss-1.9968, acc-0.3569\n",
      "Iter-15800, train loss-2.1792, acc-0.2656, valid loss-1.9936, acc-0.3554\n",
      "Iter-15900, train loss-1.9191, acc-0.3125, valid loss-1.9925, acc-0.3515\n",
      "Iter-16000, train loss-1.7255, acc-0.4844, valid loss-1.9940, acc-0.3538\n",
      "Iter-16100, train loss-1.8591, acc-0.4688, valid loss-1.9987, acc-0.3515\n",
      "Iter-16200, train loss-1.9430, acc-0.3750, valid loss-1.9910, acc-0.3523\n",
      "Iter-16300, train loss-1.8014, acc-0.3750, valid loss-1.9922, acc-0.3515\n",
      "Iter-16400, train loss-1.6567, acc-0.4219, valid loss-1.9934, acc-0.3531\n",
      "Iter-16500, train loss-2.0499, acc-0.3594, valid loss-1.9905, acc-0.3515\n",
      "Iter-16600, train loss-1.9487, acc-0.3594, valid loss-1.9906, acc-0.3515\n",
      "Iter-16700, train loss-1.7801, acc-0.3438, valid loss-1.9878, acc-0.3508\n",
      "Iter-16800, train loss-1.7824, acc-0.4844, valid loss-1.9909, acc-0.3515\n",
      "Iter-16900, train loss-1.8526, acc-0.4062, valid loss-1.9851, acc-0.3538\n",
      "Iter-17000, train loss-1.8454, acc-0.4375, valid loss-1.9860, acc-0.3523\n",
      "Iter-17100, train loss-1.8243, acc-0.3906, valid loss-1.9862, acc-0.3585\n",
      "Iter-17200, train loss-1.8837, acc-0.3906, valid loss-1.9903, acc-0.3538\n",
      "Iter-17300, train loss-1.9797, acc-0.2812, valid loss-1.9880, acc-0.3508\n",
      "Iter-17400, train loss-1.7902, acc-0.4531, valid loss-1.9891, acc-0.3546\n",
      "Iter-17500, train loss-1.8828, acc-0.5000, valid loss-1.9857, acc-0.3531\n",
      "Iter-17600, train loss-1.4410, acc-0.5938, valid loss-1.9864, acc-0.3592\n",
      "Iter-17700, train loss-2.0816, acc-0.3438, valid loss-1.9839, acc-0.3569\n",
      "Iter-17800, train loss-1.8329, acc-0.4375, valid loss-1.9835, acc-0.3577\n",
      "Iter-17900, train loss-1.8770, acc-0.4062, valid loss-1.9825, acc-0.3492\n",
      "Iter-18000, train loss-2.0869, acc-0.2969, valid loss-1.9831, acc-0.3531\n",
      "Iter-18100, train loss-2.0256, acc-0.3125, valid loss-1.9830, acc-0.3577\n",
      "Iter-18200, train loss-1.8837, acc-0.3906, valid loss-1.9851, acc-0.3485\n",
      "Iter-18300, train loss-1.8718, acc-0.4062, valid loss-1.9831, acc-0.3523\n",
      "Iter-18400, train loss-1.7786, acc-0.3906, valid loss-1.9831, acc-0.3554\n",
      "Iter-18500, train loss-2.0596, acc-0.3125, valid loss-1.9807, acc-0.3554\n",
      "Iter-18600, train loss-1.8337, acc-0.3906, valid loss-1.9827, acc-0.3515\n",
      "Iter-18700, train loss-2.0250, acc-0.3125, valid loss-1.9809, acc-0.3546\n",
      "Iter-18800, train loss-1.6624, acc-0.3594, valid loss-1.9801, acc-0.3523\n",
      "Iter-18900, train loss-1.7770, acc-0.4375, valid loss-1.9803, acc-0.3515\n",
      "Iter-19000, train loss-1.9135, acc-0.3438, valid loss-1.9777, acc-0.3500\n",
      "Iter-19100, train loss-1.7025, acc-0.4531, valid loss-1.9822, acc-0.3608\n",
      "Iter-19200, train loss-1.9794, acc-0.2969, valid loss-1.9813, acc-0.3523\n",
      "Iter-19300, train loss-1.7924, acc-0.4375, valid loss-1.9744, acc-0.3577\n",
      "Iter-19400, train loss-1.8835, acc-0.4375, valid loss-1.9744, acc-0.3592\n",
      "Iter-19500, train loss-1.9894, acc-0.3906, valid loss-1.9778, acc-0.3569\n",
      "Iter-19600, train loss-1.5708, acc-0.5000, valid loss-1.9729, acc-0.3577\n",
      "Iter-19700, train loss-2.0113, acc-0.3281, valid loss-1.9792, acc-0.3585\n",
      "Iter-19800, train loss-1.3759, acc-0.5938, valid loss-1.9777, acc-0.3562\n",
      "Iter-19900, train loss-1.8515, acc-0.4062, valid loss-1.9736, acc-0.3562\n",
      "Iter-20000, train loss-1.8803, acc-0.3906, valid loss-1.9736, acc-0.3538\n",
      "Iter-20100, train loss-2.1137, acc-0.3906, valid loss-1.9756, acc-0.3500\n",
      "Iter-20200, train loss-1.8055, acc-0.4375, valid loss-1.9782, acc-0.3554\n",
      "Iter-20300, train loss-1.9959, acc-0.3594, valid loss-1.9754, acc-0.3562\n",
      "Iter-20400, train loss-1.8845, acc-0.4062, valid loss-1.9731, acc-0.3600\n",
      "Iter-20500, train loss-1.8449, acc-0.4688, valid loss-1.9756, acc-0.3600\n",
      "Iter-20600, train loss-1.6389, acc-0.5469, valid loss-1.9749, acc-0.3538\n",
      "Iter-20700, train loss-1.8763, acc-0.3438, valid loss-1.9734, acc-0.3577\n",
      "Iter-20800, train loss-1.6393, acc-0.4688, valid loss-1.9706, acc-0.3646\n",
      "Iter-20900, train loss-1.7828, acc-0.4219, valid loss-1.9778, acc-0.3592\n",
      "Iter-21000, train loss-1.7957, acc-0.3125, valid loss-1.9724, acc-0.3623\n",
      "Iter-21100, train loss-1.8137, acc-0.4062, valid loss-1.9701, acc-0.3615\n",
      "Iter-21200, train loss-1.7654, acc-0.4531, valid loss-1.9749, acc-0.3569\n",
      "Iter-21300, train loss-1.7962, acc-0.4219, valid loss-1.9755, acc-0.3477\n",
      "Iter-21400, train loss-1.7532, acc-0.5312, valid loss-1.9752, acc-0.3577\n",
      "Iter-21500, train loss-1.6524, acc-0.4531, valid loss-1.9704, acc-0.3646\n",
      "Iter-21600, train loss-1.7595, acc-0.4062, valid loss-1.9738, acc-0.3631\n",
      "Iter-21700, train loss-1.7417, acc-0.4375, valid loss-1.9746, acc-0.3654\n",
      "Iter-21800, train loss-1.8842, acc-0.4219, valid loss-1.9696, acc-0.3608\n",
      "Iter-21900, train loss-1.5713, acc-0.4844, valid loss-1.9683, acc-0.3608\n",
      "Iter-22000, train loss-1.8637, acc-0.3438, valid loss-1.9724, acc-0.3646\n",
      "Iter-22100, train loss-1.7217, acc-0.4688, valid loss-1.9711, acc-0.3646\n",
      "Iter-22200, train loss-1.6193, acc-0.4375, valid loss-1.9741, acc-0.3577\n",
      "Iter-22300, train loss-1.8156, acc-0.3750, valid loss-1.9721, acc-0.3608\n",
      "Iter-22400, train loss-1.7876, acc-0.4531, valid loss-1.9734, acc-0.3623\n",
      "Iter-22500, train loss-1.6253, acc-0.5312, valid loss-1.9753, acc-0.3562\n",
      "Iter-22600, train loss-1.7690, acc-0.3281, valid loss-1.9756, acc-0.3585\n",
      "Iter-22700, train loss-2.0612, acc-0.3906, valid loss-1.9739, acc-0.3592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800, train loss-1.6660, acc-0.3906, valid loss-1.9730, acc-0.3646\n",
      "Iter-22900, train loss-1.7818, acc-0.3438, valid loss-1.9713, acc-0.3615\n",
      "Iter-23000, train loss-1.8170, acc-0.4531, valid loss-1.9721, acc-0.3646\n",
      "Iter-23100, train loss-1.7602, acc-0.3438, valid loss-1.9688, acc-0.3662\n",
      "Iter-23200, train loss-1.7410, acc-0.4688, valid loss-1.9674, acc-0.3638\n",
      "Iter-23300, train loss-1.9674, acc-0.4219, valid loss-1.9708, acc-0.3600\n",
      "Iter-23400, train loss-1.7731, acc-0.4688, valid loss-1.9716, acc-0.3585\n",
      "Iter-23500, train loss-1.7111, acc-0.4219, valid loss-1.9658, acc-0.3692\n",
      "Iter-23600, train loss-1.7395, acc-0.5000, valid loss-1.9681, acc-0.3623\n",
      "Iter-23700, train loss-1.7957, acc-0.4844, valid loss-1.9677, acc-0.3677\n",
      "Iter-23800, train loss-1.7975, acc-0.4688, valid loss-1.9669, acc-0.3615\n",
      "Iter-23900, train loss-1.8244, acc-0.3906, valid loss-1.9699, acc-0.3623\n",
      "Iter-24000, train loss-2.0017, acc-0.3594, valid loss-1.9678, acc-0.3677\n",
      "Iter-24100, train loss-1.8634, acc-0.3906, valid loss-1.9670, acc-0.3646\n",
      "Iter-24200, train loss-2.0213, acc-0.3906, valid loss-1.9678, acc-0.3608\n",
      "Iter-24300, train loss-1.8817, acc-0.2969, valid loss-1.9643, acc-0.3615\n",
      "Iter-24400, train loss-1.6176, acc-0.4844, valid loss-1.9636, acc-0.3615\n",
      "Iter-24500, train loss-1.6957, acc-0.4062, valid loss-1.9634, acc-0.3600\n",
      "Iter-24600, train loss-1.7632, acc-0.3750, valid loss-1.9625, acc-0.3669\n",
      "Iter-24700, train loss-1.8632, acc-0.4219, valid loss-1.9653, acc-0.3600\n",
      "Iter-24800, train loss-1.8554, acc-0.3750, valid loss-1.9638, acc-0.3600\n",
      "Iter-24900, train loss-1.9399, acc-0.4219, valid loss-1.9624, acc-0.3631\n",
      "Iter-25000, train loss-1.8936, acc-0.4062, valid loss-1.9636, acc-0.3585\n",
      "Iter-25100, train loss-2.0705, acc-0.3281, valid loss-1.9622, acc-0.3646\n",
      "Iter-25200, train loss-1.7123, acc-0.4844, valid loss-1.9566, acc-0.3715\n",
      "Iter-25300, train loss-1.8125, acc-0.3750, valid loss-1.9608, acc-0.3631\n",
      "Iter-25400, train loss-1.7076, acc-0.4688, valid loss-1.9591, acc-0.3638\n",
      "Iter-25500, train loss-1.8709, acc-0.3906, valid loss-1.9556, acc-0.3685\n",
      "Iter-25600, train loss-1.7689, acc-0.3906, valid loss-1.9569, acc-0.3685\n",
      "Iter-25700, train loss-1.7296, acc-0.3906, valid loss-1.9609, acc-0.3638\n",
      "Iter-25800, train loss-2.0033, acc-0.3438, valid loss-1.9582, acc-0.3608\n",
      "Iter-25900, train loss-1.8012, acc-0.3438, valid loss-1.9566, acc-0.3669\n",
      "Iter-26000, train loss-1.7579, acc-0.3594, valid loss-1.9583, acc-0.3677\n",
      "Iter-26100, train loss-1.6559, acc-0.4531, valid loss-1.9586, acc-0.3692\n",
      "Iter-26200, train loss-1.5552, acc-0.5469, valid loss-1.9569, acc-0.3708\n",
      "Iter-26300, train loss-2.0907, acc-0.3125, valid loss-1.9576, acc-0.3631\n",
      "Iter-26400, train loss-1.8141, acc-0.4062, valid loss-1.9630, acc-0.3715\n",
      "Iter-26500, train loss-2.0078, acc-0.3750, valid loss-1.9605, acc-0.3738\n",
      "Iter-26600, train loss-1.7840, acc-0.3906, valid loss-1.9561, acc-0.3692\n",
      "Iter-26700, train loss-1.7700, acc-0.3906, valid loss-1.9593, acc-0.3615\n",
      "Iter-26800, train loss-1.8086, acc-0.3750, valid loss-1.9594, acc-0.3638\n",
      "Iter-26900, train loss-2.0148, acc-0.2969, valid loss-1.9557, acc-0.3608\n",
      "Iter-27000, train loss-1.5534, acc-0.4531, valid loss-1.9556, acc-0.3638\n",
      "Iter-27100, train loss-1.9869, acc-0.3594, valid loss-1.9552, acc-0.3638\n",
      "Iter-27200, train loss-1.6675, acc-0.4844, valid loss-1.9575, acc-0.3654\n",
      "Iter-27300, train loss-1.8911, acc-0.3594, valid loss-1.9580, acc-0.3577\n",
      "Iter-27400, train loss-1.6270, acc-0.5000, valid loss-1.9608, acc-0.3623\n",
      "Iter-27500, train loss-2.0478, acc-0.2812, valid loss-1.9634, acc-0.3708\n",
      "Iter-27600, train loss-1.6204, acc-0.4219, valid loss-1.9613, acc-0.3677\n",
      "Iter-27700, train loss-1.6789, acc-0.4219, valid loss-1.9613, acc-0.3669\n",
      "Iter-27800, train loss-1.5509, acc-0.5312, valid loss-1.9676, acc-0.3646\n",
      "Iter-27900, train loss-1.6840, acc-0.4531, valid loss-1.9670, acc-0.3615\n",
      "Iter-28000, train loss-1.6053, acc-0.5469, valid loss-1.9591, acc-0.3638\n",
      "Iter-28100, train loss-2.0355, acc-0.3125, valid loss-1.9591, acc-0.3654\n",
      "Iter-28200, train loss-1.7754, acc-0.4375, valid loss-1.9605, acc-0.3608\n",
      "Iter-28300, train loss-1.6945, acc-0.3750, valid loss-1.9581, acc-0.3608\n",
      "Iter-28400, train loss-1.7936, acc-0.3906, valid loss-1.9600, acc-0.3623\n",
      "Iter-28500, train loss-1.7373, acc-0.4375, valid loss-1.9601, acc-0.3654\n",
      "Iter-28600, train loss-1.6392, acc-0.5000, valid loss-1.9559, acc-0.3631\n",
      "Iter-28700, train loss-1.6118, acc-0.4531, valid loss-1.9571, acc-0.3654\n",
      "Iter-28800, train loss-1.8125, acc-0.4844, valid loss-1.9607, acc-0.3662\n",
      "Iter-28900, train loss-1.5986, acc-0.4844, valid loss-1.9591, acc-0.3669\n",
      "Iter-29000, train loss-1.6104, acc-0.4844, valid loss-1.9562, acc-0.3692\n",
      "Iter-29100, train loss-1.7653, acc-0.4375, valid loss-1.9568, acc-0.3592\n",
      "Iter-29200, train loss-1.7358, acc-0.5000, valid loss-1.9537, acc-0.3646\n",
      "Iter-29300, train loss-1.7495, acc-0.4375, valid loss-1.9511, acc-0.3723\n",
      "Iter-29400, train loss-1.8890, acc-0.4219, valid loss-1.9533, acc-0.3708\n",
      "Iter-29500, train loss-1.7099, acc-0.4531, valid loss-1.9568, acc-0.3638\n",
      "Iter-29600, train loss-1.9827, acc-0.3438, valid loss-1.9612, acc-0.3585\n",
      "Iter-29700, train loss-1.9137, acc-0.3281, valid loss-1.9621, acc-0.3723\n",
      "Iter-29800, train loss-1.9524, acc-0.3594, valid loss-1.9654, acc-0.3638\n",
      "Iter-29900, train loss-1.6075, acc-0.3750, valid loss-1.9613, acc-0.3685\n",
      "Iter-30000, train loss-1.8851, acc-0.3594, valid loss-1.9634, acc-0.3654\n",
      "Iter-30100, train loss-1.7180, acc-0.4688, valid loss-1.9616, acc-0.3654\n",
      "Iter-30200, train loss-1.8492, acc-0.3594, valid loss-1.9577, acc-0.3646\n",
      "Iter-30300, train loss-1.7203, acc-0.4062, valid loss-1.9584, acc-0.3669\n",
      "Iter-30400, train loss-2.1381, acc-0.2969, valid loss-1.9644, acc-0.3654\n",
      "Iter-30500, train loss-1.7298, acc-0.5000, valid loss-1.9581, acc-0.3692\n",
      "Iter-30600, train loss-1.7957, acc-0.4844, valid loss-1.9587, acc-0.3738\n",
      "Iter-30700, train loss-1.8970, acc-0.3438, valid loss-1.9609, acc-0.3654\n",
      "Iter-30800, train loss-1.5625, acc-0.5000, valid loss-1.9605, acc-0.3685\n",
      "Iter-30900, train loss-1.8173, acc-0.4219, valid loss-1.9621, acc-0.3662\n",
      "Iter-31000, train loss-1.7062, acc-0.4531, valid loss-1.9606, acc-0.3754\n",
      "Iter-31100, train loss-1.8411, acc-0.3594, valid loss-1.9568, acc-0.3738\n",
      "Iter-31200, train loss-1.8207, acc-0.4062, valid loss-1.9593, acc-0.3685\n",
      "Iter-31300, train loss-1.8823, acc-0.4219, valid loss-1.9612, acc-0.3669\n",
      "Iter-31400, train loss-1.6603, acc-0.3906, valid loss-1.9593, acc-0.3700\n",
      "Iter-31500, train loss-1.9495, acc-0.4062, valid loss-1.9581, acc-0.3669\n",
      "Iter-31600, train loss-1.7818, acc-0.4219, valid loss-1.9602, acc-0.3685\n",
      "Iter-31700, train loss-1.7807, acc-0.4062, valid loss-1.9621, acc-0.3615\n",
      "Iter-31800, train loss-1.7551, acc-0.4219, valid loss-1.9599, acc-0.3638\n",
      "Iter-31900, train loss-1.6471, acc-0.4688, valid loss-1.9612, acc-0.3662\n",
      "Iter-32000, train loss-1.7717, acc-0.4688, valid loss-1.9615, acc-0.3692\n",
      "Iter-32100, train loss-1.8588, acc-0.4375, valid loss-1.9641, acc-0.3554\n",
      "Iter-32200, train loss-1.8075, acc-0.3594, valid loss-1.9594, acc-0.3677\n",
      "Iter-32300, train loss-1.6263, acc-0.3906, valid loss-1.9572, acc-0.3615\n",
      "Iter-32400, train loss-1.7147, acc-0.3594, valid loss-1.9608, acc-0.3669\n",
      "Iter-32500, train loss-1.7700, acc-0.3750, valid loss-1.9582, acc-0.3654\n",
      "Iter-32600, train loss-1.8459, acc-0.3594, valid loss-1.9639, acc-0.3669\n",
      "Iter-32700, train loss-1.6469, acc-0.4531, valid loss-1.9572, acc-0.3685\n",
      "Iter-32800, train loss-1.6396, acc-0.4219, valid loss-1.9591, acc-0.3715\n",
      "Iter-32900, train loss-1.6281, acc-0.4844, valid loss-1.9592, acc-0.3638\n",
      "Iter-33000, train loss-1.6475, acc-0.4844, valid loss-1.9568, acc-0.3700\n",
      "Iter-33100, train loss-1.6070, acc-0.5312, valid loss-1.9610, acc-0.3692\n",
      "Iter-33200, train loss-1.7554, acc-0.3594, valid loss-1.9626, acc-0.3623\n",
      "Iter-33300, train loss-1.7399, acc-0.4688, valid loss-1.9606, acc-0.3631\n",
      "Iter-33400, train loss-2.0174, acc-0.3125, valid loss-1.9610, acc-0.3700\n",
      "Iter-33500, train loss-1.9022, acc-0.3125, valid loss-1.9635, acc-0.3715\n",
      "Iter-33600, train loss-1.8973, acc-0.3750, valid loss-1.9642, acc-0.3600\n",
      "Iter-33700, train loss-1.7073, acc-0.4531, valid loss-1.9662, acc-0.3615\n",
      "Iter-33800, train loss-1.8991, acc-0.3906, valid loss-1.9549, acc-0.3646\n",
      "Iter-33900, train loss-1.8717, acc-0.3906, valid loss-1.9571, acc-0.3700\n",
      "Iter-34000, train loss-1.5274, acc-0.5312, valid loss-1.9607, acc-0.3662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100, train loss-1.6536, acc-0.5312, valid loss-1.9606, acc-0.3669\n",
      "Iter-34200, train loss-1.8780, acc-0.3750, valid loss-1.9575, acc-0.3746\n",
      "Iter-34300, train loss-1.8478, acc-0.3906, valid loss-1.9571, acc-0.3685\n",
      "Iter-34400, train loss-1.5754, acc-0.4531, valid loss-1.9556, acc-0.3692\n",
      "Iter-34500, train loss-1.7705, acc-0.4844, valid loss-1.9552, acc-0.3638\n",
      "Iter-34600, train loss-1.9432, acc-0.3125, valid loss-1.9575, acc-0.3662\n",
      "Iter-34700, train loss-1.8944, acc-0.3906, valid loss-1.9568, acc-0.3708\n",
      "Iter-34800, train loss-1.8677, acc-0.3281, valid loss-1.9612, acc-0.3631\n",
      "Iter-34900, train loss-1.7783, acc-0.4688, valid loss-1.9610, acc-0.3715\n",
      "Iter-35000, train loss-1.7549, acc-0.2969, valid loss-1.9593, acc-0.3669\n",
      "Iter-35100, train loss-1.8088, acc-0.3906, valid loss-1.9591, acc-0.3692\n",
      "Iter-35200, train loss-1.6578, acc-0.5156, valid loss-1.9593, acc-0.3708\n",
      "Iter-35300, train loss-1.8973, acc-0.3594, valid loss-1.9603, acc-0.3685\n",
      "Iter-35400, train loss-1.9066, acc-0.3594, valid loss-1.9623, acc-0.3677\n",
      "Iter-35500, train loss-1.8164, acc-0.3750, valid loss-1.9597, acc-0.3700\n",
      "Iter-35600, train loss-1.7394, acc-0.4375, valid loss-1.9556, acc-0.3715\n",
      "Iter-35700, train loss-1.7796, acc-0.4375, valid loss-1.9592, acc-0.3669\n",
      "Iter-35800, train loss-1.9438, acc-0.3906, valid loss-1.9620, acc-0.3700\n",
      "Iter-35900, train loss-1.5478, acc-0.5156, valid loss-1.9647, acc-0.3654\n",
      "Iter-36000, train loss-1.6823, acc-0.4531, valid loss-1.9585, acc-0.3654\n",
      "Iter-36100, train loss-1.8997, acc-0.4375, valid loss-1.9617, acc-0.3662\n",
      "Iter-36200, train loss-1.7284, acc-0.4531, valid loss-1.9609, acc-0.3654\n",
      "Iter-36300, train loss-1.8881, acc-0.3750, valid loss-1.9569, acc-0.3785\n",
      "Iter-36400, train loss-1.6517, acc-0.5469, valid loss-1.9611, acc-0.3692\n",
      "Iter-36500, train loss-1.6547, acc-0.3906, valid loss-1.9631, acc-0.3623\n",
      "Iter-36600, train loss-1.6907, acc-0.3750, valid loss-1.9613, acc-0.3685\n",
      "Iter-36700, train loss-1.7971, acc-0.3750, valid loss-1.9604, acc-0.3692\n",
      "Iter-36800, train loss-1.6326, acc-0.4688, valid loss-1.9575, acc-0.3646\n",
      "Iter-36900, train loss-1.5796, acc-0.4531, valid loss-1.9644, acc-0.3654\n",
      "Iter-37000, train loss-1.8850, acc-0.3906, valid loss-1.9588, acc-0.3708\n",
      "Iter-37100, train loss-1.8711, acc-0.3906, valid loss-1.9585, acc-0.3708\n",
      "Iter-37200, train loss-1.8168, acc-0.3750, valid loss-1.9581, acc-0.3731\n",
      "Iter-37300, train loss-1.7154, acc-0.4531, valid loss-1.9653, acc-0.3723\n",
      "Iter-37400, train loss-1.7881, acc-0.3906, valid loss-1.9572, acc-0.3638\n",
      "Iter-37500, train loss-1.9122, acc-0.3750, valid loss-1.9577, acc-0.3677\n",
      "Iter-37600, train loss-1.8917, acc-0.3438, valid loss-1.9551, acc-0.3654\n",
      "Iter-37700, train loss-1.7773, acc-0.3281, valid loss-1.9573, acc-0.3669\n",
      "Iter-37800, train loss-1.6932, acc-0.4844, valid loss-1.9605, acc-0.3662\n",
      "Iter-37900, train loss-1.8859, acc-0.4219, valid loss-1.9532, acc-0.3746\n",
      "Iter-38000, train loss-1.6399, acc-0.4688, valid loss-1.9545, acc-0.3692\n",
      "Iter-38100, train loss-1.7670, acc-0.4375, valid loss-1.9570, acc-0.3700\n",
      "Iter-38200, train loss-1.7869, acc-0.4219, valid loss-1.9573, acc-0.3723\n",
      "Iter-38300, train loss-1.5230, acc-0.4844, valid loss-1.9595, acc-0.3662\n",
      "Iter-38400, train loss-1.5687, acc-0.5156, valid loss-1.9560, acc-0.3777\n",
      "Iter-38500, train loss-2.0183, acc-0.3125, valid loss-1.9620, acc-0.3685\n",
      "Iter-38600, train loss-1.7091, acc-0.3906, valid loss-1.9576, acc-0.3662\n",
      "Iter-38700, train loss-1.7045, acc-0.4062, valid loss-1.9582, acc-0.3746\n",
      "Iter-38800, train loss-1.8477, acc-0.3750, valid loss-1.9571, acc-0.3731\n",
      "Iter-38900, train loss-1.6071, acc-0.4531, valid loss-1.9570, acc-0.3669\n",
      "Iter-39000, train loss-1.5651, acc-0.5156, valid loss-1.9636, acc-0.3700\n",
      "Iter-39100, train loss-1.8143, acc-0.4219, valid loss-1.9635, acc-0.3662\n",
      "Iter-39200, train loss-2.0080, acc-0.3125, valid loss-1.9609, acc-0.3646\n",
      "Iter-39300, train loss-1.6097, acc-0.4375, valid loss-1.9617, acc-0.3654\n",
      "Iter-39400, train loss-1.9264, acc-0.3906, valid loss-1.9652, acc-0.3669\n",
      "Iter-39500, train loss-1.6464, acc-0.4531, valid loss-1.9672, acc-0.3677\n",
      "Iter-39600, train loss-1.7619, acc-0.4375, valid loss-1.9619, acc-0.3731\n",
      "Iter-39700, train loss-1.7009, acc-0.4375, valid loss-1.9625, acc-0.3654\n",
      "Iter-39800, train loss-1.6498, acc-0.4219, valid loss-1.9623, acc-0.3692\n",
      "Iter-39900, train loss-1.5549, acc-0.5000, valid loss-1.9606, acc-0.3715\n",
      "Iter-40000, train loss-1.9968, acc-0.3750, valid loss-1.9603, acc-0.3692\n",
      "Iter-40100, train loss-1.5048, acc-0.5156, valid loss-1.9633, acc-0.3662\n",
      "Iter-40200, train loss-1.7805, acc-0.4375, valid loss-1.9633, acc-0.3715\n",
      "Iter-40300, train loss-1.7720, acc-0.4062, valid loss-1.9630, acc-0.3762\n",
      "Iter-40400, train loss-1.6469, acc-0.5000, valid loss-1.9618, acc-0.3631\n",
      "Iter-40500, train loss-1.7788, acc-0.4062, valid loss-1.9638, acc-0.3692\n",
      "Iter-40600, train loss-1.6038, acc-0.4531, valid loss-1.9685, acc-0.3638\n",
      "Iter-40700, train loss-1.6740, acc-0.4531, valid loss-1.9658, acc-0.3600\n",
      "Iter-40800, train loss-1.8332, acc-0.3750, valid loss-1.9665, acc-0.3608\n",
      "Iter-40900, train loss-1.7946, acc-0.4062, valid loss-1.9678, acc-0.3631\n",
      "Iter-41000, train loss-1.6972, acc-0.4062, valid loss-1.9619, acc-0.3631\n",
      "Iter-41100, train loss-1.8573, acc-0.3594, valid loss-1.9579, acc-0.3692\n",
      "Iter-41200, train loss-1.7642, acc-0.4375, valid loss-1.9588, acc-0.3662\n",
      "Iter-41300, train loss-2.0115, acc-0.3594, valid loss-1.9636, acc-0.3638\n",
      "Iter-41400, train loss-2.0017, acc-0.3281, valid loss-1.9618, acc-0.3700\n",
      "Iter-41500, train loss-1.6281, acc-0.4375, valid loss-1.9589, acc-0.3662\n",
      "Iter-41600, train loss-1.5728, acc-0.5000, valid loss-1.9621, acc-0.3608\n",
      "Iter-41700, train loss-1.7030, acc-0.4688, valid loss-1.9639, acc-0.3638\n",
      "Iter-41800, train loss-1.6517, acc-0.4219, valid loss-1.9644, acc-0.3638\n",
      "Iter-41900, train loss-1.6683, acc-0.4375, valid loss-1.9597, acc-0.3631\n",
      "Iter-42000, train loss-1.6305, acc-0.4531, valid loss-1.9635, acc-0.3615\n",
      "Iter-42100, train loss-1.6341, acc-0.4844, valid loss-1.9654, acc-0.3623\n",
      "Iter-42200, train loss-1.8189, acc-0.4062, valid loss-1.9632, acc-0.3685\n",
      "Iter-42300, train loss-1.6336, acc-0.4219, valid loss-1.9594, acc-0.3631\n",
      "Iter-42400, train loss-1.7159, acc-0.4375, valid loss-1.9619, acc-0.3638\n",
      "Iter-42500, train loss-2.0374, acc-0.3438, valid loss-1.9572, acc-0.3646\n",
      "Iter-42600, train loss-1.7636, acc-0.3750, valid loss-1.9581, acc-0.3623\n",
      "Iter-42700, train loss-1.7527, acc-0.4062, valid loss-1.9617, acc-0.3677\n",
      "Iter-42800, train loss-1.7576, acc-0.3594, valid loss-1.9639, acc-0.3692\n",
      "Iter-42900, train loss-1.8216, acc-0.3750, valid loss-1.9635, acc-0.3638\n",
      "Iter-43000, train loss-1.7851, acc-0.4375, valid loss-1.9619, acc-0.3700\n",
      "Iter-43100, train loss-1.8039, acc-0.3906, valid loss-1.9589, acc-0.3692\n",
      "Iter-43200, train loss-1.5905, acc-0.4844, valid loss-1.9577, acc-0.3723\n",
      "Iter-43300, train loss-2.0887, acc-0.4062, valid loss-1.9626, acc-0.3669\n",
      "Iter-43400, train loss-1.5227, acc-0.4375, valid loss-1.9616, acc-0.3692\n",
      "Iter-43500, train loss-1.6518, acc-0.4219, valid loss-1.9628, acc-0.3646\n",
      "Iter-43600, train loss-1.4487, acc-0.5000, valid loss-1.9549, acc-0.3669\n",
      "Iter-43700, train loss-1.4729, acc-0.4375, valid loss-1.9575, acc-0.3708\n",
      "Iter-43800, train loss-1.8398, acc-0.3594, valid loss-1.9634, acc-0.3723\n",
      "Iter-43900, train loss-1.9390, acc-0.2812, valid loss-1.9644, acc-0.3562\n",
      "Iter-44000, train loss-2.0988, acc-0.3125, valid loss-1.9576, acc-0.3654\n",
      "Iter-44100, train loss-1.7534, acc-0.4219, valid loss-1.9607, acc-0.3662\n",
      "Iter-44200, train loss-1.4971, acc-0.4844, valid loss-1.9622, acc-0.3646\n",
      "Iter-44300, train loss-1.6075, acc-0.4219, valid loss-1.9629, acc-0.3646\n",
      "Iter-44400, train loss-1.5008, acc-0.4688, valid loss-1.9574, acc-0.3638\n",
      "Iter-44500, train loss-1.7557, acc-0.4375, valid loss-1.9644, acc-0.3577\n",
      "Iter-44600, train loss-1.7285, acc-0.4688, valid loss-1.9618, acc-0.3631\n",
      "Iter-44700, train loss-1.8462, acc-0.3438, valid loss-1.9639, acc-0.3638\n",
      "Iter-44800, train loss-1.7273, acc-0.3594, valid loss-1.9626, acc-0.3654\n",
      "Iter-44900, train loss-1.9349, acc-0.4062, valid loss-1.9610, acc-0.3585\n",
      "Iter-45000, train loss-1.7681, acc-0.4688, valid loss-1.9605, acc-0.3631\n",
      "Iter-45100, train loss-1.9038, acc-0.3125, valid loss-1.9617, acc-0.3662\n",
      "Iter-45200, train loss-1.6804, acc-0.4219, valid loss-1.9612, acc-0.3615\n",
      "Iter-45300, train loss-1.8017, acc-0.4219, valid loss-1.9608, acc-0.3677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400, train loss-1.6339, acc-0.3281, valid loss-1.9667, acc-0.3777\n",
      "Iter-45500, train loss-1.6013, acc-0.4844, valid loss-1.9628, acc-0.3723\n",
      "Iter-45600, train loss-1.7905, acc-0.5000, valid loss-1.9636, acc-0.3723\n",
      "Iter-45700, train loss-1.5146, acc-0.4844, valid loss-1.9650, acc-0.3669\n",
      "Iter-45800, train loss-1.7822, acc-0.4688, valid loss-1.9565, acc-0.3631\n",
      "Iter-45900, train loss-1.6033, acc-0.4062, valid loss-1.9588, acc-0.3692\n",
      "Iter-46000, train loss-1.7085, acc-0.5156, valid loss-1.9592, acc-0.3669\n",
      "Iter-46100, train loss-1.7306, acc-0.4375, valid loss-1.9574, acc-0.3669\n",
      "Iter-46200, train loss-1.5879, acc-0.4844, valid loss-1.9573, acc-0.3692\n",
      "Iter-46300, train loss-1.6276, acc-0.4531, valid loss-1.9575, acc-0.3731\n",
      "Iter-46400, train loss-1.7572, acc-0.4531, valid loss-1.9630, acc-0.3677\n",
      "Iter-46500, train loss-1.6892, acc-0.4219, valid loss-1.9605, acc-0.3746\n",
      "Iter-46600, train loss-1.7626, acc-0.4219, valid loss-1.9591, acc-0.3685\n",
      "Iter-46700, train loss-1.8780, acc-0.3594, valid loss-1.9572, acc-0.3685\n",
      "Iter-46800, train loss-1.6787, acc-0.4375, valid loss-1.9646, acc-0.3585\n",
      "Iter-46900, train loss-1.6686, acc-0.4688, valid loss-1.9596, acc-0.3708\n",
      "Iter-47000, train loss-1.6786, acc-0.3281, valid loss-1.9589, acc-0.3692\n",
      "Iter-47100, train loss-1.5643, acc-0.5000, valid loss-1.9592, acc-0.3738\n",
      "Iter-47200, train loss-1.8677, acc-0.3438, valid loss-1.9624, acc-0.3692\n",
      "Iter-47300, train loss-1.8256, acc-0.4375, valid loss-1.9621, acc-0.3631\n",
      "Iter-47400, train loss-1.6812, acc-0.5000, valid loss-1.9657, acc-0.3638\n",
      "Iter-47500, train loss-1.7609, acc-0.4062, valid loss-1.9693, acc-0.3615\n",
      "Iter-47600, train loss-1.5773, acc-0.5000, valid loss-1.9655, acc-0.3692\n",
      "Iter-47700, train loss-1.6430, acc-0.3906, valid loss-1.9619, acc-0.3638\n",
      "Iter-47800, train loss-1.6883, acc-0.4219, valid loss-1.9622, acc-0.3631\n",
      "Iter-47900, train loss-1.6809, acc-0.5312, valid loss-1.9654, acc-0.3623\n",
      "Iter-48000, train loss-1.4911, acc-0.5156, valid loss-1.9666, acc-0.3662\n",
      "Iter-48100, train loss-1.5028, acc-0.5000, valid loss-1.9606, acc-0.3669\n",
      "Iter-48200, train loss-1.6947, acc-0.4219, valid loss-1.9623, acc-0.3623\n",
      "Iter-48300, train loss-1.8019, acc-0.4688, valid loss-1.9584, acc-0.3638\n",
      "Iter-48400, train loss-1.7186, acc-0.3906, valid loss-1.9608, acc-0.3692\n",
      "Iter-48500, train loss-1.5958, acc-0.4375, valid loss-1.9560, acc-0.3692\n",
      "Iter-48600, train loss-1.8638, acc-0.3906, valid loss-1.9567, acc-0.3615\n",
      "Iter-48700, train loss-1.8612, acc-0.4375, valid loss-1.9572, acc-0.3638\n",
      "Iter-48800, train loss-1.9434, acc-0.3750, valid loss-1.9529, acc-0.3662\n",
      "Iter-48900, train loss-1.6356, acc-0.4219, valid loss-1.9586, acc-0.3685\n",
      "Iter-49000, train loss-1.7440, acc-0.4375, valid loss-1.9611, acc-0.3646\n",
      "Iter-49100, train loss-1.8265, acc-0.3906, valid loss-1.9587, acc-0.3723\n",
      "Iter-49200, train loss-1.9409, acc-0.3438, valid loss-1.9588, acc-0.3677\n",
      "Iter-49300, train loss-1.7590, acc-0.3906, valid loss-1.9568, acc-0.3715\n",
      "Iter-49400, train loss-1.6183, acc-0.4375, valid loss-1.9648, acc-0.3685\n",
      "Iter-49500, train loss-1.6135, acc-0.4219, valid loss-1.9547, acc-0.3646\n",
      "Iter-49600, train loss-1.6243, acc-0.4531, valid loss-1.9628, acc-0.3608\n",
      "Iter-49700, train loss-1.8586, acc-0.3281, valid loss-1.9601, acc-0.3754\n",
      "Iter-49800, train loss-1.6820, acc-0.4844, valid loss-1.9626, acc-0.3669\n",
      "Iter-49900, train loss-1.6546, acc-0.4375, valid loss-1.9585, acc-0.3646\n",
      "Iter-50000, train loss-1.8653, acc-0.4062, valid loss-1.9581, acc-0.3638\n",
      "Iter-50100, train loss-1.6458, acc-0.4219, valid loss-1.9597, acc-0.3685\n",
      "Iter-50200, train loss-1.4253, acc-0.6094, valid loss-1.9605, acc-0.3677\n",
      "Iter-50300, train loss-1.5366, acc-0.5000, valid loss-1.9616, acc-0.3762\n",
      "Iter-50400, train loss-1.8256, acc-0.3438, valid loss-1.9639, acc-0.3692\n",
      "Iter-50500, train loss-1.3575, acc-0.5156, valid loss-1.9618, acc-0.3685\n",
      "Iter-50600, train loss-1.8290, acc-0.4219, valid loss-1.9582, acc-0.3685\n",
      "Iter-50700, train loss-1.9475, acc-0.3594, valid loss-1.9610, acc-0.3662\n",
      "Iter-50800, train loss-1.6846, acc-0.4531, valid loss-1.9605, acc-0.3623\n",
      "Iter-50900, train loss-1.5611, acc-0.5312, valid loss-1.9610, acc-0.3715\n",
      "Iter-51000, train loss-1.6327, acc-0.5000, valid loss-1.9638, acc-0.3692\n",
      "Iter-51100, train loss-1.8810, acc-0.3750, valid loss-1.9579, acc-0.3723\n",
      "Iter-51200, train loss-1.7725, acc-0.3906, valid loss-1.9668, acc-0.3662\n",
      "Iter-51300, train loss-1.4963, acc-0.5000, valid loss-1.9640, acc-0.3662\n",
      "Iter-51400, train loss-1.7864, acc-0.4219, valid loss-1.9614, acc-0.3700\n",
      "Iter-51500, train loss-1.7024, acc-0.4062, valid loss-1.9671, acc-0.3692\n",
      "Iter-51600, train loss-1.7100, acc-0.5000, valid loss-1.9600, acc-0.3615\n",
      "Iter-51700, train loss-1.6358, acc-0.4375, valid loss-1.9679, acc-0.3638\n",
      "Iter-51800, train loss-1.8540, acc-0.3750, valid loss-1.9611, acc-0.3708\n",
      "Iter-51900, train loss-1.8013, acc-0.3750, valid loss-1.9627, acc-0.3608\n",
      "Iter-52000, train loss-1.7321, acc-0.3594, valid loss-1.9673, acc-0.3646\n",
      "Iter-52100, train loss-2.0128, acc-0.2500, valid loss-1.9632, acc-0.3669\n",
      "Iter-52200, train loss-1.6370, acc-0.4219, valid loss-1.9657, acc-0.3677\n",
      "Iter-52300, train loss-1.9469, acc-0.2812, valid loss-1.9620, acc-0.3723\n",
      "Iter-52400, train loss-1.6739, acc-0.5000, valid loss-1.9674, acc-0.3677\n",
      "Iter-52500, train loss-1.7439, acc-0.4375, valid loss-1.9664, acc-0.3638\n",
      "Iter-52600, train loss-1.7492, acc-0.4844, valid loss-1.9671, acc-0.3662\n",
      "Iter-52700, train loss-1.6387, acc-0.4844, valid loss-1.9696, acc-0.3600\n",
      "Iter-52800, train loss-1.7750, acc-0.4062, valid loss-1.9675, acc-0.3669\n",
      "Iter-52900, train loss-1.6254, acc-0.4531, valid loss-1.9687, acc-0.3692\n",
      "Iter-53000, train loss-1.8689, acc-0.3906, valid loss-1.9642, acc-0.3692\n",
      "Iter-53100, train loss-1.7413, acc-0.5156, valid loss-1.9622, acc-0.3677\n",
      "Iter-53200, train loss-1.7185, acc-0.4062, valid loss-1.9660, acc-0.3677\n",
      "Iter-53300, train loss-1.7930, acc-0.4219, valid loss-1.9676, acc-0.3692\n",
      "Iter-53400, train loss-1.7144, acc-0.3750, valid loss-1.9668, acc-0.3792\n",
      "Iter-53500, train loss-1.5773, acc-0.4844, valid loss-1.9651, acc-0.3692\n",
      "Iter-53600, train loss-1.7432, acc-0.3750, valid loss-1.9693, acc-0.3685\n",
      "Iter-53700, train loss-1.7963, acc-0.3594, valid loss-1.9732, acc-0.3631\n",
      "Iter-53800, train loss-1.5585, acc-0.4844, valid loss-1.9650, acc-0.3700\n",
      "Iter-53900, train loss-1.7638, acc-0.4219, valid loss-1.9602, acc-0.3677\n",
      "Iter-54000, train loss-1.6517, acc-0.4375, valid loss-1.9677, acc-0.3662\n",
      "Iter-54100, train loss-1.9044, acc-0.4531, valid loss-1.9585, acc-0.3762\n",
      "Iter-54200, train loss-1.7151, acc-0.4219, valid loss-1.9630, acc-0.3654\n",
      "Iter-54300, train loss-1.5754, acc-0.5000, valid loss-1.9643, acc-0.3592\n",
      "Iter-54400, train loss-1.7086, acc-0.4688, valid loss-1.9643, acc-0.3692\n",
      "Iter-54500, train loss-1.6564, acc-0.4531, valid loss-1.9650, acc-0.3662\n",
      "Iter-54600, train loss-1.5575, acc-0.4531, valid loss-1.9596, acc-0.3723\n",
      "Iter-54700, train loss-1.7483, acc-0.4688, valid loss-1.9634, acc-0.3646\n",
      "Iter-54800, train loss-1.5565, acc-0.5312, valid loss-1.9627, acc-0.3654\n",
      "Iter-54900, train loss-1.4341, acc-0.4844, valid loss-1.9646, acc-0.3615\n",
      "Iter-55000, train loss-1.6128, acc-0.4375, valid loss-1.9713, acc-0.3662\n",
      "Iter-55100, train loss-1.8436, acc-0.2969, valid loss-1.9634, acc-0.3715\n",
      "Iter-55200, train loss-1.5117, acc-0.5000, valid loss-1.9625, acc-0.3777\n",
      "Iter-55300, train loss-1.5151, acc-0.4531, valid loss-1.9671, acc-0.3708\n",
      "Iter-55400, train loss-1.7245, acc-0.3438, valid loss-1.9633, acc-0.3669\n",
      "Iter-55500, train loss-1.3867, acc-0.5625, valid loss-1.9655, acc-0.3723\n",
      "Iter-55600, train loss-1.7027, acc-0.3906, valid loss-1.9653, acc-0.3715\n",
      "Iter-55700, train loss-1.5964, acc-0.4531, valid loss-1.9646, acc-0.3692\n",
      "Iter-55800, train loss-1.5479, acc-0.5312, valid loss-1.9655, acc-0.3723\n",
      "Iter-55900, train loss-2.0011, acc-0.4219, valid loss-1.9694, acc-0.3692\n",
      "Iter-56000, train loss-1.7761, acc-0.4375, valid loss-1.9669, acc-0.3769\n",
      "Iter-56100, train loss-1.6795, acc-0.4062, valid loss-1.9709, acc-0.3677\n",
      "Iter-56200, train loss-1.7617, acc-0.3906, valid loss-1.9727, acc-0.3685\n",
      "Iter-56300, train loss-1.7881, acc-0.3906, valid loss-1.9681, acc-0.3723\n",
      "Iter-56400, train loss-1.4594, acc-0.5000, valid loss-1.9644, acc-0.3677\n",
      "Iter-56500, train loss-1.8041, acc-0.3750, valid loss-1.9601, acc-0.3654\n",
      "Iter-56600, train loss-1.8136, acc-0.4219, valid loss-1.9646, acc-0.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56700, train loss-1.5355, acc-0.4375, valid loss-1.9625, acc-0.3715\n",
      "Iter-56800, train loss-1.8279, acc-0.4688, valid loss-1.9652, acc-0.3638\n",
      "Iter-56900, train loss-1.7397, acc-0.3750, valid loss-1.9636, acc-0.3746\n",
      "Iter-57000, train loss-1.7831, acc-0.3906, valid loss-1.9656, acc-0.3700\n",
      "Iter-57100, train loss-1.4691, acc-0.4375, valid loss-1.9661, acc-0.3738\n",
      "Iter-57200, train loss-1.9039, acc-0.3438, valid loss-1.9722, acc-0.3808\n",
      "Iter-57300, train loss-1.5610, acc-0.4844, valid loss-1.9684, acc-0.3692\n",
      "Iter-57400, train loss-1.9095, acc-0.3594, valid loss-1.9652, acc-0.3754\n",
      "Iter-57500, train loss-1.7937, acc-0.3750, valid loss-1.9648, acc-0.3700\n",
      "Iter-57600, train loss-1.4940, acc-0.4844, valid loss-1.9614, acc-0.3815\n",
      "Iter-57700, train loss-1.8130, acc-0.3594, valid loss-1.9702, acc-0.3746\n",
      "Iter-57800, train loss-1.7553, acc-0.4531, valid loss-1.9628, acc-0.3738\n",
      "Iter-57900, train loss-1.4753, acc-0.4531, valid loss-1.9687, acc-0.3715\n",
      "Iter-58000, train loss-1.5406, acc-0.4844, valid loss-1.9662, acc-0.3731\n",
      "Iter-58100, train loss-1.6475, acc-0.4688, valid loss-1.9699, acc-0.3762\n",
      "Iter-58200, train loss-1.7251, acc-0.4219, valid loss-1.9656, acc-0.3715\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading = labels_keys_sorted.copy()\n",
    "heading.insert(0, 'Id')\n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_logits = nn.test(X_test)\n",
    "y_prob = l.softmax(y_logits)\n",
    "y_prob.shape, X_test.shape, y_logits.shape, test_y_sample.shape, test_y_sample[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for Id, pred in enumerate(y_prob):\n",
    "#     print(Id+1, *pred)\n",
    "    pred_list.append([Id+1, *pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_file = open(file='prediction.csv', mode='w')\n",
    "pred_file.write('\\n') # because of the previous line        \n",
    "\n",
    "for idx in range(len(heading)):\n",
    "    if idx < len(heading) - 1:\n",
    "        pred_file.write(heading[idx] + ',')\n",
    "    else:\n",
    "        pred_file.write(heading[idx] + '\\n')        \n",
    "\n",
    "# len(test), test[0]\n",
    "# for key in test:\n",
    "for i in range(len(pred_list)): # rows\n",
    "    for j in range(len(pred_list[i])): # cols\n",
    "        if j < (len(pred_list[i]) - 1):\n",
    "            pred_file.write(str(pred_list[i][j]))\n",
    "            pred_file.write(',')\n",
    "        else: # last item before starting a new line\n",
    "            pred_file.write(str(pred_list[i][j]) + '\\n')        \n",
    "\n",
    "# pred_file.write(-',')\n",
    "pred_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').shape, test_y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

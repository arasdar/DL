{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "      <th>msd_track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41.08</td>\n",
       "      <td>6.579</td>\n",
       "      <td>4.307</td>\n",
       "      <td>3.421</td>\n",
       "      <td>3.192</td>\n",
       "      <td>2.076</td>\n",
       "      <td>2.179</td>\n",
       "      <td>2.052</td>\n",
       "      <td>1.794</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3470</td>\n",
       "      <td>-0.2463</td>\n",
       "      <td>-1.5470</td>\n",
       "      <td>0.17920</td>\n",
       "      <td>-1.1530</td>\n",
       "      <td>-0.7370</td>\n",
       "      <td>0.40750</td>\n",
       "      <td>-0.67190</td>\n",
       "      <td>-0.05147</td>\n",
       "      <td>TRPLTEM128F92E1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>60.80</td>\n",
       "      <td>5.973</td>\n",
       "      <td>4.344</td>\n",
       "      <td>3.261</td>\n",
       "      <td>2.835</td>\n",
       "      <td>2.725</td>\n",
       "      <td>2.446</td>\n",
       "      <td>1.884</td>\n",
       "      <td>1.962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3316</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>-1.4760</td>\n",
       "      <td>0.52700</td>\n",
       "      <td>-2.1960</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>-1.39000</td>\n",
       "      <td>0.22560</td>\n",
       "      <td>-0.72080</td>\n",
       "      <td>TRJWMBQ128F424155E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51.47</td>\n",
       "      <td>4.971</td>\n",
       "      <td>4.316</td>\n",
       "      <td>2.916</td>\n",
       "      <td>3.112</td>\n",
       "      <td>2.290</td>\n",
       "      <td>2.053</td>\n",
       "      <td>1.934</td>\n",
       "      <td>1.878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2803</td>\n",
       "      <td>-0.1603</td>\n",
       "      <td>-0.1355</td>\n",
       "      <td>1.03500</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>1.4890</td>\n",
       "      <td>0.02959</td>\n",
       "      <td>-0.13670</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>TRRZWMO12903CCFCC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>41.28</td>\n",
       "      <td>6.610</td>\n",
       "      <td>4.411</td>\n",
       "      <td>2.602</td>\n",
       "      <td>2.822</td>\n",
       "      <td>2.126</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1.973</td>\n",
       "      <td>1.945</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6930</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>-0.3953</td>\n",
       "      <td>0.26710</td>\n",
       "      <td>-1.0450</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.03724</td>\n",
       "      <td>1.04500</td>\n",
       "      <td>-0.20000</td>\n",
       "      <td>TRBZRUT12903CE6C04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54.17</td>\n",
       "      <td>8.945</td>\n",
       "      <td>4.685</td>\n",
       "      <td>4.208</td>\n",
       "      <td>3.154</td>\n",
       "      <td>3.527</td>\n",
       "      <td>2.733</td>\n",
       "      <td>2.202</td>\n",
       "      <td>2.686</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4690</td>\n",
       "      <td>-0.5449</td>\n",
       "      <td>-0.5622</td>\n",
       "      <td>-0.08968</td>\n",
       "      <td>-0.9823</td>\n",
       "      <td>-0.2445</td>\n",
       "      <td>-1.65800</td>\n",
       "      <td>-0.04825</td>\n",
       "      <td>-0.70950</td>\n",
       "      <td>TRLUJQF128F42AF5BF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9  \\\n",
       "0   1  41.08  6.579  4.307  3.421  3.192  2.076  2.179  2.052  1.794   \n",
       "1   2  60.80  5.973  4.344  3.261  2.835  2.725  2.446  1.884  1.962   \n",
       "2   3  51.47  4.971  4.316  2.916  3.112  2.290  2.053  1.934  1.878   \n",
       "3   4  41.28  6.610  4.411  2.602  2.822  2.126  1.984  1.973  1.945   \n",
       "4   5  54.17  8.945  4.685  4.208  3.154  3.527  2.733  2.202  2.686   \n",
       "\n",
       "          ...           att18   att19   att20    att21   att22   att23  \\\n",
       "0         ...          1.3470 -0.2463 -1.5470  0.17920 -1.1530 -0.7370   \n",
       "1         ...         -0.3316  0.3519 -1.4760  0.52700 -2.1960  1.5990   \n",
       "2         ...         -0.2803 -0.1603 -0.1355  1.03500  0.2370  1.4890   \n",
       "3         ...         -1.6930  1.0040 -0.3953  0.26710 -1.0450  0.4974   \n",
       "4         ...          2.4690 -0.5449 -0.5622 -0.08968 -0.9823 -0.2445   \n",
       "\n",
       "     att24    att25    att26        msd_track_id  \n",
       "0  0.40750 -0.67190 -0.05147  TRPLTEM128F92E1389  \n",
       "1 -1.39000  0.22560 -0.72080  TRJWMBQ128F424155E  \n",
       "2  0.02959 -0.13670  0.10820  TRRZWMO12903CCFCC2  \n",
       "3  0.03724  1.04500 -0.20000  TRBZRUT12903CE6C04  \n",
       "4 -1.65800 -0.04825 -0.70950  TRLUJQF128F42AF5BF  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att17</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.22</td>\n",
       "      <td>8.076</td>\n",
       "      <td>6.935</td>\n",
       "      <td>4.696</td>\n",
       "      <td>3.856</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.922</td>\n",
       "      <td>2.568</td>\n",
       "      <td>2.070</td>\n",
       "      <td>...</td>\n",
       "      <td>3.988</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>-2.2210</td>\n",
       "      <td>0.6453</td>\n",
       "      <td>-0.2923</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>-0.09179</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36.42</td>\n",
       "      <td>6.131</td>\n",
       "      <td>5.364</td>\n",
       "      <td>4.292</td>\n",
       "      <td>3.968</td>\n",
       "      <td>2.937</td>\n",
       "      <td>2.872</td>\n",
       "      <td>2.142</td>\n",
       "      <td>2.050</td>\n",
       "      <td>...</td>\n",
       "      <td>7.098</td>\n",
       "      <td>1.2290</td>\n",
       "      <td>0.5971</td>\n",
       "      <td>-1.0670</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>-1.8240</td>\n",
       "      <td>2.3130</td>\n",
       "      <td>-0.80890</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>-0.6225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70.01</td>\n",
       "      <td>5.496</td>\n",
       "      <td>4.698</td>\n",
       "      <td>3.699</td>\n",
       "      <td>3.258</td>\n",
       "      <td>2.293</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.034</td>\n",
       "      <td>...</td>\n",
       "      <td>4.449</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>1.6370</td>\n",
       "      <td>-1.0690</td>\n",
       "      <td>2.4160</td>\n",
       "      <td>-0.6299</td>\n",
       "      <td>1.4190</td>\n",
       "      <td>-0.81960</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>-0.5948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40.64</td>\n",
       "      <td>7.281</td>\n",
       "      <td>6.702</td>\n",
       "      <td>4.043</td>\n",
       "      <td>3.729</td>\n",
       "      <td>3.043</td>\n",
       "      <td>2.644</td>\n",
       "      <td>2.366</td>\n",
       "      <td>1.940</td>\n",
       "      <td>...</td>\n",
       "      <td>2.785</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>-1.1370</td>\n",
       "      <td>1.2750</td>\n",
       "      <td>1.7920</td>\n",
       "      <td>-2.1250</td>\n",
       "      <td>1.6090</td>\n",
       "      <td>-0.83230</td>\n",
       "      <td>-0.1998</td>\n",
       "      <td>-0.1218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38.85</td>\n",
       "      <td>7.118</td>\n",
       "      <td>5.703</td>\n",
       "      <td>4.825</td>\n",
       "      <td>4.088</td>\n",
       "      <td>3.823</td>\n",
       "      <td>3.254</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.193</td>\n",
       "      <td>...</td>\n",
       "      <td>4.536</td>\n",
       "      <td>2.1470</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>2.8050</td>\n",
       "      <td>0.2762</td>\n",
       "      <td>0.2504</td>\n",
       "      <td>1.04900</td>\n",
       "      <td>0.3447</td>\n",
       "      <td>-0.7689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9   ...    \\\n",
       "0   1  38.22  8.076  6.935  4.696  3.856  3.465  2.922  2.568  2.070   ...     \n",
       "1   2  36.42  6.131  5.364  4.292  3.968  2.937  2.872  2.142  2.050   ...     \n",
       "2   3  70.01  5.496  4.698  3.699  3.258  2.293  2.680  2.226  2.034   ...     \n",
       "3   4  40.64  7.281  6.702  4.043  3.729  3.043  2.644  2.366  1.940   ...     \n",
       "4   5  38.85  7.118  5.703  4.825  4.088  3.823  3.254  2.551  2.193   ...     \n",
       "\n",
       "   att17   att18   att19   att20   att21   att22   att23    att24   att25  \\\n",
       "0  3.988  0.4957  0.1836 -2.2210  0.6453 -0.2923  1.2000 -0.09179  0.4674   \n",
       "1  7.098  1.2290  0.5971 -1.0670  0.9569 -1.8240  2.3130 -0.80890  0.5612   \n",
       "2  4.449  0.4773  1.6370 -1.0690  2.4160 -0.6299  1.4190 -0.81960  0.9151   \n",
       "3  2.785  1.9000 -1.1370  1.2750  1.7920 -2.1250  1.6090 -0.83230 -0.1998   \n",
       "4  4.536  2.1470  1.0200 -0.2656  2.8050  0.2762  0.2504  1.04900  0.3447   \n",
       "\n",
       "    att26  \n",
       "0  0.2158  \n",
       "1 -0.6225  \n",
       "2 -0.5948  \n",
       "3 -0.1218  \n",
       "4 -0.7689  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/submission-random.csv')\n",
    "test_y_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Blues, Country, Electronic, Folk, International, Jazz, Latin, New_Age, Pop_Rock, Rap, Reggae, RnB, Vocal]\n",
       "Index: []"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "test_X = np.array(test_x)\n",
    "\n",
    "# Getting rid of the first and the last column: Id and msd_track_id\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)\n",
    "\n",
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Vocal', 'New_Age', 'Rap', 'Folk', 'International', 'Blues', 'Country', 'Latin', 'Reggae', 'Electronic', 'Pop_Rock', 'RnB', 'Jazz'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the freq of the keys in the training labels\n",
    "counted_labels = Counter(train_Y)\n",
    "labels_keys = counted_labels.keys()\n",
    "labels_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blues',\n",
       " 'Country',\n",
       " 'Electronic',\n",
       " 'Folk',\n",
       " 'International',\n",
       " 'Jazz',\n",
       " 'Latin',\n",
       " 'New_Age',\n",
       " 'Pop_Rock',\n",
       " 'Rap',\n",
       " 'Reggae',\n",
       " 'RnB',\n",
       " 'Vocal']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_keys_sorted = sorted(labels_keys)\n",
    "labels_keys_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Blues': 0,\n",
       " 'Country': 1,\n",
       " 'Electronic': 2,\n",
       " 'Folk': 3,\n",
       " 'International': 4,\n",
       " 'Jazz': 5,\n",
       " 'Latin': 6,\n",
       " 'New_Age': 7,\n",
       " 'Pop_Rock': 8,\n",
       " 'Rap': 9,\n",
       " 'Reggae': 10,\n",
       " 'RnB': 11,\n",
       " 'Vocal': 12}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This for loop for creating a dictionary/ vocab\n",
    "key_to_val = {key: val for val, key in enumerate(labels_keys_sorted)}\n",
    "key_to_val['Country']\n",
    "key_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Blues',\n",
       " 1: 'Country',\n",
       " 2: 'Electronic',\n",
       " 3: 'Folk',\n",
       " 4: 'International',\n",
       " 5: 'Jazz',\n",
       " 6: 'Latin',\n",
       " 7: 'New_Age',\n",
       " 8: 'Pop_Rock',\n",
       " 9: 'Rap',\n",
       " 10: 'Reggae',\n",
       " 11: 'RnB',\n",
       " 12: 'Vocal'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_to_key = {val: key for val, key in enumerate(labels_keys_sorted)}\n",
    "val_to_key[1]\n",
    "val_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_val[each])\n",
    "    Y_train_vec.append(key_to_val[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13000, 26), (10400, 26), dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std\n",
    "X_train_val.shape, X_test.shape, X_train_val.dtype, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 26), (1300, 26), (10400, 26), (1300,), (11700,))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, \n",
    "X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape \n",
    "# X_train.dtype, X_val.dtype\n",
    "# Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # Backprop\n",
    "#         dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "#         # Test the final model\n",
    "#         y_pred, y_logit = nn.test(X_test)\n",
    "#         loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "#         acc = np.mean(y_pred == y_test)\n",
    "#         print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#             acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700,), (11700, 26), (1300, 26), (1300,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1000 train loss: 2.5603 valid loss: 2.5146, valid accuracy: 0.1246\n",
      "Iter-2000 train loss: 2.4604 valid loss: 2.4720, valid accuracy: 0.1523\n",
      "Iter-3000 train loss: 2.4784 valid loss: 2.4326, valid accuracy: 0.1769\n",
      "Iter-4000 train loss: 2.4252 valid loss: 2.3950, valid accuracy: 0.1946\n",
      "Iter-5000 train loss: 2.2862 valid loss: 2.3590, valid accuracy: 0.2062\n",
      "Iter-6000 train loss: 2.2241 valid loss: 2.3254, valid accuracy: 0.2192\n",
      "Iter-7000 train loss: 2.1832 valid loss: 2.2962, valid accuracy: 0.2285\n",
      "Iter-8000 train loss: 2.2419 valid loss: 2.2711, valid accuracy: 0.2369\n",
      "Iter-9000 train loss: 2.2343 valid loss: 2.2505, valid accuracy: 0.2469\n",
      "Iter-10000 train loss: 2.2129 valid loss: 2.2330, valid accuracy: 0.2515\n",
      "Iter-11000 train loss: 2.1714 valid loss: 2.2174, valid accuracy: 0.2592\n",
      "Iter-12000 train loss: 2.2901 valid loss: 2.2037, valid accuracy: 0.2600\n",
      "Iter-13000 train loss: 2.2446 valid loss: 2.1911, valid accuracy: 0.2669\n",
      "Iter-14000 train loss: 2.0900 valid loss: 2.1801, valid accuracy: 0.2669\n",
      "Iter-15000 train loss: 2.1186 valid loss: 2.1694, valid accuracy: 0.2762\n",
      "Iter-16000 train loss: 2.2730 valid loss: 2.1596, valid accuracy: 0.2746\n",
      "Iter-17000 train loss: 2.1348 valid loss: 2.1508, valid accuracy: 0.2815\n",
      "Iter-18000 train loss: 2.2012 valid loss: 2.1425, valid accuracy: 0.2800\n",
      "Iter-19000 train loss: 2.2499 valid loss: 2.1349, valid accuracy: 0.2846\n",
      "Iter-20000 train loss: 2.0322 valid loss: 2.1272, valid accuracy: 0.2846\n",
      "Iter-21000 train loss: 2.0210 valid loss: 2.1207, valid accuracy: 0.2900\n",
      "Iter-22000 train loss: 2.0382 valid loss: 2.1141, valid accuracy: 0.2946\n",
      "Iter-23000 train loss: 1.9269 valid loss: 2.1088, valid accuracy: 0.2962\n",
      "Iter-24000 train loss: 2.1840 valid loss: 2.1032, valid accuracy: 0.2969\n",
      "Iter-25000 train loss: 1.8678 valid loss: 2.0972, valid accuracy: 0.2969\n",
      "Iter-26000 train loss: 2.0872 valid loss: 2.0932, valid accuracy: 0.3000\n",
      "Iter-27000 train loss: 2.1597 valid loss: 2.0876, valid accuracy: 0.3062\n",
      "Iter-28000 train loss: 1.9781 valid loss: 2.0839, valid accuracy: 0.3054\n",
      "Iter-29000 train loss: 1.8560 valid loss: 2.0788, valid accuracy: 0.3062\n",
      "Iter-30000 train loss: 2.0700 valid loss: 2.0750, valid accuracy: 0.3138\n",
      "Iter-31000 train loss: 1.8046 valid loss: 2.0717, valid accuracy: 0.3138\n",
      "Iter-32000 train loss: 1.8961 valid loss: 2.0686, valid accuracy: 0.3208\n",
      "Iter-33000 train loss: 1.8560 valid loss: 2.0654, valid accuracy: 0.3231\n",
      "Iter-34000 train loss: 1.8000 valid loss: 2.0626, valid accuracy: 0.3246\n",
      "Iter-35000 train loss: 1.9564 valid loss: 2.0594, valid accuracy: 0.3238\n",
      "Iter-36000 train loss: 2.1275 valid loss: 2.0565, valid accuracy: 0.3231\n",
      "Iter-37000 train loss: 1.9102 valid loss: 2.0547, valid accuracy: 0.3192\n",
      "Iter-38000 train loss: 2.0047 valid loss: 2.0523, valid accuracy: 0.3223\n",
      "Iter-39000 train loss: 2.0939 valid loss: 2.0501, valid accuracy: 0.3254\n",
      "Iter-40000 train loss: 2.0111 valid loss: 2.0479, valid accuracy: 0.3246\n",
      "Iter-41000 train loss: 2.0387 valid loss: 2.0467, valid accuracy: 0.3254\n",
      "Iter-42000 train loss: 1.9565 valid loss: 2.0450, valid accuracy: 0.3238\n",
      "Iter-43000 train loss: 1.9493 valid loss: 2.0431, valid accuracy: 0.3262\n",
      "Iter-44000 train loss: 2.0870 valid loss: 2.0416, valid accuracy: 0.3246\n",
      "Iter-45000 train loss: 1.8282 valid loss: 2.0397, valid accuracy: 0.3277\n",
      "Iter-46000 train loss: 1.7876 valid loss: 2.0388, valid accuracy: 0.3300\n",
      "Iter-47000 train loss: 1.8931 valid loss: 2.0379, valid accuracy: 0.3292\n",
      "Iter-48000 train loss: 2.2096 valid loss: 2.0362, valid accuracy: 0.3315\n",
      "Iter-49000 train loss: 2.3395 valid loss: 2.0354, valid accuracy: 0.3285\n",
      "Iter-50000 train loss: 1.9564 valid loss: 2.0337, valid accuracy: 0.3285\n",
      "Iter-51000 train loss: 2.1331 valid loss: 2.0329, valid accuracy: 0.3308\n",
      "Iter-52000 train loss: 1.8784 valid loss: 2.0321, valid accuracy: 0.3285\n",
      "Iter-53000 train loss: 1.8558 valid loss: 2.0310, valid accuracy: 0.3277\n",
      "Iter-54000 train loss: 1.9762 valid loss: 2.0303, valid accuracy: 0.3300\n",
      "Iter-55000 train loss: 2.2028 valid loss: 2.0283, valid accuracy: 0.3292\n",
      "Iter-56000 train loss: 1.8598 valid loss: 2.0279, valid accuracy: 0.3300\n",
      "Iter-57000 train loss: 2.0325 valid loss: 2.0274, valid accuracy: 0.3323\n",
      "Iter-58000 train loss: 1.6507 valid loss: 2.0264, valid accuracy: 0.3362\n",
      "Iter-59000 train loss: 1.7305 valid loss: 2.0260, valid accuracy: 0.3377\n",
      "Iter-60000 train loss: 1.8975 valid loss: 2.0251, valid accuracy: 0.3392\n",
      "Iter-61000 train loss: 1.9641 valid loss: 2.0236, valid accuracy: 0.3400\n",
      "Iter-62000 train loss: 2.1560 valid loss: 2.0237, valid accuracy: 0.3400\n",
      "Iter-63000 train loss: 1.8508 valid loss: 2.0230, valid accuracy: 0.3400\n",
      "Iter-64000 train loss: 1.9400 valid loss: 2.0220, valid accuracy: 0.3408\n",
      "Iter-65000 train loss: 2.0997 valid loss: 2.0207, valid accuracy: 0.3385\n",
      "Iter-66000 train loss: 2.1835 valid loss: 2.0196, valid accuracy: 0.3400\n",
      "Iter-67000 train loss: 2.1592 valid loss: 2.0189, valid accuracy: 0.3392\n",
      "Iter-68000 train loss: 1.7350 valid loss: 2.0186, valid accuracy: 0.3377\n",
      "Iter-69000 train loss: 2.0428 valid loss: 2.0180, valid accuracy: 0.3423\n",
      "Iter-70000 train loss: 1.8334 valid loss: 2.0175, valid accuracy: 0.3354\n",
      "Iter-71000 train loss: 1.9339 valid loss: 2.0169, valid accuracy: 0.3377\n",
      "Iter-72000 train loss: 1.9312 valid loss: 2.0161, valid accuracy: 0.3385\n",
      "Iter-73000 train loss: 1.8748 valid loss: 2.0157, valid accuracy: 0.3377\n",
      "Iter-74000 train loss: 2.1364 valid loss: 2.0149, valid accuracy: 0.3377\n",
      "Iter-75000 train loss: 1.8203 valid loss: 2.0148, valid accuracy: 0.3369\n",
      "Iter-76000 train loss: 2.1771 valid loss: 2.0147, valid accuracy: 0.3377\n",
      "Iter-77000 train loss: 1.7710 valid loss: 2.0135, valid accuracy: 0.3392\n",
      "Iter-78000 train loss: 1.9498 valid loss: 2.0126, valid accuracy: 0.3385\n",
      "Iter-79000 train loss: 1.7222 valid loss: 2.0122, valid accuracy: 0.3346\n",
      "Iter-80000 train loss: 1.8280 valid loss: 2.0116, valid accuracy: 0.3346\n",
      "Iter-81000 train loss: 2.0578 valid loss: 2.0104, valid accuracy: 0.3362\n",
      "Iter-82000 train loss: 1.9244 valid loss: 2.0107, valid accuracy: 0.3377\n",
      "Iter-83000 train loss: 1.8695 valid loss: 2.0104, valid accuracy: 0.3369\n",
      "Iter-84000 train loss: 1.7853 valid loss: 2.0093, valid accuracy: 0.3408\n",
      "Iter-85000 train loss: 1.7223 valid loss: 2.0089, valid accuracy: 0.3415\n",
      "Iter-86000 train loss: 1.7671 valid loss: 2.0088, valid accuracy: 0.3415\n",
      "Iter-87000 train loss: 2.0986 valid loss: 2.0082, valid accuracy: 0.3431\n",
      "Iter-88000 train loss: 1.9128 valid loss: 2.0075, valid accuracy: 0.3415\n",
      "Iter-89000 train loss: 1.7171 valid loss: 2.0070, valid accuracy: 0.3446\n",
      "Iter-90000 train loss: 1.7142 valid loss: 2.0069, valid accuracy: 0.3415\n",
      "Iter-91000 train loss: 1.9897 valid loss: 2.0063, valid accuracy: 0.3415\n",
      "Iter-92000 train loss: 1.7942 valid loss: 2.0049, valid accuracy: 0.3431\n",
      "Iter-93000 train loss: 1.8686 valid loss: 2.0053, valid accuracy: 0.3454\n",
      "Iter-94000 train loss: 1.6995 valid loss: 2.0051, valid accuracy: 0.3438\n",
      "Iter-95000 train loss: 2.3072 valid loss: 2.0039, valid accuracy: 0.3462\n",
      "Iter-96000 train loss: 1.9225 valid loss: 2.0051, valid accuracy: 0.3408\n",
      "Iter-97000 train loss: 2.1405 valid loss: 2.0034, valid accuracy: 0.3423\n",
      "Iter-98000 train loss: 1.8930 valid loss: 2.0025, valid accuracy: 0.3408\n",
      "Iter-99000 train loss: 1.8786 valid loss: 2.0024, valid accuracy: 0.3423\n",
      "Iter-100000 train loss: 2.1701 valid loss: 2.0015, valid accuracy: 0.3431\n",
      "Iter-101000 train loss: 2.0215 valid loss: 2.0007, valid accuracy: 0.3423\n",
      "Iter-102000 train loss: 1.8993 valid loss: 2.0013, valid accuracy: 0.3423\n",
      "Iter-103000 train loss: 1.7763 valid loss: 2.0016, valid accuracy: 0.3446\n",
      "Iter-104000 train loss: 1.5537 valid loss: 2.0008, valid accuracy: 0.3400\n",
      "Iter-105000 train loss: 1.9344 valid loss: 2.0003, valid accuracy: 0.3431\n",
      "Iter-106000 train loss: 1.8287 valid loss: 1.9996, valid accuracy: 0.3431\n",
      "Iter-107000 train loss: 1.8953 valid loss: 1.9989, valid accuracy: 0.3415\n",
      "Iter-108000 train loss: 2.0447 valid loss: 1.9984, valid accuracy: 0.3446\n",
      "Iter-109000 train loss: 1.9318 valid loss: 1.9989, valid accuracy: 0.3462\n",
      "Iter-110000 train loss: 1.5015 valid loss: 1.9992, valid accuracy: 0.3438\n",
      "Iter-111000 train loss: 1.9367 valid loss: 1.9975, valid accuracy: 0.3431\n",
      "Iter-112000 train loss: 1.6669 valid loss: 1.9971, valid accuracy: 0.3431\n",
      "Iter-113000 train loss: 1.7151 valid loss: 1.9963, valid accuracy: 0.3431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-114000 train loss: 1.8205 valid loss: 1.9965, valid accuracy: 0.3462\n",
      "Iter-115000 train loss: 1.8669 valid loss: 1.9962, valid accuracy: 0.3454\n",
      "Iter-116000 train loss: 1.6765 valid loss: 1.9961, valid accuracy: 0.3423\n",
      "Iter-117000 train loss: 1.8666 valid loss: 1.9961, valid accuracy: 0.3431\n",
      "Iter-118000 train loss: 2.0276 valid loss: 1.9954, valid accuracy: 0.3462\n",
      "Iter-119000 train loss: 1.8822 valid loss: 1.9948, valid accuracy: 0.3469\n",
      "Iter-120000 train loss: 1.9185 valid loss: 1.9945, valid accuracy: 0.3492\n",
      "Iter-121000 train loss: 1.7843 valid loss: 1.9938, valid accuracy: 0.3492\n",
      "Iter-122000 train loss: 2.0377 valid loss: 1.9931, valid accuracy: 0.3469\n",
      "Iter-123000 train loss: 1.8690 valid loss: 1.9935, valid accuracy: 0.3469\n",
      "Iter-124000 train loss: 1.8059 valid loss: 1.9933, valid accuracy: 0.3462\n",
      "Iter-125000 train loss: 1.7603 valid loss: 1.9928, valid accuracy: 0.3477\n",
      "Iter-126000 train loss: 2.1523 valid loss: 1.9924, valid accuracy: 0.3446\n",
      "Iter-127000 train loss: 1.6247 valid loss: 1.9924, valid accuracy: 0.3462\n",
      "Iter-128000 train loss: 1.9895 valid loss: 1.9932, valid accuracy: 0.3431\n",
      "Iter-129000 train loss: 1.9697 valid loss: 1.9919, valid accuracy: 0.3438\n",
      "Iter-130000 train loss: 1.9662 valid loss: 1.9918, valid accuracy: 0.3469\n",
      "Iter-131000 train loss: 1.8621 valid loss: 1.9911, valid accuracy: 0.3469\n",
      "Iter-132000 train loss: 1.7738 valid loss: 1.9906, valid accuracy: 0.3477\n",
      "Iter-133000 train loss: 1.8042 valid loss: 1.9892, valid accuracy: 0.3485\n",
      "Iter-134000 train loss: 1.7633 valid loss: 1.9901, valid accuracy: 0.3431\n",
      "Iter-135000 train loss: 1.9872 valid loss: 1.9897, valid accuracy: 0.3454\n",
      "Iter-136000 train loss: 1.9703 valid loss: 1.9892, valid accuracy: 0.3438\n",
      "Iter-137000 train loss: 1.8320 valid loss: 1.9884, valid accuracy: 0.3446\n",
      "Iter-138000 train loss: 1.8036 valid loss: 1.9882, valid accuracy: 0.3454\n",
      "Iter-139000 train loss: 1.9429 valid loss: 1.9882, valid accuracy: 0.3438\n",
      "Iter-140000 train loss: 2.0266 valid loss: 1.9885, valid accuracy: 0.3454\n",
      "Iter-141000 train loss: 1.8949 valid loss: 1.9885, valid accuracy: 0.3431\n",
      "Iter-142000 train loss: 1.8575 valid loss: 1.9880, valid accuracy: 0.3408\n",
      "Iter-143000 train loss: 1.7462 valid loss: 1.9870, valid accuracy: 0.3431\n",
      "Iter-144000 train loss: 1.8269 valid loss: 1.9865, valid accuracy: 0.3462\n",
      "Iter-145000 train loss: 1.8223 valid loss: 1.9866, valid accuracy: 0.3462\n",
      "Iter-146000 train loss: 2.0514 valid loss: 1.9853, valid accuracy: 0.3423\n",
      "Iter-147000 train loss: 1.9518 valid loss: 1.9857, valid accuracy: 0.3415\n",
      "Iter-148000 train loss: 1.9777 valid loss: 1.9856, valid accuracy: 0.3438\n",
      "Iter-149000 train loss: 1.9463 valid loss: 1.9854, valid accuracy: 0.3423\n",
      "Iter-150000 train loss: 1.8228 valid loss: 1.9847, valid accuracy: 0.3415\n",
      "Iter-151000 train loss: 1.6280 valid loss: 1.9843, valid accuracy: 0.3431\n",
      "Iter-152000 train loss: 1.8887 valid loss: 1.9844, valid accuracy: 0.3415\n",
      "Iter-153000 train loss: 1.8484 valid loss: 1.9843, valid accuracy: 0.3438\n",
      "Iter-154000 train loss: 1.6279 valid loss: 1.9837, valid accuracy: 0.3392\n",
      "Iter-155000 train loss: 2.1519 valid loss: 1.9839, valid accuracy: 0.3415\n",
      "Iter-156000 train loss: 1.6853 valid loss: 1.9846, valid accuracy: 0.3392\n",
      "Iter-157000 train loss: 1.8876 valid loss: 1.9846, valid accuracy: 0.3423\n",
      "Iter-158000 train loss: 1.7158 valid loss: 1.9838, valid accuracy: 0.3423\n",
      "Iter-159000 train loss: 1.9193 valid loss: 1.9841, valid accuracy: 0.3400\n",
      "Iter-160000 train loss: 1.7171 valid loss: 1.9826, valid accuracy: 0.3423\n",
      "Iter-161000 train loss: 1.7798 valid loss: 1.9819, valid accuracy: 0.3431\n",
      "Iter-162000 train loss: 1.9045 valid loss: 1.9836, valid accuracy: 0.3438\n",
      "Iter-163000 train loss: 1.7372 valid loss: 1.9827, valid accuracy: 0.3400\n",
      "Iter-164000 train loss: 1.6686 valid loss: 1.9819, valid accuracy: 0.3431\n",
      "Iter-165000 train loss: 1.7312 valid loss: 1.9827, valid accuracy: 0.3392\n",
      "Iter-166000 train loss: 1.6867 valid loss: 1.9818, valid accuracy: 0.3354\n",
      "Iter-167000 train loss: 1.6291 valid loss: 1.9817, valid accuracy: 0.3392\n",
      "Iter-168000 train loss: 1.9066 valid loss: 1.9820, valid accuracy: 0.3408\n",
      "Iter-169000 train loss: 2.0550 valid loss: 1.9811, valid accuracy: 0.3400\n",
      "Iter-170000 train loss: 2.0930 valid loss: 1.9806, valid accuracy: 0.3415\n",
      "Iter-171000 train loss: 2.1193 valid loss: 1.9807, valid accuracy: 0.3446\n",
      "Iter-172000 train loss: 1.9493 valid loss: 1.9809, valid accuracy: 0.3415\n",
      "Iter-173000 train loss: 1.9282 valid loss: 1.9804, valid accuracy: 0.3408\n",
      "Iter-174000 train loss: 1.8664 valid loss: 1.9803, valid accuracy: 0.3408\n",
      "Iter-175000 train loss: 1.8787 valid loss: 1.9797, valid accuracy: 0.3408\n",
      "Iter-176000 train loss: 1.8168 valid loss: 1.9794, valid accuracy: 0.3431\n",
      "Iter-177000 train loss: 1.6704 valid loss: 1.9799, valid accuracy: 0.3392\n",
      "Iter-178000 train loss: 1.9271 valid loss: 1.9796, valid accuracy: 0.3385\n",
      "Iter-179000 train loss: 1.8579 valid loss: 1.9799, valid accuracy: 0.3408\n",
      "Iter-180000 train loss: 1.8514 valid loss: 1.9798, valid accuracy: 0.3385\n",
      "Iter-181000 train loss: 2.0547 valid loss: 1.9799, valid accuracy: 0.3423\n",
      "Iter-182000 train loss: 1.6446 valid loss: 1.9797, valid accuracy: 0.3408\n",
      "Iter-183000 train loss: 2.0497 valid loss: 1.9807, valid accuracy: 0.3415\n",
      "Iter-184000 train loss: 1.7623 valid loss: 1.9798, valid accuracy: 0.3415\n",
      "Iter-185000 train loss: 1.9745 valid loss: 1.9788, valid accuracy: 0.3423\n",
      "Iter-186000 train loss: 1.8434 valid loss: 1.9783, valid accuracy: 0.3400\n",
      "Iter-187000 train loss: 1.7570 valid loss: 1.9778, valid accuracy: 0.3431\n",
      "Iter-188000 train loss: 1.9689 valid loss: 1.9775, valid accuracy: 0.3400\n",
      "Iter-189000 train loss: 1.9428 valid loss: 1.9771, valid accuracy: 0.3408\n",
      "Iter-190000 train loss: 1.7326 valid loss: 1.9781, valid accuracy: 0.3431\n",
      "Iter-191000 train loss: 2.0502 valid loss: 1.9788, valid accuracy: 0.3415\n",
      "Iter-192000 train loss: 1.7190 valid loss: 1.9786, valid accuracy: 0.3423\n",
      "Iter-193000 train loss: 1.6841 valid loss: 1.9773, valid accuracy: 0.3392\n",
      "Iter-194000 train loss: 1.6477 valid loss: 1.9788, valid accuracy: 0.3423\n",
      "Iter-195000 train loss: 1.5485 valid loss: 1.9782, valid accuracy: 0.3415\n",
      "Iter-196000 train loss: 1.6514 valid loss: 1.9775, valid accuracy: 0.3446\n",
      "Iter-197000 train loss: 1.6933 valid loss: 1.9773, valid accuracy: 0.3423\n",
      "Iter-198000 train loss: 2.1232 valid loss: 1.9766, valid accuracy: 0.3415\n",
      "Iter-199000 train loss: 1.9660 valid loss: 1.9762, valid accuracy: 0.3415\n",
      "Iter-200000 train loss: 1.7507 valid loss: 1.9759, valid accuracy: 0.3446\n",
      "Iter-201000 train loss: 1.9098 valid loss: 1.9751, valid accuracy: 0.3438\n",
      "Iter-202000 train loss: 1.8177 valid loss: 1.9751, valid accuracy: 0.3408\n",
      "Iter-203000 train loss: 1.7249 valid loss: 1.9753, valid accuracy: 0.3423\n",
      "Iter-204000 train loss: 2.0068 valid loss: 1.9763, valid accuracy: 0.3408\n",
      "Iter-205000 train loss: 2.1116 valid loss: 1.9763, valid accuracy: 0.3423\n",
      "Iter-206000 train loss: 1.8805 valid loss: 1.9757, valid accuracy: 0.3438\n",
      "Iter-207000 train loss: 1.9593 valid loss: 1.9749, valid accuracy: 0.3438\n",
      "Iter-208000 train loss: 2.1501 valid loss: 1.9748, valid accuracy: 0.3408\n",
      "Iter-209000 train loss: 1.9358 valid loss: 1.9734, valid accuracy: 0.3462\n",
      "Iter-210000 train loss: 1.8112 valid loss: 1.9743, valid accuracy: 0.3408\n",
      "Iter-211000 train loss: 1.8033 valid loss: 1.9746, valid accuracy: 0.3392\n",
      "Iter-212000 train loss: 1.9059 valid loss: 1.9750, valid accuracy: 0.3415\n",
      "Iter-213000 train loss: 1.6742 valid loss: 1.9747, valid accuracy: 0.3408\n",
      "Iter-214000 train loss: 2.0135 valid loss: 1.9755, valid accuracy: 0.3438\n",
      "Iter-215000 train loss: 1.7688 valid loss: 1.9747, valid accuracy: 0.3408\n",
      "Iter-216000 train loss: 1.9915 valid loss: 1.9751, valid accuracy: 0.3408\n",
      "Iter-217000 train loss: 1.8412 valid loss: 1.9749, valid accuracy: 0.3454\n",
      "Iter-218000 train loss: 1.7979 valid loss: 1.9746, valid accuracy: 0.3415\n",
      "Iter-219000 train loss: 1.7987 valid loss: 1.9739, valid accuracy: 0.3438\n",
      "Iter-220000 train loss: 1.9276 valid loss: 1.9740, valid accuracy: 0.3392\n",
      "Iter-221000 train loss: 1.8693 valid loss: 1.9745, valid accuracy: 0.3415\n",
      "Iter-222000 train loss: 1.8580 valid loss: 1.9751, valid accuracy: 0.3415\n",
      "Iter-223000 train loss: 1.6040 valid loss: 1.9751, valid accuracy: 0.3400\n",
      "Iter-224000 train loss: 2.1273 valid loss: 1.9735, valid accuracy: 0.3446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-225000 train loss: 1.8743 valid loss: 1.9740, valid accuracy: 0.3423\n",
      "Iter-226000 train loss: 1.6685 valid loss: 1.9738, valid accuracy: 0.3415\n",
      "Iter-227000 train loss: 1.8358 valid loss: 1.9744, valid accuracy: 0.3400\n",
      "Iter-228000 train loss: 1.9261 valid loss: 1.9726, valid accuracy: 0.3423\n",
      "Iter-229000 train loss: 1.6622 valid loss: 1.9729, valid accuracy: 0.3431\n",
      "Iter-230000 train loss: 1.7307 valid loss: 1.9727, valid accuracy: 0.3415\n",
      "Iter-231000 train loss: 1.6790 valid loss: 1.9730, valid accuracy: 0.3446\n",
      "Iter-232000 train loss: 1.6180 valid loss: 1.9740, valid accuracy: 0.3423\n",
      "Iter-233000 train loss: 1.7506 valid loss: 1.9743, valid accuracy: 0.3392\n",
      "Iter-234000 train loss: 1.8223 valid loss: 1.9734, valid accuracy: 0.3431\n",
      "Iter-235000 train loss: 1.7117 valid loss: 1.9728, valid accuracy: 0.3415\n",
      "Iter-236000 train loss: 1.9383 valid loss: 1.9742, valid accuracy: 0.3438\n",
      "Iter-237000 train loss: 1.8298 valid loss: 1.9746, valid accuracy: 0.3408\n",
      "Iter-238000 train loss: 1.9714 valid loss: 1.9730, valid accuracy: 0.3462\n",
      "Iter-239000 train loss: 1.5916 valid loss: 1.9734, valid accuracy: 0.3423\n",
      "Iter-240000 train loss: 2.0231 valid loss: 1.9729, valid accuracy: 0.3431\n",
      "Iter-241000 train loss: 1.8116 valid loss: 1.9735, valid accuracy: 0.3462\n",
      "Iter-242000 train loss: 1.8989 valid loss: 1.9725, valid accuracy: 0.3415\n",
      "Iter-243000 train loss: 1.7886 valid loss: 1.9731, valid accuracy: 0.3408\n",
      "Iter-244000 train loss: 1.7630 valid loss: 1.9721, valid accuracy: 0.3438\n",
      "Iter-245000 train loss: 2.1236 valid loss: 1.9731, valid accuracy: 0.3415\n",
      "Iter-246000 train loss: 1.4547 valid loss: 1.9725, valid accuracy: 0.3423\n",
      "Iter-247000 train loss: 1.8853 valid loss: 1.9721, valid accuracy: 0.3469\n",
      "Iter-248000 train loss: 1.5570 valid loss: 1.9731, valid accuracy: 0.3415\n",
      "Iter-249000 train loss: 1.7118 valid loss: 1.9728, valid accuracy: 0.3408\n",
      "Iter-250000 train loss: 1.6320 valid loss: 1.9731, valid accuracy: 0.3415\n",
      "Iter-251000 train loss: 1.8419 valid loss: 1.9729, valid accuracy: 0.3415\n",
      "Iter-252000 train loss: 1.7088 valid loss: 1.9735, valid accuracy: 0.3423\n",
      "Iter-253000 train loss: 1.8180 valid loss: 1.9731, valid accuracy: 0.3431\n",
      "Iter-254000 train loss: 1.6539 valid loss: 1.9728, valid accuracy: 0.3423\n",
      "Iter-255000 train loss: 2.0136 valid loss: 1.9730, valid accuracy: 0.3415\n",
      "Iter-256000 train loss: 1.8676 valid loss: 1.9726, valid accuracy: 0.3431\n",
      "Iter-257000 train loss: 1.9287 valid loss: 1.9736, valid accuracy: 0.3415\n",
      "Iter-258000 train loss: 1.9743 valid loss: 1.9726, valid accuracy: 0.3446\n",
      "Iter-259000 train loss: 1.7082 valid loss: 1.9708, valid accuracy: 0.3431\n",
      "Iter-260000 train loss: 1.7005 valid loss: 1.9710, valid accuracy: 0.3454\n",
      "Iter-261000 train loss: 1.9431 valid loss: 1.9710, valid accuracy: 0.3415\n",
      "Iter-262000 train loss: 2.0333 valid loss: 1.9716, valid accuracy: 0.3485\n",
      "Iter-263000 train loss: 1.9227 valid loss: 1.9712, valid accuracy: 0.3454\n",
      "Iter-264000 train loss: 1.4208 valid loss: 1.9709, valid accuracy: 0.3462\n",
      "Iter-265000 train loss: 1.7866 valid loss: 1.9723, valid accuracy: 0.3446\n",
      "Iter-266000 train loss: 1.9221 valid loss: 1.9723, valid accuracy: 0.3431\n",
      "Iter-267000 train loss: 1.7827 valid loss: 1.9734, valid accuracy: 0.3431\n",
      "Iter-268000 train loss: 1.8489 valid loss: 1.9733, valid accuracy: 0.3415\n",
      "Iter-269000 train loss: 1.7193 valid loss: 1.9720, valid accuracy: 0.3431\n",
      "Iter-270000 train loss: 1.7255 valid loss: 1.9728, valid accuracy: 0.3477\n",
      "Iter-271000 train loss: 1.8712 valid loss: 1.9726, valid accuracy: 0.3454\n",
      "Iter-272000 train loss: 1.7658 valid loss: 1.9720, valid accuracy: 0.3462\n",
      "Iter-273000 train loss: 1.5075 valid loss: 1.9721, valid accuracy: 0.3438\n",
      "Iter-274000 train loss: 1.7801 valid loss: 1.9715, valid accuracy: 0.3438\n",
      "Iter-275000 train loss: 1.9249 valid loss: 1.9713, valid accuracy: 0.3446\n",
      "Iter-276000 train loss: 1.7840 valid loss: 1.9719, valid accuracy: 0.3446\n",
      "Iter-277000 train loss: 1.8633 valid loss: 1.9716, valid accuracy: 0.3469\n",
      "Iter-278000 train loss: 1.8619 valid loss: 1.9722, valid accuracy: 0.3477\n",
      "Iter-279000 train loss: 1.9997 valid loss: 1.9709, valid accuracy: 0.3454\n",
      "Iter-280000 train loss: 1.9662 valid loss: 1.9705, valid accuracy: 0.3477\n",
      "Iter-281000 train loss: 2.0817 valid loss: 1.9704, valid accuracy: 0.3454\n",
      "Iter-282000 train loss: 1.5331 valid loss: 1.9714, valid accuracy: 0.3485\n",
      "Iter-283000 train loss: 2.2329 valid loss: 1.9721, valid accuracy: 0.3469\n",
      "Iter-284000 train loss: 1.6922 valid loss: 1.9722, valid accuracy: 0.3469\n",
      "Iter-285000 train loss: 1.8435 valid loss: 1.9713, valid accuracy: 0.3477\n",
      "Iter-286000 train loss: 1.7498 valid loss: 1.9715, valid accuracy: 0.3469\n",
      "Iter-287000 train loss: 1.6057 valid loss: 1.9706, valid accuracy: 0.3446\n",
      "Iter-288000 train loss: 1.4745 valid loss: 1.9701, valid accuracy: 0.3431\n",
      "Iter-289000 train loss: 1.7929 valid loss: 1.9705, valid accuracy: 0.3438\n",
      "Iter-290000 train loss: 1.4419 valid loss: 1.9709, valid accuracy: 0.3423\n",
      "Iter-291000 train loss: 1.6789 valid loss: 1.9717, valid accuracy: 0.3446\n",
      "Iter-292000 train loss: 1.7890 valid loss: 1.9705, valid accuracy: 0.3477\n",
      "Iter-293000 train loss: 1.8138 valid loss: 1.9731, valid accuracy: 0.3438\n",
      "Iter-294000 train loss: 1.8070 valid loss: 1.9724, valid accuracy: 0.3454\n",
      "Iter-295000 train loss: 1.7970 valid loss: 1.9721, valid accuracy: 0.3462\n",
      "Iter-296000 train loss: 1.5800 valid loss: 1.9709, valid accuracy: 0.3446\n",
      "Iter-297000 train loss: 1.8456 valid loss: 1.9711, valid accuracy: 0.3492\n",
      "Iter-298000 train loss: 1.4388 valid loss: 1.9704, valid accuracy: 0.3438\n",
      "Iter-299000 train loss: 1.8787 valid loss: 1.9712, valid accuracy: 0.3462\n",
      "Iter-300000 train loss: 1.9698 valid loss: 1.9713, valid accuracy: 0.3454\n",
      "Iter-301000 train loss: 1.5771 valid loss: 1.9722, valid accuracy: 0.3454\n",
      "Iter-302000 train loss: 1.8571 valid loss: 1.9729, valid accuracy: 0.3446\n",
      "Iter-303000 train loss: 2.0451 valid loss: 1.9728, valid accuracy: 0.3485\n",
      "Iter-304000 train loss: 1.8204 valid loss: 1.9716, valid accuracy: 0.3515\n",
      "Iter-305000 train loss: 1.6106 valid loss: 1.9720, valid accuracy: 0.3454\n",
      "Iter-306000 train loss: 1.7610 valid loss: 1.9742, valid accuracy: 0.3431\n",
      "Iter-307000 train loss: 1.4492 valid loss: 1.9718, valid accuracy: 0.3477\n",
      "Iter-308000 train loss: 1.6580 valid loss: 1.9719, valid accuracy: 0.3462\n",
      "Iter-309000 train loss: 1.8096 valid loss: 1.9719, valid accuracy: 0.3492\n",
      "Iter-310000 train loss: 1.7422 valid loss: 1.9727, valid accuracy: 0.3485\n",
      "Iter-311000 train loss: 1.9619 valid loss: 1.9716, valid accuracy: 0.3485\n",
      "Iter-312000 train loss: 1.7688 valid loss: 1.9718, valid accuracy: 0.3438\n",
      "Iter-313000 train loss: 1.8095 valid loss: 1.9713, valid accuracy: 0.3469\n",
      "Iter-314000 train loss: 1.6220 valid loss: 1.9734, valid accuracy: 0.3469\n",
      "Iter-315000 train loss: 1.9897 valid loss: 1.9722, valid accuracy: 0.3438\n",
      "Iter-316000 train loss: 1.8935 valid loss: 1.9726, valid accuracy: 0.3454\n",
      "Iter-317000 train loss: 1.7948 valid loss: 1.9720, valid accuracy: 0.3485\n",
      "Iter-318000 train loss: 1.7130 valid loss: 1.9719, valid accuracy: 0.3469\n",
      "Iter-319000 train loss: 1.7933 valid loss: 1.9721, valid accuracy: 0.3469\n",
      "Iter-320000 train loss: 2.0069 valid loss: 1.9716, valid accuracy: 0.3462\n",
      "Iter-321000 train loss: 1.9185 valid loss: 1.9717, valid accuracy: 0.3454\n",
      "Iter-322000 train loss: 1.7864 valid loss: 1.9720, valid accuracy: 0.3485\n",
      "Iter-323000 train loss: 1.8380 valid loss: 1.9722, valid accuracy: 0.3469\n",
      "Iter-324000 train loss: 1.7525 valid loss: 1.9713, valid accuracy: 0.3485\n",
      "Iter-325000 train loss: 1.7330 valid loss: 1.9707, valid accuracy: 0.3454\n",
      "Iter-326000 train loss: 1.7176 valid loss: 1.9704, valid accuracy: 0.3454\n",
      "Iter-327000 train loss: 1.8904 valid loss: 1.9713, valid accuracy: 0.3492\n",
      "Iter-328000 train loss: 1.9868 valid loss: 1.9710, valid accuracy: 0.3500\n",
      "Iter-329000 train loss: 1.9859 valid loss: 1.9721, valid accuracy: 0.3462\n",
      "Iter-330000 train loss: 1.6111 valid loss: 1.9704, valid accuracy: 0.3462\n",
      "Iter-331000 train loss: 1.6249 valid loss: 1.9712, valid accuracy: 0.3469\n",
      "Iter-332000 train loss: 1.8786 valid loss: 1.9716, valid accuracy: 0.3477\n",
      "Iter-333000 train loss: 1.4481 valid loss: 1.9722, valid accuracy: 0.3500\n",
      "Iter-334000 train loss: 1.7375 valid loss: 1.9735, valid accuracy: 0.3469\n",
      "Iter-335000 train loss: 1.4219 valid loss: 1.9732, valid accuracy: 0.3492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-336000 train loss: 2.0871 valid loss: 1.9719, valid accuracy: 0.3508\n",
      "Iter-337000 train loss: 1.8579 valid loss: 1.9719, valid accuracy: 0.3531\n",
      "Iter-338000 train loss: 1.8853 valid loss: 1.9725, valid accuracy: 0.3523\n",
      "Iter-339000 train loss: 1.6634 valid loss: 1.9750, valid accuracy: 0.3485\n",
      "Iter-340000 train loss: 1.7288 valid loss: 1.9731, valid accuracy: 0.3492\n",
      "Iter-341000 train loss: 1.7074 valid loss: 1.9735, valid accuracy: 0.3515\n",
      "Iter-342000 train loss: 1.7615 valid loss: 1.9716, valid accuracy: 0.3531\n",
      "Iter-343000 train loss: 1.7258 valid loss: 1.9718, valid accuracy: 0.3508\n",
      "Iter-344000 train loss: 1.7020 valid loss: 1.9737, valid accuracy: 0.3523\n",
      "Iter-345000 train loss: 1.8790 valid loss: 1.9723, valid accuracy: 0.3508\n",
      "Iter-346000 train loss: 1.6565 valid loss: 1.9719, valid accuracy: 0.3485\n",
      "Iter-347000 train loss: 1.9143 valid loss: 1.9725, valid accuracy: 0.3500\n",
      "Iter-348000 train loss: 1.8740 valid loss: 1.9737, valid accuracy: 0.3469\n",
      "Iter-349000 train loss: 1.6547 valid loss: 1.9725, valid accuracy: 0.3485\n",
      "Iter-350000 train loss: 1.9440 valid loss: 1.9725, valid accuracy: 0.3508\n",
      "Iter-351000 train loss: 1.5968 valid loss: 1.9748, valid accuracy: 0.3523\n",
      "Iter-352000 train loss: 1.6305 valid loss: 1.9744, valid accuracy: 0.3523\n",
      "Iter-353000 train loss: 1.7508 valid loss: 1.9740, valid accuracy: 0.3538\n",
      "Iter-354000 train loss: 1.7126 valid loss: 1.9747, valid accuracy: 0.3546\n",
      "Iter-355000 train loss: 1.8444 valid loss: 1.9741, valid accuracy: 0.3546\n",
      "Iter-356000 train loss: 1.6935 valid loss: 1.9743, valid accuracy: 0.3554\n",
      "Iter-357000 train loss: 2.1110 valid loss: 1.9735, valid accuracy: 0.3538\n",
      "Iter-358000 train loss: 1.7136 valid loss: 1.9742, valid accuracy: 0.3523\n",
      "Iter-359000 train loss: 1.6477 valid loss: 1.9751, valid accuracy: 0.3523\n",
      "Iter-360000 train loss: 1.8521 valid loss: 1.9748, valid accuracy: 0.3538\n",
      "Iter-361000 train loss: 1.7716 valid loss: 1.9751, valid accuracy: 0.3515\n",
      "Iter-362000 train loss: 1.8817 valid loss: 1.9740, valid accuracy: 0.3538\n",
      "Iter-363000 train loss: 1.4806 valid loss: 1.9735, valid accuracy: 0.3515\n",
      "Iter-364000 train loss: 1.7323 valid loss: 1.9732, valid accuracy: 0.3500\n",
      "Iter-365000 train loss: 1.9259 valid loss: 1.9739, valid accuracy: 0.3508\n",
      "Iter-366000 train loss: 1.6024 valid loss: 1.9731, valid accuracy: 0.3500\n",
      "Iter-367000 train loss: 1.9431 valid loss: 1.9741, valid accuracy: 0.3538\n",
      "Iter-368000 train loss: 1.7871 valid loss: 1.9754, valid accuracy: 0.3554\n",
      "Iter-369000 train loss: 1.5574 valid loss: 1.9746, valid accuracy: 0.3523\n",
      "Iter-370000 train loss: 1.7767 valid loss: 1.9753, valid accuracy: 0.3531\n",
      "Iter-371000 train loss: 1.7161 valid loss: 1.9751, valid accuracy: 0.3500\n",
      "Iter-372000 train loss: 1.8174 valid loss: 1.9740, valid accuracy: 0.3515\n",
      "Iter-373000 train loss: 1.6790 valid loss: 1.9755, valid accuracy: 0.3577\n",
      "Iter-374000 train loss: 1.7356 valid loss: 1.9746, valid accuracy: 0.3508\n",
      "Iter-375000 train loss: 1.5071 valid loss: 1.9739, valid accuracy: 0.3546\n",
      "Iter-376000 train loss: 1.8999 valid loss: 1.9736, valid accuracy: 0.3546\n",
      "Iter-377000 train loss: 1.7765 valid loss: 1.9743, valid accuracy: 0.3546\n",
      "Iter-378000 train loss: 1.9868 valid loss: 1.9749, valid accuracy: 0.3546\n",
      "Iter-379000 train loss: 1.7064 valid loss: 1.9752, valid accuracy: 0.3562\n",
      "Iter-380000 train loss: 1.8699 valid loss: 1.9737, valid accuracy: 0.3531\n",
      "Iter-381000 train loss: 1.7931 valid loss: 1.9754, valid accuracy: 0.3515\n",
      "Iter-382000 train loss: 1.3892 valid loss: 1.9748, valid accuracy: 0.3523\n",
      "Iter-383000 train loss: 1.6178 valid loss: 1.9756, valid accuracy: 0.3538\n",
      "Iter-384000 train loss: 1.7676 valid loss: 1.9757, valid accuracy: 0.3546\n",
      "Iter-385000 train loss: 1.8390 valid loss: 1.9763, valid accuracy: 0.3577\n",
      "Iter-386000 train loss: 1.8306 valid loss: 1.9753, valid accuracy: 0.3538\n",
      "Iter-387000 train loss: 1.5936 valid loss: 1.9760, valid accuracy: 0.3508\n",
      "Iter-388000 train loss: 1.7259 valid loss: 1.9763, valid accuracy: 0.3531\n",
      "Iter-389000 train loss: 1.7613 valid loss: 1.9754, valid accuracy: 0.3515\n",
      "Iter-390000 train loss: 1.5795 valid loss: 1.9758, valid accuracy: 0.3523\n",
      "Iter-391000 train loss: 1.9035 valid loss: 1.9760, valid accuracy: 0.3554\n",
      "Iter-392000 train loss: 1.8991 valid loss: 1.9758, valid accuracy: 0.3531\n",
      "Iter-393000 train loss: 1.9378 valid loss: 1.9773, valid accuracy: 0.3562\n",
      "Iter-394000 train loss: 1.8042 valid loss: 1.9766, valid accuracy: 0.3531\n",
      "Iter-395000 train loss: 1.5551 valid loss: 1.9764, valid accuracy: 0.3577\n",
      "Iter-396000 train loss: 1.6638 valid loss: 1.9765, valid accuracy: 0.3577\n",
      "Iter-397000 train loss: 1.6583 valid loss: 1.9782, valid accuracy: 0.3554\n",
      "Iter-398000 train loss: 1.9514 valid loss: 1.9774, valid accuracy: 0.3546\n",
      "Iter-399000 train loss: 2.0500 valid loss: 1.9784, valid accuracy: 0.3546\n",
      "Iter-400000 train loss: 1.6703 valid loss: 1.9774, valid accuracy: 0.3515\n",
      "Iter-401000 train loss: 1.7636 valid loss: 1.9783, valid accuracy: 0.3592\n",
      "Iter-402000 train loss: 1.6823 valid loss: 1.9768, valid accuracy: 0.3585\n",
      "Iter-403000 train loss: 1.7378 valid loss: 1.9776, valid accuracy: 0.3562\n",
      "Iter-404000 train loss: 1.7772 valid loss: 1.9777, valid accuracy: 0.3515\n",
      "Iter-405000 train loss: 1.5238 valid loss: 1.9777, valid accuracy: 0.3577\n",
      "Iter-406000 train loss: 1.9768 valid loss: 1.9779, valid accuracy: 0.3600\n",
      "Iter-407000 train loss: 1.8205 valid loss: 1.9790, valid accuracy: 0.3569\n",
      "Iter-408000 train loss: 1.7256 valid loss: 1.9785, valid accuracy: 0.3585\n",
      "Iter-409000 train loss: 1.9547 valid loss: 1.9791, valid accuracy: 0.3592\n",
      "Iter-410000 train loss: 1.7913 valid loss: 1.9776, valid accuracy: 0.3592\n",
      "Iter-411000 train loss: 1.7324 valid loss: 1.9798, valid accuracy: 0.3569\n",
      "Iter-412000 train loss: 1.6123 valid loss: 1.9785, valid accuracy: 0.3600\n",
      "Iter-413000 train loss: 1.7266 valid loss: 1.9785, valid accuracy: 0.3615\n",
      "Iter-414000 train loss: 1.6312 valid loss: 1.9782, valid accuracy: 0.3546\n",
      "Iter-415000 train loss: 1.7466 valid loss: 1.9760, valid accuracy: 0.3554\n",
      "Iter-416000 train loss: 1.4935 valid loss: 1.9781, valid accuracy: 0.3600\n",
      "Iter-417000 train loss: 1.4779 valid loss: 1.9797, valid accuracy: 0.3554\n",
      "Iter-418000 train loss: 1.7406 valid loss: 1.9801, valid accuracy: 0.3577\n",
      "Iter-419000 train loss: 1.6377 valid loss: 1.9794, valid accuracy: 0.3600\n",
      "Iter-420000 train loss: 1.5369 valid loss: 1.9798, valid accuracy: 0.3577\n",
      "Iter-421000 train loss: 1.8153 valid loss: 1.9799, valid accuracy: 0.3577\n",
      "Iter-422000 train loss: 1.6380 valid loss: 1.9794, valid accuracy: 0.3569\n",
      "Iter-423000 train loss: 1.8026 valid loss: 1.9791, valid accuracy: 0.3577\n",
      "Iter-424000 train loss: 1.9562 valid loss: 1.9799, valid accuracy: 0.3577\n",
      "Iter-425000 train loss: 1.7640 valid loss: 1.9783, valid accuracy: 0.3600\n",
      "Iter-426000 train loss: 1.7546 valid loss: 1.9787, valid accuracy: 0.3538\n",
      "Iter-427000 train loss: 1.8922 valid loss: 1.9806, valid accuracy: 0.3569\n",
      "Iter-428000 train loss: 1.9578 valid loss: 1.9812, valid accuracy: 0.3600\n",
      "Iter-429000 train loss: 1.6078 valid loss: 1.9824, valid accuracy: 0.3585\n",
      "Iter-430000 train loss: 1.7195 valid loss: 1.9819, valid accuracy: 0.3585\n",
      "Iter-431000 train loss: 1.3812 valid loss: 1.9820, valid accuracy: 0.3569\n",
      "Iter-432000 train loss: 1.8101 valid loss: 1.9805, valid accuracy: 0.3592\n",
      "Iter-433000 train loss: 1.5176 valid loss: 1.9810, valid accuracy: 0.3585\n",
      "Iter-434000 train loss: 1.7446 valid loss: 1.9817, valid accuracy: 0.3569\n",
      "Iter-435000 train loss: 1.7896 valid loss: 1.9814, valid accuracy: 0.3546\n",
      "Iter-436000 train loss: 1.8247 valid loss: 1.9820, valid accuracy: 0.3631\n",
      "Iter-437000 train loss: 1.8282 valid loss: 1.9826, valid accuracy: 0.3600\n",
      "Iter-438000 train loss: 1.9204 valid loss: 1.9812, valid accuracy: 0.3615\n",
      "Iter-439000 train loss: 2.0740 valid loss: 1.9823, valid accuracy: 0.3592\n",
      "Iter-440000 train loss: 1.6405 valid loss: 1.9829, valid accuracy: 0.3562\n",
      "Iter-441000 train loss: 1.8920 valid loss: 1.9820, valid accuracy: 0.3569\n",
      "Iter-442000 train loss: 1.9000 valid loss: 1.9811, valid accuracy: 0.3623\n",
      "Iter-443000 train loss: 1.8871 valid loss: 1.9827, valid accuracy: 0.3577\n",
      "Iter-444000 train loss: 1.7671 valid loss: 1.9812, valid accuracy: 0.3638\n",
      "Iter-445000 train loss: 1.7878 valid loss: 1.9824, valid accuracy: 0.3585\n",
      "Iter-446000 train loss: 1.7330 valid loss: 1.9834, valid accuracy: 0.3569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-447000 train loss: 1.8769 valid loss: 1.9832, valid accuracy: 0.3638\n",
      "Iter-448000 train loss: 1.6836 valid loss: 1.9839, valid accuracy: 0.3623\n",
      "Iter-449000 train loss: 1.7194 valid loss: 1.9845, valid accuracy: 0.3615\n",
      "Iter-450000 train loss: 1.5618 valid loss: 1.9819, valid accuracy: 0.3646\n",
      "Iter-451000 train loss: 1.6949 valid loss: 1.9816, valid accuracy: 0.3615\n",
      "Iter-452000 train loss: 1.6640 valid loss: 1.9822, valid accuracy: 0.3638\n",
      "Iter-453000 train loss: 1.8140 valid loss: 1.9831, valid accuracy: 0.3623\n",
      "Iter-454000 train loss: 1.7045 valid loss: 1.9832, valid accuracy: 0.3638\n",
      "Iter-455000 train loss: 1.5852 valid loss: 1.9827, valid accuracy: 0.3585\n",
      "Iter-456000 train loss: 1.9973 valid loss: 1.9856, valid accuracy: 0.3585\n",
      "Iter-457000 train loss: 1.9220 valid loss: 1.9854, valid accuracy: 0.3608\n",
      "Iter-458000 train loss: 1.8857 valid loss: 1.9854, valid accuracy: 0.3585\n",
      "Iter-459000 train loss: 1.7775 valid loss: 1.9853, valid accuracy: 0.3608\n",
      "Iter-460000 train loss: 1.6827 valid loss: 1.9841, valid accuracy: 0.3631\n",
      "Iter-461000 train loss: 1.7485 valid loss: 1.9854, valid accuracy: 0.3585\n",
      "Iter-462000 train loss: 1.7640 valid loss: 1.9844, valid accuracy: 0.3608\n",
      "Iter-463000 train loss: 1.7369 valid loss: 1.9844, valid accuracy: 0.3592\n",
      "Iter-464000 train loss: 1.8566 valid loss: 1.9864, valid accuracy: 0.3608\n",
      "Iter-465000 train loss: 2.0165 valid loss: 1.9876, valid accuracy: 0.3623\n",
      "Iter-466000 train loss: 1.5028 valid loss: 1.9865, valid accuracy: 0.3569\n",
      "Iter-467000 train loss: 1.7765 valid loss: 1.9876, valid accuracy: 0.3562\n",
      "Iter-468000 train loss: 1.8122 valid loss: 1.9867, valid accuracy: 0.3600\n",
      "Iter-469000 train loss: 1.6576 valid loss: 1.9873, valid accuracy: 0.3569\n",
      "Iter-470000 train loss: 1.5891 valid loss: 1.9866, valid accuracy: 0.3631\n",
      "Iter-471000 train loss: 1.7495 valid loss: 1.9860, valid accuracy: 0.3577\n",
      "Iter-472000 train loss: 1.8386 valid loss: 1.9876, valid accuracy: 0.3531\n",
      "Iter-473000 train loss: 1.6092 valid loss: 1.9875, valid accuracy: 0.3577\n",
      "Iter-474000 train loss: 1.8126 valid loss: 1.9870, valid accuracy: 0.3577\n",
      "Iter-475000 train loss: 1.6991 valid loss: 1.9870, valid accuracy: 0.3608\n",
      "Iter-476000 train loss: 1.6128 valid loss: 1.9868, valid accuracy: 0.3592\n",
      "Iter-477000 train loss: 1.5423 valid loss: 1.9863, valid accuracy: 0.3608\n",
      "Iter-478000 train loss: 1.7607 valid loss: 1.9854, valid accuracy: 0.3615\n",
      "Iter-479000 train loss: 1.6202 valid loss: 1.9866, valid accuracy: 0.3600\n",
      "Iter-480000 train loss: 1.5901 valid loss: 1.9870, valid accuracy: 0.3646\n",
      "Iter-481000 train loss: 1.8573 valid loss: 1.9861, valid accuracy: 0.3638\n",
      "Iter-482000 train loss: 1.7049 valid loss: 1.9886, valid accuracy: 0.3608\n",
      "Iter-483000 train loss: 1.7890 valid loss: 1.9884, valid accuracy: 0.3638\n",
      "Iter-484000 train loss: 1.7041 valid loss: 1.9876, valid accuracy: 0.3631\n",
      "Iter-485000 train loss: 1.6157 valid loss: 1.9879, valid accuracy: 0.3608\n",
      "Iter-486000 train loss: 2.0441 valid loss: 1.9878, valid accuracy: 0.3631\n",
      "Iter-487000 train loss: 1.8957 valid loss: 1.9891, valid accuracy: 0.3585\n",
      "Iter-488000 train loss: 1.6837 valid loss: 1.9879, valid accuracy: 0.3585\n",
      "Iter-489000 train loss: 2.0036 valid loss: 1.9870, valid accuracy: 0.3585\n",
      "Iter-490000 train loss: 1.5976 valid loss: 1.9866, valid accuracy: 0.3615\n",
      "Iter-491000 train loss: 1.8156 valid loss: 1.9866, valid accuracy: 0.3592\n",
      "Iter-492000 train loss: 1.8404 valid loss: 1.9873, valid accuracy: 0.3600\n",
      "Iter-493000 train loss: 1.9155 valid loss: 1.9879, valid accuracy: 0.3569\n",
      "Iter-494000 train loss: 1.8763 valid loss: 1.9883, valid accuracy: 0.3592\n",
      "Iter-495000 train loss: 1.7282 valid loss: 1.9897, valid accuracy: 0.3554\n",
      "Iter-496000 train loss: 1.9287 valid loss: 1.9894, valid accuracy: 0.3562\n",
      "Iter-497000 train loss: 2.0214 valid loss: 1.9901, valid accuracy: 0.3538\n",
      "Iter-498000 train loss: 2.0962 valid loss: 1.9890, valid accuracy: 0.3562\n",
      "Iter-499000 train loss: 1.7987 valid loss: 1.9904, valid accuracy: 0.3577\n",
      "Iter-500000 train loss: 1.5510 valid loss: 1.9903, valid accuracy: 0.3608\n",
      "Iter-501000 train loss: 1.7161 valid loss: 1.9901, valid accuracy: 0.3592\n",
      "Iter-502000 train loss: 1.7281 valid loss: 1.9901, valid accuracy: 0.3608\n",
      "Iter-503000 train loss: 1.6624 valid loss: 1.9895, valid accuracy: 0.3562\n",
      "Iter-504000 train loss: 1.6908 valid loss: 1.9902, valid accuracy: 0.3600\n",
      "Iter-505000 train loss: 1.5113 valid loss: 1.9910, valid accuracy: 0.3562\n",
      "Iter-506000 train loss: 1.9124 valid loss: 1.9903, valid accuracy: 0.3585\n",
      "Iter-507000 train loss: 1.5558 valid loss: 1.9915, valid accuracy: 0.3554\n",
      "Iter-508000 train loss: 2.0296 valid loss: 1.9925, valid accuracy: 0.3569\n",
      "Iter-509000 train loss: 1.6956 valid loss: 1.9924, valid accuracy: 0.3592\n",
      "Iter-510000 train loss: 1.8094 valid loss: 1.9933, valid accuracy: 0.3538\n",
      "Iter-511000 train loss: 2.0010 valid loss: 1.9913, valid accuracy: 0.3592\n",
      "Iter-512000 train loss: 1.8230 valid loss: 1.9906, valid accuracy: 0.3585\n",
      "Iter-513000 train loss: 1.8571 valid loss: 1.9926, valid accuracy: 0.3585\n",
      "Iter-514000 train loss: 1.7080 valid loss: 1.9912, valid accuracy: 0.3523\n",
      "Iter-515000 train loss: 1.9112 valid loss: 1.9910, valid accuracy: 0.3546\n",
      "Iter-516000 train loss: 1.8637 valid loss: 1.9932, valid accuracy: 0.3538\n",
      "Iter-517000 train loss: 1.6635 valid loss: 1.9912, valid accuracy: 0.3500\n",
      "Iter-518000 train loss: 1.6644 valid loss: 1.9940, valid accuracy: 0.3515\n",
      "Iter-519000 train loss: 1.9959 valid loss: 1.9941, valid accuracy: 0.3523\n",
      "Iter-520000 train loss: 1.4709 valid loss: 1.9937, valid accuracy: 0.3562\n",
      "Iter-521000 train loss: 1.9477 valid loss: 1.9929, valid accuracy: 0.3546\n",
      "Iter-522000 train loss: 1.6322 valid loss: 1.9936, valid accuracy: 0.3500\n",
      "Iter-523000 train loss: 1.6612 valid loss: 1.9935, valid accuracy: 0.3531\n",
      "Iter-524000 train loss: 1.5139 valid loss: 1.9949, valid accuracy: 0.3531\n",
      "Iter-525000 train loss: 1.6472 valid loss: 1.9942, valid accuracy: 0.3508\n",
      "Iter-526000 train loss: 1.8830 valid loss: 1.9942, valid accuracy: 0.3592\n",
      "Iter-527000 train loss: 1.9185 valid loss: 1.9926, valid accuracy: 0.3546\n",
      "Iter-528000 train loss: 1.5161 valid loss: 1.9951, valid accuracy: 0.3531\n",
      "Iter-529000 train loss: 1.6156 valid loss: 1.9941, valid accuracy: 0.3538\n",
      "Iter-530000 train loss: 1.5223 valid loss: 1.9950, valid accuracy: 0.3546\n",
      "Iter-531000 train loss: 1.5547 valid loss: 1.9940, valid accuracy: 0.3577\n",
      "Iter-532000 train loss: 1.7142 valid loss: 1.9960, valid accuracy: 0.3523\n",
      "Iter-533000 train loss: 1.4425 valid loss: 1.9959, valid accuracy: 0.3515\n",
      "Iter-534000 train loss: 1.9575 valid loss: 1.9963, valid accuracy: 0.3585\n",
      "Iter-535000 train loss: 1.5701 valid loss: 1.9973, valid accuracy: 0.3515\n",
      "Iter-536000 train loss: 1.7342 valid loss: 1.9982, valid accuracy: 0.3500\n",
      "Iter-537000 train loss: 1.5677 valid loss: 1.9982, valid accuracy: 0.3454\n",
      "Iter-538000 train loss: 1.8300 valid loss: 1.9963, valid accuracy: 0.3508\n",
      "Iter-539000 train loss: 1.9067 valid loss: 1.9977, valid accuracy: 0.3515\n",
      "Iter-540000 train loss: 1.6746 valid loss: 1.9969, valid accuracy: 0.3515\n",
      "Iter-541000 train loss: 1.8375 valid loss: 1.9966, valid accuracy: 0.3523\n",
      "Iter-542000 train loss: 1.4334 valid loss: 1.9975, valid accuracy: 0.3531\n",
      "Iter-543000 train loss: 1.9038 valid loss: 1.9966, valid accuracy: 0.3562\n",
      "Iter-544000 train loss: 1.8270 valid loss: 1.9978, valid accuracy: 0.3531\n",
      "Iter-545000 train loss: 1.4346 valid loss: 1.9980, valid accuracy: 0.3500\n",
      "Iter-546000 train loss: 1.6378 valid loss: 1.9966, valid accuracy: 0.3492\n",
      "Iter-547000 train loss: 1.7567 valid loss: 1.9978, valid accuracy: 0.3546\n",
      "Iter-548000 train loss: 1.8686 valid loss: 1.9988, valid accuracy: 0.3500\n",
      "Iter-549000 train loss: 1.7538 valid loss: 1.9997, valid accuracy: 0.3485\n",
      "Iter-550000 train loss: 1.7605 valid loss: 2.0010, valid accuracy: 0.3485\n",
      "Iter-551000 train loss: 1.7982 valid loss: 1.9991, valid accuracy: 0.3523\n",
      "Iter-552000 train loss: 2.0456 valid loss: 1.9984, valid accuracy: 0.3523\n",
      "Iter-553000 train loss: 1.5533 valid loss: 1.9982, valid accuracy: 0.3523\n",
      "Iter-554000 train loss: 1.7145 valid loss: 1.9988, valid accuracy: 0.3523\n",
      "Iter-555000 train loss: 1.2557 valid loss: 1.9990, valid accuracy: 0.3515\n",
      "Iter-556000 train loss: 1.9037 valid loss: 1.9985, valid accuracy: 0.3538\n",
      "Iter-557000 train loss: 1.7245 valid loss: 1.9982, valid accuracy: 0.3554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-558000 train loss: 1.8923 valid loss: 1.9980, valid accuracy: 0.3531\n",
      "Iter-559000 train loss: 2.0134 valid loss: 1.9970, valid accuracy: 0.3515\n",
      "Iter-560000 train loss: 1.7058 valid loss: 1.9983, valid accuracy: 0.3531\n",
      "Iter-561000 train loss: 1.8150 valid loss: 1.9966, valid accuracy: 0.3546\n",
      "Iter-562000 train loss: 1.5376 valid loss: 1.9996, valid accuracy: 0.3485\n",
      "Iter-563000 train loss: 1.7031 valid loss: 1.9998, valid accuracy: 0.3508\n",
      "Iter-564000 train loss: 1.6090 valid loss: 1.9985, valid accuracy: 0.3500\n",
      "Iter-565000 train loss: 1.4837 valid loss: 1.9998, valid accuracy: 0.3538\n",
      "Iter-566000 train loss: 1.7642 valid loss: 2.0006, valid accuracy: 0.3454\n",
      "Iter-567000 train loss: 1.8408 valid loss: 1.9988, valid accuracy: 0.3523\n",
      "Iter-568000 train loss: 1.5572 valid loss: 1.9992, valid accuracy: 0.3492\n",
      "Iter-569000 train loss: 1.7766 valid loss: 1.9976, valid accuracy: 0.3523\n",
      "Iter-570000 train loss: 1.5863 valid loss: 1.9979, valid accuracy: 0.3500\n",
      "Iter-571000 train loss: 1.5575 valid loss: 1.9978, valid accuracy: 0.3538\n",
      "Iter-572000 train loss: 1.7691 valid loss: 1.9991, valid accuracy: 0.3515\n",
      "Iter-573000 train loss: 1.6939 valid loss: 1.9999, valid accuracy: 0.3515\n",
      "Iter-574000 train loss: 1.8008 valid loss: 1.9984, valid accuracy: 0.3523\n",
      "Iter-575000 train loss: 1.7579 valid loss: 1.9994, valid accuracy: 0.3508\n",
      "Iter-576000 train loss: 1.5606 valid loss: 2.0004, valid accuracy: 0.3523\n",
      "Iter-577000 train loss: 1.8379 valid loss: 2.0002, valid accuracy: 0.3508\n",
      "Iter-578000 train loss: 1.8278 valid loss: 2.0002, valid accuracy: 0.3477\n",
      "Iter-579000 train loss: 1.8663 valid loss: 1.9996, valid accuracy: 0.3485\n",
      "Iter-580000 train loss: 1.7027 valid loss: 1.9994, valid accuracy: 0.3531\n",
      "Iter-581000 train loss: 1.7041 valid loss: 2.0004, valid accuracy: 0.3492\n",
      "Iter-582000 train loss: 1.3434 valid loss: 2.0018, valid accuracy: 0.3492\n",
      "Iter-583000 train loss: 1.7434 valid loss: 1.9997, valid accuracy: 0.3523\n",
      "Iter-584000 train loss: 1.7226 valid loss: 2.0022, valid accuracy: 0.3500\n",
      "Iter-585000 train loss: 1.6632 valid loss: 2.0013, valid accuracy: 0.3492\n",
      "Iter-586000 train loss: 1.7147 valid loss: 2.0020, valid accuracy: 0.3477\n",
      "Iter-587000 train loss: 1.5918 valid loss: 2.0012, valid accuracy: 0.3538\n",
      "Iter-588000 train loss: 1.7067 valid loss: 2.0007, valid accuracy: 0.3508\n",
      "Iter-589000 train loss: 1.7847 valid loss: 2.0015, valid accuracy: 0.3500\n",
      "Iter-590000 train loss: 1.7599 valid loss: 2.0007, valid accuracy: 0.3477\n",
      "Iter-591000 train loss: 1.9186 valid loss: 2.0016, valid accuracy: 0.3500\n",
      "Iter-592000 train loss: 1.9723 valid loss: 2.0001, valid accuracy: 0.3508\n",
      "Iter-593000 train loss: 1.4363 valid loss: 2.0012, valid accuracy: 0.3538\n",
      "Iter-594000 train loss: 1.7950 valid loss: 1.9998, valid accuracy: 0.3523\n",
      "Iter-595000 train loss: 1.5890 valid loss: 2.0004, valid accuracy: 0.3500\n",
      "Iter-596000 train loss: 1.9958 valid loss: 2.0021, valid accuracy: 0.3500\n",
      "Iter-597000 train loss: 1.8360 valid loss: 2.0034, valid accuracy: 0.3492\n",
      "Iter-598000 train loss: 1.4628 valid loss: 2.0016, valid accuracy: 0.3515\n",
      "Iter-599000 train loss: 1.6724 valid loss: 2.0021, valid accuracy: 0.3469\n",
      "Iter-600000 train loss: 1.7985 valid loss: 2.0017, valid accuracy: 0.3523\n",
      "Iter-601000 train loss: 1.6459 valid loss: 2.0028, valid accuracy: 0.3538\n",
      "Iter-602000 train loss: 1.6099 valid loss: 2.0023, valid accuracy: 0.3554\n",
      "Iter-603000 train loss: 1.5222 valid loss: 2.0038, valid accuracy: 0.3469\n",
      "Iter-604000 train loss: 1.5033 valid loss: 2.0040, valid accuracy: 0.3538\n",
      "Iter-605000 train loss: 1.5601 valid loss: 2.0039, valid accuracy: 0.3500\n",
      "Iter-606000 train loss: 1.5843 valid loss: 2.0041, valid accuracy: 0.3515\n",
      "Iter-607000 train loss: 1.7968 valid loss: 2.0013, valid accuracy: 0.3562\n",
      "Iter-608000 train loss: 1.5485 valid loss: 2.0023, valid accuracy: 0.3538\n",
      "Iter-609000 train loss: 1.5215 valid loss: 2.0035, valid accuracy: 0.3531\n",
      "Iter-610000 train loss: 1.8147 valid loss: 2.0040, valid accuracy: 0.3477\n",
      "Iter-611000 train loss: 1.5890 valid loss: 2.0032, valid accuracy: 0.3523\n",
      "Iter-612000 train loss: 1.6288 valid loss: 2.0029, valid accuracy: 0.3523\n",
      "Iter-613000 train loss: 1.5273 valid loss: 2.0042, valid accuracy: 0.3500\n",
      "Iter-614000 train loss: 1.7164 valid loss: 2.0040, valid accuracy: 0.3538\n",
      "Iter-615000 train loss: 1.7671 valid loss: 2.0015, valid accuracy: 0.3538\n",
      "Iter-616000 train loss: 1.5593 valid loss: 2.0032, valid accuracy: 0.3500\n",
      "Iter-617000 train loss: 1.8091 valid loss: 2.0040, valid accuracy: 0.3531\n",
      "Iter-618000 train loss: 1.4172 valid loss: 2.0036, valid accuracy: 0.3515\n",
      "Iter-619000 train loss: 1.6077 valid loss: 2.0061, valid accuracy: 0.3508\n",
      "Iter-620000 train loss: 1.9191 valid loss: 2.0053, valid accuracy: 0.3515\n",
      "Iter-621000 train loss: 1.8944 valid loss: 2.0061, valid accuracy: 0.3477\n",
      "Iter-622000 train loss: 1.8419 valid loss: 2.0043, valid accuracy: 0.3523\n",
      "Iter-623000 train loss: 1.6000 valid loss: 2.0040, valid accuracy: 0.3477\n",
      "Iter-624000 train loss: 1.8194 valid loss: 2.0071, valid accuracy: 0.3469\n",
      "Iter-625000 train loss: 1.9813 valid loss: 2.0068, valid accuracy: 0.3446\n",
      "Iter-626000 train loss: 1.5074 valid loss: 2.0045, valid accuracy: 0.3500\n",
      "Iter-627000 train loss: 1.5143 valid loss: 2.0044, valid accuracy: 0.3508\n",
      "Iter-628000 train loss: 1.5446 valid loss: 2.0048, valid accuracy: 0.3531\n",
      "Iter-629000 train loss: 1.7895 valid loss: 2.0057, valid accuracy: 0.3485\n",
      "Iter-630000 train loss: 1.6192 valid loss: 2.0072, valid accuracy: 0.3462\n",
      "Iter-631000 train loss: 1.7078 valid loss: 2.0068, valid accuracy: 0.3546\n",
      "Iter-632000 train loss: 1.8984 valid loss: 2.0067, valid accuracy: 0.3523\n",
      "Iter-633000 train loss: 1.5492 valid loss: 2.0068, valid accuracy: 0.3523\n",
      "Iter-634000 train loss: 1.7758 valid loss: 2.0062, valid accuracy: 0.3538\n",
      "Iter-635000 train loss: 1.4194 valid loss: 2.0070, valid accuracy: 0.3515\n",
      "Iter-636000 train loss: 1.6943 valid loss: 2.0065, valid accuracy: 0.3515\n",
      "Iter-637000 train loss: 1.5485 valid loss: 2.0072, valid accuracy: 0.3554\n",
      "Iter-638000 train loss: 1.6698 valid loss: 2.0080, valid accuracy: 0.3562\n",
      "Iter-639000 train loss: 1.7789 valid loss: 2.0067, valid accuracy: 0.3531\n",
      "Iter-640000 train loss: 1.7085 valid loss: 2.0062, valid accuracy: 0.3546\n",
      "Iter-641000 train loss: 1.8530 valid loss: 2.0098, valid accuracy: 0.3477\n",
      "Iter-642000 train loss: 1.6576 valid loss: 2.0066, valid accuracy: 0.3531\n",
      "Iter-643000 train loss: 1.6641 valid loss: 2.0063, valid accuracy: 0.3538\n",
      "Iter-644000 train loss: 1.6690 valid loss: 2.0089, valid accuracy: 0.3538\n",
      "Iter-645000 train loss: 1.7669 valid loss: 2.0073, valid accuracy: 0.3523\n",
      "Iter-646000 train loss: 1.9109 valid loss: 2.0073, valid accuracy: 0.3554\n",
      "Iter-647000 train loss: 1.5970 valid loss: 2.0096, valid accuracy: 0.3531\n",
      "Iter-648000 train loss: 1.7801 valid loss: 2.0076, valid accuracy: 0.3562\n",
      "Iter-649000 train loss: 2.0600 valid loss: 2.0094, valid accuracy: 0.3515\n",
      "Iter-650000 train loss: 1.8175 valid loss: 2.0096, valid accuracy: 0.3546\n",
      "Iter-651000 train loss: 1.5348 valid loss: 2.0090, valid accuracy: 0.3523\n",
      "Iter-652000 train loss: 1.4009 valid loss: 2.0089, valid accuracy: 0.3508\n",
      "Iter-653000 train loss: 1.4800 valid loss: 2.0095, valid accuracy: 0.3562\n",
      "Iter-654000 train loss: 1.7770 valid loss: 2.0093, valid accuracy: 0.3515\n",
      "Iter-655000 train loss: 1.7251 valid loss: 2.0080, valid accuracy: 0.3562\n",
      "Iter-656000 train loss: 1.6053 valid loss: 2.0104, valid accuracy: 0.3546\n",
      "Iter-657000 train loss: 1.4227 valid loss: 2.0090, valid accuracy: 0.3554\n",
      "Iter-658000 train loss: 1.5599 valid loss: 2.0083, valid accuracy: 0.3538\n",
      "Iter-659000 train loss: 1.8598 valid loss: 2.0091, valid accuracy: 0.3515\n",
      "Iter-660000 train loss: 1.4286 valid loss: 2.0096, valid accuracy: 0.3531\n",
      "Iter-661000 train loss: 1.7651 valid loss: 2.0094, valid accuracy: 0.3546\n",
      "Iter-662000 train loss: 1.6627 valid loss: 2.0103, valid accuracy: 0.3569\n",
      "Iter-663000 train loss: 1.9015 valid loss: 2.0124, valid accuracy: 0.3538\n",
      "Iter-664000 train loss: 1.6558 valid loss: 2.0120, valid accuracy: 0.3523\n",
      "Iter-665000 train loss: 1.8013 valid loss: 2.0106, valid accuracy: 0.3615\n",
      "Iter-666000 train loss: 1.6784 valid loss: 2.0112, valid accuracy: 0.3531\n",
      "Iter-667000 train loss: 1.8561 valid loss: 2.0124, valid accuracy: 0.3554\n",
      "Iter-668000 train loss: 1.6107 valid loss: 2.0122, valid accuracy: 0.3515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-669000 train loss: 1.5261 valid loss: 2.0112, valid accuracy: 0.3538\n",
      "Iter-670000 train loss: 1.9046 valid loss: 2.0104, valid accuracy: 0.3508\n",
      "Iter-671000 train loss: 1.7125 valid loss: 2.0123, valid accuracy: 0.3546\n",
      "Iter-672000 train loss: 1.6547 valid loss: 2.0127, valid accuracy: 0.3538\n",
      "Iter-673000 train loss: 1.5064 valid loss: 2.0117, valid accuracy: 0.3569\n",
      "Iter-674000 train loss: 2.0309 valid loss: 2.0118, valid accuracy: 0.3538\n",
      "Iter-675000 train loss: 1.6054 valid loss: 2.0127, valid accuracy: 0.3608\n",
      "Iter-676000 train loss: 1.6547 valid loss: 2.0114, valid accuracy: 0.3577\n",
      "Iter-677000 train loss: 1.4439 valid loss: 2.0133, valid accuracy: 0.3569\n",
      "Iter-678000 train loss: 1.5641 valid loss: 2.0107, valid accuracy: 0.3585\n",
      "Iter-679000 train loss: 1.8433 valid loss: 2.0121, valid accuracy: 0.3546\n",
      "Iter-680000 train loss: 1.5805 valid loss: 2.0102, valid accuracy: 0.3577\n",
      "Iter-681000 train loss: 1.6560 valid loss: 2.0100, valid accuracy: 0.3538\n",
      "Iter-682000 train loss: 1.6367 valid loss: 2.0092, valid accuracy: 0.3577\n",
      "Iter-683000 train loss: 1.5728 valid loss: 2.0095, valid accuracy: 0.3592\n",
      "Iter-684000 train loss: 1.7169 valid loss: 2.0116, valid accuracy: 0.3546\n",
      "Iter-685000 train loss: 1.7875 valid loss: 2.0113, valid accuracy: 0.3592\n",
      "Iter-686000 train loss: 1.5567 valid loss: 2.0132, valid accuracy: 0.3562\n",
      "Iter-687000 train loss: 1.6242 valid loss: 2.0147, valid accuracy: 0.3515\n",
      "Iter-688000 train loss: 1.5716 valid loss: 2.0136, valid accuracy: 0.3546\n",
      "Iter-689000 train loss: 1.7281 valid loss: 2.0143, valid accuracy: 0.3531\n",
      "Iter-690000 train loss: 1.7483 valid loss: 2.0125, valid accuracy: 0.3577\n",
      "Iter-691000 train loss: 1.6382 valid loss: 2.0142, valid accuracy: 0.3554\n",
      "Iter-692000 train loss: 1.5918 valid loss: 2.0141, valid accuracy: 0.3546\n",
      "Iter-693000 train loss: 1.8810 valid loss: 2.0145, valid accuracy: 0.3569\n",
      "Iter-694000 train loss: 1.7977 valid loss: 2.0151, valid accuracy: 0.3538\n",
      "Iter-695000 train loss: 1.9798 valid loss: 2.0157, valid accuracy: 0.3523\n",
      "Iter-696000 train loss: 1.7330 valid loss: 2.0140, valid accuracy: 0.3554\n",
      "Iter-697000 train loss: 1.7917 valid loss: 2.0133, valid accuracy: 0.3569\n",
      "Iter-698000 train loss: 1.4877 valid loss: 2.0158, valid accuracy: 0.3554\n",
      "Iter-699000 train loss: 1.6639 valid loss: 2.0152, valid accuracy: 0.3546\n",
      "Iter-700000 train loss: 1.4652 valid loss: 2.0154, valid accuracy: 0.3577\n",
      "Iter-701000 train loss: 1.4179 valid loss: 2.0151, valid accuracy: 0.3554\n",
      "Iter-702000 train loss: 1.6819 valid loss: 2.0181, valid accuracy: 0.3500\n",
      "Iter-703000 train loss: 1.6360 valid loss: 2.0183, valid accuracy: 0.3515\n",
      "Iter-704000 train loss: 1.5349 valid loss: 2.0207, valid accuracy: 0.3546\n",
      "Iter-705000 train loss: 1.7316 valid loss: 2.0217, valid accuracy: 0.3515\n",
      "Iter-706000 train loss: 1.8821 valid loss: 2.0185, valid accuracy: 0.3538\n",
      "Iter-707000 train loss: 1.7391 valid loss: 2.0193, valid accuracy: 0.3562\n",
      "Iter-708000 train loss: 1.8327 valid loss: 2.0185, valid accuracy: 0.3531\n",
      "Iter-709000 train loss: 1.7540 valid loss: 2.0208, valid accuracy: 0.3554\n",
      "Iter-710000 train loss: 1.7504 valid loss: 2.0190, valid accuracy: 0.3585\n",
      "Iter-711000 train loss: 1.4601 valid loss: 2.0192, valid accuracy: 0.3562\n",
      "Iter-712000 train loss: 1.7520 valid loss: 2.0208, valid accuracy: 0.3546\n",
      "Iter-713000 train loss: 1.3272 valid loss: 2.0197, valid accuracy: 0.3515\n",
      "Iter-714000 train loss: 1.8212 valid loss: 2.0187, valid accuracy: 0.3531\n",
      "Iter-715000 train loss: 1.7184 valid loss: 2.0207, valid accuracy: 0.3562\n",
      "Iter-716000 train loss: 1.7717 valid loss: 2.0212, valid accuracy: 0.3554\n",
      "Iter-717000 train loss: 1.6195 valid loss: 2.0196, valid accuracy: 0.3523\n",
      "Iter-718000 train loss: 1.4645 valid loss: 2.0197, valid accuracy: 0.3538\n",
      "Iter-719000 train loss: 1.3512 valid loss: 2.0219, valid accuracy: 0.3523\n",
      "Iter-720000 train loss: 1.5961 valid loss: 2.0209, valid accuracy: 0.3569\n",
      "Iter-721000 train loss: 2.1734 valid loss: 2.0201, valid accuracy: 0.3569\n",
      "Iter-722000 train loss: 1.4959 valid loss: 2.0206, valid accuracy: 0.3538\n",
      "Iter-723000 train loss: 1.5828 valid loss: 2.0195, valid accuracy: 0.3523\n",
      "Iter-724000 train loss: 1.8337 valid loss: 2.0203, valid accuracy: 0.3531\n",
      "Iter-725000 train loss: 1.4771 valid loss: 2.0210, valid accuracy: 0.3546\n",
      "Iter-726000 train loss: 1.6421 valid loss: 2.0215, valid accuracy: 0.3538\n",
      "Iter-727000 train loss: 1.3132 valid loss: 2.0213, valid accuracy: 0.3531\n",
      "Iter-728000 train loss: 1.5046 valid loss: 2.0223, valid accuracy: 0.3546\n",
      "Iter-729000 train loss: 1.5908 valid loss: 2.0221, valid accuracy: 0.3523\n",
      "Iter-730000 train loss: 1.7075 valid loss: 2.0218, valid accuracy: 0.3538\n",
      "Iter-731000 train loss: 1.5056 valid loss: 2.0217, valid accuracy: 0.3554\n",
      "Iter-732000 train loss: 1.6041 valid loss: 2.0216, valid accuracy: 0.3500\n",
      "Iter-733000 train loss: 1.8509 valid loss: 2.0189, valid accuracy: 0.3538\n",
      "Iter-734000 train loss: 1.4543 valid loss: 2.0198, valid accuracy: 0.3562\n",
      "Iter-735000 train loss: 1.7579 valid loss: 2.0204, valid accuracy: 0.3523\n",
      "Iter-736000 train loss: 1.7410 valid loss: 2.0217, valid accuracy: 0.3554\n",
      "Iter-737000 train loss: 1.4825 valid loss: 2.0213, valid accuracy: 0.3515\n",
      "Iter-738000 train loss: 1.4141 valid loss: 2.0226, valid accuracy: 0.3523\n",
      "Iter-739000 train loss: 1.6750 valid loss: 2.0211, valid accuracy: 0.3523\n",
      "Iter-740000 train loss: 1.3648 valid loss: 2.0216, valid accuracy: 0.3477\n",
      "Iter-741000 train loss: 1.4667 valid loss: 2.0236, valid accuracy: 0.3531\n",
      "Iter-742000 train loss: 1.5154 valid loss: 2.0230, valid accuracy: 0.3508\n",
      "Iter-743000 train loss: 1.6488 valid loss: 2.0225, valid accuracy: 0.3477\n",
      "Iter-744000 train loss: 1.4057 valid loss: 2.0229, valid accuracy: 0.3515\n",
      "Iter-745000 train loss: 1.7870 valid loss: 2.0222, valid accuracy: 0.3531\n",
      "Iter-746000 train loss: 1.5530 valid loss: 2.0226, valid accuracy: 0.3531\n",
      "Iter-747000 train loss: 1.5314 valid loss: 2.0220, valid accuracy: 0.3500\n",
      "Iter-748000 train loss: 1.5537 valid loss: 2.0218, valid accuracy: 0.3515\n",
      "Iter-749000 train loss: 1.6548 valid loss: 2.0233, valid accuracy: 0.3523\n",
      "Iter-750000 train loss: 1.5517 valid loss: 2.0232, valid accuracy: 0.3515\n",
      "Iter-751000 train loss: 1.8282 valid loss: 2.0238, valid accuracy: 0.3492\n",
      "Iter-752000 train loss: 1.4529 valid loss: 2.0213, valid accuracy: 0.3492\n",
      "Iter-753000 train loss: 1.5171 valid loss: 2.0242, valid accuracy: 0.3500\n",
      "Iter-754000 train loss: 1.6582 valid loss: 2.0240, valid accuracy: 0.3508\n",
      "Iter-755000 train loss: 1.4157 valid loss: 2.0248, valid accuracy: 0.3492\n",
      "Iter-756000 train loss: 1.3010 valid loss: 2.0262, valid accuracy: 0.3462\n",
      "Iter-757000 train loss: 1.8301 valid loss: 2.0245, valid accuracy: 0.3477\n",
      "Iter-758000 train loss: 2.0022 valid loss: 2.0243, valid accuracy: 0.3492\n",
      "Iter-759000 train loss: 1.8821 valid loss: 2.0231, valid accuracy: 0.3515\n",
      "Iter-760000 train loss: 1.6520 valid loss: 2.0249, valid accuracy: 0.3508\n",
      "Iter-761000 train loss: 1.5857 valid loss: 2.0259, valid accuracy: 0.3492\n",
      "Iter-762000 train loss: 1.5204 valid loss: 2.0265, valid accuracy: 0.3492\n",
      "Iter-763000 train loss: 1.6302 valid loss: 2.0250, valid accuracy: 0.3492\n",
      "Iter-764000 train loss: 1.7654 valid loss: 2.0243, valid accuracy: 0.3500\n",
      "Iter-765000 train loss: 1.6528 valid loss: 2.0237, valid accuracy: 0.3500\n",
      "Iter-766000 train loss: 1.5075 valid loss: 2.0244, valid accuracy: 0.3500\n",
      "Iter-767000 train loss: 1.5229 valid loss: 2.0250, valid accuracy: 0.3508\n",
      "Iter-768000 train loss: 1.8667 valid loss: 2.0267, valid accuracy: 0.3523\n",
      "Iter-769000 train loss: 1.7293 valid loss: 2.0275, valid accuracy: 0.3500\n",
      "Iter-770000 train loss: 1.5122 valid loss: 2.0240, valid accuracy: 0.3523\n",
      "Iter-771000 train loss: 1.6744 valid loss: 2.0248, valid accuracy: 0.3500\n",
      "Iter-772000 train loss: 1.7596 valid loss: 2.0250, valid accuracy: 0.3500\n",
      "Iter-773000 train loss: 1.4426 valid loss: 2.0269, valid accuracy: 0.3523\n",
      "Iter-774000 train loss: 1.6701 valid loss: 2.0245, valid accuracy: 0.3538\n",
      "Iter-775000 train loss: 1.7552 valid loss: 2.0246, valid accuracy: 0.3515\n",
      "Iter-776000 train loss: 1.7659 valid loss: 2.0247, valid accuracy: 0.3469\n",
      "Iter-777000 train loss: 2.0522 valid loss: 2.0235, valid accuracy: 0.3500\n",
      "Iter-778000 train loss: 1.9667 valid loss: 2.0266, valid accuracy: 0.3508\n",
      "Iter-779000 train loss: 1.6658 valid loss: 2.0252, valid accuracy: 0.3538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-780000 train loss: 1.7552 valid loss: 2.0273, valid accuracy: 0.3485\n",
      "Iter-781000 train loss: 1.5395 valid loss: 2.0267, valid accuracy: 0.3492\n",
      "Iter-782000 train loss: 1.5447 valid loss: 2.0269, valid accuracy: 0.3562\n",
      "Iter-783000 train loss: 1.6251 valid loss: 2.0269, valid accuracy: 0.3508\n",
      "Iter-784000 train loss: 1.5674 valid loss: 2.0266, valid accuracy: 0.3523\n",
      "Iter-785000 train loss: 1.7104 valid loss: 2.0262, valid accuracy: 0.3554\n",
      "Iter-786000 train loss: 1.8127 valid loss: 2.0261, valid accuracy: 0.3500\n",
      "Iter-787000 train loss: 1.5208 valid loss: 2.0270, valid accuracy: 0.3469\n",
      "Iter-788000 train loss: 1.5761 valid loss: 2.0282, valid accuracy: 0.3546\n",
      "Iter-789000 train loss: 1.6907 valid loss: 2.0272, valid accuracy: 0.3500\n",
      "Iter-790000 train loss: 1.9503 valid loss: 2.0272, valid accuracy: 0.3500\n",
      "Iter-791000 train loss: 1.7036 valid loss: 2.0273, valid accuracy: 0.3523\n",
      "Iter-792000 train loss: 1.3376 valid loss: 2.0311, valid accuracy: 0.3500\n",
      "Iter-793000 train loss: 1.3852 valid loss: 2.0294, valid accuracy: 0.3508\n",
      "Iter-794000 train loss: 1.7129 valid loss: 2.0273, valid accuracy: 0.3485\n",
      "Iter-795000 train loss: 1.7156 valid loss: 2.0275, valid accuracy: 0.3554\n",
      "Iter-796000 train loss: 1.6431 valid loss: 2.0282, valid accuracy: 0.3515\n",
      "Iter-797000 train loss: 1.6646 valid loss: 2.0267, valid accuracy: 0.3500\n",
      "Iter-798000 train loss: 1.3734 valid loss: 2.0313, valid accuracy: 0.3492\n",
      "Iter-799000 train loss: 1.6114 valid loss: 2.0297, valid accuracy: 0.3485\n",
      "Iter-800000 train loss: 1.7918 valid loss: 2.0284, valid accuracy: 0.3485\n",
      "Iter-801000 train loss: 1.5750 valid loss: 2.0291, valid accuracy: 0.3485\n",
      "Iter-802000 train loss: 2.0954 valid loss: 2.0310, valid accuracy: 0.3500\n",
      "Iter-803000 train loss: 1.6551 valid loss: 2.0293, valid accuracy: 0.3569\n",
      "Iter-804000 train loss: 1.6478 valid loss: 2.0315, valid accuracy: 0.3523\n",
      "Iter-805000 train loss: 1.7702 valid loss: 2.0291, valid accuracy: 0.3508\n",
      "Iter-806000 train loss: 1.6761 valid loss: 2.0310, valid accuracy: 0.3492\n",
      "Iter-807000 train loss: 1.8176 valid loss: 2.0294, valid accuracy: 0.3500\n",
      "Iter-808000 train loss: 1.7938 valid loss: 2.0303, valid accuracy: 0.3515\n",
      "Iter-809000 train loss: 1.7871 valid loss: 2.0299, valid accuracy: 0.3515\n",
      "Iter-810000 train loss: 1.7339 valid loss: 2.0284, valid accuracy: 0.3546\n",
      "Iter-811000 train loss: 2.0234 valid loss: 2.0299, valid accuracy: 0.3515\n",
      "Iter-812000 train loss: 1.9838 valid loss: 2.0295, valid accuracy: 0.3577\n",
      "Iter-813000 train loss: 1.4952 valid loss: 2.0293, valid accuracy: 0.3515\n",
      "Iter-814000 train loss: 1.6931 valid loss: 2.0329, valid accuracy: 0.3485\n",
      "Iter-815000 train loss: 1.6264 valid loss: 2.0331, valid accuracy: 0.3515\n",
      "Iter-816000 train loss: 1.8214 valid loss: 2.0308, valid accuracy: 0.3515\n",
      "Iter-817000 train loss: 1.5839 valid loss: 2.0310, valid accuracy: 0.3523\n",
      "Iter-818000 train loss: 1.5775 valid loss: 2.0303, valid accuracy: 0.3554\n",
      "Iter-819000 train loss: 1.5720 valid loss: 2.0330, valid accuracy: 0.3492\n",
      "Iter-820000 train loss: 1.4654 valid loss: 2.0317, valid accuracy: 0.3531\n",
      "Iter-821000 train loss: 1.4843 valid loss: 2.0329, valid accuracy: 0.3477\n",
      "Iter-822000 train loss: 1.8691 valid loss: 2.0333, valid accuracy: 0.3508\n",
      "Iter-823000 train loss: 1.8380 valid loss: 2.0316, valid accuracy: 0.3515\n",
      "Iter-824000 train loss: 1.8841 valid loss: 2.0311, valid accuracy: 0.3515\n",
      "Iter-825000 train loss: 1.7793 valid loss: 2.0326, valid accuracy: 0.3538\n",
      "Iter-826000 train loss: 1.7496 valid loss: 2.0340, valid accuracy: 0.3531\n",
      "Iter-827000 train loss: 1.6159 valid loss: 2.0340, valid accuracy: 0.3523\n",
      "Iter-828000 train loss: 1.6152 valid loss: 2.0326, valid accuracy: 0.3508\n",
      "Iter-829000 train loss: 1.8429 valid loss: 2.0352, valid accuracy: 0.3454\n",
      "Iter-830000 train loss: 1.7181 valid loss: 2.0358, valid accuracy: 0.3531\n",
      "Iter-831000 train loss: 1.7004 valid loss: 2.0349, valid accuracy: 0.3485\n",
      "Iter-832000 train loss: 1.6207 valid loss: 2.0347, valid accuracy: 0.3515\n",
      "Iter-833000 train loss: 1.4973 valid loss: 2.0335, valid accuracy: 0.3508\n",
      "Iter-834000 train loss: 1.6152 valid loss: 2.0340, valid accuracy: 0.3462\n",
      "Iter-835000 train loss: 1.5270 valid loss: 2.0337, valid accuracy: 0.3477\n",
      "Iter-836000 train loss: 1.6321 valid loss: 2.0320, valid accuracy: 0.3515\n",
      "Iter-837000 train loss: 1.4510 valid loss: 2.0340, valid accuracy: 0.3477\n",
      "Iter-838000 train loss: 1.7083 valid loss: 2.0347, valid accuracy: 0.3531\n",
      "Iter-839000 train loss: 1.4691 valid loss: 2.0341, valid accuracy: 0.3523\n",
      "Iter-840000 train loss: 1.5191 valid loss: 2.0344, valid accuracy: 0.3485\n",
      "Iter-841000 train loss: 1.6908 valid loss: 2.0340, valid accuracy: 0.3492\n",
      "Iter-842000 train loss: 1.8058 valid loss: 2.0348, valid accuracy: 0.3492\n",
      "Iter-843000 train loss: 1.9506 valid loss: 2.0364, valid accuracy: 0.3515\n",
      "Iter-844000 train loss: 1.5414 valid loss: 2.0362, valid accuracy: 0.3554\n",
      "Iter-845000 train loss: 1.5948 valid loss: 2.0364, valid accuracy: 0.3515\n",
      "Iter-846000 train loss: 1.5796 valid loss: 2.0339, valid accuracy: 0.3531\n",
      "Iter-847000 train loss: 1.7442 valid loss: 2.0357, valid accuracy: 0.3577\n",
      "Iter-848000 train loss: 1.7722 valid loss: 2.0362, valid accuracy: 0.3562\n",
      "Iter-849000 train loss: 1.5144 valid loss: 2.0386, valid accuracy: 0.3515\n",
      "Iter-850000 train loss: 1.9348 valid loss: 2.0384, valid accuracy: 0.3515\n",
      "Iter-851000 train loss: 1.5120 valid loss: 2.0387, valid accuracy: 0.3531\n",
      "Iter-852000 train loss: 1.4524 valid loss: 2.0372, valid accuracy: 0.3508\n",
      "Iter-853000 train loss: 1.9112 valid loss: 2.0374, valid accuracy: 0.3554\n",
      "Iter-854000 train loss: 1.5336 valid loss: 2.0381, valid accuracy: 0.3538\n",
      "Iter-855000 train loss: 1.7076 valid loss: 2.0389, valid accuracy: 0.3531\n",
      "Iter-856000 train loss: 1.5692 valid loss: 2.0383, valid accuracy: 0.3500\n",
      "Iter-857000 train loss: 1.4078 valid loss: 2.0374, valid accuracy: 0.3531\n",
      "Iter-858000 train loss: 1.7580 valid loss: 2.0381, valid accuracy: 0.3515\n",
      "Iter-859000 train loss: 1.6738 valid loss: 2.0367, valid accuracy: 0.3492\n",
      "Iter-860000 train loss: 1.5103 valid loss: 2.0380, valid accuracy: 0.3492\n",
      "Iter-861000 train loss: 1.6191 valid loss: 2.0364, valid accuracy: 0.3508\n",
      "Iter-862000 train loss: 1.7372 valid loss: 2.0372, valid accuracy: 0.3485\n",
      "Iter-863000 train loss: 1.5568 valid loss: 2.0371, valid accuracy: 0.3546\n",
      "Iter-864000 train loss: 1.4574 valid loss: 2.0376, valid accuracy: 0.3523\n",
      "Iter-865000 train loss: 1.5129 valid loss: 2.0383, valid accuracy: 0.3515\n",
      "Iter-866000 train loss: 1.3874 valid loss: 2.0405, valid accuracy: 0.3469\n",
      "Iter-867000 train loss: 1.6499 valid loss: 2.0388, valid accuracy: 0.3508\n",
      "Iter-868000 train loss: 1.5813 valid loss: 2.0393, valid accuracy: 0.3531\n",
      "Iter-869000 train loss: 1.8663 valid loss: 2.0376, valid accuracy: 0.3577\n",
      "Iter-870000 train loss: 1.7609 valid loss: 2.0385, valid accuracy: 0.3523\n",
      "Iter-871000 train loss: 1.5107 valid loss: 2.0392, valid accuracy: 0.3462\n",
      "Iter-872000 train loss: 1.7826 valid loss: 2.0400, valid accuracy: 0.3515\n",
      "Iter-873000 train loss: 1.7125 valid loss: 2.0391, valid accuracy: 0.3492\n",
      "Iter-874000 train loss: 1.5612 valid loss: 2.0385, valid accuracy: 0.3531\n",
      "Iter-875000 train loss: 1.5569 valid loss: 2.0401, valid accuracy: 0.3523\n",
      "Iter-876000 train loss: 1.5386 valid loss: 2.0399, valid accuracy: 0.3515\n",
      "Iter-877000 train loss: 1.4635 valid loss: 2.0383, valid accuracy: 0.3523\n",
      "Iter-878000 train loss: 1.4718 valid loss: 2.0374, valid accuracy: 0.3492\n",
      "Iter-879000 train loss: 1.5635 valid loss: 2.0421, valid accuracy: 0.3508\n",
      "Iter-880000 train loss: 1.7356 valid loss: 2.0416, valid accuracy: 0.3492\n",
      "Iter-881000 train loss: 1.5119 valid loss: 2.0413, valid accuracy: 0.3508\n",
      "Iter-882000 train loss: 1.5795 valid loss: 2.0423, valid accuracy: 0.3523\n",
      "Iter-883000 train loss: 1.6348 valid loss: 2.0413, valid accuracy: 0.3531\n",
      "Iter-884000 train loss: 1.6828 valid loss: 2.0418, valid accuracy: 0.3538\n",
      "Iter-885000 train loss: 1.4992 valid loss: 2.0431, valid accuracy: 0.3485\n",
      "Iter-886000 train loss: 1.9310 valid loss: 2.0402, valid accuracy: 0.3485\n",
      "Iter-887000 train loss: 1.5903 valid loss: 2.0409, valid accuracy: 0.3508\n",
      "Iter-888000 train loss: 1.6250 valid loss: 2.0408, valid accuracy: 0.3477\n",
      "Iter-889000 train loss: 1.7687 valid loss: 2.0422, valid accuracy: 0.3515\n",
      "Iter-890000 train loss: 1.3338 valid loss: 2.0439, valid accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-891000 train loss: 1.6402 valid loss: 2.0428, valid accuracy: 0.3546\n",
      "Iter-892000 train loss: 1.4655 valid loss: 2.0436, valid accuracy: 0.3515\n",
      "Iter-893000 train loss: 1.5982 valid loss: 2.0446, valid accuracy: 0.3500\n",
      "Iter-894000 train loss: 1.7058 valid loss: 2.0437, valid accuracy: 0.3500\n",
      "Iter-895000 train loss: 1.7331 valid loss: 2.0443, valid accuracy: 0.3515\n",
      "Iter-896000 train loss: 1.6047 valid loss: 2.0423, valid accuracy: 0.3492\n",
      "Iter-897000 train loss: 1.6494 valid loss: 2.0415, valid accuracy: 0.3492\n",
      "Iter-898000 train loss: 1.5811 valid loss: 2.0419, valid accuracy: 0.3546\n",
      "Iter-899000 train loss: 1.5413 valid loss: 2.0426, valid accuracy: 0.3492\n",
      "Iter-900000 train loss: 1.6444 valid loss: 2.0447, valid accuracy: 0.3485\n",
      "Iter-901000 train loss: 1.7227 valid loss: 2.0427, valid accuracy: 0.3546\n",
      "Iter-902000 train loss: 1.4700 valid loss: 2.0410, valid accuracy: 0.3462\n",
      "Iter-903000 train loss: 1.5886 valid loss: 2.0411, valid accuracy: 0.3500\n",
      "Iter-904000 train loss: 1.5965 valid loss: 2.0437, valid accuracy: 0.3500\n",
      "Iter-905000 train loss: 1.7638 valid loss: 2.0418, valid accuracy: 0.3477\n",
      "Iter-906000 train loss: 1.4648 valid loss: 2.0416, valid accuracy: 0.3492\n",
      "Iter-907000 train loss: 2.0069 valid loss: 2.0421, valid accuracy: 0.3538\n",
      "Iter-908000 train loss: 1.6015 valid loss: 2.0437, valid accuracy: 0.3554\n",
      "Iter-909000 train loss: 1.5539 valid loss: 2.0459, valid accuracy: 0.3546\n",
      "Iter-910000 train loss: 1.5126 valid loss: 2.0440, valid accuracy: 0.3546\n",
      "Iter-911000 train loss: 1.7479 valid loss: 2.0430, valid accuracy: 0.3500\n",
      "Iter-912000 train loss: 1.5955 valid loss: 2.0454, valid accuracy: 0.3508\n",
      "Iter-913000 train loss: 1.2704 valid loss: 2.0453, valid accuracy: 0.3538\n",
      "Iter-914000 train loss: 1.6439 valid loss: 2.0430, valid accuracy: 0.3492\n",
      "Iter-915000 train loss: 2.0073 valid loss: 2.0434, valid accuracy: 0.3515\n",
      "Iter-916000 train loss: 1.5463 valid loss: 2.0436, valid accuracy: 0.3592\n",
      "Iter-917000 train loss: 1.7273 valid loss: 2.0444, valid accuracy: 0.3515\n",
      "Iter-918000 train loss: 1.3971 valid loss: 2.0441, valid accuracy: 0.3554\n",
      "Iter-919000 train loss: 1.3201 valid loss: 2.0443, valid accuracy: 0.3508\n",
      "Iter-920000 train loss: 1.8225 valid loss: 2.0452, valid accuracy: 0.3562\n",
      "Iter-921000 train loss: 1.5897 valid loss: 2.0460, valid accuracy: 0.3515\n",
      "Iter-922000 train loss: 1.3896 valid loss: 2.0455, valid accuracy: 0.3531\n",
      "Iter-923000 train loss: 1.4319 valid loss: 2.0459, valid accuracy: 0.3538\n",
      "Iter-924000 train loss: 1.4381 valid loss: 2.0468, valid accuracy: 0.3485\n",
      "Iter-925000 train loss: 1.5435 valid loss: 2.0477, valid accuracy: 0.3515\n",
      "Iter-926000 train loss: 1.7756 valid loss: 2.0485, valid accuracy: 0.3500\n",
      "Iter-927000 train loss: 1.8001 valid loss: 2.0459, valid accuracy: 0.3485\n",
      "Iter-928000 train loss: 1.5238 valid loss: 2.0477, valid accuracy: 0.3500\n",
      "Iter-929000 train loss: 1.5310 valid loss: 2.0460, valid accuracy: 0.3523\n",
      "Iter-930000 train loss: 1.5663 valid loss: 2.0483, valid accuracy: 0.3531\n",
      "Iter-931000 train loss: 1.6190 valid loss: 2.0485, valid accuracy: 0.3538\n",
      "Iter-932000 train loss: 1.3599 valid loss: 2.0476, valid accuracy: 0.3515\n",
      "Iter-933000 train loss: 1.7590 valid loss: 2.0484, valid accuracy: 0.3508\n",
      "Iter-934000 train loss: 1.4360 valid loss: 2.0492, valid accuracy: 0.3538\n",
      "Iter-935000 train loss: 1.7095 valid loss: 2.0481, valid accuracy: 0.3508\n",
      "Iter-936000 train loss: 1.8215 valid loss: 2.0487, valid accuracy: 0.3523\n",
      "Iter-937000 train loss: 1.5676 valid loss: 2.0488, valid accuracy: 0.3531\n",
      "Iter-938000 train loss: 1.7171 valid loss: 2.0498, valid accuracy: 0.3500\n",
      "Iter-939000 train loss: 1.7857 valid loss: 2.0499, valid accuracy: 0.3531\n",
      "Iter-940000 train loss: 1.9937 valid loss: 2.0520, valid accuracy: 0.3508\n",
      "Iter-941000 train loss: 1.4801 valid loss: 2.0495, valid accuracy: 0.3485\n",
      "Iter-942000 train loss: 1.6550 valid loss: 2.0523, valid accuracy: 0.3492\n",
      "Iter-943000 train loss: 1.7623 valid loss: 2.0503, valid accuracy: 0.3523\n",
      "Iter-944000 train loss: 1.4507 valid loss: 2.0531, valid accuracy: 0.3462\n",
      "Iter-945000 train loss: 1.6012 valid loss: 2.0502, valid accuracy: 0.3523\n",
      "Iter-946000 train loss: 1.7505 valid loss: 2.0495, valid accuracy: 0.3546\n",
      "Iter-947000 train loss: 1.6832 valid loss: 2.0505, valid accuracy: 0.3531\n",
      "Iter-948000 train loss: 1.5638 valid loss: 2.0509, valid accuracy: 0.3515\n",
      "Iter-949000 train loss: 1.6012 valid loss: 2.0492, valid accuracy: 0.3546\n",
      "Iter-950000 train loss: 1.2815 valid loss: 2.0514, valid accuracy: 0.3531\n",
      "Iter-951000 train loss: 1.4978 valid loss: 2.0528, valid accuracy: 0.3523\n",
      "Iter-952000 train loss: 1.6035 valid loss: 2.0518, valid accuracy: 0.3523\n",
      "Iter-953000 train loss: 1.7116 valid loss: 2.0522, valid accuracy: 0.3546\n",
      "Iter-954000 train loss: 1.5517 valid loss: 2.0552, valid accuracy: 0.3469\n",
      "Iter-955000 train loss: 1.7716 valid loss: 2.0510, valid accuracy: 0.3500\n",
      "Iter-956000 train loss: 1.4177 valid loss: 2.0501, valid accuracy: 0.3554\n",
      "Iter-957000 train loss: 1.4724 valid loss: 2.0527, valid accuracy: 0.3569\n",
      "Iter-958000 train loss: 1.7889 valid loss: 2.0531, valid accuracy: 0.3492\n",
      "Iter-959000 train loss: 1.5471 valid loss: 2.0503, valid accuracy: 0.3531\n",
      "Iter-960000 train loss: 1.7610 valid loss: 2.0507, valid accuracy: 0.3538\n",
      "Iter-961000 train loss: 1.7136 valid loss: 2.0536, valid accuracy: 0.3508\n",
      "Iter-962000 train loss: 1.2543 valid loss: 2.0532, valid accuracy: 0.3469\n",
      "Iter-963000 train loss: 1.6826 valid loss: 2.0532, valid accuracy: 0.3515\n",
      "Iter-964000 train loss: 1.9307 valid loss: 2.0540, valid accuracy: 0.3508\n",
      "Iter-965000 train loss: 1.7987 valid loss: 2.0530, valid accuracy: 0.3508\n",
      "Iter-966000 train loss: 1.7514 valid loss: 2.0549, valid accuracy: 0.3554\n",
      "Iter-967000 train loss: 1.4202 valid loss: 2.0550, valid accuracy: 0.3531\n",
      "Iter-968000 train loss: 1.5791 valid loss: 2.0542, valid accuracy: 0.3554\n",
      "Iter-969000 train loss: 1.7526 valid loss: 2.0540, valid accuracy: 0.3515\n",
      "Iter-970000 train loss: 1.6596 valid loss: 2.0552, valid accuracy: 0.3546\n",
      "Iter-971000 train loss: 1.5585 valid loss: 2.0533, valid accuracy: 0.3485\n",
      "Iter-972000 train loss: 1.6590 valid loss: 2.0552, valid accuracy: 0.3523\n",
      "Iter-973000 train loss: 1.5658 valid loss: 2.0528, valid accuracy: 0.3508\n",
      "Iter-974000 train loss: 1.3799 valid loss: 2.0526, valid accuracy: 0.3508\n",
      "Iter-975000 train loss: 1.5512 valid loss: 2.0528, valid accuracy: 0.3531\n",
      "Iter-976000 train loss: 1.5622 valid loss: 2.0537, valid accuracy: 0.3469\n",
      "Iter-977000 train loss: 1.6961 valid loss: 2.0539, valid accuracy: 0.3515\n",
      "Iter-978000 train loss: 1.7629 valid loss: 2.0543, valid accuracy: 0.3500\n",
      "Iter-979000 train loss: 1.3758 valid loss: 2.0543, valid accuracy: 0.3508\n",
      "Iter-980000 train loss: 1.4801 valid loss: 2.0574, valid accuracy: 0.3500\n",
      "Iter-981000 train loss: 1.4608 valid loss: 2.0559, valid accuracy: 0.3508\n",
      "Iter-982000 train loss: 1.3934 valid loss: 2.0582, valid accuracy: 0.3523\n",
      "Iter-983000 train loss: 1.6902 valid loss: 2.0572, valid accuracy: 0.3523\n",
      "Iter-984000 train loss: 1.3478 valid loss: 2.0581, valid accuracy: 0.3515\n",
      "Iter-985000 train loss: 1.9187 valid loss: 2.0583, valid accuracy: 0.3500\n",
      "Iter-986000 train loss: 1.5882 valid loss: 2.0560, valid accuracy: 0.3523\n",
      "Iter-987000 train loss: 1.6520 valid loss: 2.0538, valid accuracy: 0.3538\n",
      "Iter-988000 train loss: 1.7048 valid loss: 2.0544, valid accuracy: 0.3523\n",
      "Iter-989000 train loss: 1.8565 valid loss: 2.0538, valid accuracy: 0.3523\n",
      "Iter-990000 train loss: 1.9603 valid loss: 2.0546, valid accuracy: 0.3531\n",
      "Iter-991000 train loss: 1.5875 valid loss: 2.0563, valid accuracy: 0.3492\n",
      "Iter-992000 train loss: 1.6642 valid loss: 2.0556, valid accuracy: 0.3492\n",
      "Iter-993000 train loss: 1.7337 valid loss: 2.0575, valid accuracy: 0.3492\n",
      "Iter-994000 train loss: 1.7632 valid loss: 2.0571, valid accuracy: 0.3508\n",
      "Iter-995000 train loss: 1.5782 valid loss: 2.0552, valid accuracy: 0.3523\n",
      "Iter-996000 train loss: 1.3199 valid loss: 2.0562, valid accuracy: 0.3531\n",
      "Iter-997000 train loss: 1.4263 valid loss: 2.0574, valid accuracy: 0.3531\n",
      "Iter-998000 train loss: 1.4201 valid loss: 2.0582, valid accuracy: 0.3508\n",
      "Iter-999000 train loss: 1.4227 valid loss: 2.0576, valid accuracy: 0.3538\n",
      "Iter-1000000 train loss: 1.5560 valid loss: 2.0566, valid accuracy: 0.3523\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 1000000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 1000 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "# num_output_units = Y_train.shape[1] # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgghHCQ4CCGEcPj/OeIgbGeaMdgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d7cfdf160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niESmpCEiIpEpaYiISGT/H779c8ZhEkknAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d7cfdf7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'Blues',\n",
       " 'Country',\n",
       " 'Electronic',\n",
       " 'Folk',\n",
       " 'International',\n",
       " 'Jazz',\n",
       " 'Latin',\n",
       " 'New_Age',\n",
       " 'Pop_Rock',\n",
       " 'Rap',\n",
       " 'Reggae',\n",
       " 'RnB',\n",
       " 'Vocal']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = labels_keys_sorted.copy()\n",
    "heading.insert(0, 'Id')\n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10400, 13),\n",
       " (10400, 26),\n",
       " (10400, 13),\n",
       " (10400, 14),\n",
       "    Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       " 0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       " \n",
       "    New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       " 0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  )"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, y_logits = nn.test(X_test)\n",
    "y_prob = l.softmax(y_logits)\n",
    "y_prob.shape, X_test.shape, y_logits.shape, test_y_sample.shape, test_y_sample[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for Id, pred in enumerate(y_prob):\n",
    "#     print(Id+1, *pred)\n",
    "    pred_list.append([Id+1, *pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_file = open(file='prediction.csv', mode='w')\n",
    "pred_file.write('\\n') # because of the previous line        \n",
    "\n",
    "for idx in range(len(heading)):\n",
    "    if idx < len(heading) - 1:\n",
    "        pred_file.write(heading[idx] + ',')\n",
    "    else:\n",
    "        pred_file.write(heading[idx] + '\\n')        \n",
    "\n",
    "# len(test), test[0]\n",
    "# for key in test:\n",
    "for i in range(len(pred_list)): # rows\n",
    "    for j in range(len(pred_list[i])): # cols\n",
    "        if j < (len(pred_list[i]) - 1):\n",
    "            pred_file.write(str(pred_list[i][j]))\n",
    "            pred_file.write(',')\n",
    "        else: # last item before starting a new line\n",
    "            pred_file.write(str(pred_list[i][j]) + '\\n')        \n",
    "\n",
    "# pred_file.write(-',')\n",
    "pred_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.020769</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.008920</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>0.015555</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>0.471912</td>\n",
       "      <td>0.443211</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>0.004146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.111895</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.052704</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>0.171247</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>0.067255</td>\n",
       "      <td>0.545532</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.027999</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005783</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.026342</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.032167</td>\n",
       "      <td>0.173455</td>\n",
       "      <td>0.691620</td>\n",
       "      <td>0.022625</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.026503</td>\n",
       "      <td>0.006638</td>\n",
       "      <td>0.009442</td>\n",
       "      <td>0.014912</td>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.004456</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>0.015035</td>\n",
       "      <td>0.030826</td>\n",
       "      <td>0.806138</td>\n",
       "      <td>0.031182</td>\n",
       "      <td>0.036088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.020792</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.006638</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.701781</td>\n",
       "      <td>0.234770</td>\n",
       "      <td>0.006261</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id     Blues   Country  Electronic      Folk  International      Jazz  \\\n",
       "0   1  0.020769  0.000896    0.005076  0.001435       0.008920  0.010687   \n",
       "1   2  0.111895  0.002969    0.001409  0.004355       0.052704  0.006349   \n",
       "2   3  0.027999  0.005736    0.005783  0.001734       0.026342  0.000934   \n",
       "3   4  0.026503  0.006638    0.009442  0.014912       0.012117  0.004456   \n",
       "4   5  0.008471  0.000145    0.020792  0.000240       0.006638  0.001272   \n",
       "\n",
       "      Latin   New_Age  Pop_Rock       Rap    Reggae       RnB     Vocal  \n",
       "0  0.015555  0.007284  0.004823  0.471912  0.443211  0.005287  0.004146  \n",
       "1  0.171247  0.008251  0.010501  0.067255  0.545532  0.011674  0.005859  \n",
       "2  0.005800  0.001806  0.032167  0.173455  0.691620  0.022625  0.004001  \n",
       "3  0.002983  0.003679  0.015035  0.030826  0.806138  0.031182  0.036088  \n",
       "4  0.010574  0.000702  0.008070  0.701781  0.234770  0.006261  0.000282  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10400, 14), (10400, 14))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').shape, test_y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

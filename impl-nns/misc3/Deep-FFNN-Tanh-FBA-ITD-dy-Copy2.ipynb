{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y)\n",
    "\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def onehot(self, labels):\n",
    "#         y = np.zeros([labels.size, np.max(labels) + 1])\n",
    "        y = np.zeros([labels.size, self.C])\n",
    "        y[range(labels.size), labels] = 1.\n",
    "        return y\n",
    "\n",
    "    def squared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        data_loss = 0.5 * np.sum((y_pred - self.onehot(y_train))**2)/ m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dsquared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = (y_pred - self.onehot(y_train))/ m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.squared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        dy = self.dsquared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        \n",
    "        return loss, dy\n",
    "\n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # Softmax backward\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "#         dy =  dy @ self.W_fixed[2].T # done\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "#             dy =  dy @ self.W_fixed[2].T # done\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-0.4473, acc-0.1400, valid loss-0.4485, acc-0.1162, test loss-0.4485, acc-0.1156\n",
      "Iter-20, train loss-0.4495, acc-0.1000, valid loss-0.4488, acc-0.1158, test loss-0.4488, acc-0.1141\n",
      "Iter-30, train loss-0.4492, acc-0.1400, valid loss-0.4490, acc-0.1154, test loss-0.4489, acc-0.1181\n",
      "Iter-40, train loss-0.4496, acc-0.1000, valid loss-0.4494, acc-0.1144, test loss-0.4493, acc-0.1188\n",
      "Iter-50, train loss-0.4484, acc-0.1800, valid loss-0.4497, acc-0.1152, test loss-0.4495, acc-0.1182\n",
      "Iter-60, train loss-0.4504, acc-0.0800, valid loss-0.4500, acc-0.1166, test loss-0.4498, acc-0.1214\n",
      "Iter-70, train loss-0.4504, acc-0.0900, valid loss-0.4503, acc-0.1194, test loss-0.4500, acc-0.1242\n",
      "Iter-80, train loss-0.4490, acc-0.1700, valid loss-0.4504, acc-0.1226, test loss-0.4501, acc-0.1295\n",
      "Iter-90, train loss-0.4502, acc-0.0900, valid loss-0.4504, acc-0.1282, test loss-0.4501, acc-0.1374\n",
      "Iter-100, train loss-0.4489, acc-0.1600, valid loss-0.4503, acc-0.1306, test loss-0.4500, acc-0.1431\n",
      "Iter-110, train loss-0.4492, acc-0.1200, valid loss-0.4503, acc-0.1334, test loss-0.4499, acc-0.1469\n",
      "Iter-120, train loss-0.4486, acc-0.1200, valid loss-0.4500, acc-0.1372, test loss-0.4496, acc-0.1510\n",
      "Iter-130, train loss-0.4494, acc-0.0900, valid loss-0.4499, acc-0.1388, test loss-0.4495, acc-0.1520\n",
      "Iter-140, train loss-0.4501, acc-0.1200, valid loss-0.4497, acc-0.1418, test loss-0.4492, acc-0.1550\n",
      "Iter-150, train loss-0.4492, acc-0.2000, valid loss-0.4494, acc-0.1456, test loss-0.4489, acc-0.1573\n",
      "Iter-160, train loss-0.4500, acc-0.1200, valid loss-0.4491, acc-0.1508, test loss-0.4486, acc-0.1624\n",
      "Iter-170, train loss-0.4473, acc-0.1800, valid loss-0.4486, acc-0.1536, test loss-0.4481, acc-0.1670\n",
      "Iter-180, train loss-0.4474, acc-0.1400, valid loss-0.4481, acc-0.1560, test loss-0.4475, acc-0.1696\n",
      "Iter-190, train loss-0.4465, acc-0.1900, valid loss-0.4475, acc-0.1600, test loss-0.4469, acc-0.1747\n",
      "Iter-200, train loss-0.4484, acc-0.1600, valid loss-0.4468, acc-0.1628, test loss-0.4462, acc-0.1779\n",
      "Iter-210, train loss-0.4438, acc-0.2000, valid loss-0.4462, acc-0.1656, test loss-0.4456, acc-0.1785\n",
      "Iter-220, train loss-0.4473, acc-0.1700, valid loss-0.4455, acc-0.1686, test loss-0.4449, acc-0.1803\n",
      "Iter-230, train loss-0.4468, acc-0.1100, valid loss-0.4446, acc-0.1728, test loss-0.4440, acc-0.1863\n",
      "Iter-240, train loss-0.4460, acc-0.1600, valid loss-0.4439, acc-0.1718, test loss-0.4432, acc-0.1885\n",
      "Iter-250, train loss-0.4407, acc-0.2400, valid loss-0.4428, acc-0.1766, test loss-0.4422, acc-0.1924\n",
      "Iter-260, train loss-0.4397, acc-0.2000, valid loss-0.4416, acc-0.1846, test loss-0.4410, acc-0.1997\n",
      "Iter-270, train loss-0.4376, acc-0.2800, valid loss-0.4408, acc-0.1888, test loss-0.4401, acc-0.2044\n",
      "Iter-280, train loss-0.4422, acc-0.1700, valid loss-0.4396, acc-0.1952, test loss-0.4389, acc-0.2091\n",
      "Iter-290, train loss-0.4353, acc-0.1600, valid loss-0.4387, acc-0.1982, test loss-0.4380, acc-0.2103\n",
      "Iter-300, train loss-0.4398, acc-0.1700, valid loss-0.4375, acc-0.1998, test loss-0.4367, acc-0.2151\n",
      "Iter-310, train loss-0.4354, acc-0.2300, valid loss-0.4364, acc-0.2062, test loss-0.4356, acc-0.2189\n",
      "Iter-320, train loss-0.4392, acc-0.2000, valid loss-0.4350, acc-0.2108, test loss-0.4343, acc-0.2214\n",
      "Iter-330, train loss-0.4344, acc-0.1700, valid loss-0.4337, acc-0.2148, test loss-0.4329, acc-0.2231\n",
      "Iter-340, train loss-0.4333, acc-0.2200, valid loss-0.4324, acc-0.2214, test loss-0.4316, acc-0.2280\n",
      "Iter-350, train loss-0.4281, acc-0.2700, valid loss-0.4311, acc-0.2286, test loss-0.4304, acc-0.2362\n",
      "Iter-360, train loss-0.4346, acc-0.2200, valid loss-0.4299, acc-0.2376, test loss-0.4291, acc-0.2446\n",
      "Iter-370, train loss-0.4355, acc-0.1500, valid loss-0.4286, acc-0.2484, test loss-0.4278, acc-0.2568\n",
      "Iter-380, train loss-0.4303, acc-0.2600, valid loss-0.4271, acc-0.2668, test loss-0.4264, acc-0.2763\n",
      "Iter-390, train loss-0.4273, acc-0.3000, valid loss-0.4257, acc-0.2864, test loss-0.4250, acc-0.2966\n",
      "Iter-400, train loss-0.4240, acc-0.2500, valid loss-0.4242, acc-0.3008, test loss-0.4234, acc-0.3093\n",
      "Iter-410, train loss-0.4225, acc-0.3000, valid loss-0.4226, acc-0.3104, test loss-0.4218, acc-0.3182\n",
      "Iter-420, train loss-0.4230, acc-0.3300, valid loss-0.4212, acc-0.3190, test loss-0.4204, acc-0.3267\n",
      "Iter-430, train loss-0.4174, acc-0.3700, valid loss-0.4196, acc-0.3272, test loss-0.4189, acc-0.3365\n",
      "Iter-440, train loss-0.4099, acc-0.4000, valid loss-0.4180, acc-0.3306, test loss-0.4173, acc-0.3428\n",
      "Iter-450, train loss-0.4127, acc-0.3700, valid loss-0.4164, acc-0.3324, test loss-0.4157, acc-0.3473\n",
      "Iter-460, train loss-0.4143, acc-0.3500, valid loss-0.4146, acc-0.3408, test loss-0.4139, acc-0.3522\n",
      "Iter-470, train loss-0.4111, acc-0.4500, valid loss-0.4128, acc-0.3474, test loss-0.4122, acc-0.3568\n",
      "Iter-480, train loss-0.4082, acc-0.3900, valid loss-0.4112, acc-0.3584, test loss-0.4105, acc-0.3668\n",
      "Iter-490, train loss-0.4147, acc-0.2700, valid loss-0.4097, acc-0.3662, test loss-0.4091, acc-0.3755\n",
      "Iter-500, train loss-0.4117, acc-0.3100, valid loss-0.4079, acc-0.3728, test loss-0.4073, acc-0.3823\n",
      "Iter-510, train loss-0.4138, acc-0.3600, valid loss-0.4062, acc-0.3800, test loss-0.4056, acc-0.3838\n",
      "Iter-520, train loss-0.4026, acc-0.4500, valid loss-0.4045, acc-0.3836, test loss-0.4039, acc-0.3901\n",
      "Iter-530, train loss-0.3978, acc-0.4400, valid loss-0.4031, acc-0.3954, test loss-0.4024, acc-0.3967\n",
      "Iter-540, train loss-0.4085, acc-0.3200, valid loss-0.4015, acc-0.4002, test loss-0.4008, acc-0.4002\n",
      "Iter-550, train loss-0.4108, acc-0.2900, valid loss-0.3996, acc-0.4000, test loss-0.3990, acc-0.4006\n",
      "Iter-560, train loss-0.4014, acc-0.3100, valid loss-0.3978, acc-0.4004, test loss-0.3972, acc-0.3995\n",
      "Iter-570, train loss-0.3973, acc-0.4400, valid loss-0.3960, acc-0.3972, test loss-0.3954, acc-0.3960\n",
      "Iter-580, train loss-0.4013, acc-0.3500, valid loss-0.3943, acc-0.3968, test loss-0.3937, acc-0.3957\n",
      "Iter-590, train loss-0.4028, acc-0.3700, valid loss-0.3926, acc-0.3960, test loss-0.3921, acc-0.3949\n",
      "Iter-600, train loss-0.3884, acc-0.3900, valid loss-0.3910, acc-0.3928, test loss-0.3905, acc-0.3912\n",
      "Iter-610, train loss-0.3886, acc-0.4400, valid loss-0.3891, acc-0.3930, test loss-0.3885, acc-0.3927\n",
      "Iter-620, train loss-0.3933, acc-0.3600, valid loss-0.3874, acc-0.3936, test loss-0.3869, acc-0.3932\n",
      "Iter-630, train loss-0.3753, acc-0.4600, valid loss-0.3856, acc-0.3906, test loss-0.3851, acc-0.3899\n",
      "Iter-640, train loss-0.3845, acc-0.4000, valid loss-0.3838, acc-0.3920, test loss-0.3834, acc-0.3905\n",
      "Iter-650, train loss-0.3799, acc-0.4300, valid loss-0.3821, acc-0.3942, test loss-0.3817, acc-0.3893\n",
      "Iter-660, train loss-0.3801, acc-0.4500, valid loss-0.3803, acc-0.4024, test loss-0.3800, acc-0.3937\n",
      "Iter-670, train loss-0.3831, acc-0.3700, valid loss-0.3785, acc-0.4052, test loss-0.3783, acc-0.3955\n",
      "Iter-680, train loss-0.3767, acc-0.4400, valid loss-0.3771, acc-0.4070, test loss-0.3769, acc-0.3967\n",
      "Iter-690, train loss-0.3749, acc-0.4100, valid loss-0.3755, acc-0.4094, test loss-0.3753, acc-0.4021\n",
      "Iter-700, train loss-0.3698, acc-0.4200, valid loss-0.3739, acc-0.4144, test loss-0.3737, acc-0.4066\n",
      "Iter-710, train loss-0.3742, acc-0.4000, valid loss-0.3721, acc-0.4198, test loss-0.3720, acc-0.4108\n",
      "Iter-720, train loss-0.3692, acc-0.4500, valid loss-0.3705, acc-0.4230, test loss-0.3704, acc-0.4129\n",
      "Iter-730, train loss-0.3657, acc-0.4200, valid loss-0.3689, acc-0.4292, test loss-0.3689, acc-0.4204\n",
      "Iter-740, train loss-0.3665, acc-0.4000, valid loss-0.3673, acc-0.4330, test loss-0.3673, acc-0.4229\n",
      "Iter-750, train loss-0.3709, acc-0.4500, valid loss-0.3659, acc-0.4354, test loss-0.3659, acc-0.4237\n",
      "Iter-760, train loss-0.3722, acc-0.4000, valid loss-0.3640, acc-0.4458, test loss-0.3642, acc-0.4301\n",
      "Iter-770, train loss-0.3677, acc-0.4500, valid loss-0.3627, acc-0.4478, test loss-0.3628, acc-0.4321\n",
      "Iter-780, train loss-0.3757, acc-0.3500, valid loss-0.3608, acc-0.4556, test loss-0.3610, acc-0.4379\n",
      "Iter-790, train loss-0.3478, acc-0.5200, valid loss-0.3593, acc-0.4622, test loss-0.3595, acc-0.4436\n",
      "Iter-800, train loss-0.3505, acc-0.5100, valid loss-0.3580, acc-0.4624, test loss-0.3583, acc-0.4438\n",
      "Iter-810, train loss-0.3719, acc-0.4000, valid loss-0.3562, acc-0.4670, test loss-0.3564, acc-0.4471\n",
      "Iter-820, train loss-0.3579, acc-0.4500, valid loss-0.3549, acc-0.4726, test loss-0.3551, acc-0.4525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-0.3725, acc-0.3300, valid loss-0.3531, acc-0.4782, test loss-0.3534, acc-0.4602\n",
      "Iter-840, train loss-0.3511, acc-0.4900, valid loss-0.3517, acc-0.4862, test loss-0.3521, acc-0.4695\n",
      "Iter-850, train loss-0.3389, acc-0.5300, valid loss-0.3501, acc-0.4896, test loss-0.3505, acc-0.4773\n",
      "Iter-860, train loss-0.3480, acc-0.4400, valid loss-0.3488, acc-0.4994, test loss-0.3492, acc-0.4869\n",
      "Iter-870, train loss-0.3678, acc-0.4100, valid loss-0.3471, acc-0.5028, test loss-0.3476, acc-0.4907\n",
      "Iter-880, train loss-0.3501, acc-0.5500, valid loss-0.3454, acc-0.5048, test loss-0.3460, acc-0.4932\n",
      "Iter-890, train loss-0.3595, acc-0.4800, valid loss-0.3439, acc-0.5070, test loss-0.3444, acc-0.4933\n",
      "Iter-900, train loss-0.3344, acc-0.5500, valid loss-0.3425, acc-0.5084, test loss-0.3431, acc-0.4988\n",
      "Iter-910, train loss-0.3642, acc-0.4300, valid loss-0.3409, acc-0.5150, test loss-0.3415, acc-0.5061\n",
      "Iter-920, train loss-0.3349, acc-0.5300, valid loss-0.3393, acc-0.5188, test loss-0.3399, acc-0.5094\n",
      "Iter-930, train loss-0.3588, acc-0.4200, valid loss-0.3378, acc-0.5202, test loss-0.3385, acc-0.5139\n",
      "Iter-940, train loss-0.3382, acc-0.5500, valid loss-0.3360, acc-0.5234, test loss-0.3368, acc-0.5179\n",
      "Iter-950, train loss-0.3445, acc-0.5100, valid loss-0.3345, acc-0.5266, test loss-0.3353, acc-0.5262\n",
      "Iter-960, train loss-0.3546, acc-0.4100, valid loss-0.3328, acc-0.5300, test loss-0.3336, acc-0.5290\n",
      "Iter-970, train loss-0.3226, acc-0.5600, valid loss-0.3312, acc-0.5328, test loss-0.3321, acc-0.5318\n",
      "Iter-980, train loss-0.3327, acc-0.5500, valid loss-0.3296, acc-0.5334, test loss-0.3305, acc-0.5310\n",
      "Iter-990, train loss-0.3218, acc-0.6200, valid loss-0.3280, acc-0.5376, test loss-0.3288, acc-0.5356\n",
      "Iter-1000, train loss-0.3228, acc-0.5900, valid loss-0.3263, acc-0.5416, test loss-0.3271, acc-0.5381\n",
      "Iter-1010, train loss-0.3316, acc-0.5400, valid loss-0.3247, acc-0.5446, test loss-0.3255, acc-0.5425\n",
      "Iter-1020, train loss-0.3128, acc-0.5600, valid loss-0.3233, acc-0.5458, test loss-0.3241, acc-0.5454\n",
      "Iter-1030, train loss-0.3176, acc-0.5600, valid loss-0.3217, acc-0.5506, test loss-0.3224, acc-0.5499\n",
      "Iter-1040, train loss-0.3343, acc-0.5100, valid loss-0.3201, acc-0.5542, test loss-0.3208, acc-0.5533\n",
      "Iter-1050, train loss-0.3183, acc-0.5700, valid loss-0.3189, acc-0.5532, test loss-0.3196, acc-0.5525\n",
      "Iter-1060, train loss-0.3114, acc-0.5500, valid loss-0.3173, acc-0.5554, test loss-0.3180, acc-0.5578\n",
      "Iter-1070, train loss-0.3232, acc-0.5000, valid loss-0.3162, acc-0.5566, test loss-0.3168, acc-0.5595\n",
      "Iter-1080, train loss-0.3156, acc-0.5700, valid loss-0.3149, acc-0.5582, test loss-0.3156, acc-0.5596\n",
      "Iter-1090, train loss-0.3201, acc-0.5500, valid loss-0.3134, acc-0.5586, test loss-0.3142, acc-0.5625\n",
      "Iter-1100, train loss-0.3223, acc-0.5500, valid loss-0.3120, acc-0.5616, test loss-0.3127, acc-0.5620\n",
      "Iter-1110, train loss-0.3138, acc-0.5400, valid loss-0.3105, acc-0.5646, test loss-0.3111, acc-0.5660\n",
      "Iter-1120, train loss-0.2741, acc-0.7100, valid loss-0.3090, acc-0.5666, test loss-0.3096, acc-0.5675\n",
      "Iter-1130, train loss-0.3051, acc-0.5700, valid loss-0.3076, acc-0.5674, test loss-0.3081, acc-0.5693\n",
      "Iter-1140, train loss-0.3026, acc-0.6000, valid loss-0.3062, acc-0.5696, test loss-0.3066, acc-0.5727\n",
      "Iter-1150, train loss-0.3152, acc-0.5000, valid loss-0.3046, acc-0.5718, test loss-0.3050, acc-0.5757\n",
      "Iter-1160, train loss-0.2929, acc-0.6200, valid loss-0.3034, acc-0.5736, test loss-0.3038, acc-0.5783\n",
      "Iter-1170, train loss-0.3055, acc-0.5800, valid loss-0.3020, acc-0.5740, test loss-0.3024, acc-0.5793\n",
      "Iter-1180, train loss-0.2949, acc-0.5900, valid loss-0.3003, acc-0.5784, test loss-0.3007, acc-0.5837\n",
      "Iter-1190, train loss-0.3091, acc-0.5600, valid loss-0.2990, acc-0.5796, test loss-0.2994, acc-0.5850\n",
      "Iter-1200, train loss-0.2935, acc-0.5700, valid loss-0.2977, acc-0.5800, test loss-0.2980, acc-0.5852\n",
      "Iter-1210, train loss-0.2968, acc-0.5800, valid loss-0.2964, acc-0.5810, test loss-0.2967, acc-0.5871\n",
      "Iter-1220, train loss-0.2829, acc-0.6300, valid loss-0.2952, acc-0.5836, test loss-0.2956, acc-0.5894\n",
      "Iter-1230, train loss-0.2961, acc-0.5500, valid loss-0.2941, acc-0.5838, test loss-0.2943, acc-0.5902\n",
      "Iter-1240, train loss-0.2988, acc-0.5600, valid loss-0.2929, acc-0.5848, test loss-0.2931, acc-0.5903\n",
      "Iter-1250, train loss-0.3064, acc-0.5600, valid loss-0.2917, acc-0.5866, test loss-0.2919, acc-0.5926\n",
      "Iter-1260, train loss-0.3028, acc-0.6000, valid loss-0.2906, acc-0.5898, test loss-0.2907, acc-0.5930\n",
      "Iter-1270, train loss-0.2854, acc-0.5800, valid loss-0.2895, acc-0.5914, test loss-0.2896, acc-0.5953\n",
      "Iter-1280, train loss-0.2781, acc-0.6500, valid loss-0.2883, acc-0.5916, test loss-0.2884, acc-0.5979\n",
      "Iter-1290, train loss-0.2846, acc-0.5700, valid loss-0.2872, acc-0.5926, test loss-0.2871, acc-0.6003\n",
      "Iter-1300, train loss-0.2992, acc-0.5400, valid loss-0.2859, acc-0.5944, test loss-0.2858, acc-0.6006\n",
      "Iter-1310, train loss-0.2776, acc-0.5800, valid loss-0.2848, acc-0.5958, test loss-0.2847, acc-0.6017\n",
      "Iter-1320, train loss-0.2771, acc-0.6400, valid loss-0.2835, acc-0.5966, test loss-0.2834, acc-0.6020\n",
      "Iter-1330, train loss-0.2830, acc-0.6000, valid loss-0.2826, acc-0.5972, test loss-0.2824, acc-0.6016\n",
      "Iter-1340, train loss-0.2614, acc-0.6500, valid loss-0.2814, acc-0.6016, test loss-0.2811, acc-0.6055\n",
      "Iter-1350, train loss-0.2947, acc-0.5600, valid loss-0.2799, acc-0.6060, test loss-0.2796, acc-0.6082\n",
      "Iter-1360, train loss-0.2867, acc-0.5700, valid loss-0.2788, acc-0.6048, test loss-0.2785, acc-0.6098\n",
      "Iter-1370, train loss-0.2765, acc-0.6000, valid loss-0.2774, acc-0.6072, test loss-0.2772, acc-0.6110\n",
      "Iter-1380, train loss-0.2785, acc-0.5800, valid loss-0.2765, acc-0.6076, test loss-0.2761, acc-0.6143\n",
      "Iter-1390, train loss-0.2985, acc-0.5500, valid loss-0.2757, acc-0.6102, test loss-0.2753, acc-0.6155\n",
      "Iter-1400, train loss-0.3167, acc-0.4500, valid loss-0.2742, acc-0.6108, test loss-0.2739, acc-0.6173\n",
      "Iter-1410, train loss-0.2978, acc-0.5900, valid loss-0.2734, acc-0.6118, test loss-0.2729, acc-0.6172\n",
      "Iter-1420, train loss-0.2392, acc-0.6600, valid loss-0.2722, acc-0.6152, test loss-0.2717, acc-0.6193\n",
      "Iter-1430, train loss-0.2586, acc-0.6300, valid loss-0.2713, acc-0.6166, test loss-0.2708, acc-0.6221\n",
      "Iter-1440, train loss-0.2445, acc-0.6900, valid loss-0.2699, acc-0.6180, test loss-0.2694, acc-0.6245\n",
      "Iter-1450, train loss-0.2605, acc-0.6300, valid loss-0.2686, acc-0.6206, test loss-0.2681, acc-0.6263\n",
      "Iter-1460, train loss-0.2675, acc-0.5900, valid loss-0.2675, acc-0.6230, test loss-0.2670, acc-0.6260\n",
      "Iter-1470, train loss-0.2526, acc-0.6300, valid loss-0.2668, acc-0.6234, test loss-0.2662, acc-0.6262\n",
      "Iter-1480, train loss-0.2779, acc-0.6000, valid loss-0.2657, acc-0.6248, test loss-0.2650, acc-0.6293\n",
      "Iter-1490, train loss-0.2877, acc-0.6100, valid loss-0.2641, acc-0.6288, test loss-0.2635, acc-0.6314\n",
      "Iter-1500, train loss-0.2650, acc-0.5800, valid loss-0.2630, acc-0.6310, test loss-0.2623, acc-0.6342\n",
      "Iter-1510, train loss-0.2734, acc-0.5900, valid loss-0.2618, acc-0.6316, test loss-0.2611, acc-0.6370\n",
      "Iter-1520, train loss-0.2516, acc-0.6500, valid loss-0.2604, acc-0.6354, test loss-0.2596, acc-0.6402\n",
      "Iter-1530, train loss-0.2620, acc-0.6800, valid loss-0.2593, acc-0.6374, test loss-0.2585, acc-0.6421\n",
      "Iter-1540, train loss-0.2576, acc-0.6500, valid loss-0.2581, acc-0.6388, test loss-0.2574, acc-0.6447\n",
      "Iter-1550, train loss-0.2506, acc-0.6700, valid loss-0.2571, acc-0.6400, test loss-0.2563, acc-0.6460\n",
      "Iter-1560, train loss-0.2383, acc-0.7100, valid loss-0.2561, acc-0.6414, test loss-0.2553, acc-0.6477\n",
      "Iter-1570, train loss-0.2511, acc-0.6600, valid loss-0.2549, acc-0.6428, test loss-0.2540, acc-0.6480\n",
      "Iter-1580, train loss-0.2346, acc-0.7000, valid loss-0.2540, acc-0.6440, test loss-0.2532, acc-0.6503\n",
      "Iter-1590, train loss-0.2595, acc-0.6400, valid loss-0.2528, acc-0.6464, test loss-0.2520, acc-0.6512\n",
      "Iter-1600, train loss-0.2278, acc-0.7200, valid loss-0.2517, acc-0.6462, test loss-0.2506, acc-0.6521\n",
      "Iter-1610, train loss-0.2655, acc-0.6100, valid loss-0.2511, acc-0.6482, test loss-0.2499, acc-0.6534\n",
      "Iter-1620, train loss-0.2310, acc-0.6500, valid loss-0.2501, acc-0.6504, test loss-0.2487, acc-0.6555\n",
      "Iter-1630, train loss-0.2505, acc-0.6500, valid loss-0.2491, acc-0.6516, test loss-0.2476, acc-0.6575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-0.2666, acc-0.6100, valid loss-0.2486, acc-0.6516, test loss-0.2469, acc-0.6583\n",
      "Iter-1650, train loss-0.2683, acc-0.6700, valid loss-0.2477, acc-0.6532, test loss-0.2460, acc-0.6626\n",
      "Iter-1660, train loss-0.2467, acc-0.6600, valid loss-0.2468, acc-0.6554, test loss-0.2452, acc-0.6639\n",
      "Iter-1670, train loss-0.2416, acc-0.6500, valid loss-0.2460, acc-0.6562, test loss-0.2444, acc-0.6668\n",
      "Iter-1680, train loss-0.2767, acc-0.5900, valid loss-0.2450, acc-0.6590, test loss-0.2435, acc-0.6698\n",
      "Iter-1690, train loss-0.2572, acc-0.6000, valid loss-0.2439, acc-0.6616, test loss-0.2422, acc-0.6717\n",
      "Iter-1700, train loss-0.2312, acc-0.6700, valid loss-0.2428, acc-0.6648, test loss-0.2412, acc-0.6721\n",
      "Iter-1710, train loss-0.2221, acc-0.6800, valid loss-0.2416, acc-0.6658, test loss-0.2400, acc-0.6746\n",
      "Iter-1720, train loss-0.2419, acc-0.6700, valid loss-0.2405, acc-0.6684, test loss-0.2388, acc-0.6770\n",
      "Iter-1730, train loss-0.2659, acc-0.6300, valid loss-0.2392, acc-0.6696, test loss-0.2375, acc-0.6811\n",
      "Iter-1740, train loss-0.2359, acc-0.6700, valid loss-0.2382, acc-0.6710, test loss-0.2365, acc-0.6824\n",
      "Iter-1750, train loss-0.2504, acc-0.6000, valid loss-0.2374, acc-0.6726, test loss-0.2356, acc-0.6828\n",
      "Iter-1760, train loss-0.2222, acc-0.7000, valid loss-0.2368, acc-0.6732, test loss-0.2349, acc-0.6833\n",
      "Iter-1770, train loss-0.2310, acc-0.7100, valid loss-0.2361, acc-0.6732, test loss-0.2343, acc-0.6839\n",
      "Iter-1780, train loss-0.2312, acc-0.6600, valid loss-0.2353, acc-0.6764, test loss-0.2334, acc-0.6876\n",
      "Iter-1790, train loss-0.2139, acc-0.7000, valid loss-0.2342, acc-0.6764, test loss-0.2323, acc-0.6889\n",
      "Iter-1800, train loss-0.2425, acc-0.6800, valid loss-0.2331, acc-0.6768, test loss-0.2313, acc-0.6905\n",
      "Iter-1810, train loss-0.2091, acc-0.6800, valid loss-0.2324, acc-0.6786, test loss-0.2304, acc-0.6930\n",
      "Iter-1820, train loss-0.1954, acc-0.7600, valid loss-0.2316, acc-0.6782, test loss-0.2295, acc-0.6937\n",
      "Iter-1830, train loss-0.2105, acc-0.7000, valid loss-0.2308, acc-0.6808, test loss-0.2288, acc-0.6945\n",
      "Iter-1840, train loss-0.1744, acc-0.7900, valid loss-0.2301, acc-0.6826, test loss-0.2280, acc-0.6959\n",
      "Iter-1850, train loss-0.2409, acc-0.7000, valid loss-0.2295, acc-0.6830, test loss-0.2273, acc-0.6962\n",
      "Iter-1860, train loss-0.2135, acc-0.7300, valid loss-0.2290, acc-0.6836, test loss-0.2269, acc-0.6967\n",
      "Iter-1870, train loss-0.2587, acc-0.5800, valid loss-0.2282, acc-0.6856, test loss-0.2261, acc-0.6983\n",
      "Iter-1880, train loss-0.2502, acc-0.6600, valid loss-0.2273, acc-0.6870, test loss-0.2253, acc-0.7004\n",
      "Iter-1890, train loss-0.2185, acc-0.7100, valid loss-0.2268, acc-0.6874, test loss-0.2247, acc-0.7006\n",
      "Iter-1900, train loss-0.2136, acc-0.7200, valid loss-0.2258, acc-0.6896, test loss-0.2236, acc-0.7026\n",
      "Iter-1910, train loss-0.2201, acc-0.6800, valid loss-0.2251, acc-0.6902, test loss-0.2230, acc-0.7053\n",
      "Iter-1920, train loss-0.2344, acc-0.6400, valid loss-0.2246, acc-0.6922, test loss-0.2225, acc-0.7066\n",
      "Iter-1930, train loss-0.2057, acc-0.7200, valid loss-0.2239, acc-0.6944, test loss-0.2217, acc-0.7078\n",
      "Iter-1940, train loss-0.2321, acc-0.6900, valid loss-0.2235, acc-0.6980, test loss-0.2213, acc-0.7094\n",
      "Iter-1950, train loss-0.2207, acc-0.7100, valid loss-0.2226, acc-0.6990, test loss-0.2205, acc-0.7111\n",
      "Iter-1960, train loss-0.1980, acc-0.7500, valid loss-0.2221, acc-0.7000, test loss-0.2199, acc-0.7121\n",
      "Iter-1970, train loss-0.2123, acc-0.7400, valid loss-0.2213, acc-0.7004, test loss-0.2191, acc-0.7129\n",
      "Iter-1980, train loss-0.2143, acc-0.7300, valid loss-0.2211, acc-0.7012, test loss-0.2188, acc-0.7126\n",
      "Iter-1990, train loss-0.1963, acc-0.7200, valid loss-0.2205, acc-0.7024, test loss-0.2182, acc-0.7139\n",
      "Iter-2000, train loss-0.2307, acc-0.6700, valid loss-0.2202, acc-0.7034, test loss-0.2179, acc-0.7140\n",
      "Iter-2010, train loss-0.2228, acc-0.6700, valid loss-0.2197, acc-0.7038, test loss-0.2174, acc-0.7144\n",
      "Iter-2020, train loss-0.2356, acc-0.6600, valid loss-0.2190, acc-0.7066, test loss-0.2168, acc-0.7153\n",
      "Iter-2030, train loss-0.2521, acc-0.6200, valid loss-0.2184, acc-0.7088, test loss-0.2162, acc-0.7155\n",
      "Iter-2040, train loss-0.2275, acc-0.6600, valid loss-0.2180, acc-0.7076, test loss-0.2157, acc-0.7165\n",
      "Iter-2050, train loss-0.1954, acc-0.7800, valid loss-0.2179, acc-0.7086, test loss-0.2155, acc-0.7170\n",
      "Iter-2060, train loss-0.2029, acc-0.7100, valid loss-0.2171, acc-0.7090, test loss-0.2148, acc-0.7182\n",
      "Iter-2070, train loss-0.2188, acc-0.7400, valid loss-0.2166, acc-0.7092, test loss-0.2143, acc-0.7197\n",
      "Iter-2080, train loss-0.2127, acc-0.7000, valid loss-0.2162, acc-0.7110, test loss-0.2140, acc-0.7201\n",
      "Iter-2090, train loss-0.2107, acc-0.7700, valid loss-0.2156, acc-0.7142, test loss-0.2135, acc-0.7199\n",
      "Iter-2100, train loss-0.2287, acc-0.6700, valid loss-0.2152, acc-0.7132, test loss-0.2131, acc-0.7192\n",
      "Iter-2110, train loss-0.2383, acc-0.6500, valid loss-0.2149, acc-0.7122, test loss-0.2128, acc-0.7196\n",
      "Iter-2120, train loss-0.2071, acc-0.6900, valid loss-0.2146, acc-0.7132, test loss-0.2124, acc-0.7191\n",
      "Iter-2130, train loss-0.2315, acc-0.6900, valid loss-0.2143, acc-0.7130, test loss-0.2121, acc-0.7202\n",
      "Iter-2140, train loss-0.2276, acc-0.6700, valid loss-0.2139, acc-0.7140, test loss-0.2119, acc-0.7192\n",
      "Iter-2150, train loss-0.2690, acc-0.5300, valid loss-0.2136, acc-0.7124, test loss-0.2114, acc-0.7192\n",
      "Iter-2160, train loss-0.2427, acc-0.7000, valid loss-0.2132, acc-0.7114, test loss-0.2110, acc-0.7200\n",
      "Iter-2170, train loss-0.2192, acc-0.6700, valid loss-0.2127, acc-0.7122, test loss-0.2105, acc-0.7197\n",
      "Iter-2180, train loss-0.2144, acc-0.7400, valid loss-0.2122, acc-0.7126, test loss-0.2100, acc-0.7202\n",
      "Iter-2190, train loss-0.1869, acc-0.7500, valid loss-0.2119, acc-0.7130, test loss-0.2098, acc-0.7191\n",
      "Iter-2200, train loss-0.2157, acc-0.6900, valid loss-0.2115, acc-0.7142, test loss-0.2093, acc-0.7188\n",
      "Iter-2210, train loss-0.1641, acc-0.8000, valid loss-0.2111, acc-0.7140, test loss-0.2089, acc-0.7194\n",
      "Iter-2220, train loss-0.1866, acc-0.7500, valid loss-0.2109, acc-0.7146, test loss-0.2087, acc-0.7212\n",
      "Iter-2230, train loss-0.2419, acc-0.6700, valid loss-0.2104, acc-0.7146, test loss-0.2083, acc-0.7230\n",
      "Iter-2240, train loss-0.1806, acc-0.7600, valid loss-0.2104, acc-0.7132, test loss-0.2082, acc-0.7222\n",
      "Iter-2250, train loss-0.2184, acc-0.6900, valid loss-0.2100, acc-0.7158, test loss-0.2079, acc-0.7222\n",
      "Iter-2260, train loss-0.2053, acc-0.7400, valid loss-0.2095, acc-0.7166, test loss-0.2074, acc-0.7236\n",
      "Iter-2270, train loss-0.1971, acc-0.7400, valid loss-0.2092, acc-0.7162, test loss-0.2070, acc-0.7240\n",
      "Iter-2280, train loss-0.1946, acc-0.7400, valid loss-0.2089, acc-0.7182, test loss-0.2068, acc-0.7246\n",
      "Iter-2290, train loss-0.2229, acc-0.6900, valid loss-0.2090, acc-0.7174, test loss-0.2067, acc-0.7241\n",
      "Iter-2300, train loss-0.1840, acc-0.7900, valid loss-0.2084, acc-0.7182, test loss-0.2062, acc-0.7259\n",
      "Iter-2310, train loss-0.2126, acc-0.7200, valid loss-0.2082, acc-0.7194, test loss-0.2060, acc-0.7260\n",
      "Iter-2320, train loss-0.2122, acc-0.7300, valid loss-0.2076, acc-0.7208, test loss-0.2055, acc-0.7262\n",
      "Iter-2330, train loss-0.2064, acc-0.7600, valid loss-0.2073, acc-0.7198, test loss-0.2052, acc-0.7254\n",
      "Iter-2340, train loss-0.1999, acc-0.7400, valid loss-0.2069, acc-0.7210, test loss-0.2047, acc-0.7265\n",
      "Iter-2350, train loss-0.2030, acc-0.7500, valid loss-0.2065, acc-0.7216, test loss-0.2043, acc-0.7268\n",
      "Iter-2360, train loss-0.1946, acc-0.7200, valid loss-0.2064, acc-0.7214, test loss-0.2042, acc-0.7267\n",
      "Iter-2370, train loss-0.2177, acc-0.7000, valid loss-0.2061, acc-0.7212, test loss-0.2039, acc-0.7276\n",
      "Iter-2380, train loss-0.2098, acc-0.6700, valid loss-0.2059, acc-0.7236, test loss-0.2036, acc-0.7284\n",
      "Iter-2390, train loss-0.1699, acc-0.7800, valid loss-0.2058, acc-0.7234, test loss-0.2036, acc-0.7284\n",
      "Iter-2400, train loss-0.2307, acc-0.6700, valid loss-0.2054, acc-0.7246, test loss-0.2032, acc-0.7277\n",
      "Iter-2410, train loss-0.1746, acc-0.7700, valid loss-0.2053, acc-0.7238, test loss-0.2030, acc-0.7281\n",
      "Iter-2420, train loss-0.2013, acc-0.7100, valid loss-0.2049, acc-0.7230, test loss-0.2027, acc-0.7288\n",
      "Iter-2430, train loss-0.2077, acc-0.7300, valid loss-0.2049, acc-0.7234, test loss-0.2026, acc-0.7285\n",
      "Iter-2440, train loss-0.2225, acc-0.6900, valid loss-0.2047, acc-0.7228, test loss-0.2025, acc-0.7280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-0.2212, acc-0.7300, valid loss-0.2044, acc-0.7234, test loss-0.2023, acc-0.7282\n",
      "Iter-2460, train loss-0.2193, acc-0.7400, valid loss-0.2042, acc-0.7246, test loss-0.2022, acc-0.7292\n",
      "Iter-2470, train loss-0.1657, acc-0.8300, valid loss-0.2039, acc-0.7252, test loss-0.2020, acc-0.7291\n",
      "Iter-2480, train loss-0.2295, acc-0.6700, valid loss-0.2040, acc-0.7230, test loss-0.2021, acc-0.7271\n",
      "Iter-2490, train loss-0.2094, acc-0.7100, valid loss-0.2036, acc-0.7242, test loss-0.2017, acc-0.7272\n",
      "Iter-2500, train loss-0.1915, acc-0.7500, valid loss-0.2031, acc-0.7250, test loss-0.2014, acc-0.7279\n",
      "Iter-2510, train loss-0.1904, acc-0.7100, valid loss-0.2027, acc-0.7260, test loss-0.2011, acc-0.7288\n",
      "Iter-2520, train loss-0.2504, acc-0.6600, valid loss-0.2025, acc-0.7268, test loss-0.2010, acc-0.7291\n",
      "Iter-2530, train loss-0.2162, acc-0.7000, valid loss-0.2020, acc-0.7280, test loss-0.2005, acc-0.7302\n",
      "Iter-2540, train loss-0.1942, acc-0.7400, valid loss-0.2017, acc-0.7284, test loss-0.2002, acc-0.7306\n",
      "Iter-2550, train loss-0.2174, acc-0.7100, valid loss-0.2013, acc-0.7306, test loss-0.1999, acc-0.7306\n",
      "Iter-2560, train loss-0.2041, acc-0.6900, valid loss-0.2011, acc-0.7306, test loss-0.1997, acc-0.7306\n",
      "Iter-2570, train loss-0.2354, acc-0.6400, valid loss-0.2009, acc-0.7290, test loss-0.1993, acc-0.7316\n",
      "Iter-2580, train loss-0.2102, acc-0.7000, valid loss-0.2006, acc-0.7308, test loss-0.1991, acc-0.7328\n",
      "Iter-2590, train loss-0.1955, acc-0.7100, valid loss-0.2004, acc-0.7292, test loss-0.1987, acc-0.7344\n",
      "Iter-2600, train loss-0.1921, acc-0.7000, valid loss-0.2003, acc-0.7314, test loss-0.1985, acc-0.7338\n",
      "Iter-2610, train loss-0.2146, acc-0.7100, valid loss-0.2000, acc-0.7320, test loss-0.1983, acc-0.7356\n",
      "Iter-2620, train loss-0.2193, acc-0.6800, valid loss-0.1998, acc-0.7322, test loss-0.1980, acc-0.7355\n",
      "Iter-2630, train loss-0.2049, acc-0.7600, valid loss-0.1996, acc-0.7314, test loss-0.1979, acc-0.7358\n",
      "Iter-2640, train loss-0.1879, acc-0.7700, valid loss-0.1993, acc-0.7324, test loss-0.1977, acc-0.7364\n",
      "Iter-2650, train loss-0.1856, acc-0.7500, valid loss-0.1990, acc-0.7324, test loss-0.1974, acc-0.7368\n",
      "Iter-2660, train loss-0.2084, acc-0.6800, valid loss-0.1990, acc-0.7328, test loss-0.1973, acc-0.7372\n",
      "Iter-2670, train loss-0.2229, acc-0.6700, valid loss-0.1989, acc-0.7316, test loss-0.1973, acc-0.7376\n",
      "Iter-2680, train loss-0.2011, acc-0.7600, valid loss-0.1986, acc-0.7326, test loss-0.1969, acc-0.7383\n",
      "Iter-2690, train loss-0.1596, acc-0.8300, valid loss-0.1987, acc-0.7316, test loss-0.1969, acc-0.7367\n",
      "Iter-2700, train loss-0.1822, acc-0.8000, valid loss-0.1988, acc-0.7308, test loss-0.1970, acc-0.7362\n",
      "Iter-2710, train loss-0.1661, acc-0.8000, valid loss-0.1985, acc-0.7308, test loss-0.1967, acc-0.7380\n",
      "Iter-2720, train loss-0.2375, acc-0.6500, valid loss-0.1982, acc-0.7310, test loss-0.1964, acc-0.7384\n",
      "Iter-2730, train loss-0.1797, acc-0.7500, valid loss-0.1980, acc-0.7298, test loss-0.1962, acc-0.7392\n",
      "Iter-2740, train loss-0.2276, acc-0.6800, valid loss-0.1977, acc-0.7310, test loss-0.1960, acc-0.7391\n",
      "Iter-2750, train loss-0.1920, acc-0.7100, valid loss-0.1974, acc-0.7328, test loss-0.1959, acc-0.7389\n",
      "Iter-2760, train loss-0.2252, acc-0.6700, valid loss-0.1970, acc-0.7326, test loss-0.1955, acc-0.7400\n",
      "Iter-2770, train loss-0.1955, acc-0.7400, valid loss-0.1967, acc-0.7358, test loss-0.1954, acc-0.7390\n",
      "Iter-2780, train loss-0.2123, acc-0.7100, valid loss-0.1966, acc-0.7354, test loss-0.1952, acc-0.7397\n",
      "Iter-2790, train loss-0.1630, acc-0.7800, valid loss-0.1966, acc-0.7362, test loss-0.1953, acc-0.7395\n",
      "Iter-2800, train loss-0.1989, acc-0.7300, valid loss-0.1964, acc-0.7370, test loss-0.1951, acc-0.7400\n",
      "Iter-2810, train loss-0.1999, acc-0.7300, valid loss-0.1962, acc-0.7368, test loss-0.1950, acc-0.7408\n",
      "Iter-2820, train loss-0.1837, acc-0.7400, valid loss-0.1960, acc-0.7382, test loss-0.1949, acc-0.7399\n",
      "Iter-2830, train loss-0.1662, acc-0.8200, valid loss-0.1957, acc-0.7388, test loss-0.1946, acc-0.7412\n",
      "Iter-2840, train loss-0.1950, acc-0.7300, valid loss-0.1956, acc-0.7376, test loss-0.1945, acc-0.7413\n",
      "Iter-2850, train loss-0.2047, acc-0.7100, valid loss-0.1953, acc-0.7360, test loss-0.1941, acc-0.7415\n",
      "Iter-2860, train loss-0.2035, acc-0.7300, valid loss-0.1953, acc-0.7390, test loss-0.1942, acc-0.7418\n",
      "Iter-2870, train loss-0.2279, acc-0.7000, valid loss-0.1950, acc-0.7388, test loss-0.1941, acc-0.7412\n",
      "Iter-2880, train loss-0.2158, acc-0.7000, valid loss-0.1950, acc-0.7392, test loss-0.1940, acc-0.7430\n",
      "Iter-2890, train loss-0.1923, acc-0.7500, valid loss-0.1946, acc-0.7420, test loss-0.1938, acc-0.7430\n",
      "Iter-2900, train loss-0.2194, acc-0.6700, valid loss-0.1944, acc-0.7414, test loss-0.1937, acc-0.7430\n",
      "Iter-2910, train loss-0.2030, acc-0.7400, valid loss-0.1943, acc-0.7414, test loss-0.1937, acc-0.7431\n",
      "Iter-2920, train loss-0.1813, acc-0.7800, valid loss-0.1942, acc-0.7416, test loss-0.1935, acc-0.7437\n",
      "Iter-2930, train loss-0.2156, acc-0.6900, valid loss-0.1939, acc-0.7424, test loss-0.1933, acc-0.7433\n",
      "Iter-2940, train loss-0.2024, acc-0.7200, valid loss-0.1940, acc-0.7420, test loss-0.1934, acc-0.7429\n",
      "Iter-2950, train loss-0.2380, acc-0.6500, valid loss-0.1939, acc-0.7418, test loss-0.1932, acc-0.7440\n",
      "Iter-2960, train loss-0.1960, acc-0.7500, valid loss-0.1938, acc-0.7426, test loss-0.1931, acc-0.7443\n",
      "Iter-2970, train loss-0.1892, acc-0.7500, valid loss-0.1934, acc-0.7434, test loss-0.1927, acc-0.7446\n",
      "Iter-2980, train loss-0.1566, acc-0.8000, valid loss-0.1934, acc-0.7418, test loss-0.1926, acc-0.7437\n",
      "Iter-2990, train loss-0.1968, acc-0.7000, valid loss-0.1933, acc-0.7428, test loss-0.1925, acc-0.7438\n",
      "Iter-3000, train loss-0.1707, acc-0.7900, valid loss-0.1932, acc-0.7414, test loss-0.1924, acc-0.7444\n",
      "Iter-3010, train loss-0.1910, acc-0.7400, valid loss-0.1930, acc-0.7422, test loss-0.1922, acc-0.7451\n",
      "Iter-3020, train loss-0.2205, acc-0.7000, valid loss-0.1929, acc-0.7436, test loss-0.1921, acc-0.7446\n",
      "Iter-3030, train loss-0.1939, acc-0.7400, valid loss-0.1925, acc-0.7444, test loss-0.1918, acc-0.7452\n",
      "Iter-3040, train loss-0.2025, acc-0.7400, valid loss-0.1923, acc-0.7448, test loss-0.1917, acc-0.7455\n",
      "Iter-3050, train loss-0.1953, acc-0.7100, valid loss-0.1920, acc-0.7456, test loss-0.1915, acc-0.7463\n",
      "Iter-3060, train loss-0.2222, acc-0.7000, valid loss-0.1919, acc-0.7464, test loss-0.1914, acc-0.7467\n",
      "Iter-3070, train loss-0.2131, acc-0.7400, valid loss-0.1920, acc-0.7446, test loss-0.1915, acc-0.7468\n",
      "Iter-3080, train loss-0.2093, acc-0.7200, valid loss-0.1917, acc-0.7468, test loss-0.1914, acc-0.7469\n",
      "Iter-3090, train loss-0.2101, acc-0.6700, valid loss-0.1916, acc-0.7472, test loss-0.1913, acc-0.7474\n",
      "Iter-3100, train loss-0.1974, acc-0.7200, valid loss-0.1915, acc-0.7480, test loss-0.1913, acc-0.7467\n",
      "Iter-3110, train loss-0.1668, acc-0.7800, valid loss-0.1916, acc-0.7490, test loss-0.1915, acc-0.7464\n",
      "Iter-3120, train loss-0.2034, acc-0.7300, valid loss-0.1914, acc-0.7482, test loss-0.1914, acc-0.7469\n",
      "Iter-3130, train loss-0.1766, acc-0.7900, valid loss-0.1914, acc-0.7488, test loss-0.1915, acc-0.7461\n",
      "Iter-3140, train loss-0.1915, acc-0.7100, valid loss-0.1912, acc-0.7492, test loss-0.1914, acc-0.7454\n",
      "Iter-3150, train loss-0.2101, acc-0.6800, valid loss-0.1913, acc-0.7488, test loss-0.1914, acc-0.7451\n",
      "Iter-3160, train loss-0.2228, acc-0.7100, valid loss-0.1912, acc-0.7470, test loss-0.1913, acc-0.7451\n",
      "Iter-3170, train loss-0.2049, acc-0.7000, valid loss-0.1908, acc-0.7498, test loss-0.1911, acc-0.7465\n",
      "Iter-3180, train loss-0.1665, acc-0.7900, valid loss-0.1909, acc-0.7478, test loss-0.1911, acc-0.7461\n",
      "Iter-3190, train loss-0.1577, acc-0.7900, valid loss-0.1907, acc-0.7496, test loss-0.1909, acc-0.7467\n",
      "Iter-3200, train loss-0.1758, acc-0.8100, valid loss-0.1906, acc-0.7494, test loss-0.1907, acc-0.7469\n",
      "Iter-3210, train loss-0.1968, acc-0.7100, valid loss-0.1904, acc-0.7498, test loss-0.1906, acc-0.7476\n",
      "Iter-3220, train loss-0.2334, acc-0.6300, valid loss-0.1903, acc-0.7484, test loss-0.1905, acc-0.7480\n",
      "Iter-3230, train loss-0.1387, acc-0.8200, valid loss-0.1902, acc-0.7496, test loss-0.1903, acc-0.7477\n",
      "Iter-3240, train loss-0.1740, acc-0.7500, valid loss-0.1899, acc-0.7502, test loss-0.1902, acc-0.7489\n",
      "Iter-3250, train loss-0.2156, acc-0.7000, valid loss-0.1898, acc-0.7502, test loss-0.1900, acc-0.7491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-0.1968, acc-0.7300, valid loss-0.1897, acc-0.7492, test loss-0.1898, acc-0.7495\n",
      "Iter-3270, train loss-0.1652, acc-0.8000, valid loss-0.1897, acc-0.7494, test loss-0.1899, acc-0.7492\n",
      "Iter-3280, train loss-0.1918, acc-0.7400, valid loss-0.1899, acc-0.7488, test loss-0.1900, acc-0.7483\n",
      "Iter-3290, train loss-0.1625, acc-0.7900, valid loss-0.1898, acc-0.7500, test loss-0.1901, acc-0.7475\n",
      "Iter-3300, train loss-0.2045, acc-0.7200, valid loss-0.1895, acc-0.7508, test loss-0.1898, acc-0.7479\n",
      "Iter-3310, train loss-0.1645, acc-0.7700, valid loss-0.1897, acc-0.7486, test loss-0.1899, acc-0.7480\n",
      "Iter-3320, train loss-0.2314, acc-0.6700, valid loss-0.1896, acc-0.7500, test loss-0.1898, acc-0.7485\n",
      "Iter-3330, train loss-0.2071, acc-0.7100, valid loss-0.1895, acc-0.7492, test loss-0.1896, acc-0.7495\n",
      "Iter-3340, train loss-0.2140, acc-0.7400, valid loss-0.1896, acc-0.7488, test loss-0.1896, acc-0.7490\n",
      "Iter-3350, train loss-0.1697, acc-0.8000, valid loss-0.1895, acc-0.7476, test loss-0.1896, acc-0.7488\n",
      "Iter-3360, train loss-0.1533, acc-0.8200, valid loss-0.1894, acc-0.7486, test loss-0.1896, acc-0.7492\n",
      "Iter-3370, train loss-0.1641, acc-0.7600, valid loss-0.1896, acc-0.7484, test loss-0.1898, acc-0.7487\n",
      "Iter-3380, train loss-0.2297, acc-0.6900, valid loss-0.1894, acc-0.7480, test loss-0.1897, acc-0.7482\n",
      "Iter-3390, train loss-0.2082, acc-0.7100, valid loss-0.1891, acc-0.7486, test loss-0.1894, acc-0.7489\n",
      "Iter-3400, train loss-0.1833, acc-0.7700, valid loss-0.1889, acc-0.7492, test loss-0.1894, acc-0.7491\n",
      "Iter-3410, train loss-0.1586, acc-0.7900, valid loss-0.1889, acc-0.7494, test loss-0.1894, acc-0.7495\n",
      "Iter-3420, train loss-0.2000, acc-0.7500, valid loss-0.1887, acc-0.7500, test loss-0.1891, acc-0.7502\n",
      "Iter-3430, train loss-0.2042, acc-0.7000, valid loss-0.1884, acc-0.7508, test loss-0.1889, acc-0.7503\n",
      "Iter-3440, train loss-0.2110, acc-0.6900, valid loss-0.1883, acc-0.7506, test loss-0.1888, acc-0.7509\n",
      "Iter-3450, train loss-0.2122, acc-0.7500, valid loss-0.1883, acc-0.7514, test loss-0.1889, acc-0.7504\n",
      "Iter-3460, train loss-0.1978, acc-0.7200, valid loss-0.1883, acc-0.7508, test loss-0.1888, acc-0.7503\n",
      "Iter-3470, train loss-0.1791, acc-0.7600, valid loss-0.1882, acc-0.7518, test loss-0.1888, acc-0.7506\n",
      "Iter-3480, train loss-0.1860, acc-0.7500, valid loss-0.1880, acc-0.7532, test loss-0.1887, acc-0.7510\n",
      "Iter-3490, train loss-0.2018, acc-0.7200, valid loss-0.1880, acc-0.7530, test loss-0.1888, acc-0.7508\n",
      "Iter-3500, train loss-0.1584, acc-0.8200, valid loss-0.1880, acc-0.7522, test loss-0.1888, acc-0.7511\n",
      "Iter-3510, train loss-0.1764, acc-0.7400, valid loss-0.1881, acc-0.7516, test loss-0.1889, acc-0.7504\n",
      "Iter-3520, train loss-0.1785, acc-0.7700, valid loss-0.1878, acc-0.7526, test loss-0.1885, acc-0.7508\n",
      "Iter-3530, train loss-0.1803, acc-0.7600, valid loss-0.1879, acc-0.7524, test loss-0.1886, acc-0.7519\n",
      "Iter-3540, train loss-0.2253, acc-0.6900, valid loss-0.1876, acc-0.7522, test loss-0.1884, acc-0.7517\n",
      "Iter-3550, train loss-0.1869, acc-0.7700, valid loss-0.1877, acc-0.7520, test loss-0.1887, acc-0.7513\n",
      "Iter-3560, train loss-0.2234, acc-0.7000, valid loss-0.1877, acc-0.7522, test loss-0.1887, acc-0.7512\n",
      "Iter-3570, train loss-0.1531, acc-0.8400, valid loss-0.1881, acc-0.7518, test loss-0.1891, acc-0.7517\n",
      "Iter-3580, train loss-0.1825, acc-0.7500, valid loss-0.1880, acc-0.7522, test loss-0.1890, acc-0.7515\n",
      "Iter-3590, train loss-0.1782, acc-0.7400, valid loss-0.1883, acc-0.7532, test loss-0.1892, acc-0.7516\n",
      "Iter-3600, train loss-0.1887, acc-0.7900, valid loss-0.1885, acc-0.7532, test loss-0.1894, acc-0.7509\n",
      "Iter-3610, train loss-0.2025, acc-0.7300, valid loss-0.1886, acc-0.7520, test loss-0.1896, acc-0.7497\n",
      "Iter-3620, train loss-0.1958, acc-0.7400, valid loss-0.1883, acc-0.7528, test loss-0.1894, acc-0.7503\n",
      "Iter-3630, train loss-0.2052, acc-0.7200, valid loss-0.1885, acc-0.7538, test loss-0.1895, acc-0.7506\n",
      "Iter-3640, train loss-0.1928, acc-0.6900, valid loss-0.1883, acc-0.7530, test loss-0.1893, acc-0.7499\n",
      "Iter-3650, train loss-0.1743, acc-0.7600, valid loss-0.1882, acc-0.7516, test loss-0.1893, acc-0.7505\n",
      "Iter-3660, train loss-0.1857, acc-0.7800, valid loss-0.1881, acc-0.7516, test loss-0.1892, acc-0.7505\n",
      "Iter-3670, train loss-0.1860, acc-0.7700, valid loss-0.1881, acc-0.7516, test loss-0.1893, acc-0.7504\n",
      "Iter-3680, train loss-0.1762, acc-0.8100, valid loss-0.1882, acc-0.7512, test loss-0.1894, acc-0.7505\n",
      "Iter-3690, train loss-0.1605, acc-0.8000, valid loss-0.1881, acc-0.7512, test loss-0.1893, acc-0.7506\n",
      "Iter-3700, train loss-0.1822, acc-0.7800, valid loss-0.1880, acc-0.7532, test loss-0.1894, acc-0.7496\n",
      "Iter-3710, train loss-0.2171, acc-0.7000, valid loss-0.1879, acc-0.7536, test loss-0.1894, acc-0.7503\n",
      "Iter-3720, train loss-0.1833, acc-0.7400, valid loss-0.1881, acc-0.7532, test loss-0.1896, acc-0.7497\n",
      "Iter-3730, train loss-0.2001, acc-0.7300, valid loss-0.1884, acc-0.7534, test loss-0.1898, acc-0.7486\n",
      "Iter-3740, train loss-0.1749, acc-0.7800, valid loss-0.1887, acc-0.7522, test loss-0.1902, acc-0.7484\n",
      "Iter-3750, train loss-0.1469, acc-0.8100, valid loss-0.1884, acc-0.7540, test loss-0.1899, acc-0.7493\n",
      "Iter-3760, train loss-0.1835, acc-0.7700, valid loss-0.1884, acc-0.7532, test loss-0.1898, acc-0.7507\n",
      "Iter-3770, train loss-0.1987, acc-0.7600, valid loss-0.1883, acc-0.7544, test loss-0.1897, acc-0.7506\n",
      "Iter-3780, train loss-0.1444, acc-0.8500, valid loss-0.1883, acc-0.7538, test loss-0.1899, acc-0.7499\n",
      "Iter-3790, train loss-0.1895, acc-0.7500, valid loss-0.1882, acc-0.7540, test loss-0.1898, acc-0.7499\n",
      "Iter-3800, train loss-0.1873, acc-0.7600, valid loss-0.1882, acc-0.7548, test loss-0.1899, acc-0.7500\n",
      "Iter-3810, train loss-0.2097, acc-0.6900, valid loss-0.1882, acc-0.7554, test loss-0.1902, acc-0.7502\n",
      "Iter-3820, train loss-0.1776, acc-0.7600, valid loss-0.1880, acc-0.7564, test loss-0.1901, acc-0.7501\n",
      "Iter-3830, train loss-0.2019, acc-0.7600, valid loss-0.1879, acc-0.7556, test loss-0.1901, acc-0.7500\n",
      "Iter-3840, train loss-0.2290, acc-0.6700, valid loss-0.1879, acc-0.7554, test loss-0.1900, acc-0.7500\n",
      "Iter-3850, train loss-0.1709, acc-0.8000, valid loss-0.1881, acc-0.7546, test loss-0.1902, acc-0.7505\n",
      "Iter-3860, train loss-0.2032, acc-0.7200, valid loss-0.1879, acc-0.7554, test loss-0.1901, acc-0.7514\n",
      "Iter-3870, train loss-0.1990, acc-0.7400, valid loss-0.1880, acc-0.7550, test loss-0.1902, acc-0.7510\n",
      "Iter-3880, train loss-0.1854, acc-0.7300, valid loss-0.1879, acc-0.7552, test loss-0.1901, acc-0.7513\n",
      "Iter-3890, train loss-0.1619, acc-0.8100, valid loss-0.1880, acc-0.7558, test loss-0.1902, acc-0.7501\n",
      "Iter-3900, train loss-0.1820, acc-0.7700, valid loss-0.1877, acc-0.7558, test loss-0.1899, acc-0.7501\n",
      "Iter-3910, train loss-0.1990, acc-0.7300, valid loss-0.1876, acc-0.7564, test loss-0.1898, acc-0.7513\n",
      "Iter-3920, train loss-0.2013, acc-0.7300, valid loss-0.1876, acc-0.7568, test loss-0.1901, acc-0.7509\n",
      "Iter-3930, train loss-0.1760, acc-0.7900, valid loss-0.1878, acc-0.7562, test loss-0.1902, acc-0.7513\n",
      "Iter-3940, train loss-0.2147, acc-0.6800, valid loss-0.1881, acc-0.7544, test loss-0.1903, acc-0.7520\n",
      "Iter-3950, train loss-0.1900, acc-0.7900, valid loss-0.1881, acc-0.7550, test loss-0.1903, acc-0.7517\n",
      "Iter-3960, train loss-0.2267, acc-0.6700, valid loss-0.1883, acc-0.7544, test loss-0.1906, acc-0.7520\n",
      "Iter-3970, train loss-0.2231, acc-0.7100, valid loss-0.1883, acc-0.7568, test loss-0.1907, acc-0.7516\n",
      "Iter-3980, train loss-0.1797, acc-0.7900, valid loss-0.1885, acc-0.7556, test loss-0.1910, acc-0.7511\n",
      "Iter-3990, train loss-0.1673, acc-0.8000, valid loss-0.1884, acc-0.7562, test loss-0.1910, acc-0.7515\n",
      "Iter-4000, train loss-0.2007, acc-0.7200, valid loss-0.1885, acc-0.7566, test loss-0.1911, acc-0.7512\n",
      "Iter-4010, train loss-0.1672, acc-0.7800, valid loss-0.1883, acc-0.7580, test loss-0.1910, acc-0.7510\n",
      "Iter-4020, train loss-0.2367, acc-0.6900, valid loss-0.1883, acc-0.7570, test loss-0.1910, acc-0.7510\n",
      "Iter-4030, train loss-0.1574, acc-0.8100, valid loss-0.1881, acc-0.7574, test loss-0.1909, acc-0.7521\n",
      "Iter-4040, train loss-0.1627, acc-0.7900, valid loss-0.1882, acc-0.7566, test loss-0.1910, acc-0.7517\n",
      "Iter-4050, train loss-0.1752, acc-0.7600, valid loss-0.1883, acc-0.7568, test loss-0.1909, acc-0.7522\n",
      "Iter-4060, train loss-0.1990, acc-0.7500, valid loss-0.1883, acc-0.7580, test loss-0.1911, acc-0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-0.1708, acc-0.7600, valid loss-0.1883, acc-0.7584, test loss-0.1912, acc-0.7525\n",
      "Iter-4080, train loss-0.1712, acc-0.8100, valid loss-0.1885, acc-0.7564, test loss-0.1914, acc-0.7517\n",
      "Iter-4090, train loss-0.2161, acc-0.7200, valid loss-0.1887, acc-0.7562, test loss-0.1917, acc-0.7508\n",
      "Iter-4100, train loss-0.1727, acc-0.7400, valid loss-0.1887, acc-0.7568, test loss-0.1917, acc-0.7504\n",
      "Iter-4110, train loss-0.1551, acc-0.8300, valid loss-0.1889, acc-0.7570, test loss-0.1920, acc-0.7500\n",
      "Iter-4120, train loss-0.1827, acc-0.7600, valid loss-0.1888, acc-0.7580, test loss-0.1919, acc-0.7506\n",
      "Iter-4130, train loss-0.1976, acc-0.7500, valid loss-0.1891, acc-0.7562, test loss-0.1923, acc-0.7507\n",
      "Iter-4140, train loss-0.1851, acc-0.7400, valid loss-0.1891, acc-0.7556, test loss-0.1923, acc-0.7502\n",
      "Iter-4150, train loss-0.1964, acc-0.7600, valid loss-0.1894, acc-0.7564, test loss-0.1925, acc-0.7505\n",
      "Iter-4160, train loss-0.2393, acc-0.6500, valid loss-0.1894, acc-0.7556, test loss-0.1926, acc-0.7502\n",
      "Iter-4170, train loss-0.1907, acc-0.7500, valid loss-0.1892, acc-0.7564, test loss-0.1924, acc-0.7507\n",
      "Iter-4180, train loss-0.2056, acc-0.7400, valid loss-0.1896, acc-0.7564, test loss-0.1929, acc-0.7509\n",
      "Iter-4190, train loss-0.1554, acc-0.8400, valid loss-0.1902, acc-0.7546, test loss-0.1935, acc-0.7491\n",
      "Iter-4200, train loss-0.2143, acc-0.6700, valid loss-0.1902, acc-0.7552, test loss-0.1937, acc-0.7490\n",
      "Iter-4210, train loss-0.1911, acc-0.7500, valid loss-0.1904, acc-0.7568, test loss-0.1939, acc-0.7492\n",
      "Iter-4220, train loss-0.1894, acc-0.7300, valid loss-0.1903, acc-0.7556, test loss-0.1938, acc-0.7489\n",
      "Iter-4230, train loss-0.1891, acc-0.7500, valid loss-0.1903, acc-0.7554, test loss-0.1938, acc-0.7486\n",
      "Iter-4240, train loss-0.2272, acc-0.7100, valid loss-0.1903, acc-0.7550, test loss-0.1938, acc-0.7489\n",
      "Iter-4250, train loss-0.1948, acc-0.7700, valid loss-0.1904, acc-0.7562, test loss-0.1940, acc-0.7492\n",
      "Iter-4260, train loss-0.2143, acc-0.7200, valid loss-0.1905, acc-0.7566, test loss-0.1941, acc-0.7488\n",
      "Iter-4270, train loss-0.1736, acc-0.7900, valid loss-0.1905, acc-0.7560, test loss-0.1942, acc-0.7485\n",
      "Iter-4280, train loss-0.1866, acc-0.7200, valid loss-0.1906, acc-0.7552, test loss-0.1941, acc-0.7484\n",
      "Iter-4290, train loss-0.1626, acc-0.8100, valid loss-0.1906, acc-0.7556, test loss-0.1942, acc-0.7471\n",
      "Iter-4300, train loss-0.2315, acc-0.6900, valid loss-0.1909, acc-0.7552, test loss-0.1944, acc-0.7471\n",
      "Iter-4310, train loss-0.1829, acc-0.7700, valid loss-0.1913, acc-0.7540, test loss-0.1948, acc-0.7482\n",
      "Iter-4320, train loss-0.1865, acc-0.7100, valid loss-0.1913, acc-0.7552, test loss-0.1949, acc-0.7488\n",
      "Iter-4330, train loss-0.1784, acc-0.7700, valid loss-0.1915, acc-0.7532, test loss-0.1949, acc-0.7483\n",
      "Iter-4340, train loss-0.1801, acc-0.7800, valid loss-0.1918, acc-0.7532, test loss-0.1953, acc-0.7477\n",
      "Iter-4350, train loss-0.1879, acc-0.7400, valid loss-0.1920, acc-0.7554, test loss-0.1954, acc-0.7478\n",
      "Iter-4360, train loss-0.2155, acc-0.6700, valid loss-0.1917, acc-0.7548, test loss-0.1952, acc-0.7482\n",
      "Iter-4370, train loss-0.1935, acc-0.7900, valid loss-0.1921, acc-0.7554, test loss-0.1955, acc-0.7479\n",
      "Iter-4380, train loss-0.1804, acc-0.8000, valid loss-0.1923, acc-0.7534, test loss-0.1956, acc-0.7482\n",
      "Iter-4390, train loss-0.1872, acc-0.7300, valid loss-0.1926, acc-0.7536, test loss-0.1958, acc-0.7476\n",
      "Iter-4400, train loss-0.1897, acc-0.7600, valid loss-0.1926, acc-0.7542, test loss-0.1960, acc-0.7475\n",
      "Iter-4410, train loss-0.2115, acc-0.7600, valid loss-0.1927, acc-0.7564, test loss-0.1962, acc-0.7476\n",
      "Iter-4420, train loss-0.1772, acc-0.7800, valid loss-0.1927, acc-0.7552, test loss-0.1963, acc-0.7483\n",
      "Iter-4430, train loss-0.1935, acc-0.7400, valid loss-0.1928, acc-0.7536, test loss-0.1962, acc-0.7486\n",
      "Iter-4440, train loss-0.1598, acc-0.8300, valid loss-0.1930, acc-0.7546, test loss-0.1965, acc-0.7491\n",
      "Iter-4450, train loss-0.1748, acc-0.7600, valid loss-0.1934, acc-0.7546, test loss-0.1968, acc-0.7481\n",
      "Iter-4460, train loss-0.2331, acc-0.6800, valid loss-0.1935, acc-0.7564, test loss-0.1970, acc-0.7483\n",
      "Iter-4470, train loss-0.1666, acc-0.7800, valid loss-0.1934, acc-0.7540, test loss-0.1968, acc-0.7483\n",
      "Iter-4480, train loss-0.2670, acc-0.6400, valid loss-0.1937, acc-0.7556, test loss-0.1972, acc-0.7487\n",
      "Iter-4490, train loss-0.2342, acc-0.7100, valid loss-0.1935, acc-0.7576, test loss-0.1969, acc-0.7495\n",
      "Iter-4500, train loss-0.1942, acc-0.7600, valid loss-0.1938, acc-0.7578, test loss-0.1971, acc-0.7499\n",
      "Iter-4510, train loss-0.2192, acc-0.6900, valid loss-0.1940, acc-0.7586, test loss-0.1973, acc-0.7498\n",
      "Iter-4520, train loss-0.1974, acc-0.8000, valid loss-0.1941, acc-0.7592, test loss-0.1975, acc-0.7501\n",
      "Iter-4530, train loss-0.1702, acc-0.7800, valid loss-0.1944, acc-0.7580, test loss-0.1978, acc-0.7499\n",
      "Iter-4540, train loss-0.2493, acc-0.6600, valid loss-0.1945, acc-0.7584, test loss-0.1978, acc-0.7504\n",
      "Iter-4550, train loss-0.1845, acc-0.7500, valid loss-0.1945, acc-0.7570, test loss-0.1979, acc-0.7502\n",
      "Iter-4560, train loss-0.2081, acc-0.7100, valid loss-0.1943, acc-0.7588, test loss-0.1978, acc-0.7510\n",
      "Iter-4570, train loss-0.1953, acc-0.7200, valid loss-0.1944, acc-0.7592, test loss-0.1980, acc-0.7516\n",
      "Iter-4580, train loss-0.1762, acc-0.7800, valid loss-0.1949, acc-0.7578, test loss-0.1985, acc-0.7493\n",
      "Iter-4590, train loss-0.1939, acc-0.7700, valid loss-0.1951, acc-0.7558, test loss-0.1985, acc-0.7486\n",
      "Iter-4600, train loss-0.2206, acc-0.6800, valid loss-0.1954, acc-0.7574, test loss-0.1990, acc-0.7487\n",
      "Iter-4610, train loss-0.2055, acc-0.7300, valid loss-0.1956, acc-0.7552, test loss-0.1993, acc-0.7475\n",
      "Iter-4620, train loss-0.2178, acc-0.7200, valid loss-0.1956, acc-0.7564, test loss-0.1995, acc-0.7473\n",
      "Iter-4630, train loss-0.2137, acc-0.7400, valid loss-0.1960, acc-0.7548, test loss-0.1999, acc-0.7473\n",
      "Iter-4640, train loss-0.2079, acc-0.7300, valid loss-0.1958, acc-0.7556, test loss-0.1997, acc-0.7486\n",
      "Iter-4650, train loss-0.2203, acc-0.6800, valid loss-0.1957, acc-0.7578, test loss-0.1998, acc-0.7489\n",
      "Iter-4660, train loss-0.1938, acc-0.7500, valid loss-0.1959, acc-0.7564, test loss-0.1999, acc-0.7485\n",
      "Iter-4670, train loss-0.1909, acc-0.7600, valid loss-0.1966, acc-0.7566, test loss-0.2007, acc-0.7477\n",
      "Iter-4680, train loss-0.2496, acc-0.6900, valid loss-0.1970, acc-0.7546, test loss-0.2011, acc-0.7466\n",
      "Iter-4690, train loss-0.1813, acc-0.7600, valid loss-0.1969, acc-0.7566, test loss-0.2011, acc-0.7477\n",
      "Iter-4700, train loss-0.1722, acc-0.7700, valid loss-0.1971, acc-0.7578, test loss-0.2014, acc-0.7476\n",
      "Iter-4710, train loss-0.1889, acc-0.7800, valid loss-0.1973, acc-0.7578, test loss-0.2014, acc-0.7475\n",
      "Iter-4720, train loss-0.1884, acc-0.7700, valid loss-0.1976, acc-0.7570, test loss-0.2017, acc-0.7470\n",
      "Iter-4730, train loss-0.2146, acc-0.7600, valid loss-0.1981, acc-0.7570, test loss-0.2023, acc-0.7463\n",
      "Iter-4740, train loss-0.1446, acc-0.8700, valid loss-0.1982, acc-0.7572, test loss-0.2023, acc-0.7463\n",
      "Iter-4750, train loss-0.1994, acc-0.7700, valid loss-0.1985, acc-0.7564, test loss-0.2026, acc-0.7463\n",
      "Iter-4760, train loss-0.1892, acc-0.7600, valid loss-0.1988, acc-0.7558, test loss-0.2028, acc-0.7467\n",
      "Iter-4770, train loss-0.2121, acc-0.7600, valid loss-0.1994, acc-0.7550, test loss-0.2034, acc-0.7461\n",
      "Iter-4780, train loss-0.2441, acc-0.7000, valid loss-0.1994, acc-0.7562, test loss-0.2034, acc-0.7456\n",
      "Iter-4790, train loss-0.2083, acc-0.7000, valid loss-0.2001, acc-0.7550, test loss-0.2041, acc-0.7450\n",
      "Iter-4800, train loss-0.2229, acc-0.7300, valid loss-0.2001, acc-0.7560, test loss-0.2042, acc-0.7448\n",
      "Iter-4810, train loss-0.2657, acc-0.6400, valid loss-0.2005, acc-0.7558, test loss-0.2046, acc-0.7452\n",
      "Iter-4820, train loss-0.1651, acc-0.7700, valid loss-0.2003, acc-0.7550, test loss-0.2045, acc-0.7464\n",
      "Iter-4830, train loss-0.2226, acc-0.7200, valid loss-0.2005, acc-0.7552, test loss-0.2047, acc-0.7458\n",
      "Iter-4840, train loss-0.2183, acc-0.7100, valid loss-0.2007, acc-0.7558, test loss-0.2049, acc-0.7462\n",
      "Iter-4850, train loss-0.2050, acc-0.7100, valid loss-0.2013, acc-0.7552, test loss-0.2056, acc-0.7447\n",
      "Iter-4860, train loss-0.2102, acc-0.7800, valid loss-0.2015, acc-0.7548, test loss-0.2058, acc-0.7436\n",
      "Iter-4870, train loss-0.2441, acc-0.6600, valid loss-0.2017, acc-0.7552, test loss-0.2060, acc-0.7451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-0.2245, acc-0.7100, valid loss-0.2019, acc-0.7540, test loss-0.2062, acc-0.7450\n",
      "Iter-4890, train loss-0.1904, acc-0.7900, valid loss-0.2021, acc-0.7544, test loss-0.2064, acc-0.7441\n",
      "Iter-4900, train loss-0.2066, acc-0.7300, valid loss-0.2026, acc-0.7516, test loss-0.2069, acc-0.7426\n",
      "Iter-4910, train loss-0.1923, acc-0.7400, valid loss-0.2029, acc-0.7514, test loss-0.2073, acc-0.7433\n",
      "Iter-4920, train loss-0.1678, acc-0.7900, valid loss-0.2026, acc-0.7530, test loss-0.2071, acc-0.7438\n",
      "Iter-4930, train loss-0.2370, acc-0.6900, valid loss-0.2028, acc-0.7524, test loss-0.2072, acc-0.7419\n",
      "Iter-4940, train loss-0.1992, acc-0.7100, valid loss-0.2031, acc-0.7520, test loss-0.2076, acc-0.7419\n",
      "Iter-4950, train loss-0.2109, acc-0.7200, valid loss-0.2029, acc-0.7532, test loss-0.2075, acc-0.7440\n",
      "Iter-4960, train loss-0.2407, acc-0.6800, valid loss-0.2026, acc-0.7544, test loss-0.2072, acc-0.7448\n",
      "Iter-4970, train loss-0.2387, acc-0.6900, valid loss-0.2028, acc-0.7536, test loss-0.2076, acc-0.7454\n",
      "Iter-4980, train loss-0.1976, acc-0.7600, valid loss-0.2028, acc-0.7538, test loss-0.2074, acc-0.7457\n",
      "Iter-4990, train loss-0.1683, acc-0.8200, valid loss-0.2029, acc-0.7544, test loss-0.2077, acc-0.7461\n",
      "Iter-5000, train loss-0.2307, acc-0.6900, valid loss-0.2032, acc-0.7548, test loss-0.2080, acc-0.7463\n",
      "Iter-5010, train loss-0.2031, acc-0.7400, valid loss-0.2033, acc-0.7550, test loss-0.2081, acc-0.7460\n",
      "Iter-5020, train loss-0.2079, acc-0.7700, valid loss-0.2032, acc-0.7542, test loss-0.2082, acc-0.7463\n",
      "Iter-5030, train loss-0.2087, acc-0.7400, valid loss-0.2031, acc-0.7566, test loss-0.2082, acc-0.7450\n",
      "Iter-5040, train loss-0.2309, acc-0.7300, valid loss-0.2033, acc-0.7558, test loss-0.2083, acc-0.7450\n",
      "Iter-5050, train loss-0.1932, acc-0.7600, valid loss-0.2036, acc-0.7554, test loss-0.2087, acc-0.7445\n",
      "Iter-5060, train loss-0.1845, acc-0.7800, valid loss-0.2040, acc-0.7554, test loss-0.2090, acc-0.7443\n",
      "Iter-5070, train loss-0.1955, acc-0.8100, valid loss-0.2040, acc-0.7554, test loss-0.2091, acc-0.7451\n",
      "Iter-5080, train loss-0.2434, acc-0.7000, valid loss-0.2045, acc-0.7546, test loss-0.2097, acc-0.7442\n",
      "Iter-5090, train loss-0.2002, acc-0.7700, valid loss-0.2045, acc-0.7544, test loss-0.2096, acc-0.7453\n",
      "Iter-5100, train loss-0.1800, acc-0.7500, valid loss-0.2043, acc-0.7548, test loss-0.2095, acc-0.7459\n",
      "Iter-5110, train loss-0.2514, acc-0.6100, valid loss-0.2043, acc-0.7542, test loss-0.2095, acc-0.7440\n",
      "Iter-5120, train loss-0.2485, acc-0.6900, valid loss-0.2050, acc-0.7526, test loss-0.2100, acc-0.7426\n",
      "Iter-5130, train loss-0.2627, acc-0.6600, valid loss-0.2052, acc-0.7522, test loss-0.2103, acc-0.7436\n",
      "Iter-5140, train loss-0.2134, acc-0.7300, valid loss-0.2050, acc-0.7530, test loss-0.2101, acc-0.7441\n",
      "Iter-5150, train loss-0.2312, acc-0.7300, valid loss-0.2054, acc-0.7522, test loss-0.2106, acc-0.7435\n",
      "Iter-5160, train loss-0.2170, acc-0.7200, valid loss-0.2054, acc-0.7530, test loss-0.2106, acc-0.7422\n",
      "Iter-5170, train loss-0.2178, acc-0.7400, valid loss-0.2056, acc-0.7514, test loss-0.2109, acc-0.7410\n",
      "Iter-5180, train loss-0.2023, acc-0.7500, valid loss-0.2059, acc-0.7498, test loss-0.2112, acc-0.7405\n",
      "Iter-5190, train loss-0.2231, acc-0.7100, valid loss-0.2062, acc-0.7498, test loss-0.2114, acc-0.7405\n",
      "Iter-5200, train loss-0.2270, acc-0.7000, valid loss-0.2063, acc-0.7506, test loss-0.2114, acc-0.7408\n",
      "Iter-5210, train loss-0.2275, acc-0.7100, valid loss-0.2067, acc-0.7500, test loss-0.2119, acc-0.7409\n",
      "Iter-5220, train loss-0.2119, acc-0.7300, valid loss-0.2064, acc-0.7506, test loss-0.2117, acc-0.7416\n",
      "Iter-5230, train loss-0.1653, acc-0.8100, valid loss-0.2066, acc-0.7490, test loss-0.2119, acc-0.7411\n",
      "Iter-5240, train loss-0.2216, acc-0.7700, valid loss-0.2071, acc-0.7490, test loss-0.2122, acc-0.7396\n",
      "Iter-5250, train loss-0.1982, acc-0.7500, valid loss-0.2069, acc-0.7498, test loss-0.2120, acc-0.7395\n",
      "Iter-5260, train loss-0.2032, acc-0.7700, valid loss-0.2069, acc-0.7498, test loss-0.2121, acc-0.7395\n",
      "Iter-5270, train loss-0.1940, acc-0.7700, valid loss-0.2070, acc-0.7496, test loss-0.2122, acc-0.7390\n",
      "Iter-5280, train loss-0.1730, acc-0.8200, valid loss-0.2077, acc-0.7490, test loss-0.2128, acc-0.7382\n",
      "Iter-5290, train loss-0.1973, acc-0.7700, valid loss-0.2076, acc-0.7490, test loss-0.2128, acc-0.7394\n",
      "Iter-5300, train loss-0.2167, acc-0.7400, valid loss-0.2080, acc-0.7472, test loss-0.2132, acc-0.7387\n",
      "Iter-5310, train loss-0.1954, acc-0.7900, valid loss-0.2082, acc-0.7484, test loss-0.2136, acc-0.7396\n",
      "Iter-5320, train loss-0.2141, acc-0.7100, valid loss-0.2082, acc-0.7492, test loss-0.2135, acc-0.7396\n",
      "Iter-5330, train loss-0.2266, acc-0.7000, valid loss-0.2083, acc-0.7502, test loss-0.2137, acc-0.7393\n",
      "Iter-5340, train loss-0.2480, acc-0.6900, valid loss-0.2086, acc-0.7512, test loss-0.2140, acc-0.7378\n",
      "Iter-5350, train loss-0.2109, acc-0.7400, valid loss-0.2091, acc-0.7490, test loss-0.2144, acc-0.7369\n",
      "Iter-5360, train loss-0.2151, acc-0.7100, valid loss-0.2091, acc-0.7486, test loss-0.2142, acc-0.7367\n",
      "Iter-5370, train loss-0.2610, acc-0.6200, valid loss-0.2094, acc-0.7492, test loss-0.2147, acc-0.7365\n",
      "Iter-5380, train loss-0.2461, acc-0.7200, valid loss-0.2101, acc-0.7488, test loss-0.2154, acc-0.7367\n",
      "Iter-5390, train loss-0.2270, acc-0.7400, valid loss-0.2103, acc-0.7458, test loss-0.2157, acc-0.7355\n",
      "Iter-5400, train loss-0.2084, acc-0.7500, valid loss-0.2102, acc-0.7464, test loss-0.2155, acc-0.7362\n",
      "Iter-5410, train loss-0.2262, acc-0.7300, valid loss-0.2108, acc-0.7452, test loss-0.2163, acc-0.7357\n",
      "Iter-5420, train loss-0.2310, acc-0.7100, valid loss-0.2113, acc-0.7436, test loss-0.2167, acc-0.7351\n",
      "Iter-5430, train loss-0.2235, acc-0.7500, valid loss-0.2115, acc-0.7430, test loss-0.2169, acc-0.7350\n",
      "Iter-5440, train loss-0.2343, acc-0.7200, valid loss-0.2119, acc-0.7418, test loss-0.2173, acc-0.7336\n",
      "Iter-5450, train loss-0.2353, acc-0.7600, valid loss-0.2122, acc-0.7418, test loss-0.2175, acc-0.7337\n",
      "Iter-5460, train loss-0.2075, acc-0.7500, valid loss-0.2127, acc-0.7418, test loss-0.2181, acc-0.7335\n",
      "Iter-5470, train loss-0.2261, acc-0.7500, valid loss-0.2126, acc-0.7422, test loss-0.2179, acc-0.7338\n",
      "Iter-5480, train loss-0.2180, acc-0.7500, valid loss-0.2128, acc-0.7416, test loss-0.2181, acc-0.7332\n",
      "Iter-5490, train loss-0.2136, acc-0.7500, valid loss-0.2131, acc-0.7414, test loss-0.2184, acc-0.7333\n",
      "Iter-5500, train loss-0.2133, acc-0.7300, valid loss-0.2133, acc-0.7404, test loss-0.2187, acc-0.7333\n",
      "Iter-5510, train loss-0.2556, acc-0.7000, valid loss-0.2137, acc-0.7412, test loss-0.2190, acc-0.7316\n",
      "Iter-5520, train loss-0.2435, acc-0.6300, valid loss-0.2141, acc-0.7406, test loss-0.2194, acc-0.7319\n",
      "Iter-5530, train loss-0.1852, acc-0.7400, valid loss-0.2139, acc-0.7410, test loss-0.2192, acc-0.7328\n",
      "Iter-5540, train loss-0.2086, acc-0.7500, valid loss-0.2139, acc-0.7410, test loss-0.2192, acc-0.7322\n",
      "Iter-5550, train loss-0.2262, acc-0.7400, valid loss-0.2142, acc-0.7414, test loss-0.2196, acc-0.7322\n",
      "Iter-5560, train loss-0.2707, acc-0.6600, valid loss-0.2144, acc-0.7406, test loss-0.2199, acc-0.7316\n",
      "Iter-5570, train loss-0.2379, acc-0.7000, valid loss-0.2140, acc-0.7428, test loss-0.2195, acc-0.7328\n",
      "Iter-5580, train loss-0.2212, acc-0.7100, valid loss-0.2145, acc-0.7420, test loss-0.2198, acc-0.7329\n",
      "Iter-5590, train loss-0.2005, acc-0.7400, valid loss-0.2147, acc-0.7404, test loss-0.2200, acc-0.7316\n",
      "Iter-5600, train loss-0.2312, acc-0.7300, valid loss-0.2152, acc-0.7384, test loss-0.2205, acc-0.7311\n",
      "Iter-5610, train loss-0.2516, acc-0.6900, valid loss-0.2155, acc-0.7374, test loss-0.2208, acc-0.7309\n",
      "Iter-5620, train loss-0.2368, acc-0.7400, valid loss-0.2162, acc-0.7352, test loss-0.2214, acc-0.7293\n",
      "Iter-5630, train loss-0.2380, acc-0.7200, valid loss-0.2163, acc-0.7354, test loss-0.2217, acc-0.7275\n",
      "Iter-5640, train loss-0.1960, acc-0.7700, valid loss-0.2166, acc-0.7354, test loss-0.2220, acc-0.7289\n",
      "Iter-5650, train loss-0.2319, acc-0.7300, valid loss-0.2169, acc-0.7370, test loss-0.2222, acc-0.7297\n",
      "Iter-5660, train loss-0.2584, acc-0.6400, valid loss-0.2170, acc-0.7386, test loss-0.2222, acc-0.7296\n",
      "Iter-5670, train loss-0.2705, acc-0.6300, valid loss-0.2171, acc-0.7374, test loss-0.2222, acc-0.7288\n",
      "Iter-5680, train loss-0.2149, acc-0.7200, valid loss-0.2175, acc-0.7390, test loss-0.2226, acc-0.7289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-0.2181, acc-0.7500, valid loss-0.2181, acc-0.7386, test loss-0.2231, acc-0.7284\n",
      "Iter-5700, train loss-0.2226, acc-0.7000, valid loss-0.2184, acc-0.7374, test loss-0.2235, acc-0.7282\n",
      "Iter-5710, train loss-0.2161, acc-0.7500, valid loss-0.2183, acc-0.7384, test loss-0.2234, acc-0.7293\n",
      "Iter-5720, train loss-0.2018, acc-0.7500, valid loss-0.2184, acc-0.7366, test loss-0.2234, acc-0.7286\n",
      "Iter-5730, train loss-0.1959, acc-0.8000, valid loss-0.2183, acc-0.7374, test loss-0.2232, acc-0.7289\n",
      "Iter-5740, train loss-0.2653, acc-0.6700, valid loss-0.2185, acc-0.7358, test loss-0.2233, acc-0.7281\n",
      "Iter-5750, train loss-0.2703, acc-0.6300, valid loss-0.2184, acc-0.7372, test loss-0.2232, acc-0.7295\n",
      "Iter-5760, train loss-0.2620, acc-0.6900, valid loss-0.2185, acc-0.7378, test loss-0.2234, acc-0.7307\n",
      "Iter-5770, train loss-0.2524, acc-0.6800, valid loss-0.2186, acc-0.7370, test loss-0.2234, acc-0.7300\n",
      "Iter-5780, train loss-0.2335, acc-0.6800, valid loss-0.2185, acc-0.7344, test loss-0.2232, acc-0.7294\n",
      "Iter-5790, train loss-0.2440, acc-0.6400, valid loss-0.2188, acc-0.7348, test loss-0.2234, acc-0.7290\n",
      "Iter-5800, train loss-0.2544, acc-0.6600, valid loss-0.2185, acc-0.7340, test loss-0.2231, acc-0.7286\n",
      "Iter-5810, train loss-0.2341, acc-0.7000, valid loss-0.2189, acc-0.7344, test loss-0.2233, acc-0.7285\n",
      "Iter-5820, train loss-0.2292, acc-0.7200, valid loss-0.2192, acc-0.7332, test loss-0.2236, acc-0.7282\n",
      "Iter-5830, train loss-0.2427, acc-0.7200, valid loss-0.2195, acc-0.7314, test loss-0.2238, acc-0.7266\n",
      "Iter-5840, train loss-0.2212, acc-0.7600, valid loss-0.2195, acc-0.7308, test loss-0.2240, acc-0.7259\n",
      "Iter-5850, train loss-0.2251, acc-0.7400, valid loss-0.2197, acc-0.7318, test loss-0.2241, acc-0.7259\n",
      "Iter-5860, train loss-0.2381, acc-0.7000, valid loss-0.2197, acc-0.7314, test loss-0.2241, acc-0.7272\n",
      "Iter-5870, train loss-0.2606, acc-0.6600, valid loss-0.2197, acc-0.7312, test loss-0.2240, acc-0.7275\n",
      "Iter-5880, train loss-0.2103, acc-0.7900, valid loss-0.2201, acc-0.7306, test loss-0.2244, acc-0.7274\n",
      "Iter-5890, train loss-0.1988, acc-0.7700, valid loss-0.2199, acc-0.7320, test loss-0.2242, acc-0.7282\n",
      "Iter-5900, train loss-0.2296, acc-0.7400, valid loss-0.2202, acc-0.7318, test loss-0.2243, acc-0.7271\n",
      "Iter-5910, train loss-0.1983, acc-0.8100, valid loss-0.2202, acc-0.7316, test loss-0.2243, acc-0.7273\n",
      "Iter-5920, train loss-0.2251, acc-0.6900, valid loss-0.2203, acc-0.7314, test loss-0.2243, acc-0.7266\n",
      "Iter-5930, train loss-0.2152, acc-0.7600, valid loss-0.2206, acc-0.7306, test loss-0.2247, acc-0.7261\n",
      "Iter-5940, train loss-0.1924, acc-0.7700, valid loss-0.2207, acc-0.7310, test loss-0.2249, acc-0.7259\n",
      "Iter-5950, train loss-0.1923, acc-0.8000, valid loss-0.2204, acc-0.7312, test loss-0.2246, acc-0.7262\n",
      "Iter-5960, train loss-0.2286, acc-0.7200, valid loss-0.2205, acc-0.7314, test loss-0.2248, acc-0.7254\n",
      "Iter-5970, train loss-0.2076, acc-0.7600, valid loss-0.2205, acc-0.7304, test loss-0.2248, acc-0.7262\n",
      "Iter-5980, train loss-0.2401, acc-0.7200, valid loss-0.2209, acc-0.7302, test loss-0.2251, acc-0.7264\n",
      "Iter-5990, train loss-0.2451, acc-0.6500, valid loss-0.2211, acc-0.7306, test loss-0.2253, acc-0.7271\n",
      "Iter-6000, train loss-0.2464, acc-0.7000, valid loss-0.2208, acc-0.7308, test loss-0.2251, acc-0.7278\n",
      "Iter-6010, train loss-0.2064, acc-0.7700, valid loss-0.2213, acc-0.7304, test loss-0.2257, acc-0.7268\n",
      "Iter-6020, train loss-0.2285, acc-0.7000, valid loss-0.2210, acc-0.7316, test loss-0.2253, acc-0.7269\n",
      "Iter-6030, train loss-0.2167, acc-0.7400, valid loss-0.2209, acc-0.7316, test loss-0.2252, acc-0.7267\n",
      "Iter-6040, train loss-0.2330, acc-0.7500, valid loss-0.2211, acc-0.7318, test loss-0.2254, acc-0.7272\n",
      "Iter-6050, train loss-0.1953, acc-0.8300, valid loss-0.2215, acc-0.7310, test loss-0.2258, acc-0.7251\n",
      "Iter-6060, train loss-0.2594, acc-0.6700, valid loss-0.2215, acc-0.7320, test loss-0.2259, acc-0.7253\n",
      "Iter-6070, train loss-0.2131, acc-0.7800, valid loss-0.2216, acc-0.7320, test loss-0.2260, acc-0.7250\n",
      "Iter-6080, train loss-0.2229, acc-0.7000, valid loss-0.2213, acc-0.7316, test loss-0.2256, acc-0.7243\n",
      "Iter-6090, train loss-0.2092, acc-0.6700, valid loss-0.2212, acc-0.7338, test loss-0.2256, acc-0.7266\n",
      "Iter-6100, train loss-0.2527, acc-0.6500, valid loss-0.2213, acc-0.7316, test loss-0.2257, acc-0.7253\n",
      "Iter-6110, train loss-0.2238, acc-0.7500, valid loss-0.2217, acc-0.7304, test loss-0.2259, acc-0.7240\n",
      "Iter-6120, train loss-0.2424, acc-0.6800, valid loss-0.2219, acc-0.7306, test loss-0.2261, acc-0.7239\n",
      "Iter-6130, train loss-0.2073, acc-0.7400, valid loss-0.2222, acc-0.7298, test loss-0.2263, acc-0.7216\n",
      "Iter-6140, train loss-0.1903, acc-0.8000, valid loss-0.2225, acc-0.7294, test loss-0.2266, acc-0.7206\n",
      "Iter-6150, train loss-0.2057, acc-0.7600, valid loss-0.2224, acc-0.7298, test loss-0.2266, acc-0.7236\n",
      "Iter-6160, train loss-0.1841, acc-0.7700, valid loss-0.2224, acc-0.7304, test loss-0.2264, acc-0.7237\n",
      "Iter-6170, train loss-0.2146, acc-0.7500, valid loss-0.2224, acc-0.7332, test loss-0.2264, acc-0.7253\n",
      "Iter-6180, train loss-0.2155, acc-0.7800, valid loss-0.2226, acc-0.7322, test loss-0.2266, acc-0.7249\n",
      "Iter-6190, train loss-0.2289, acc-0.7000, valid loss-0.2228, acc-0.7320, test loss-0.2267, acc-0.7250\n",
      "Iter-6200, train loss-0.2846, acc-0.6600, valid loss-0.2226, acc-0.7318, test loss-0.2265, acc-0.7266\n",
      "Iter-6210, train loss-0.2190, acc-0.7400, valid loss-0.2229, acc-0.7328, test loss-0.2267, acc-0.7260\n",
      "Iter-6220, train loss-0.2183, acc-0.7400, valid loss-0.2232, acc-0.7314, test loss-0.2269, acc-0.7238\n",
      "Iter-6230, train loss-0.2839, acc-0.5900, valid loss-0.2231, acc-0.7326, test loss-0.2268, acc-0.7242\n",
      "Iter-6240, train loss-0.2403, acc-0.7000, valid loss-0.2230, acc-0.7302, test loss-0.2266, acc-0.7225\n",
      "Iter-6250, train loss-0.2163, acc-0.7000, valid loss-0.2229, acc-0.7312, test loss-0.2264, acc-0.7238\n",
      "Iter-6260, train loss-0.1932, acc-0.7600, valid loss-0.2229, acc-0.7300, test loss-0.2263, acc-0.7226\n",
      "Iter-6270, train loss-0.2168, acc-0.7500, valid loss-0.2226, acc-0.7320, test loss-0.2261, acc-0.7232\n",
      "Iter-6280, train loss-0.2143, acc-0.7400, valid loss-0.2226, acc-0.7322, test loss-0.2261, acc-0.7246\n",
      "Iter-6290, train loss-0.2606, acc-0.6600, valid loss-0.2226, acc-0.7312, test loss-0.2260, acc-0.7249\n",
      "Iter-6300, train loss-0.2260, acc-0.7500, valid loss-0.2226, acc-0.7320, test loss-0.2260, acc-0.7245\n",
      "Iter-6310, train loss-0.2195, acc-0.7300, valid loss-0.2226, acc-0.7324, test loss-0.2260, acc-0.7251\n",
      "Iter-6320, train loss-0.2276, acc-0.7500, valid loss-0.2222, acc-0.7326, test loss-0.2255, acc-0.7268\n",
      "Iter-6330, train loss-0.2332, acc-0.7000, valid loss-0.2223, acc-0.7328, test loss-0.2255, acc-0.7269\n",
      "Iter-6340, train loss-0.2408, acc-0.6700, valid loss-0.2220, acc-0.7334, test loss-0.2252, acc-0.7278\n",
      "Iter-6350, train loss-0.2033, acc-0.7600, valid loss-0.2223, acc-0.7332, test loss-0.2254, acc-0.7268\n",
      "Iter-6360, train loss-0.2616, acc-0.7100, valid loss-0.2225, acc-0.7348, test loss-0.2256, acc-0.7285\n",
      "Iter-6370, train loss-0.2311, acc-0.7200, valid loss-0.2226, acc-0.7346, test loss-0.2258, acc-0.7274\n",
      "Iter-6380, train loss-0.2080, acc-0.7900, valid loss-0.2229, acc-0.7352, test loss-0.2260, acc-0.7280\n",
      "Iter-6390, train loss-0.1930, acc-0.7400, valid loss-0.2229, acc-0.7360, test loss-0.2260, acc-0.7278\n",
      "Iter-6400, train loss-0.1920, acc-0.8100, valid loss-0.2228, acc-0.7354, test loss-0.2258, acc-0.7276\n",
      "Iter-6410, train loss-0.2062, acc-0.7200, valid loss-0.2230, acc-0.7344, test loss-0.2259, acc-0.7278\n",
      "Iter-6420, train loss-0.2122, acc-0.7100, valid loss-0.2227, acc-0.7352, test loss-0.2257, acc-0.7265\n",
      "Iter-6430, train loss-0.2442, acc-0.7200, valid loss-0.2229, acc-0.7348, test loss-0.2258, acc-0.7256\n",
      "Iter-6440, train loss-0.1796, acc-0.7500, valid loss-0.2227, acc-0.7340, test loss-0.2257, acc-0.7253\n",
      "Iter-6450, train loss-0.2297, acc-0.7000, valid loss-0.2224, acc-0.7338, test loss-0.2253, acc-0.7262\n",
      "Iter-6460, train loss-0.2359, acc-0.7200, valid loss-0.2223, acc-0.7346, test loss-0.2252, acc-0.7256\n",
      "Iter-6470, train loss-0.2166, acc-0.7100, valid loss-0.2227, acc-0.7362, test loss-0.2256, acc-0.7254\n",
      "Iter-6480, train loss-0.2187, acc-0.7500, valid loss-0.2230, acc-0.7332, test loss-0.2259, acc-0.7245\n",
      "Iter-6490, train loss-0.2091, acc-0.7700, valid loss-0.2225, acc-0.7356, test loss-0.2254, acc-0.7261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-0.1927, acc-0.7400, valid loss-0.2224, acc-0.7356, test loss-0.2253, acc-0.7259\n",
      "Iter-6510, train loss-0.2015, acc-0.7700, valid loss-0.2223, acc-0.7348, test loss-0.2254, acc-0.7260\n",
      "Iter-6520, train loss-0.2428, acc-0.6400, valid loss-0.2223, acc-0.7388, test loss-0.2253, acc-0.7288\n",
      "Iter-6530, train loss-0.1943, acc-0.8000, valid loss-0.2222, acc-0.7382, test loss-0.2252, acc-0.7290\n",
      "Iter-6540, train loss-0.2212, acc-0.7400, valid loss-0.2222, acc-0.7372, test loss-0.2250, acc-0.7286\n",
      "Iter-6550, train loss-0.2341, acc-0.6800, valid loss-0.2222, acc-0.7378, test loss-0.2250, acc-0.7288\n",
      "Iter-6560, train loss-0.1829, acc-0.7600, valid loss-0.2219, acc-0.7388, test loss-0.2247, acc-0.7299\n",
      "Iter-6570, train loss-0.2603, acc-0.6800, valid loss-0.2217, acc-0.7386, test loss-0.2245, acc-0.7300\n",
      "Iter-6580, train loss-0.2298, acc-0.7200, valid loss-0.2216, acc-0.7392, test loss-0.2242, acc-0.7312\n",
      "Iter-6590, train loss-0.2601, acc-0.6900, valid loss-0.2217, acc-0.7384, test loss-0.2244, acc-0.7298\n",
      "Iter-6600, train loss-0.2574, acc-0.6400, valid loss-0.2215, acc-0.7392, test loss-0.2242, acc-0.7302\n",
      "Iter-6610, train loss-0.2417, acc-0.6700, valid loss-0.2213, acc-0.7398, test loss-0.2240, acc-0.7310\n",
      "Iter-6620, train loss-0.2238, acc-0.7300, valid loss-0.2211, acc-0.7404, test loss-0.2237, acc-0.7326\n",
      "Iter-6630, train loss-0.1908, acc-0.7600, valid loss-0.2210, acc-0.7380, test loss-0.2237, acc-0.7316\n",
      "Iter-6640, train loss-0.2143, acc-0.7100, valid loss-0.2211, acc-0.7392, test loss-0.2238, acc-0.7320\n",
      "Iter-6650, train loss-0.2254, acc-0.7200, valid loss-0.2208, acc-0.7392, test loss-0.2236, acc-0.7320\n",
      "Iter-6660, train loss-0.2282, acc-0.7100, valid loss-0.2209, acc-0.7380, test loss-0.2236, acc-0.7318\n",
      "Iter-6670, train loss-0.2373, acc-0.6600, valid loss-0.2208, acc-0.7372, test loss-0.2235, acc-0.7310\n",
      "Iter-6680, train loss-0.2145, acc-0.7100, valid loss-0.2208, acc-0.7366, test loss-0.2234, acc-0.7305\n",
      "Iter-6690, train loss-0.2041, acc-0.7500, valid loss-0.2205, acc-0.7338, test loss-0.2232, acc-0.7290\n",
      "Iter-6700, train loss-0.2116, acc-0.7300, valid loss-0.2205, acc-0.7380, test loss-0.2231, acc-0.7321\n",
      "Iter-6710, train loss-0.2328, acc-0.7200, valid loss-0.2206, acc-0.7376, test loss-0.2233, acc-0.7312\n",
      "Iter-6720, train loss-0.2302, acc-0.7300, valid loss-0.2210, acc-0.7380, test loss-0.2237, acc-0.7305\n",
      "Iter-6730, train loss-0.1867, acc-0.7600, valid loss-0.2209, acc-0.7376, test loss-0.2236, acc-0.7304\n",
      "Iter-6740, train loss-0.2148, acc-0.7700, valid loss-0.2207, acc-0.7370, test loss-0.2234, acc-0.7296\n",
      "Iter-6750, train loss-0.1892, acc-0.8200, valid loss-0.2203, acc-0.7372, test loss-0.2228, acc-0.7306\n",
      "Iter-6760, train loss-0.1687, acc-0.8000, valid loss-0.2200, acc-0.7368, test loss-0.2227, acc-0.7299\n",
      "Iter-6770, train loss-0.2061, acc-0.7500, valid loss-0.2201, acc-0.7384, test loss-0.2227, acc-0.7298\n",
      "Iter-6780, train loss-0.2290, acc-0.7200, valid loss-0.2201, acc-0.7372, test loss-0.2226, acc-0.7304\n",
      "Iter-6790, train loss-0.2331, acc-0.7300, valid loss-0.2199, acc-0.7372, test loss-0.2224, acc-0.7281\n",
      "Iter-6800, train loss-0.2609, acc-0.7000, valid loss-0.2199, acc-0.7362, test loss-0.2223, acc-0.7287\n",
      "Iter-6810, train loss-0.2131, acc-0.7600, valid loss-0.2199, acc-0.7366, test loss-0.2222, acc-0.7301\n",
      "Iter-6820, train loss-0.2297, acc-0.7200, valid loss-0.2198, acc-0.7360, test loss-0.2220, acc-0.7306\n",
      "Iter-6830, train loss-0.2055, acc-0.7200, valid loss-0.2198, acc-0.7376, test loss-0.2218, acc-0.7309\n",
      "Iter-6840, train loss-0.2160, acc-0.7000, valid loss-0.2197, acc-0.7382, test loss-0.2218, acc-0.7302\n",
      "Iter-6850, train loss-0.2186, acc-0.7100, valid loss-0.2195, acc-0.7384, test loss-0.2215, acc-0.7313\n",
      "Iter-6860, train loss-0.2376, acc-0.7100, valid loss-0.2196, acc-0.7384, test loss-0.2215, acc-0.7313\n",
      "Iter-6870, train loss-0.2043, acc-0.7700, valid loss-0.2194, acc-0.7382, test loss-0.2212, acc-0.7306\n",
      "Iter-6880, train loss-0.2109, acc-0.7300, valid loss-0.2190, acc-0.7380, test loss-0.2208, acc-0.7305\n",
      "Iter-6890, train loss-0.1709, acc-0.7900, valid loss-0.2191, acc-0.7392, test loss-0.2210, acc-0.7298\n",
      "Iter-6900, train loss-0.1951, acc-0.7900, valid loss-0.2192, acc-0.7376, test loss-0.2211, acc-0.7288\n",
      "Iter-6910, train loss-0.2301, acc-0.7200, valid loss-0.2194, acc-0.7382, test loss-0.2212, acc-0.7288\n",
      "Iter-6920, train loss-0.2676, acc-0.6500, valid loss-0.2193, acc-0.7362, test loss-0.2211, acc-0.7270\n",
      "Iter-6930, train loss-0.2083, acc-0.7700, valid loss-0.2194, acc-0.7366, test loss-0.2211, acc-0.7288\n",
      "Iter-6940, train loss-0.2106, acc-0.7800, valid loss-0.2193, acc-0.7384, test loss-0.2209, acc-0.7302\n",
      "Iter-6950, train loss-0.2549, acc-0.6700, valid loss-0.2191, acc-0.7374, test loss-0.2207, acc-0.7301\n",
      "Iter-6960, train loss-0.2074, acc-0.7500, valid loss-0.2191, acc-0.7370, test loss-0.2206, acc-0.7288\n",
      "Iter-6970, train loss-0.2592, acc-0.6900, valid loss-0.2190, acc-0.7378, test loss-0.2206, acc-0.7305\n",
      "Iter-6980, train loss-0.2249, acc-0.7000, valid loss-0.2190, acc-0.7390, test loss-0.2206, acc-0.7314\n",
      "Iter-6990, train loss-0.2116, acc-0.7400, valid loss-0.2188, acc-0.7368, test loss-0.2203, acc-0.7303\n",
      "Iter-7000, train loss-0.2065, acc-0.7100, valid loss-0.2188, acc-0.7350, test loss-0.2203, acc-0.7308\n",
      "Iter-7010, train loss-0.2575, acc-0.6700, valid loss-0.2187, acc-0.7362, test loss-0.2201, acc-0.7312\n",
      "Iter-7020, train loss-0.2272, acc-0.7500, valid loss-0.2186, acc-0.7334, test loss-0.2199, acc-0.7291\n",
      "Iter-7030, train loss-0.2378, acc-0.7200, valid loss-0.2184, acc-0.7336, test loss-0.2196, acc-0.7296\n",
      "Iter-7040, train loss-0.2298, acc-0.6900, valid loss-0.2185, acc-0.7324, test loss-0.2198, acc-0.7285\n",
      "Iter-7050, train loss-0.1858, acc-0.7600, valid loss-0.2187, acc-0.7290, test loss-0.2199, acc-0.7274\n",
      "Iter-7060, train loss-0.2080, acc-0.7200, valid loss-0.2184, acc-0.7308, test loss-0.2194, acc-0.7287\n",
      "Iter-7070, train loss-0.2512, acc-0.6200, valid loss-0.2184, acc-0.7316, test loss-0.2192, acc-0.7304\n",
      "Iter-7080, train loss-0.2218, acc-0.7300, valid loss-0.2184, acc-0.7320, test loss-0.2191, acc-0.7307\n",
      "Iter-7090, train loss-0.1809, acc-0.8100, valid loss-0.2186, acc-0.7324, test loss-0.2193, acc-0.7302\n",
      "Iter-7100, train loss-0.2262, acc-0.6700, valid loss-0.2181, acc-0.7326, test loss-0.2188, acc-0.7311\n",
      "Iter-7110, train loss-0.2576, acc-0.6500, valid loss-0.2182, acc-0.7354, test loss-0.2188, acc-0.7327\n",
      "Iter-7120, train loss-0.2223, acc-0.7500, valid loss-0.2183, acc-0.7336, test loss-0.2189, acc-0.7307\n",
      "Iter-7130, train loss-0.2147, acc-0.7500, valid loss-0.2182, acc-0.7336, test loss-0.2187, acc-0.7312\n",
      "Iter-7140, train loss-0.2151, acc-0.7100, valid loss-0.2178, acc-0.7354, test loss-0.2182, acc-0.7338\n",
      "Iter-7150, train loss-0.2233, acc-0.6800, valid loss-0.2177, acc-0.7348, test loss-0.2183, acc-0.7334\n",
      "Iter-7160, train loss-0.2256, acc-0.7600, valid loss-0.2174, acc-0.7374, test loss-0.2178, acc-0.7345\n",
      "Iter-7170, train loss-0.1786, acc-0.8300, valid loss-0.2175, acc-0.7386, test loss-0.2178, acc-0.7352\n",
      "Iter-7180, train loss-0.2243, acc-0.6900, valid loss-0.2171, acc-0.7396, test loss-0.2174, acc-0.7356\n",
      "Iter-7190, train loss-0.2182, acc-0.7400, valid loss-0.2169, acc-0.7358, test loss-0.2171, acc-0.7348\n",
      "Iter-7200, train loss-0.2138, acc-0.7400, valid loss-0.2169, acc-0.7366, test loss-0.2169, acc-0.7360\n",
      "Iter-7210, train loss-0.2621, acc-0.6600, valid loss-0.2171, acc-0.7348, test loss-0.2171, acc-0.7357\n",
      "Iter-7220, train loss-0.2310, acc-0.7200, valid loss-0.2172, acc-0.7334, test loss-0.2170, acc-0.7355\n",
      "Iter-7230, train loss-0.2100, acc-0.7800, valid loss-0.2172, acc-0.7330, test loss-0.2168, acc-0.7361\n",
      "Iter-7240, train loss-0.2009, acc-0.7900, valid loss-0.2174, acc-0.7360, test loss-0.2169, acc-0.7375\n",
      "Iter-7250, train loss-0.2314, acc-0.6500, valid loss-0.2173, acc-0.7338, test loss-0.2168, acc-0.7357\n",
      "Iter-7260, train loss-0.2104, acc-0.7200, valid loss-0.2176, acc-0.7340, test loss-0.2170, acc-0.7356\n",
      "Iter-7270, train loss-0.2422, acc-0.6500, valid loss-0.2174, acc-0.7326, test loss-0.2168, acc-0.7338\n",
      "Iter-7280, train loss-0.2202, acc-0.6900, valid loss-0.2175, acc-0.7324, test loss-0.2169, acc-0.7345\n",
      "Iter-7290, train loss-0.2197, acc-0.7600, valid loss-0.2172, acc-0.7348, test loss-0.2166, acc-0.7357\n",
      "Iter-7300, train loss-0.1934, acc-0.7800, valid loss-0.2171, acc-0.7322, test loss-0.2165, acc-0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-0.2055, acc-0.7400, valid loss-0.2168, acc-0.7360, test loss-0.2160, acc-0.7368\n",
      "Iter-7320, train loss-0.1902, acc-0.7500, valid loss-0.2170, acc-0.7336, test loss-0.2161, acc-0.7361\n",
      "Iter-7330, train loss-0.2388, acc-0.6800, valid loss-0.2168, acc-0.7364, test loss-0.2158, acc-0.7381\n",
      "Iter-7340, train loss-0.2419, acc-0.7200, valid loss-0.2170, acc-0.7358, test loss-0.2159, acc-0.7380\n",
      "Iter-7350, train loss-0.2405, acc-0.6700, valid loss-0.2169, acc-0.7358, test loss-0.2157, acc-0.7394\n",
      "Iter-7360, train loss-0.2338, acc-0.7000, valid loss-0.2169, acc-0.7338, test loss-0.2156, acc-0.7380\n",
      "Iter-7370, train loss-0.2125, acc-0.7300, valid loss-0.2171, acc-0.7358, test loss-0.2157, acc-0.7376\n",
      "Iter-7380, train loss-0.2040, acc-0.7600, valid loss-0.2173, acc-0.7330, test loss-0.2160, acc-0.7380\n",
      "Iter-7390, train loss-0.2117, acc-0.7400, valid loss-0.2174, acc-0.7338, test loss-0.2160, acc-0.7370\n",
      "Iter-7400, train loss-0.2503, acc-0.7000, valid loss-0.2171, acc-0.7342, test loss-0.2157, acc-0.7383\n",
      "Iter-7410, train loss-0.1856, acc-0.7700, valid loss-0.2177, acc-0.7348, test loss-0.2162, acc-0.7367\n",
      "Iter-7420, train loss-0.2109, acc-0.7200, valid loss-0.2180, acc-0.7316, test loss-0.2164, acc-0.7354\n",
      "Iter-7430, train loss-0.2237, acc-0.7600, valid loss-0.2179, acc-0.7312, test loss-0.2162, acc-0.7366\n",
      "Iter-7440, train loss-0.2297, acc-0.6700, valid loss-0.2183, acc-0.7318, test loss-0.2165, acc-0.7362\n",
      "Iter-7450, train loss-0.2129, acc-0.7000, valid loss-0.2183, acc-0.7314, test loss-0.2164, acc-0.7367\n",
      "Iter-7460, train loss-0.2260, acc-0.7300, valid loss-0.2187, acc-0.7306, test loss-0.2167, acc-0.7355\n",
      "Iter-7470, train loss-0.2309, acc-0.6800, valid loss-0.2193, acc-0.7292, test loss-0.2173, acc-0.7333\n",
      "Iter-7480, train loss-0.2562, acc-0.6800, valid loss-0.2192, acc-0.7272, test loss-0.2172, acc-0.7324\n",
      "Iter-7490, train loss-0.2828, acc-0.5900, valid loss-0.2195, acc-0.7274, test loss-0.2174, acc-0.7333\n",
      "Iter-7500, train loss-0.2335, acc-0.7000, valid loss-0.2202, acc-0.7252, test loss-0.2181, acc-0.7317\n",
      "Iter-7510, train loss-0.2127, acc-0.7500, valid loss-0.2202, acc-0.7254, test loss-0.2180, acc-0.7316\n",
      "Iter-7520, train loss-0.2337, acc-0.6900, valid loss-0.2207, acc-0.7232, test loss-0.2184, acc-0.7302\n",
      "Iter-7530, train loss-0.2039, acc-0.7600, valid loss-0.2210, acc-0.7248, test loss-0.2185, acc-0.7298\n",
      "Iter-7540, train loss-0.2392, acc-0.6700, valid loss-0.2219, acc-0.7238, test loss-0.2191, acc-0.7287\n",
      "Iter-7550, train loss-0.2531, acc-0.6400, valid loss-0.2220, acc-0.7248, test loss-0.2192, acc-0.7288\n",
      "Iter-7560, train loss-0.2368, acc-0.7000, valid loss-0.2221, acc-0.7228, test loss-0.2192, acc-0.7276\n",
      "Iter-7570, train loss-0.2506, acc-0.6500, valid loss-0.2222, acc-0.7232, test loss-0.2192, acc-0.7292\n",
      "Iter-7580, train loss-0.2378, acc-0.7000, valid loss-0.2227, acc-0.7232, test loss-0.2196, acc-0.7274\n",
      "Iter-7590, train loss-0.2190, acc-0.6900, valid loss-0.2230, acc-0.7236, test loss-0.2197, acc-0.7282\n",
      "Iter-7600, train loss-0.2608, acc-0.6500, valid loss-0.2229, acc-0.7228, test loss-0.2197, acc-0.7283\n",
      "Iter-7610, train loss-0.2485, acc-0.7300, valid loss-0.2230, acc-0.7220, test loss-0.2197, acc-0.7277\n",
      "Iter-7620, train loss-0.2118, acc-0.7600, valid loss-0.2236, acc-0.7206, test loss-0.2202, acc-0.7268\n",
      "Iter-7630, train loss-0.1910, acc-0.7400, valid loss-0.2238, acc-0.7202, test loss-0.2204, acc-0.7262\n",
      "Iter-7640, train loss-0.2099, acc-0.7200, valid loss-0.2246, acc-0.7194, test loss-0.2209, acc-0.7261\n",
      "Iter-7650, train loss-0.2323, acc-0.7400, valid loss-0.2243, acc-0.7202, test loss-0.2206, acc-0.7253\n",
      "Iter-7660, train loss-0.1961, acc-0.7300, valid loss-0.2247, acc-0.7190, test loss-0.2210, acc-0.7235\n",
      "Iter-7670, train loss-0.2429, acc-0.6900, valid loss-0.2243, acc-0.7180, test loss-0.2206, acc-0.7239\n",
      "Iter-7680, train loss-0.2270, acc-0.7100, valid loss-0.2246, acc-0.7178, test loss-0.2209, acc-0.7240\n",
      "Iter-7690, train loss-0.2061, acc-0.7700, valid loss-0.2236, acc-0.7190, test loss-0.2200, acc-0.7255\n",
      "Iter-7700, train loss-0.2333, acc-0.7000, valid loss-0.2242, acc-0.7190, test loss-0.2203, acc-0.7248\n",
      "Iter-7710, train loss-0.1982, acc-0.7900, valid loss-0.2244, acc-0.7182, test loss-0.2206, acc-0.7239\n",
      "Iter-7720, train loss-0.2074, acc-0.7700, valid loss-0.2247, acc-0.7178, test loss-0.2209, acc-0.7236\n",
      "Iter-7730, train loss-0.1929, acc-0.7700, valid loss-0.2251, acc-0.7170, test loss-0.2213, acc-0.7225\n",
      "Iter-7740, train loss-0.2437, acc-0.6900, valid loss-0.2251, acc-0.7170, test loss-0.2214, acc-0.7223\n",
      "Iter-7750, train loss-0.1904, acc-0.7500, valid loss-0.2255, acc-0.7156, test loss-0.2219, acc-0.7231\n",
      "Iter-7760, train loss-0.2027, acc-0.7400, valid loss-0.2246, acc-0.7180, test loss-0.2211, acc-0.7232\n",
      "Iter-7770, train loss-0.2294, acc-0.7100, valid loss-0.2247, acc-0.7182, test loss-0.2211, acc-0.7236\n",
      "Iter-7780, train loss-0.2505, acc-0.7000, valid loss-0.2249, acc-0.7180, test loss-0.2214, acc-0.7220\n",
      "Iter-7790, train loss-0.2312, acc-0.6900, valid loss-0.2252, acc-0.7180, test loss-0.2216, acc-0.7226\n",
      "Iter-7800, train loss-0.2069, acc-0.7200, valid loss-0.2259, acc-0.7140, test loss-0.2224, acc-0.7211\n",
      "Iter-7810, train loss-0.2577, acc-0.6400, valid loss-0.2260, acc-0.7132, test loss-0.2225, acc-0.7207\n",
      "Iter-7820, train loss-0.2195, acc-0.6900, valid loss-0.2263, acc-0.7118, test loss-0.2228, acc-0.7199\n",
      "Iter-7830, train loss-0.2125, acc-0.7400, valid loss-0.2267, acc-0.7104, test loss-0.2233, acc-0.7168\n",
      "Iter-7840, train loss-0.2492, acc-0.6900, valid loss-0.2266, acc-0.7096, test loss-0.2233, acc-0.7154\n",
      "Iter-7850, train loss-0.2389, acc-0.6900, valid loss-0.2268, acc-0.7108, test loss-0.2235, acc-0.7157\n",
      "Iter-7860, train loss-0.2265, acc-0.6900, valid loss-0.2270, acc-0.7106, test loss-0.2237, acc-0.7143\n",
      "Iter-7870, train loss-0.2372, acc-0.7300, valid loss-0.2272, acc-0.7088, test loss-0.2240, acc-0.7142\n",
      "Iter-7880, train loss-0.2277, acc-0.6900, valid loss-0.2265, acc-0.7112, test loss-0.2234, acc-0.7170\n",
      "Iter-7890, train loss-0.2295, acc-0.6600, valid loss-0.2269, acc-0.7098, test loss-0.2237, acc-0.7164\n",
      "Iter-7900, train loss-0.1847, acc-0.8100, valid loss-0.2275, acc-0.7070, test loss-0.2243, acc-0.7135\n",
      "Iter-7910, train loss-0.2449, acc-0.6600, valid loss-0.2270, acc-0.7088, test loss-0.2239, acc-0.7156\n",
      "Iter-7920, train loss-0.2578, acc-0.6400, valid loss-0.2271, acc-0.7084, test loss-0.2242, acc-0.7148\n",
      "Iter-7930, train loss-0.2317, acc-0.7100, valid loss-0.2277, acc-0.7072, test loss-0.2247, acc-0.7145\n",
      "Iter-7940, train loss-0.2054, acc-0.7200, valid loss-0.2279, acc-0.7050, test loss-0.2249, acc-0.7149\n",
      "Iter-7950, train loss-0.2401, acc-0.6800, valid loss-0.2282, acc-0.7048, test loss-0.2253, acc-0.7132\n",
      "Iter-7960, train loss-0.2048, acc-0.7200, valid loss-0.2285, acc-0.7038, test loss-0.2256, acc-0.7126\n",
      "Iter-7970, train loss-0.2072, acc-0.7700, valid loss-0.2285, acc-0.7038, test loss-0.2256, acc-0.7134\n",
      "Iter-7980, train loss-0.2237, acc-0.7300, valid loss-0.2285, acc-0.7038, test loss-0.2257, acc-0.7139\n",
      "Iter-7990, train loss-0.2761, acc-0.6700, valid loss-0.2287, acc-0.7044, test loss-0.2260, acc-0.7139\n",
      "Iter-8000, train loss-0.2919, acc-0.5800, valid loss-0.2289, acc-0.7018, test loss-0.2263, acc-0.7136\n",
      "Iter-8010, train loss-0.2410, acc-0.7400, valid loss-0.2293, acc-0.7010, test loss-0.2267, acc-0.7117\n",
      "Iter-8020, train loss-0.2166, acc-0.7300, valid loss-0.2292, acc-0.7008, test loss-0.2267, acc-0.7101\n",
      "Iter-8030, train loss-0.1922, acc-0.7600, valid loss-0.2293, acc-0.7016, test loss-0.2268, acc-0.7097\n",
      "Iter-8040, train loss-0.2040, acc-0.7400, valid loss-0.2293, acc-0.7008, test loss-0.2268, acc-0.7096\n",
      "Iter-8050, train loss-0.2530, acc-0.6500, valid loss-0.2298, acc-0.7008, test loss-0.2273, acc-0.7086\n",
      "Iter-8060, train loss-0.2497, acc-0.7000, valid loss-0.2299, acc-0.7006, test loss-0.2275, acc-0.7086\n",
      "Iter-8070, train loss-0.1986, acc-0.7600, valid loss-0.2303, acc-0.6996, test loss-0.2279, acc-0.7065\n",
      "Iter-8080, train loss-0.2035, acc-0.7400, valid loss-0.2302, acc-0.7002, test loss-0.2277, acc-0.7073\n",
      "Iter-8090, train loss-0.2448, acc-0.6700, valid loss-0.2304, acc-0.6994, test loss-0.2280, acc-0.7054\n",
      "Iter-8100, train loss-0.2422, acc-0.6700, valid loss-0.2305, acc-0.6982, test loss-0.2282, acc-0.7052\n",
      "Iter-8110, train loss-0.2782, acc-0.6300, valid loss-0.2298, acc-0.6996, test loss-0.2275, acc-0.7061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-0.2431, acc-0.7100, valid loss-0.2298, acc-0.6988, test loss-0.2275, acc-0.7043\n",
      "Iter-8130, train loss-0.2287, acc-0.7100, valid loss-0.2295, acc-0.6994, test loss-0.2273, acc-0.7049\n",
      "Iter-8140, train loss-0.2307, acc-0.7300, valid loss-0.2300, acc-0.6982, test loss-0.2279, acc-0.7040\n",
      "Iter-8150, train loss-0.1824, acc-0.7900, valid loss-0.2302, acc-0.6982, test loss-0.2279, acc-0.7045\n",
      "Iter-8160, train loss-0.2299, acc-0.7200, valid loss-0.2297, acc-0.6986, test loss-0.2276, acc-0.7036\n",
      "Iter-8170, train loss-0.1988, acc-0.7800, valid loss-0.2297, acc-0.6988, test loss-0.2277, acc-0.7016\n",
      "Iter-8180, train loss-0.2423, acc-0.6400, valid loss-0.2288, acc-0.7010, test loss-0.2269, acc-0.7041\n",
      "Iter-8190, train loss-0.2570, acc-0.6700, valid loss-0.2291, acc-0.6998, test loss-0.2273, acc-0.7044\n",
      "Iter-8200, train loss-0.2241, acc-0.7000, valid loss-0.2286, acc-0.6988, test loss-0.2268, acc-0.7043\n",
      "Iter-8210, train loss-0.2318, acc-0.7100, valid loss-0.2284, acc-0.7014, test loss-0.2265, acc-0.7049\n",
      "Iter-8220, train loss-0.2259, acc-0.6900, valid loss-0.2279, acc-0.6996, test loss-0.2261, acc-0.7051\n",
      "Iter-8230, train loss-0.2305, acc-0.6900, valid loss-0.2277, acc-0.6996, test loss-0.2260, acc-0.7039\n",
      "Iter-8240, train loss-0.2455, acc-0.7000, valid loss-0.2274, acc-0.6990, test loss-0.2258, acc-0.7042\n",
      "Iter-8250, train loss-0.2222, acc-0.7500, valid loss-0.2278, acc-0.6976, test loss-0.2262, acc-0.7042\n",
      "Iter-8260, train loss-0.1856, acc-0.7600, valid loss-0.2282, acc-0.6964, test loss-0.2266, acc-0.7029\n",
      "Iter-8270, train loss-0.2171, acc-0.7100, valid loss-0.2279, acc-0.6972, test loss-0.2264, acc-0.7021\n",
      "Iter-8280, train loss-0.1917, acc-0.8000, valid loss-0.2278, acc-0.6976, test loss-0.2263, acc-0.7006\n",
      "Iter-8290, train loss-0.2482, acc-0.6000, valid loss-0.2278, acc-0.6986, test loss-0.2263, acc-0.7012\n",
      "Iter-8300, train loss-0.2511, acc-0.6800, valid loss-0.2276, acc-0.6988, test loss-0.2260, acc-0.7014\n",
      "Iter-8310, train loss-0.2362, acc-0.7300, valid loss-0.2282, acc-0.6976, test loss-0.2267, acc-0.6989\n",
      "Iter-8320, train loss-0.2471, acc-0.6500, valid loss-0.2282, acc-0.6968, test loss-0.2267, acc-0.7004\n",
      "Iter-8330, train loss-0.2312, acc-0.7400, valid loss-0.2282, acc-0.6966, test loss-0.2267, acc-0.7001\n",
      "Iter-8340, train loss-0.1915, acc-0.7800, valid loss-0.2279, acc-0.7006, test loss-0.2263, acc-0.7043\n",
      "Iter-8350, train loss-0.2323, acc-0.6900, valid loss-0.2277, acc-0.7008, test loss-0.2262, acc-0.7056\n",
      "Iter-8360, train loss-0.2606, acc-0.6700, valid loss-0.2271, acc-0.7034, test loss-0.2257, acc-0.7074\n",
      "Iter-8370, train loss-0.2563, acc-0.6800, valid loss-0.2273, acc-0.6998, test loss-0.2259, acc-0.7038\n",
      "Iter-8380, train loss-0.2419, acc-0.6700, valid loss-0.2271, acc-0.7026, test loss-0.2257, acc-0.7073\n",
      "Iter-8390, train loss-0.1886, acc-0.7800, valid loss-0.2271, acc-0.7032, test loss-0.2257, acc-0.7070\n",
      "Iter-8400, train loss-0.2021, acc-0.7500, valid loss-0.2271, acc-0.7032, test loss-0.2257, acc-0.7069\n",
      "Iter-8410, train loss-0.2355, acc-0.6900, valid loss-0.2267, acc-0.7060, test loss-0.2253, acc-0.7093\n",
      "Iter-8420, train loss-0.2162, acc-0.7200, valid loss-0.2267, acc-0.7062, test loss-0.2254, acc-0.7088\n",
      "Iter-8430, train loss-0.1869, acc-0.8000, valid loss-0.2265, acc-0.7054, test loss-0.2252, acc-0.7092\n",
      "Iter-8440, train loss-0.2391, acc-0.6500, valid loss-0.2270, acc-0.7060, test loss-0.2257, acc-0.7093\n",
      "Iter-8450, train loss-0.2470, acc-0.6500, valid loss-0.2269, acc-0.7040, test loss-0.2256, acc-0.7084\n",
      "Iter-8460, train loss-0.2115, acc-0.7400, valid loss-0.2271, acc-0.7052, test loss-0.2258, acc-0.7085\n",
      "Iter-8470, train loss-0.1696, acc-0.8000, valid loss-0.2271, acc-0.7060, test loss-0.2258, acc-0.7080\n",
      "Iter-8480, train loss-0.1985, acc-0.7700, valid loss-0.2262, acc-0.7070, test loss-0.2250, acc-0.7085\n",
      "Iter-8490, train loss-0.2529, acc-0.6500, valid loss-0.2258, acc-0.7088, test loss-0.2246, acc-0.7113\n",
      "Iter-8500, train loss-0.2413, acc-0.6900, valid loss-0.2260, acc-0.7090, test loss-0.2248, acc-0.7113\n",
      "Iter-8510, train loss-0.1760, acc-0.8000, valid loss-0.2266, acc-0.7072, test loss-0.2254, acc-0.7116\n",
      "Iter-8520, train loss-0.2205, acc-0.7700, valid loss-0.2259, acc-0.7078, test loss-0.2248, acc-0.7126\n",
      "Iter-8530, train loss-0.2320, acc-0.6900, valid loss-0.2263, acc-0.7070, test loss-0.2251, acc-0.7113\n",
      "Iter-8540, train loss-0.2143, acc-0.7100, valid loss-0.2259, acc-0.7086, test loss-0.2248, acc-0.7118\n",
      "Iter-8550, train loss-0.2487, acc-0.6700, valid loss-0.2262, acc-0.7086, test loss-0.2251, acc-0.7112\n",
      "Iter-8560, train loss-0.2514, acc-0.6800, valid loss-0.2266, acc-0.7074, test loss-0.2256, acc-0.7094\n",
      "Iter-8570, train loss-0.2074, acc-0.7700, valid loss-0.2267, acc-0.7082, test loss-0.2257, acc-0.7097\n",
      "Iter-8580, train loss-0.2077, acc-0.7500, valid loss-0.2266, acc-0.7074, test loss-0.2257, acc-0.7093\n",
      "Iter-8590, train loss-0.2543, acc-0.6400, valid loss-0.2263, acc-0.7082, test loss-0.2254, acc-0.7107\n",
      "Iter-8600, train loss-0.2122, acc-0.6900, valid loss-0.2263, acc-0.7076, test loss-0.2256, acc-0.7106\n",
      "Iter-8610, train loss-0.2167, acc-0.7400, valid loss-0.2262, acc-0.7062, test loss-0.2255, acc-0.7097\n",
      "Iter-8620, train loss-0.3037, acc-0.5700, valid loss-0.2260, acc-0.7058, test loss-0.2254, acc-0.7119\n",
      "Iter-8630, train loss-0.2526, acc-0.6800, valid loss-0.2260, acc-0.7056, test loss-0.2253, acc-0.7120\n",
      "Iter-8640, train loss-0.1974, acc-0.7700, valid loss-0.2257, acc-0.7058, test loss-0.2250, acc-0.7118\n",
      "Iter-8650, train loss-0.2361, acc-0.7100, valid loss-0.2257, acc-0.7036, test loss-0.2251, acc-0.7118\n",
      "Iter-8660, train loss-0.2411, acc-0.7300, valid loss-0.2254, acc-0.7066, test loss-0.2247, acc-0.7141\n",
      "Iter-8670, train loss-0.2475, acc-0.6800, valid loss-0.2258, acc-0.7044, test loss-0.2250, acc-0.7110\n",
      "Iter-8680, train loss-0.2450, acc-0.6600, valid loss-0.2260, acc-0.7034, test loss-0.2251, acc-0.7116\n",
      "Iter-8690, train loss-0.2549, acc-0.6800, valid loss-0.2262, acc-0.7032, test loss-0.2255, acc-0.7102\n",
      "Iter-8700, train loss-0.2477, acc-0.6700, valid loss-0.2262, acc-0.7024, test loss-0.2256, acc-0.7104\n",
      "Iter-8710, train loss-0.2533, acc-0.6700, valid loss-0.2259, acc-0.7016, test loss-0.2252, acc-0.7105\n",
      "Iter-8720, train loss-0.2298, acc-0.7100, valid loss-0.2256, acc-0.7014, test loss-0.2248, acc-0.7105\n",
      "Iter-8730, train loss-0.2401, acc-0.6900, valid loss-0.2251, acc-0.7008, test loss-0.2244, acc-0.7107\n",
      "Iter-8740, train loss-0.1776, acc-0.8000, valid loss-0.2251, acc-0.7004, test loss-0.2243, acc-0.7119\n",
      "Iter-8750, train loss-0.2078, acc-0.7300, valid loss-0.2249, acc-0.7018, test loss-0.2242, acc-0.7122\n",
      "Iter-8760, train loss-0.2345, acc-0.7000, valid loss-0.2246, acc-0.7014, test loss-0.2238, acc-0.7125\n",
      "Iter-8770, train loss-0.2366, acc-0.7100, valid loss-0.2245, acc-0.7006, test loss-0.2237, acc-0.7130\n",
      "Iter-8780, train loss-0.2276, acc-0.7100, valid loss-0.2246, acc-0.7016, test loss-0.2238, acc-0.7116\n",
      "Iter-8790, train loss-0.2101, acc-0.8000, valid loss-0.2243, acc-0.7016, test loss-0.2236, acc-0.7115\n",
      "Iter-8800, train loss-0.2832, acc-0.5700, valid loss-0.2242, acc-0.7020, test loss-0.2235, acc-0.7123\n",
      "Iter-8810, train loss-0.2271, acc-0.7100, valid loss-0.2238, acc-0.7018, test loss-0.2231, acc-0.7118\n",
      "Iter-8820, train loss-0.2251, acc-0.6800, valid loss-0.2237, acc-0.7022, test loss-0.2230, acc-0.7120\n",
      "Iter-8830, train loss-0.2266, acc-0.6900, valid loss-0.2236, acc-0.7020, test loss-0.2228, acc-0.7127\n",
      "Iter-8840, train loss-0.2264, acc-0.7000, valid loss-0.2239, acc-0.7006, test loss-0.2232, acc-0.7108\n",
      "Iter-8850, train loss-0.2276, acc-0.6500, valid loss-0.2238, acc-0.6994, test loss-0.2231, acc-0.7106\n",
      "Iter-8860, train loss-0.2799, acc-0.6000, valid loss-0.2238, acc-0.7012, test loss-0.2230, acc-0.7106\n",
      "Iter-8870, train loss-0.2701, acc-0.6400, valid loss-0.2238, acc-0.6996, test loss-0.2232, acc-0.7097\n",
      "Iter-8880, train loss-0.2336, acc-0.6800, valid loss-0.2236, acc-0.7002, test loss-0.2231, acc-0.7093\n",
      "Iter-8890, train loss-0.2460, acc-0.6700, valid loss-0.2237, acc-0.6994, test loss-0.2233, acc-0.7082\n",
      "Iter-8900, train loss-0.2130, acc-0.7300, valid loss-0.2238, acc-0.6996, test loss-0.2234, acc-0.7088\n",
      "Iter-8910, train loss-0.2214, acc-0.7400, valid loss-0.2242, acc-0.6992, test loss-0.2237, acc-0.7080\n",
      "Iter-8920, train loss-0.2686, acc-0.6300, valid loss-0.2239, acc-0.6994, test loss-0.2234, acc-0.7087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-0.1909, acc-0.7700, valid loss-0.2241, acc-0.7002, test loss-0.2237, acc-0.7090\n",
      "Iter-8940, train loss-0.2704, acc-0.6100, valid loss-0.2240, acc-0.6998, test loss-0.2235, acc-0.7096\n",
      "Iter-8950, train loss-0.2395, acc-0.7000, valid loss-0.2241, acc-0.6996, test loss-0.2235, acc-0.7095\n",
      "Iter-8960, train loss-0.2511, acc-0.6500, valid loss-0.2246, acc-0.6994, test loss-0.2239, acc-0.7097\n",
      "Iter-8970, train loss-0.2467, acc-0.6700, valid loss-0.2244, acc-0.7000, test loss-0.2238, acc-0.7102\n",
      "Iter-8980, train loss-0.2160, acc-0.7100, valid loss-0.2243, acc-0.6996, test loss-0.2237, acc-0.7097\n",
      "Iter-8990, train loss-0.2262, acc-0.6900, valid loss-0.2242, acc-0.6996, test loss-0.2236, acc-0.7097\n",
      "Iter-9000, train loss-0.1943, acc-0.7300, valid loss-0.2238, acc-0.6994, test loss-0.2233, acc-0.7100\n",
      "Iter-9010, train loss-0.2239, acc-0.7100, valid loss-0.2239, acc-0.6988, test loss-0.2233, acc-0.7097\n",
      "Iter-9020, train loss-0.1899, acc-0.7800, valid loss-0.2244, acc-0.6982, test loss-0.2238, acc-0.7086\n",
      "Iter-9030, train loss-0.2177, acc-0.7300, valid loss-0.2243, acc-0.6978, test loss-0.2237, acc-0.7079\n",
      "Iter-9040, train loss-0.2213, acc-0.7100, valid loss-0.2247, acc-0.6982, test loss-0.2240, acc-0.7078\n",
      "Iter-9050, train loss-0.2339, acc-0.6800, valid loss-0.2251, acc-0.6966, test loss-0.2243, acc-0.7068\n",
      "Iter-9060, train loss-0.2345, acc-0.6600, valid loss-0.2252, acc-0.6954, test loss-0.2243, acc-0.7065\n",
      "Iter-9070, train loss-0.2614, acc-0.6300, valid loss-0.2253, acc-0.6956, test loss-0.2245, acc-0.7066\n",
      "Iter-9080, train loss-0.1871, acc-0.7400, valid loss-0.2254, acc-0.6944, test loss-0.2246, acc-0.7045\n",
      "Iter-9090, train loss-0.2670, acc-0.6400, valid loss-0.2253, acc-0.6952, test loss-0.2245, acc-0.7057\n",
      "Iter-9100, train loss-0.2643, acc-0.6500, valid loss-0.2250, acc-0.6950, test loss-0.2244, acc-0.7042\n",
      "Iter-9110, train loss-0.2029, acc-0.7700, valid loss-0.2248, acc-0.6952, test loss-0.2241, acc-0.7031\n",
      "Iter-9120, train loss-0.2109, acc-0.7200, valid loss-0.2246, acc-0.6950, test loss-0.2239, acc-0.7040\n",
      "Iter-9130, train loss-0.2133, acc-0.7500, valid loss-0.2243, acc-0.6946, test loss-0.2235, acc-0.7044\n",
      "Iter-9140, train loss-0.2568, acc-0.6400, valid loss-0.2241, acc-0.6958, test loss-0.2233, acc-0.7036\n",
      "Iter-9150, train loss-0.2500, acc-0.6800, valid loss-0.2245, acc-0.6944, test loss-0.2236, acc-0.7034\n",
      "Iter-9160, train loss-0.2356, acc-0.7200, valid loss-0.2241, acc-0.6940, test loss-0.2233, acc-0.7035\n",
      "Iter-9170, train loss-0.2283, acc-0.6500, valid loss-0.2242, acc-0.6944, test loss-0.2234, acc-0.7040\n",
      "Iter-9180, train loss-0.2595, acc-0.6100, valid loss-0.2241, acc-0.6958, test loss-0.2233, acc-0.7048\n",
      "Iter-9190, train loss-0.2458, acc-0.6300, valid loss-0.2241, acc-0.6952, test loss-0.2232, acc-0.7045\n",
      "Iter-9200, train loss-0.2161, acc-0.6900, valid loss-0.2242, acc-0.6968, test loss-0.2234, acc-0.7047\n",
      "Iter-9210, train loss-0.2077, acc-0.7100, valid loss-0.2238, acc-0.6964, test loss-0.2228, acc-0.7056\n",
      "Iter-9220, train loss-0.2874, acc-0.5900, valid loss-0.2237, acc-0.6946, test loss-0.2228, acc-0.7046\n",
      "Iter-9230, train loss-0.2027, acc-0.7200, valid loss-0.2237, acc-0.6968, test loss-0.2227, acc-0.7057\n",
      "Iter-9240, train loss-0.2452, acc-0.6300, valid loss-0.2234, acc-0.6968, test loss-0.2224, acc-0.7068\n",
      "Iter-9250, train loss-0.2601, acc-0.6600, valid loss-0.2236, acc-0.6968, test loss-0.2224, acc-0.7066\n",
      "Iter-9260, train loss-0.2050, acc-0.7400, valid loss-0.2238, acc-0.6966, test loss-0.2226, acc-0.7066\n",
      "Iter-9270, train loss-0.2394, acc-0.6700, valid loss-0.2238, acc-0.6962, test loss-0.2224, acc-0.7075\n",
      "Iter-9280, train loss-0.1657, acc-0.8100, valid loss-0.2240, acc-0.6958, test loss-0.2227, acc-0.7060\n",
      "Iter-9290, train loss-0.2651, acc-0.6300, valid loss-0.2241, acc-0.6952, test loss-0.2227, acc-0.7056\n",
      "Iter-9300, train loss-0.2526, acc-0.6200, valid loss-0.2239, acc-0.6964, test loss-0.2225, acc-0.7078\n",
      "Iter-9310, train loss-0.2084, acc-0.7000, valid loss-0.2243, acc-0.6952, test loss-0.2228, acc-0.7072\n",
      "Iter-9320, train loss-0.2377, acc-0.6600, valid loss-0.2243, acc-0.6960, test loss-0.2228, acc-0.7078\n",
      "Iter-9330, train loss-0.2244, acc-0.7100, valid loss-0.2245, acc-0.6958, test loss-0.2230, acc-0.7069\n",
      "Iter-9340, train loss-0.2530, acc-0.6600, valid loss-0.2245, acc-0.6958, test loss-0.2230, acc-0.7073\n",
      "Iter-9350, train loss-0.2230, acc-0.6600, valid loss-0.2244, acc-0.6956, test loss-0.2229, acc-0.7081\n",
      "Iter-9360, train loss-0.1999, acc-0.7300, valid loss-0.2243, acc-0.6976, test loss-0.2227, acc-0.7076\n",
      "Iter-9370, train loss-0.2450, acc-0.6900, valid loss-0.2243, acc-0.6984, test loss-0.2227, acc-0.7088\n",
      "Iter-9380, train loss-0.2150, acc-0.7200, valid loss-0.2246, acc-0.6982, test loss-0.2229, acc-0.7083\n",
      "Iter-9390, train loss-0.2237, acc-0.7300, valid loss-0.2246, acc-0.6990, test loss-0.2228, acc-0.7086\n",
      "Iter-9400, train loss-0.2330, acc-0.6600, valid loss-0.2247, acc-0.6994, test loss-0.2229, acc-0.7086\n",
      "Iter-9410, train loss-0.2423, acc-0.6700, valid loss-0.2251, acc-0.7000, test loss-0.2234, acc-0.7082\n",
      "Iter-9420, train loss-0.2477, acc-0.6400, valid loss-0.2250, acc-0.7006, test loss-0.2233, acc-0.7081\n",
      "Iter-9430, train loss-0.2038, acc-0.7300, valid loss-0.2251, acc-0.6994, test loss-0.2235, acc-0.7078\n",
      "Iter-9440, train loss-0.2288, acc-0.6900, valid loss-0.2254, acc-0.6988, test loss-0.2238, acc-0.7072\n",
      "Iter-9450, train loss-0.2555, acc-0.6300, valid loss-0.2255, acc-0.6988, test loss-0.2239, acc-0.7072\n",
      "Iter-9460, train loss-0.2457, acc-0.6500, valid loss-0.2257, acc-0.6972, test loss-0.2242, acc-0.7052\n",
      "Iter-9470, train loss-0.1979, acc-0.7200, valid loss-0.2259, acc-0.6968, test loss-0.2243, acc-0.7053\n",
      "Iter-9480, train loss-0.2071, acc-0.7300, valid loss-0.2259, acc-0.6972, test loss-0.2242, acc-0.7053\n",
      "Iter-9490, train loss-0.2093, acc-0.7300, valid loss-0.2260, acc-0.6984, test loss-0.2242, acc-0.7066\n",
      "Iter-9500, train loss-0.1982, acc-0.7600, valid loss-0.2267, acc-0.6970, test loss-0.2248, acc-0.7048\n",
      "Iter-9510, train loss-0.2148, acc-0.7100, valid loss-0.2269, acc-0.6960, test loss-0.2251, acc-0.7044\n",
      "Iter-9520, train loss-0.2222, acc-0.7100, valid loss-0.2270, acc-0.6946, test loss-0.2252, acc-0.7036\n",
      "Iter-9530, train loss-0.2433, acc-0.6600, valid loss-0.2270, acc-0.6958, test loss-0.2251, acc-0.7044\n",
      "Iter-9540, train loss-0.2146, acc-0.7100, valid loss-0.2271, acc-0.6948, test loss-0.2251, acc-0.7047\n",
      "Iter-9550, train loss-0.2206, acc-0.7200, valid loss-0.2276, acc-0.6928, test loss-0.2256, acc-0.7028\n",
      "Iter-9560, train loss-0.2597, acc-0.6500, valid loss-0.2275, acc-0.6920, test loss-0.2256, acc-0.7025\n",
      "Iter-9570, train loss-0.1738, acc-0.8100, valid loss-0.2278, acc-0.6930, test loss-0.2257, acc-0.7019\n",
      "Iter-9580, train loss-0.2468, acc-0.6600, valid loss-0.2276, acc-0.6922, test loss-0.2257, acc-0.7027\n",
      "Iter-9590, train loss-0.2487, acc-0.6500, valid loss-0.2275, acc-0.6918, test loss-0.2255, acc-0.7026\n",
      "Iter-9600, train loss-0.2336, acc-0.6700, valid loss-0.2272, acc-0.6920, test loss-0.2252, acc-0.7029\n",
      "Iter-9610, train loss-0.2358, acc-0.6700, valid loss-0.2276, acc-0.6918, test loss-0.2257, acc-0.7020\n",
      "Iter-9620, train loss-0.2671, acc-0.6100, valid loss-0.2276, acc-0.6916, test loss-0.2255, acc-0.7009\n",
      "Iter-9630, train loss-0.2337, acc-0.6700, valid loss-0.2278, acc-0.6904, test loss-0.2259, acc-0.7000\n",
      "Iter-9640, train loss-0.2287, acc-0.7200, valid loss-0.2279, acc-0.6908, test loss-0.2260, acc-0.7005\n",
      "Iter-9650, train loss-0.2162, acc-0.7400, valid loss-0.2278, acc-0.6906, test loss-0.2259, acc-0.7000\n",
      "Iter-9660, train loss-0.2239, acc-0.6700, valid loss-0.2277, acc-0.6904, test loss-0.2257, acc-0.7009\n",
      "Iter-9670, train loss-0.2111, acc-0.6800, valid loss-0.2278, acc-0.6924, test loss-0.2256, acc-0.7023\n",
      "Iter-9680, train loss-0.2167, acc-0.7000, valid loss-0.2280, acc-0.6928, test loss-0.2259, acc-0.7010\n",
      "Iter-9690, train loss-0.2570, acc-0.6700, valid loss-0.2279, acc-0.6934, test loss-0.2259, acc-0.7001\n",
      "Iter-9700, train loss-0.2351, acc-0.6900, valid loss-0.2282, acc-0.6926, test loss-0.2261, acc-0.7003\n",
      "Iter-9710, train loss-0.2460, acc-0.6400, valid loss-0.2285, acc-0.6922, test loss-0.2264, acc-0.6993\n",
      "Iter-9720, train loss-0.2203, acc-0.6900, valid loss-0.2285, acc-0.6932, test loss-0.2263, acc-0.6998\n",
      "Iter-9730, train loss-0.2374, acc-0.6800, valid loss-0.2283, acc-0.6924, test loss-0.2262, acc-0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-0.1896, acc-0.7500, valid loss-0.2288, acc-0.6926, test loss-0.2268, acc-0.6984\n",
      "Iter-9750, train loss-0.2484, acc-0.6800, valid loss-0.2292, acc-0.6900, test loss-0.2271, acc-0.6975\n",
      "Iter-9760, train loss-0.1960, acc-0.7600, valid loss-0.2290, acc-0.6910, test loss-0.2269, acc-0.6974\n",
      "Iter-9770, train loss-0.2202, acc-0.7200, valid loss-0.2289, acc-0.6910, test loss-0.2269, acc-0.6978\n",
      "Iter-9780, train loss-0.2431, acc-0.6700, valid loss-0.2288, acc-0.6914, test loss-0.2268, acc-0.6976\n",
      "Iter-9790, train loss-0.1676, acc-0.7700, valid loss-0.2291, acc-0.6916, test loss-0.2272, acc-0.6958\n",
      "Iter-9800, train loss-0.2641, acc-0.6200, valid loss-0.2290, acc-0.6918, test loss-0.2272, acc-0.6958\n",
      "Iter-9810, train loss-0.2442, acc-0.6100, valid loss-0.2294, acc-0.6912, test loss-0.2275, acc-0.6954\n",
      "Iter-9820, train loss-0.2521, acc-0.6400, valid loss-0.2292, acc-0.6916, test loss-0.2272, acc-0.6963\n",
      "Iter-9830, train loss-0.2576, acc-0.6400, valid loss-0.2294, acc-0.6908, test loss-0.2273, acc-0.6966\n",
      "Iter-9840, train loss-0.2223, acc-0.7200, valid loss-0.2293, acc-0.6898, test loss-0.2274, acc-0.6949\n",
      "Iter-9850, train loss-0.2120, acc-0.7200, valid loss-0.2293, acc-0.6910, test loss-0.2273, acc-0.6945\n",
      "Iter-9860, train loss-0.2226, acc-0.7100, valid loss-0.2297, acc-0.6892, test loss-0.2277, acc-0.6945\n",
      "Iter-9870, train loss-0.2118, acc-0.6700, valid loss-0.2298, acc-0.6902, test loss-0.2277, acc-0.6940\n",
      "Iter-9880, train loss-0.2563, acc-0.6200, valid loss-0.2299, acc-0.6892, test loss-0.2279, acc-0.6932\n",
      "Iter-9890, train loss-0.2122, acc-0.7100, valid loss-0.2302, acc-0.6900, test loss-0.2280, acc-0.6948\n",
      "Iter-9900, train loss-0.2252, acc-0.6900, valid loss-0.2301, acc-0.6888, test loss-0.2281, acc-0.6923\n",
      "Iter-9910, train loss-0.2241, acc-0.6900, valid loss-0.2304, acc-0.6892, test loss-0.2284, acc-0.6926\n",
      "Iter-9920, train loss-0.2077, acc-0.7100, valid loss-0.2307, acc-0.6886, test loss-0.2286, acc-0.6927\n",
      "Iter-9930, train loss-0.2004, acc-0.7400, valid loss-0.2309, acc-0.6872, test loss-0.2287, acc-0.6910\n",
      "Iter-9940, train loss-0.2553, acc-0.6700, valid loss-0.2307, acc-0.6864, test loss-0.2285, acc-0.6916\n",
      "Iter-9950, train loss-0.2548, acc-0.6200, valid loss-0.2310, acc-0.6872, test loss-0.2287, acc-0.6924\n",
      "Iter-9960, train loss-0.2028, acc-0.7300, valid loss-0.2309, acc-0.6880, test loss-0.2285, acc-0.6931\n",
      "Iter-9970, train loss-0.2871, acc-0.5600, valid loss-0.2310, acc-0.6876, test loss-0.2286, acc-0.6933\n",
      "Iter-9980, train loss-0.1841, acc-0.7500, valid loss-0.2313, acc-0.6870, test loss-0.2288, acc-0.6923\n",
      "Iter-9990, train loss-0.2296, acc-0.7200, valid loss-0.2310, acc-0.6878, test loss-0.2285, acc-0.6924\n",
      "Iter-10000, train loss-0.2775, acc-0.5500, valid loss-0.2313, acc-0.6872, test loss-0.2289, acc-0.6925\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noK10XY18AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118fb4780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnWd4VNXWgN8dCC2QkNCkJpSoKCBKE7EE0AuiAiqocEH0\nE8QCKgrIVRAUK6IXvV6vBUSKgB2xoIiKCtIUEJDei/QSOgnJ+n6cmUzJtCTTkqz3ec4z5+y6zp6Z\ns85uaxkRQVEURSmexERaAEVRFCVyqBJQFEUpxqgSUBRFKcaoElAURSnGqBJQFEUpxqgSUBRFKcb4\nVQLGmAnGmH3GmJU+0rxujNlojFlhjGkaXBEVRVGUUBFIT2Ai0MFbpDHmeqC+iKQC/YG3giSboiiK\nEmL8KgERmQ8c8ZGkCzDZlnYxkGCMqRYc8RRFUZRQEow5gZrATqfr3bYwRVEUJcrRiWFFUZRiTMkg\nlLEbqO10XcsWlgtjjBoqUhRFyQciYkJRbqA9AWM7PDELuBPAGHM5cFRE9nkrSET0EGHkyJERlyFa\nDm0LbQttC99HKPHbEzDGTAPSgErGmB3ASKCU9TyXd0TkG2NMJ2PMJuAkcHcoBVYURVGCh18lICI9\nA0gzIDjiKIqiKOFEJ4YjRFpaWqRFiBq0LRxoWzjQtggPJtTjTS6VGSPhrE9RFKUoYIxBQjQxHIzV\nQYqi5JOUlBS2b98eaTGUKCE5OZlt27aFtU7tCShKBLG94UVaDCVK8PZ7CGVPQOcEFEVRijGqBBRF\nUYoxqgQURVGKMaoEFEUJOdnZ2VSoUIFdu3ZFWhTFDVUCiqLkokKFCsTHxxMfH0+JEiUoV65cTtj0\n6dPzXF5MTAzHjx+nVq1aIZBWKQi6OkhRIkhhWB1Ur149JkyYQNu2bb2mycrKokSJEmGUKnyE8950\ndZCiKFGHJyNmI0aM4I477qBnz54kJCTwwQcfsGjRIlq3bk1iYiI1a9bk4YcfJisrC7AepDExMezY\nsQOA3r178/DDD9OpUyfi4+Np06aN1/0SIkL37t2pXr06SUlJtGvXjnXr1uXEnz59mkGDBpGcnExi\nYiJpaWlkZmYC8Msvv9C6dWsqVqxIcnIyH3zwAQBXXXUVkydPzinDWcnZZf3f//5HamoqDRs2BGDg\nwIHUrl2bihUr0qpVKxYuXJiTPysri9GjR9OgQQMSEhJo2bIle/fu5b777mPYsGEu93PDDTfw3//+\nN+9fRIhQJaAoSr6YOXMmvXr1Ij09ndtvv53Y2Fhef/11Dh8+zIIFC/juu+94++23c9Ib4/oiO336\ndJ577jmOHDlC7dq1GTFihNe6brrpJjZv3szevXtp1KgRvXv3zol75JFHWL16NUuXLuXw4cM8//zz\nxMTEsHXrVm644QYGDx7M4cOHWb58OY0bN/Zah7t8X375Jb///jurVq0C4PLLL2f16tUcPnyYbt26\n0b179xxlM2bMGD777DPmzJlDeno648ePp0yZMvTp04cZM2bklLl//35+/vlnevb0a5ItfITZHKoo\niuLA338CgnMUhJSUFPnhhx9cwoYPHy7t27f3mW/s2LFy2223iYjIuXPnxBgj27dvFxGRXr16yf33\n35+TdtasWdK4ceOA5Dlw4IAYY+TUqVOSlZUlpUuXlrVr1+ZKN3r06Jz63bnyyitl0qRJOdfjx4+X\ntm3busg6f/58rzJkZ2dLhQoVZM2aNSIiUr9+fZk9e7bHtBdccIHMmzdPRETGjRsnXbp08Vqut9+D\nLTwkz2XtCShKFBMsNRAKateu7XK9fv16brzxRqpXr05CQgIjR47k4MGDXvOfd955OeflypXjxIkT\nHtNlZ2czdOhQ6tevT8WKFUlNTcUYw8GDB9m3bx+ZmZnUq1cvV76dO3dSv379fN4duSaxx4wZQ8OG\nDUlMTCQpKYlTp07l3N/OnTs9ygDW0NfUqVMBmDp1qksvJhpQJaAoSr5wHz7p378/jRs3ZsuWLaSn\np/P0008HZdJ78uTJfPvtt8ybN4+jR4+yadOmnLfYatWqUapUKTZv3pwrX+3atdm0aZPHMuPi4jh1\n6lTO9d69e3Olcb6/efPm8e9//5vPP/+cI0eOcOTIEeLi4nLur06dOh5lAEsJfP7556xYsYItW7Zw\n00035en+Q40qASWi3HknfPZZpKVQgsHx48dJSEigbNmyrF271mU+oKDlli5dmsTERE6ePMkTTzyR\n84COiYnhrrvu4pFHHmHfvn1kZ2fz22+/kZWVRa9evfjuu+/4/PPPycrK4tChQ6xcuRKApk2b8umn\nn3LmzBk2bNjAe++951eG2NhYkpKSyMjIYOTIkS5K5J577mH48OFs2bIFgD///JOjR48CloJo0qQJ\nffr0oXv37pQqVSoo7RIsVAkoEWXKFPDz/1MijPsbvzdeeeUV3n//feLj47n//vu54447vJYTaJkA\nd999N9WrV6dGjRo0btyYK6+80iX+1VdfpWHDhjRr1oxKlSrx5JNPIiKkpKTw5Zdf8uKLL5KUlESz\nZs1YvXo1AIMHDwagWrVq9O3bN9cQjbt8nTp1on379qSmplKvXj0qVqxI9erVc+KHDBlC165dad++\nPQkJCfTv358zZ87kxPfp04fVq1dz5513Bnzf4UL3CSgRxRi44Qb46qtISxIZCsM+AaXg/PTTT/Tt\n29frkJEd3SegKIpSxMjIyOC1117j3nvvjbQoHlEloCiKEiJWr15NUlISR48eZeDAgZEWxyPqWUyJ\nODoaohRVGjVq5HXpa7SgPQElz2RkQHZ2pKVQFCUYqBJQ8kxcHAwZEmkpFEUJBqoElDxz7hzYllsH\nhTysFlQUJcioEihkrF0LiYmRliK44/ihnhPo1CmwvQjVq8Py5aGVRVGiDVUChYwVK8C2EVEJkNmz\n4cMP/afbuxeWLAm9PIoSTagSUALixAl44AHPcVu3wsiR1vmpU3DffeGTK1DsQ04vvww2y8A+0ykF\nY/v27cTExJBtW0HQqVMnpkyZElBaJbyoEihkROohtXIl/O9/nuMmT4ZnnrHO166FvJqMCecS0aFD\nYdy48NVXWLn++usZNWpUrvAvvviC6tWrB/TAdja98M033/i0npkXMxJKcFElUMjw9l85exZ++y3v\n5WVmwoIFrmHffQcvvOAa5u1BvXIlzJ/vW74ff4RffvEuw44dYLO7VWDOnIGFCy2Zfv/dMXTmLJcv\npZPXZ9Hu3fDBB3DyZN5ljWb69OmTY/7YGbsp5JiY4vPoKOpmPYrPN1lE8PaQmjAB2rTJe3kzZoCb\nPS46doQnnoAjR/znv+QSmDvXd5r27eGaa7zH//UXFMDsuwtvvQVXXAFXXQUtWsBjj1nhgT7c86oE\n2reHXr3gpZfyli/a6dq1K4cOHWK+k4Y/evQoX331VY4RtG+++YbLLruMhIQEkpOTefrpp72W17Zt\n2xxLndnZ2QwePJgqVarQoEEDvv76a5+yvPTSSzRo0ID4+HgaNWrEzJkzXeLfffddLrroopz4FStW\nALBr1y5uvfVWqlatSpUqVXjooYcAePrpp116Je7DUW3btmX48OFceeWVxMXFsXXrVt5///2cOho0\naMA777zjIsMXX3zBpZdeSkJCAqmpqcyZM4dPPvmE5s2bu6R79dVXufnmm33eb7hRJRBhDh92/fQW\nbz8/ftw6//tv13Q2L3cB1ef8YnPunPe0zj1+95chTy9HR4/CsWOByREI3trEmawsSE93XLu3Q0aG\n9XnihDXxC757Ap58oLi3mbNsdmvCJ044vpuiQJkyZejevbuLH94PP/yQhg0b0qhRIwDKly/PlClT\nSE9P5+uvv+att95i1qxZfst+5513+Oabb/jzzz/5/fff+eSTT3ymb9CgAQsWLODYsWOMHDmSXr16\nsW/fPgA+/vhjnnnmGaZOncqxY8eYNWsWlSpVIjs7mxtvvJG6deuyY8cOdu/e7WLV1H34yf166tSp\njB8/nuPHj1OnTh2qVavGN998w7Fjx5g4cSKDBg3KUTZLliyhT58+vPLKK6Snp/PLL7+QkpJC586d\n2bZtG+vXr3cpt0+fPn7bKKyEymWZpwN1L5kLEJk71/pcuNBz/OLFjnNvLgPHjQvMjSCITJ3quJ44\nMXc+e/kHDjjCfvnFkQ5E2rXzLJMn2Xy5OPSW5+DBwO5n1CjXdGPGuJbZu3du2e66y7csnsJnzswd\ntnWrSO3ajnyWkdy84e8/wSiCcuSH+fPnS8WKFeXs2bMiItKmTRsZN26c1/SPPPKIPProoyIism3b\nNomJiZGsrCwREUlLS5MJEyaIiEi7du3k7bffzsk3Z84cl7T+aNq0qcyaNUtERDp06CCvv/56rjQL\nFy6UqlWreixz1KhR0rt375xrT7KOHDnSpwxdu3bNqbd///459+3OAw88IMOHDxcRkdWrV0tSUpJk\nZGR4Ldfb74EQupdU20FRgP3t09vwSyBvxHkZtvTgRCnPhHqY1MkUu0+2bfMdH6z5xv37c4e5zwOE\nok1kZOTGo9u0aUOVKlWYOXMmzZs3Z+nSpXz++ec58UuWLGHYsGGsXr2ajIwMMjIy6N69u99y//77\nbxfXlMnJyT7TT548mX//+99ss33ZJ0+edHHr6MmF5M6dO0lOTs733IW768zZs2fzzDPPsGHDBrKz\nszl9+jRNmjTJqeuGG27wWM6dd95Jz549GT16NFOnTuW2224jNjY2XzKFCh0OiiI6dfIcnpeHS2qq\n9eCLjYUffnCE9+oF/ft7L+/aa63P8893hHXsmD8ZvNGiheM8OdlyKOOOMd4f3BdcAO++67iuVg3e\nf986r1TJGo4aOtQ1j9NohguPPgpdu/qW9/77oWdPh1x27KMX/frBzp2e886ZA2XL+i6/MNC7d28m\nTZrE1KlT6dChA1WqVMmJ69mzJ127dmX37t0cPXqU/v37BzSJWr16dXY6Ndz27du9pt2xYwf33nsv\nb775Zo5bx4svvjinntq1a3t1Lbljxw6Pq5jcXUvu2bMnVxrn4aGMjAy6devG0KFDOXDgAEeOHOH6\n66/3KwNAq1atKFWqFL/++ivTpk2LOv/CoEqgUBDIA9j+m7W7VD13DpYtc8R/8IHnB6I9n11hbNzo\niPvjj7zJ4I/ff3ec79gBP/+ct/wbNsD33zuund/ODx8OrMcE1r1Mnw5ffOE73eTJVjpwVQK//mp9\nLlzoPe/ixYH3ZqKZO++8k7lz5zJ+/PhcY9knTpwgMTGR2NhYlixZwrRp01zivSmE2267jddff53d\nu3dz5MgRXvIxq37y5EliYmKoXLky2dnZTJw4Mcc7GEDfvn0ZO3Ysy2w/9s2bN7Nz505atmxJ9erV\nGTZsGKdOneLs2bP8Zls+17RpU3755Rd27txJeno6L774os82sPdyKleuTExMDLNnz2bOnDk58ffc\ncw8TJ07kp59+QkT4+++/XeYBevfuzYABAyhVqhRXXHGFz7oigSqBKOOtt/Ke59FHPT+QPvvMWttv\nf4DZP4cMCfwB9d138OST1jJPgEWLrM+ffvI91HLppdYk7XffeU8zYYL3uFq1PId//HHBh3gmTXIM\nidWrB/feC/HxjnhjrAll92fYv/8NH33ku+xnn3W9NgbcX3R37YLbbsuf7OEmOTmZK664glOnTtG5\nc2eXuDfffJMRI0aQkJDAs88+y+233+4S782dZL9+/ejQoQOXXHIJzZs359Zbb/Vaf8OGDXnssce4\n/PLLOe+88/jrr79c3Et269aNJ598kp49exIfH8/NN9/M4cOHiYmJ4csvv2Tjxo3UqVOH2rVr85Ht\ny7v22mu5/fbbadKkCS1atMjl+N19krh8+fK8/vrrdO/enaSkJGbMmEGXLl1y4lu0aMHEiRN55JFH\nSEhIIC0tjR07duTE9+7dm9WrV0dlLwAIbGIY6AisAzYAj3uIjwdmASuAVcBdXsrxOiFSXAGRGTN8\nT6p+9ZXj3FM6b5Oz7ke5co7zbdusvJMmuZbnnictzfqsUMH67NAh8PoOHhRp1ixwmX3do3ucp7Sb\nNgUum79jzx6RsmUd1+PHW5916ogMHOg7r4jI6NGO63ffdb2PyZOd70H/E0Wd06dPS3x8vGzatMlv\nWm+/B0I4Mey3J2CMiQHeADoAFwM9jDEXuiV7EPhLRJoCbYFXjDE66Wzj3DkIYOVcDsuXW6YY7Hl8\nrdfPywYxp2FQXnrJsXzSjqdl3vbhJfvyR19v9u7ExLgOKYFlm8dt6XQOH3+cO2zaNBg+3DH2Hy6W\nLIHTpx3XDz5ofQbSC5k925Lbjth6FJ995ppOjdUVD958801atGjhcQI7GgjkQd0S2Cgi2wGMMTOA\nLlg9AzsCVLCdVwAOiYiPFejFi19/hS5dAh9Xv+wya4J2wwbr+tlnrYldT+RngxhYw0Rpaa5hHqwE\nsGtX/soHzw/MVq28p/c0RPLPfwZeX6DtGwhOvX3A2pEdKO4T/Ha5br3VdR/DZZflTzal8FC3bl2A\nXBvcoolAlEBNwHkNxC4sxeDMG8AsY8zfQHngdpQcsrJ8x9vftp1xfqB5W4FSUFatcl0NFGz87AEq\nEN9+mzvMfQNdKNi/H5zm/AJCBNassc6LwmSxEjhbt27NV74dO6BOnSAL44VgDdl0AJaLSDtjTH3g\ne2NMExHJ5VzT2ShVWloaae6vo0WQN97wHT98uO9452GcYPLss56XaQaLfv1CV/b11+cO82WaIlic\nPm0t/8wLS5Y4LKsOGgRXXx18uZSiRXLyPJ58ch4lwzCoHkgVuwFnnVTLFubM3cALACKy2RizFbgQ\n+N0tnUfLhEUdT6YI/BHMoQ0lsjibk9i9W81VK4GQxpNPpuXsNfFll6mgBLJEdCnQwBiTbIwpBdyB\ntRLIme3AtQDGmGrA+UCQ7EIWPpw3PBmT20rnrFl5fxB4eqsOxsPk/vsLXobiG+cJ79mzIVpXCirR\nRbheFvz2BEQkyxgzAJiDpTQmiMhaY0x/K1reAZ4F3jfG2D3PDhWRALfuFD/y45/Xabd+UDmRa8BO\nUZTiRECbxUTkWxG5QERSReRFW9jbNgWAiOwRkQ4i0sR2TA+l0NHEokVw442Bp69d22FzxteGIffJ\n4kOH8i6boiiFD/tyZLDMooe6R6A7hgvIF1+AH3PoLuza5Xige1oXryhK8ebNNx3nvkyTBAtVAgXE\nfQL31Ve9p7X753A2gqYo0UiFChWIj48nPj6eEiVKUK5cuZyw6dPz39Fv3bp1LhtDimdeeSU89eiu\n3iBj92TliauuCp8cilIQjjstaapXrx4TJkygbdu2EZQoPGRlZVGiRIlIiwH4XzoeLLQn4IdVq6y3\n/VWrPMc79wTc0+Rzn0jxw2QDAjEBukfLhUDNJVDqONR2s6NR8rStfCW/2G3MOJOdnc3o0aOpX78+\nVatWpXfv3hyzuZU7deoUPXr0oFKlSiQmJtK6dWvS09MZPHgwS5cupW/fvsTHxzNkyJBcdWVlZdGt\nWzfOO+88kpKSaN++PRvsW+dtZT/00EPUqVOHxMRE2rZtm2Muet68ebRu3ZqKFSuSkpLCjBkzgNy9\nj7fffpvrrrsOgLNnzxITE8Nbb71FgwYNaNy4MQAPPPAAtWvXJiEhgcsvv5zFixe7yPj0009Tv359\nEhISaNWqFfv376dv374Md3tyd+jQgbfffjvfbR8OtCfghyZNLMue993ne+3+6dNWWmfq1QutbFFJ\nzDnIdv5ZCcSegs79oPF0EAPG1pDrusCFfuw5O5NeG37vDxX2QMPP4Eg9qLPAfz53VvaEHVfCH/0t\neWougbo/wbX/suJ/fAaW3wPHa+S97GLCyy+/zNy5c/ntt99ITEzkvvvuY9CgQUyYMIHx48eTlZXF\nnj17KFmyJMuXL6dUqVKMHTuWBQsW8NBDD9GjRw+vZXft2pUpU6ZQokQJBg0aRJ8+fVhoGxwfOHAg\nu3fv5o8//qBy5cr89ttvGGPYtGkTN910E1OmTKFz584cOXKEv31sIXe3FPr111+zbNkySpcuDcAV\nV1zBiy++SFxcHGPGjKF79+5s3bqVEiVK8PzzzzNr1izmzp1L3bp1+fPPPylTpgx9+vShT58+PGsz\nJbtnzx4WLFjAhx9+WNDmDimqBAIgEN+xHnxXFE5iMgEDZQ9B7YVQ5hDUWkrpeUMofaISx0wcLWr9\nh/b1/sXpMhkMXAL1j8DxUvD5hfDpRVDxDFxwEMZdDulloMs6KHMOUubBM/OEk7EQlwnwBVsqQr2j\njurn14ZSWVD5FHS7DZY7P4cTdkJ7pzetCpYzkCZ7ocfXV1Ky0l8cSTrC81dbL//irZ/bZJp13PiA\n5/h2T1mHnU3/gNV3wMZOcLoSJP8CBy+A4zXz3Lx5JlhLQ4K8+/Dtt9/mgw8+oFq1agCMGDGCRo0a\nMWHCBGJjYzlw4AAbN27k4osvplmzZm6ieJelRIkS9HIylDVixAhq1qxJRkYGMTExTJkyhTVr1uQ4\nt2ljM541depUOnfuTFebp6BKlSpRqVKlgO9n+PDhxDvZE3eWYdiwYTz77LNs2bKF1NTUHEVntwt0\nySWXAHDVVVcRExPDggULaNOmDdOmTaNjx45UrFgxYDkigSqBAlLYd/ZWMbvpe0EXbjvyB3sqwORL\noP0W6Lsch4nAFQC2Lq1gWZLaCTvioY7NsXyFDOi2xnDnSkeDPDHfc51xTqM+iUcrApYWyMZw5U5H\n/mXv5OVO5udYuHruR9eYndSiNru4k0mU4QyLacWp5MVs7tMfiYGbx7xF7TPHSc+uzCTuIoGjnLql\nL5lNPrUKaDDHOjwx6QfY2i4vguaNKP2B7dy5k06dOuW8Udsf7IcPH+aee+5h7969dOvWjZMnT9K7\nd2+effbZXG/fnsjKymLo0KHMnDmTQ4cO5eQ5dOgQ586dIysri3oeutje3EwGSi03BxYvvPACkyZN\nynFof/bsWQ4ePEhqaiq7d+/2KANYvgOmTp1KmzZtmDp1auGwkBAqG9WeDgqh7XQQefnl3Lbt7QwZ\nYsWdOBE8W/bBPspxQrrxkeyMregxQUaM98yDKjwiAnJvy4vkqSvKyegrSkmlUpu91hXDOYFsgWyp\nyl65nen5lrsum2UcD8n/6O810dv0k+v5OifoEpaLgKykkeylaoEb7zXzgLQo8bOcV2GJcOkEYSRS\npvYczw7d+7YSOj0gDKkiNJ4aYBXR/59ISUmRH374IVfYsmXL/ObdunWrpKamyrRp00REpHXr1vLB\nBx94Tf/uu+/KJZdcIjt37hQRkb1790pMTIzs3r1bMjMzpVSpUrJhw4Zc+UaOHCk9evTwWGb79u3l\nXSenDqNGjZLrrrtORETOnDkjxhjZvXt3Tvz3338vNWrUkHXr1omISHZ2tsTFxcmCBQty7n3OnDke\n69qyZYtUrlxZ/vjjD6lcubJkZmZ6vVdPAL5+J0F7FjsfISnUa2WF4AfvDoiMHSu5lEBKihWWnCxR\noQQMWXIrH8v3tJcp/NNrwkyDfHE+MiIN6XcjkvBYaaH0UcGcExApSUZE7yOUR1lOCoj8kykykT45\nEXcwTSBbEjgidzBNejHZb2HvcZdczCqhZ0fPCmEUwuCqrtf3NRFu7F8klMALL7wg1113Xc7Det++\nffLll1+KiMjcuXNlzZo1kp2dLQcOHJCGDRvKhx9+KCIiXbt2ldGjR3ut69VXX5VWrVrJiRMn5Pjx\n43LPPffkKAERkb59+8r1118v+/btk6ysLJk/f75kZ2fLpk2bJD4+XmbOnCnnzp2TAwcOyMqVK0VE\n5LHHHpMOHTrImTNnZO3atZKSkuJTCXz22WeSkpIiBw4ckDNnzsiTTz4pJUuWzFECo0ePlubNm8uW\nLVtERGT58uWSnp6ek/+qq66SJk2ayIMPPpjntlYlEIWA556A+5cUKSUQwzmvkROaIk+lIckPIzFP\n2R5EZEf8YRxdh+/2KEGmxHFcuvOhnKSsz8J+LNFavojpILPM9XL+AC+KwX60HyYkbJfC8J+oW7du\nLiWQnZ0tY8aMkdTUVImPj5fU1FR55plnRERk0qRJkpqaKuXLl5fq1avLkCFDcvL9/PPP0qBBA0lK\nSpLHH388V13p6elyww03SPny5aVevXoyadIkFyVw8uRJGTBggNSoUUMSExOlXbt2kpWVJSIiP/30\nk7Ro0ULi4+MlJSVFZsyYISKWgmrXrp3Ex8fLNddcIyNGjHBRAs7li4hkZmZK7969JT4+XmrVqiWv\nvfaaVK9ePUcJZGZmysiRIyUlJUXi4+Pl8ssvl/379+fkHz9+vMTExMjixYvz3NaRUALGqjc8GGMk\nnPUFA2Pg5Zctv7z2r+O+++Adt/HqadOgZ8/QydGXd7mcRVTmINXYx2hG8DWu9iouGAAbKkPZDDhd\nyhb43VhrFUxmObzPlCp5pRRnac1C5pG/tfO9b4aV1WDlW1DY/hOKb77//nsefPBBl6WtgWLNgXj6\nPRhEJDQGJEKlXTwdFIK3HnfAdTjozJnQv53GcE4SOSRv089v4ueuRKoORmjxhhVkzglXPWu9bVb5\nKwretIvTkS0xnJNSnJFXGBRQpsL4n1C8c/bsWbnxxhtl7Nix+cpvaQBPPxVERHsCYWH+fLjySse1\nMTB2LAweDMuWWd5+KlcOVm3CJhpQhQP8QHua8QfpJNCY1QAcJYGKpPP4tfDRxbCtImA3UZ0N8sf9\n8PWb3otXIoxgfWH2TyvMIKznAkbyNNP5J9H+n1AC488//6RNmza0bNmSr7/+mrJ2ZwB5IBI9AVUC\nTuzZAzVq2PSuDWclAPDIIzBuXMHqiSWDidzNP/FsQ2VP6bI0evg0h8s5Ba6+HT6dRs5mKx3aKSIY\nVQJKDpFQAsV+n8DRo2Dfy+HNF7Czk/GCmIIozRn+oBkXsyYnrNJQXB/2AJy2Pk5UhXHb4VwZ1+gQ\nDQ0qilL8KPY9AWNg3jzLP+2uXZa9f/eeQDCIJ510LG0zmJcZ12U5WZfaegIHLoS/m1smFOa+AL/f\nB2UPw6kqwalciWK0J6A40J5AhDhwILTl12QXu6gNQGKtrzna9wZH5EuH4HRS7kyqABRFCQOqBHB9\n8wf4/nup+WrGAAAgAElEQVTo3t0aKioo57Oe9VwIwIXVp7oqgFH6BljcKV06OSBzCkrxIK50TU6e\n9Z8umKgScMKuDBYsgPT0gpc3muEM5zkAGrKG9bd1siJGZeNYLRL91KsHW7b4TvPww/Daa+GRpyhx\n9uy2SIsQxdj+kF3vgqaTHcFny8O8UdBhMNWOQ8vdMLcenH7pLGSV8lSQd2JPwrmyUCIDLpgF9b6H\nQxdY4S3/C3H5HCY4kwBlPDxEJs/mqn3HWZqYROnuXbl31QnGzHWKP7sb+7NhNh25nm+B0D4tdE7A\nwK23wiefwM6d1hLQkSPh6afhP/+BgQPzV24jVrGKJnzCrXQv+zb0bwYVt8NzJyAzLrg3EWKaN4ff\nf/edZtQo61CUIofJsizH1p9jmS5P/hXmPQXL+lk+LCqvt8ycH6ttXR9xM2R33gq471KfVZTMgtRD\ncOefMMyDdXQDukQ0VNh74iK5lUB+qcFudlOL2XSkU9wkGGKZ22XKd7D5HwUXOsy89x783//5TnP2\nLNhMsSuK4k5MJpQ+BhX+hnbD4Whd+O5Va6XfRZ/CvsZQ+ji0eg0umQpArz9hfSVYWgsYFToloMNB\nTixfbn1u3pz/MnoxhSncyS5q0rnB/dDLpgBe3WG9KRRCSgbwKymVx164ohQrsmMtXxSnK8EMN0dK\na7o5zj+fYh3A1DrzIbMs7L0UCJ3LS+0JOPUECjo/Z8gmmxJMowf/HOXkjPv545BRvmCFh5Enn4Tn\nnnNcT54Md97pO08w2k9RFG+Ebomobju1cehQwcvItmnre/9lUwB/9rJWABUiBZCWBjbveIUaNx8h\niqJ4QZWAjYLaA7ody6n1tb3hZGngjTU53brCRPPmucOuvjqwvFddZX3eeqv16a9n4Gyjyc7YsYHV\n5Yt27aBVq4KXoyjFAVUCQaAcJ5lBD36rGscP9bFWAB1sGPR63Fy1euTCC3OHvfee49x574MIlHAb\naqzhwbd6crLnB7Y7v/xilfnJJ9b1qlWOuGrVcu/HmDEjdxmPPea/Hl8MHQo//ADVqzvC3nqrYGUq\nSlFGlUAQOIk13HN1/5PwvxVRtwTU+Y3c/e28Rw/o5jQv1aSJa/z113vO54zN17fPej1RoYLv+GBx\n8cXhqSfc2Hy8K0qBUCVQQJZijZ/0vQmySgD7LilQeS+/7D0uO7tARQO5H8xTpsDHHzuslrdv74ir\nUAG++cZ/mfO9OJR3xt4LmDDBERYfD7/95j+vL266yX8ab72Yiy6C6dM9xxUGpnk2Qhsw8fGBp/3n\nPz2HR9k6DyUfFFslkJZWsKWgJcnkLKVozh/MS4YJzYAXgrDN2AeB/OHq188d5jzfEchyz2DhrHDO\nP9/6dH/weLqn2FjvZTZ0G2VzV2rnnReYbCVLem6rwkRBH8B16gSeVld+FV2KrRL4+efA3mC9kUkp\nSpFJz1ug7TXtrFVAZ/PwauUFX382f3/6Xbust3p3brgB/vwT9u6FsmUtu0gFlSVQu0qPP259fved\n9Wmf19i71/q039MPPzjyHDoEAwY4rrdssay7AixZ4gif4mHe/aGHXGW31+PM7t1w8CB8+KHnNnVX\nNPmhRYvA0u3c6Xr9yCOO8z17Ci6HJ+691/qcOBESElzjbrrJ8yIJY6B1a+t8xQrvZbdrFxwZlfBR\npJVAdjb897/e48ePz1+5V/MzADUfhem168LkuX5yBAd/SqBiResh744x1li/fQzZ/Y+fHwIto7xt\ndWy5cg5ZwCFLTEzu8ipUgJYtHdfVqkHVqq7lgfWwcu7Z1KuXe6Lb07h5jRpWfWXLOup3xtcwSaDO\nouICnBZynsAGh7ID/72a/PYE7Du7S5Vy9NDs1KyZWyawvjf7d3DJJY5zdwp776o4UqSVgPsbpTv5\n6wkIYxJup+ct8PcfT8NrWwiXMThPf/pJk2D4cM/pu3WDtWuDU7f94d2njyOsd2//eQYNslYN2UlO\nhtmzHdetWlm9hEsvhW+/zV3G7NmWAvE0H3LttdbKn6VLXWX0hfscxC23wFwvOvzXXx1lXnaZ9blm\nDbz/viNNs2YwZ07uvL4e0OvXO87dlZC9Puc2Cgb9+llv/s6y5Xiv9VC/P7zdn3O4N0Xhjzej2GOq\nt7mRwkyRVQJrHM672LOn4BOQdrrHTqRc6X3MaAT8/FTA+Xx1k1NSAivD0x/vzjsdQw/uf+C6dT0v\nGS0IzkMFycn+08fFOfYPgCVjx46O65gY+Mc/rM8OHRzh9nu1p3W/93LlrF5AlSqe9zZ4wz6kYSc2\n1nUy3C4jWBPK9p6HvceRkuKqCGNj4brrAq+/bFnXt2/378x+HcjejLz0BC69FO66yzXMk2I1xrMi\ncA/z5oXPuUxPq7IC2b9x//3+0wQL59+cJ9x7Rc7DdUWFIqkEzp61foD2P0mNGtYyxoKurlnAFXyU\neQ9vtgB57kye8jp38919FP/nP47z/MwJXHGF/7zOPPggjBjhO83777vuL3j6aWsjl3MdvXo5fC97\noiCTie73OmYMvP669/RvveXflPVFF3mPc37wOI/nO781e8K+u3r6dEthdemSO820aY7ew6ef+pbx\n5putz0Dazq783O/LeSjN2wqiHj0gNRVeeMH6Pb77rqNeT/MZAwe6tsH48Y6ehZ1bbnHteXsaaits\nlmadzadA8F+qogIRCdthVRd6Tp2yOrr79tk7vNaRnS1y7pxIVpZreCDHzXwqArI+CSH1qzznv+8+\nx7mIa5zz9auvei/jootyh9kB676dyxo6NPhtO2SIa73eAJF16/Jfz3vvea8HRMqV853/oYdyt8+Y\nMb7zgMjgwSJPPeXIW768dd68ee7yzjsvdxm//GLFXX219XnHHb7rtJfl/ltw/y49Hfb4338XefBB\n67x3b5GuXXOnefNNx/mAAd7lGDjQagNPv7HOnT1/JyDyyCO57+faax3nV1xhfZ48KVK5suf7scvt\n6b6Nyft/1n6UKeM4r1dPZP58x3WHDt7zff21yOrVjusWLSzZhg/Pvyz5PxCR0DyXA+oJGGM6GmPW\nGWM2GGMe95ImzRiz3Biz2hjzUxD1VJ7x9hb100+OIYS8cCnL+IxbebgjXNDoGdh4g/9MAcqUF+zr\n3f11qe33l5ZW8Drdads2sGGggnLppb5NU/trz/btrTfdgtK/v2VGu0cPx8Y5sMaGPZnXdh/a69TJ\nfx233WZNVDsPk/nDeXixZk1He1x/PXTuHHg5zqSmWu3Wtq01lGifB7HTpQtcfnnufA0aeB7ubNTI\nce78fXmb8O7c2ftvO79Lm/v1g/vuc/wWLrrI9kgNgPPPd5U70HyFDn9aAmvIaBOQDMQCK4AL3dIk\nAH8BNW3Xlb2U5fkVJMicPm1pT/eewIQJedfAF7BWBOStZgijyLcmf+ABx7mIa5yI463JuSeQlCTy\n5ZeO6xkzHPfo/pYG1n1HC1CwnoC/suPi8p4nPz2B/GLvCeQH8N0T6NQpd54BA1zre/RR199aID0B\nd44ezd892N+4333XIXObNtbnyZMi7dvn/k+89lruMpyP0qV9/7+eeMJxfsstuf8fCxda10OGiPz8\nsyPeV09ARGTNGsd18+ZWmK+eQGKi97h77snfsyMaegItgY0isl1EMoEZgPvIZ0/gUxHZbXvSHyyY\naioY3t4S8/o2XoFjrKMh82vDfTcBz2TkWyYJ8C3CWcbLLsvb24f78shIE6oNRhddlLfJ4LwS6ES9\nLwpqqsLTeLodT+2al6WZ9erlXZ6CUKKE//bwZ/XV33LZCy5wnPv73SUmBp7WTrlyrj0bbxRGUx6B\nKIGagPOWll22MGfOB5KMMT8ZY5YaY/wsHox+ynGS90pZRnXa9QFeOmg5hggRzj/Gffvg9Gn4+mvX\nNO4K4Qsn3xTHj/veaVuUWLbM87LMYHHXXXDiRMHKeP11OHUqf3mPH3cdChswwArzxUMPucrs7eF2\n4oTlDzqcHD/u2K+T817rRM+ejglxb9gXP4Bn+f35u3CmcWNLpkD+M/Z2PHTIMXnuCfv34ywnWBs1\no51gGREoCVwGtAPigIXGmIUisilI5ecJZ0cxztitW/qjEgc5SBXIgCv+DzKfzwipAnCmTBnX9dW+\nfqTOG6zKR6HLglD1TELtxtKYwDd7eaNkyfyPY7t/l+XL+/9+Y2JcZbZvznMnL/flqzcSCGXKWJ/u\nG+zs92L/f9at6/+N3Pn+nd/kPeGpLPv/yB5nL8+fEUP7d2i/F/C8YdBennvd+el1hJtAvubdgLOV\nkVq2MGd2Ad+JyBkROQT8Ani0pDZq1KicY968efkQOf8EYgytJJkcpArHS0HVwbDwp7lBVQD2jU2L\nFlmmHOxNYP+B3HOPa3rnNwtnpbZ4ceB2/iPBokXhH3ZQHPzrX9bvqyBUqGB9j/nljjscv3dn3n/f\ndcPcE0/4L+uVVxznQ4f6l2vRItc07pPcduwb05z/S716Oc7r189d16OPOs79+SF54w3XDZsLPDiR\nd2bxYvvZPGCU0xE6AnlXWQo0MMYkA3uAO4Aebmm+AP5jjCkBlAZaAa96KmxUGBcK52U8HaAN85mP\ntbPp/IFw4KvPYGt7P7nyhn0s29sqCPc3f29vD85rwaMRdeoSOgJ5oyxbNrdZ8PxQkO+xZEnPczeJ\nia5vyN56Lc44v7GXLetbLmNyx3trM3tvunFja2d7pUquhvU8leXcK0hMtOxQudO4seVPIyHBtcfu\nPlzkjuN/nWY77DztO2MB8KsERCTLGDMAmIPVc5ggImuNMf2taHlHRNYZY74DVgJZwDsissZHsWEh\nL0ogmW05CqBVX9g781vY7Gc7oRcuvhj++ss1rFw53xurvBEXZzlaeeWVvCs1JW+8/jrceGOkpXBl\n3Djo2tU6f/VVa54hEPPZzrzwgsPbWzj49FPPE6SDB7sOpTz2WGCTrc542vDn/F7Zq5fVC/JGIAq0\nf//ATV48/3xug4yjR8OxY65OlZzrfuMNz+ZsnnkmsDqDTqiWHXk6KOi6uwA5e9aaftq1K9DlV9k5\nF7HDEWovyPdSrsqVRSZNyh0+eLBvmf/xD8eyNE+AyAcfBLedihOBLBFVogf3zWEikrOs1E6PHq7/\nGRD58UfvZYLI4497DrcvsU1KCkw+u1zr17vK17evdf7cc7ll69cvd373e3SOa9QoepaIFlp8WRB1\n5husHT0lR0Dm+wthp58+mx/sXbq77nJs2Apk05CiKBZPPQVPPuk7ze23u/6vqlTJbRU1UILRy77u\nOv+rnLzhqYfywAMFkydQwuhiJPxs2eI/TWUOcD3f0q07ZL2zosCewYyx7Ivk9UcVSDdVh4OU4oJ9\niOe557zbjOrSxdVW0/79/ssN1gqd0qUtG2XO+Fu27KnuRx+1hvk82TW7//7wKIIiqQQC3phFFgeo\nytIa8Om+ZwqsACC0TjXCYa6hKBMMMxJK4cabw6DGja1JbGdDj4Hg7VnjKbxx48Dzh5MiqQTs+Gvg\nsbWvgJ3wUJV74Rc/ZjW9cNll1jK4EiXgyy/zvznE3xtKVlbB120XZ7T9FG9WhO3hdo9rgeDv2eIe\nHwz/4KGiyP0tnnoKfv/dOvf1Rb1FPx7duYTxl8KiP9/Od33VqjkeLuXLh25DiD7ACoa2n+LLV4Lz\nEQg1a1rLSQNNn5eynQn1xkgogj2B0aNhk599ylc3+T/6r5zIgXLQb+OufNe1a5dj/fLata72SxRF\nKbosXWq93R8+nL/8cXFw8qRnxbBmjSN8+3ZrgnzChPzL6o8ipwTAsSvvq688x/+8ciIAVc8dh4z8\n21uo6WRBqUg6m1AUxSOVKlmf3pSAv+EiuxLwhPO8RbVqkJSUd/nyQpHsJO/YYX2ePp07rmbKZADi\nK/xVIAWgKEromTbNYRxu5kzv/rQLG3kZGgqFXxBnimRPwBe7tvUB4PhxH74GI0C0GpdSlEjSw8lA\njSfXndFKoKt+AkkX6j1GxUoJxDaYCZugc6mpkH/XAIqiKIA1JOzJ9IW/h/tzz8GePfmfUwgmxUgJ\nZJOxydrO91WGu/27yKM9AUUpfJQvn9tGUCDYrQU/9lhw5ckPhXZOICXF1bysPx6tcwsAL5a9h2i0\nlpGWlveNKoqiKAUl+p6GAbJ9u8MWfyC0qPAFq5Ni+dfp8SGTqSAMGeKY0FYUpXATDTuBA6XQKgFP\neHUa03QijfbD0+nvhVUeRVEUX/jznRwOCrUScB9H/89/PKeLvfH/aHQAFmalBa1uf84hFEUpvgTa\nExg0CPbuDa0s/ijUSsCZVaucXbM5UWkDM2dYp7upFbT6/PkmVRSl+BKomZKSJT074AknhVoJOPcE\n0tLgyBG3BDHnYOAFdArA3b0vh+523n3XsQtZV/MoiuKNRx+Fn36KtBSBUeiUwL59cPSode7c5Tp3\nzkPiy8cRY7Pe1wrfnqlfftl/3X37OqyEqhJQFMUbFSqEfqdvsCh0+wSqV/fsvPrYMQ+JL/qE/3xe\nATjOEnx7zA7E2bWd0aPh6qsDT68oihKtFDolIGLttAP/b+OlT8TzwKrjAZVbokTusKQka0dfYqLr\nUFNRsV+iKIpS6JQAWCac7bRoYW0cy4XJ5sPl3wPQEk8zxq7Yh5bi4uCiiyxTsWqDXlGUok6hfswZ\nYzmQ8bhp7JJJXLUdDpPIUlr6LcuuBPr3hyVLcocriqIURQq1ErBz8KCHwAtnsrA2/B+BbRCzP+zV\nj6+iKMWJQjkcFBAXzqLRl6VYjQcTfx4QsSaX4+IcYboCSFGUok6h7gl88YX3uPj9Nah0sgRbqBdQ\nWbGx1rIu53kA3RCmKEpRp2j2BDoMot2hvzGU82sxtGFDeP99uPRS1/AtW8Lj5FlRFCWSFD0lYLKh\n9Ti6fg4Hqew3eb9+0NLDvHHduk5F6rCQoihFlEI9HOSRhO0A9PkTNpLqNdmwYdanrv5RFKU4U2iU\nwJkzXkxDuFNlLWy7mj+4jCd43mOSGjXghResc1UCiqIUZwrNcFDVqnDrrQEkvOlezj9gaMYurz0B\n+4P/wgsDM//w6KOwf3/gsiqKohQWjITxVdgYI/mtzxi47DJYtsxPwlEGGWXLg+e6qleHv//OlxiK\noihhxxiDiIRkdrLQDAcFRMnT1Ey3TlPYGllZFEVRCgFFSwnceB+3roXN1GM7KV6T6TyAoiiKRaFS\nAn6XajadzGvfQraf21IloCiKYlGolIBPyh3kGtsIUEuWuESpP2BFURTPFCol8McfPiKvep55k6zT\noyS6RN18s2tS7QkoiqJYBKQEjDEdjTHrjDEbjDGP+0jXwhiTaYy5JXgiBkaZ5v8GoBwnPcjlel2/\nfjgkUhRFiX78KgFjTAzwBtABuBjoYYy50Eu6F4Hvgi3kcX/OwcoeZvon1ulpcvuJdFcCnryIKYqi\nFEcC2SzWEtgoItsBjDEzgC7AOrd0A4FPgBZBlRBrU5dPenWk67vwE2k+k115pWUo7qqrgiaaoihK\noSYQJVAT2Ol0vQtcXXUZY2oAXUWkrTHGvxuvPOK6sUtofU0HFq9+lexDjaDMUeqWXQrAdXyfK++T\nTzrmAH79NdiSKYqiFG6CNTE8DnCeKwiZ3c37693Cbz9/T9ahxlDiNHR8hCmfW3FZHnRa06Y6Eawo\niuKNQHoCu4E6Tte1bGHONAdmGGMMUBm43hiTKSKz3AsbNWpUznlaWhppaWl5ErjtiTU555JVjn/G\nQJudsI+qHtN36wZjx+apCkVRlIgyb9485nl0nh58/NoOMsaUANYD7YE9wBKgh4is9ZJ+IvCliHzm\nIS5ftoOcJ3YFw4UNXuXDQyO45IhjJZAhG08dEBF4+WUYOlR7BIqiFE4iajtIRLKAAcAc4C9ghois\nNcb0N8bc6ylLkGXM4RrmAbD+WFuaHjlBKutZa1JpynJ8jUBlZ4dKIkVRlMJNobAiau8JiO1Bb0qe\ngnNlA8orAmPGwOOPa09AUZTCiVoRdeLuLgSsAOw8+CDMmRMaeRRFUQozUa8EduywPstwGoCP5PY8\nlxEXB9ddF0ypFEVRigZRrwSSk63PVDaypmJZTq3sH1mBFEVRihBRrwTsXBC7jHXnnYa9l0ZaFEVR\nlCJDVCuB5538xDcp8wt/JZaFMxUjJ5CiKEoRI6qVwOLFjvN7T3/KusymkRNGURSlCBLVSuC0NRdM\nPOlUO3eM70uq5TdFUZRgEtVK4MgR67M7H/N7YkUOHG+Wp/xVPVuSUBRFUWxEtRKw7/RtwwImXX4U\n0mvnKX9MVN+doihK5Inqx6R9h+8lJRezpCawOzAr1f/4R+hkUhRFKUpEtRLIzoZYMriQtayqCoh/\nl2AXXwylSoVeNkVRlKJAIKakI0Z2NjQ0q9hWEU7Pmh5QHmNg1Cg4ehRGjAitfIqiKIWdqOwJnDlj\nfZ49C43jfmRlNWC1b3MR115rfRoDzZpZXsR0WEhRFMU3UdcT2L4dUlKs+YANG+CupEX8FXM+vkxF\n33QT3HwzVKkCrVqFTVRFUZRCT9QpgaNHXa8vZg3vG++e5o8fh/LlrfO77w6hYIqiKEWQqBwOcuaC\njL9Zl9E80mIoiqIUSaJaCZThNBecOMbWE1d4TWNC5tJeURSl6BO1SmDfPqjLVgDOHDvfazpVAoqi\nKPknapXAnDmQGLOfhTUNnKjuNV25cmEUSlEUpYgRtUpgwQKoVHYjh0qVhmzP89ctA9tArCiKongh\n6pSAfXjn7behcux2DsbGucR36QLnnWedq20gRVGUghFVj9GsLIflUIB6ZjOncHUqP24c/Otf1rnO\nByiKohSMqFICY8ZAWprjeviRGXTYfcQljd2oHMDAgeGRS1EUpagSVZvFtm93vf4o6VI+S64Eyx1h\nzkqgR4/wyKUoilJUiaqegDuJHOZIhqsPgQoVXBWBoiiKkn+iWwlkH+dwRh0AhgyxHv5VqkRYKEVR\nlCJEVCuBqhmnOJiRDLhOAicmRkggRVGUIkbUKoFYMqh25iw7MxrmiuvVC7ZtC79MiqIoRY2ITwwv\nWwb790PNmq7hKWxjV4UYsk7VAFx7AjExkJwcRiEVRVGKKBFXAs2aOc7vvddxXs9sYEtSNmy3lMCw\nYWEWTFEUpRgQVcNBH3zgOK9XZhmby8flmIyoWDFCQimKohRhokoJnDzpOK9X8Ue2VDsROWEURVGK\nAVGlBJzpu38Ru9JbR1oMRVGUIk1UKoEYsqiYdZYjB7w7k1EURVEKTlQqgTEMBeC7M3cAkJ0dSWkU\nRVGKLkbCaIPBGCP2+pYuhQ4dXK2G2hGs9aAm9iRkllMzEYqiFGuMMYhISOwmB9QTMMZ0NMasM8Zs\nMMY87iG+pzHmT9sx3xjT2F+ZX33lWQHUwbIilzAMyCxHQkIgEiqKoij5wa8SMMbEAG8AHYCLgR7G\nmAvdkm0BrhaRS4BngXf9l+s5fDspbKkIx8pY140a+StJURRFyS+B9ARaAhtFZLuIZAIzgC7OCURk\nkYik2y4XAW77f3PjSQk0YhUAH10MvLMUgFdfDUBCRVEUJV8EogRqAjudrnfh+yHfF5idH2Gm0BuA\nf10H/G1tJVZjcYqiKKEjqGYjjDFtgbuBK72lGTVqFADz5gGk2Q6LLErwQoMmsKwF2CeH1YWkoijF\njHnz5jHPekiGHL+rg4wxlwOjRKSj7XoYICLyklu6JsCnQEcR2eylLBERMjJg1Ch44QXnWOFvanBl\nvzNs+ehPSLf8CGzaBPXr5/PuFEVRigChXB0USE9gKdDAGJMM7AHuAFwcOxpj6mApgN7eFIAzpUvn\nDltMK6qzly0lGuUoAKvsACRUFEVR8oVfJSAiWcaYAcAcrDmECSKy1hjT34qWd4ARQBLwpjHGAJki\n0jIvghwjnifaoU99RVGUMBL2zWKQu77x3MM9vEeloXD4m+mw+o6cuC1boG7dsImoKIoSdUR8s1io\nuYf3mFqlKYfLAes7u8Rpx0BRFCV0RFwJtGIRe6lG7wdWWAGZ5VziVQkoiqKEjogrgUW05ofzEq0V\noc8fi7Q4iqIoxYqIKoEEjgLw3C3rQAxkVMiVJibiakpRFKXoEtFH7DR6ArC2KjA6w2MaHQ5SFEUJ\nHRFTAuU5Tidm89xVtoDsiPu8VxRFKXZETAm8wQAAhrcDnsn0mi4pKUwCKYqiFEMi8vodSwZ9mEz/\nDuXgUG2vvQB1JqMoihJaItITaGwzGT2xxSnIKB8JERRFURQipASasoLJTSCzJPDO77ni69TJnUdR\nFEUJPhFRAhPoy44EYNqXHuMr5F4pqiiKooSACCgBa6D/o4uBDTd6TNG1axjFURRFKcaEXQmIrcpV\nm4eEu2pFURTFjcgtzp/7osfgDz+EG29UB/OKoijhIOympAdfB2Obl4cXjueKr1cPNvt1SaMoilK8\nCKUp6Qj4E8jG7j/Ynf/8BwYMCJs4iqIohYIi5k/A+3106hRGMRRFUZTIm5J2pmLFSEugKIpSvIga\nq21qIkJRFCX8RLwnUKJEpCVQFEUpvkRUCfTuDWXKRFICRVGU4k1ElUBqqioBRVGUSBLxOYGlSyHT\nuzsBRVEUJYREVAmIQN26kZRAURSleBPxiWFFURQlcoRdCcybF+4aFUVRFG+E34qoeD5XFEVRwk9E\nlYCiKIoSWcKuBCpXdpynpIS7dkVRFMWZsFsRFRFOngRjoGxZ61NRFEXxTpEyJR3O+hRFUYoCRcyU\ntKIoihItqBJQFEUpxqgSUBRFKcaoElAURSnGBKQEjDEdjTHrjDEbjDGPe0nzujFmozFmhTGmaXDF\nVBRFUUKBXyVgjIkB3gA6ABcDPYwxF7qluR6oLyKpQH/grRDIWqSYp/YzctC2cKBt4UDbIjwE0hNo\nCWwUke0ikgnMALq4pekCTAYQkcVAgjGmWlAlLWLoD9yBtoUDbQsH2hbhIRAlUBPY6XS9yxbmK81u\nD2kURVGUKEMnhhVFUYoxfncMG2MuB0aJSEfb9TBAROQlpzRvAT+JyIe263XANSKyz60s3S6sKIqS\nDxB2IRgAAAQrSURBVEK1YzgQz2JLgQbGmGRgD3AH0MMtzSzgQeBDm9I46q4AIHQ3oSiKouQPv0pA\nRLKMMQOAOVjDRxNEZK0xpr8VLe+IyDfGmE7GmE3ASeDu0IqtKIqiBIOwGpBTFEVRoouwTQwHsuGs\nMGOMqWWM+dEY85cxZpUx5iFbeKIxZo4xZr0x5jtjTIJTnn/ZNtitNcb8wyn8MmPMSltbjYvE/QQD\nY0yMMWaZMWaW7bpYtoUxJsEY87Ht3v4yxrQqxm0xyBiz2nYfHxhjShWXtjDGTDDG7DPGrHQKC9q9\n29pyhi3PQmNMnYAEE5GQH1jKZhOQDMQCK4ALw1F3uA7gPKCp7bw8sB64EHgJGGoLfxx40XZ+EbAc\na0guxdY+9p7ZYqCF7fwboEOk7y+fbTIImArMsl0Xy7YA3gfutp2XBBKKY1sANYAtQCnb9YdAn+LS\nFsCVQFNgpVNY0O4duB9403Z+OzAjELnC1RMIZMNZoUZE9orICtv5CWAtUAvrPifZkk0CutrOO2N9\nSedEZBuwEWhpjDkPqCAiS23pJjvlKTQYY2oBnYDxTsHFri2MMfHAVSIyEcB2j+kUw7awUQKIM8aU\nBMpi7SkqFm0hIvOBI27Bwbx357I+AdoHIle4lEAgG86KDMaYFCyNvwioJraVUiKyF6hqS+Ztg11N\nrPaxU1jb6t/AEMB50qk4tkVd4KAxZqJtaOwdY0w5imFbiMjfwCvADqz7SheRuRTDtnCiahDvPSeP\niGQBR40xSf4E0M1iQcYYUx5LCz9s6xG4z7wX+Zl4Y8wNwD5bz8jXsuAi3xZY3fnLgP+KyGVYq+eG\nUTx/FxWx3laTsYaG4owx/6QYtoUPgnnvAS3JD5cS2A04T1LUsoUVKWxd3E+AKSLyhS14n92Okq0r\nt98Wvhuo7ZTd3ibewgsTbYDOxpgtwHSgnTFmCrC3GLbFLmCniPxuu/4USykUx9/FtcAWETlse1P9\nHLiC4tkWdoJ57zlxxpgSQLyIHPYnQLiUQM6GM2NMKawNZ7PCVHc4eQ9YIyKvOYXNAu6ynfcBvnAK\nv8M2o18XaAAssXUJ040xLY0xBrjTKU+hQESeEJE6IlIP67v+UUR6A19S/NpiH7DTGHO+Lag98BfF\n8HeBNQx0uTGmjO0e2gNrKF5tYXB9Qw/mvc+ylQHQHfgxIInCODPeEWvFzEZgWCRm50N8f22ALKyV\nT8uBZbZ7TgLm2u59DlDRKc+/sGb91wL/cApvBqyytdVrkb63ArbLNThWBxXLtgAuwXoRWgF8hrU6\nqLi2xUjbfa3EmsSMLS5tAUwD/gbOYinEu4HEYN07UBr4yBa+CEgJRC7dLKYoilKM0YlhRVGUYowq\nAUVRlGKMKgFFUZRijCoBRVGUYowqAUVRlGKMKgFFUZRijCoBRVGUYowqAUVRlGLM/wOnWm902PKa\nEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118fb44a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

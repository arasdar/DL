{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # vanilla Backprop\n",
    "#         dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "#         dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "#         self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "# #         dy =  dy @ self.W_fixed[2].T # done\n",
    "#         dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "# #             dy =  dy @ self.W_fixed[2].T # done\n",
    "#             dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#         dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3092, acc-0.1200, valid loss-2.3169, acc-0.0736, test loss-2.3132, acc-0.0864\n",
      "Iter-20, train loss-2.3009, acc-0.1400, valid loss-2.3141, acc-0.0778, test loss-2.3104, acc-0.0912\n",
      "Iter-30, train loss-2.3261, acc-0.0400, valid loss-2.3112, acc-0.0822, test loss-2.3074, acc-0.0957\n",
      "Iter-40, train loss-2.3269, acc-0.0200, valid loss-2.3081, acc-0.0874, test loss-2.3044, acc-0.1009\n",
      "Iter-50, train loss-2.3025, acc-0.0800, valid loss-2.3052, acc-0.0924, test loss-2.3014, acc-0.1068\n",
      "Iter-60, train loss-2.2883, acc-0.1800, valid loss-2.3023, acc-0.0984, test loss-2.2985, acc-0.1143\n",
      "Iter-70, train loss-2.3196, acc-0.0200, valid loss-2.2992, acc-0.1040, test loss-2.2955, acc-0.1210\n",
      "Iter-80, train loss-2.2822, acc-0.1400, valid loss-2.2962, acc-0.1116, test loss-2.2925, acc-0.1262\n",
      "Iter-90, train loss-2.3076, acc-0.1000, valid loss-2.2932, acc-0.1182, test loss-2.2895, acc-0.1311\n",
      "Iter-100, train loss-2.2728, acc-0.2000, valid loss-2.2903, acc-0.1252, test loss-2.2866, acc-0.1393\n",
      "Iter-110, train loss-2.3094, acc-0.1000, valid loss-2.2875, acc-0.1318, test loss-2.2838, acc-0.1459\n",
      "Iter-120, train loss-2.2800, acc-0.1200, valid loss-2.2846, acc-0.1394, test loss-2.2810, acc-0.1522\n",
      "Iter-130, train loss-2.2923, acc-0.1400, valid loss-2.2817, acc-0.1460, test loss-2.2780, acc-0.1598\n",
      "Iter-140, train loss-2.2828, acc-0.1000, valid loss-2.2790, acc-0.1524, test loss-2.2753, acc-0.1671\n",
      "Iter-150, train loss-2.2498, acc-0.2400, valid loss-2.2761, acc-0.1622, test loss-2.2724, acc-0.1744\n",
      "Iter-160, train loss-2.2661, acc-0.2400, valid loss-2.2734, acc-0.1698, test loss-2.2697, acc-0.1813\n",
      "Iter-170, train loss-2.2769, acc-0.1600, valid loss-2.2704, acc-0.1816, test loss-2.2667, acc-0.1911\n",
      "Iter-180, train loss-2.2744, acc-0.2000, valid loss-2.2677, acc-0.1894, test loss-2.2639, acc-0.1997\n",
      "Iter-190, train loss-2.2570, acc-0.1600, valid loss-2.2647, acc-0.1990, test loss-2.2610, acc-0.2090\n",
      "Iter-200, train loss-2.2677, acc-0.2400, valid loss-2.2618, acc-0.2086, test loss-2.2581, acc-0.2171\n",
      "Iter-210, train loss-2.2619, acc-0.2400, valid loss-2.2590, acc-0.2162, test loss-2.2553, acc-0.2262\n",
      "Iter-220, train loss-2.2544, acc-0.2600, valid loss-2.2564, acc-0.2226, test loss-2.2527, acc-0.2335\n",
      "Iter-230, train loss-2.2747, acc-0.1800, valid loss-2.2536, acc-0.2318, test loss-2.2499, acc-0.2407\n",
      "Iter-240, train loss-2.2491, acc-0.3000, valid loss-2.2506, acc-0.2396, test loss-2.2469, acc-0.2478\n",
      "Iter-250, train loss-2.2501, acc-0.2800, valid loss-2.2479, acc-0.2450, test loss-2.2442, acc-0.2575\n",
      "Iter-260, train loss-2.2389, acc-0.2400, valid loss-2.2450, acc-0.2528, test loss-2.2413, acc-0.2662\n",
      "Iter-270, train loss-2.2189, acc-0.3200, valid loss-2.2422, acc-0.2592, test loss-2.2384, acc-0.2737\n",
      "Iter-280, train loss-2.2182, acc-0.3200, valid loss-2.2393, acc-0.2678, test loss-2.2356, acc-0.2813\n",
      "Iter-290, train loss-2.2339, acc-0.3200, valid loss-2.2365, acc-0.2734, test loss-2.2328, acc-0.2893\n",
      "Iter-300, train loss-2.2397, acc-0.2800, valid loss-2.2337, acc-0.2804, test loss-2.2300, acc-0.2944\n",
      "Iter-310, train loss-2.2380, acc-0.3000, valid loss-2.2310, acc-0.2872, test loss-2.2272, acc-0.2998\n",
      "Iter-320, train loss-2.2267, acc-0.3200, valid loss-2.2282, acc-0.2942, test loss-2.2245, acc-0.3061\n",
      "Iter-330, train loss-2.2214, acc-0.3200, valid loss-2.2254, acc-0.2986, test loss-2.2217, acc-0.3114\n",
      "Iter-340, train loss-2.2151, acc-0.3200, valid loss-2.2225, acc-0.3034, test loss-2.2188, acc-0.3181\n",
      "Iter-350, train loss-2.2109, acc-0.2800, valid loss-2.2197, acc-0.3080, test loss-2.2160, acc-0.3240\n",
      "Iter-360, train loss-2.2086, acc-0.4000, valid loss-2.2167, acc-0.3142, test loss-2.2130, acc-0.3280\n",
      "Iter-370, train loss-2.2023, acc-0.3400, valid loss-2.2138, acc-0.3216, test loss-2.2101, acc-0.3350\n",
      "Iter-380, train loss-2.1935, acc-0.4400, valid loss-2.2109, acc-0.3262, test loss-2.2072, acc-0.3399\n",
      "Iter-390, train loss-2.1931, acc-0.4200, valid loss-2.2080, acc-0.3312, test loss-2.2043, acc-0.3445\n",
      "Iter-400, train loss-2.2058, acc-0.3600, valid loss-2.2052, acc-0.3368, test loss-2.2016, acc-0.3493\n",
      "Iter-410, train loss-2.1847, acc-0.4200, valid loss-2.2024, acc-0.3414, test loss-2.1988, acc-0.3547\n",
      "Iter-420, train loss-2.2383, acc-0.2400, valid loss-2.1998, acc-0.3450, test loss-2.1961, acc-0.3589\n",
      "Iter-430, train loss-2.1836, acc-0.3800, valid loss-2.1969, acc-0.3484, test loss-2.1932, acc-0.3646\n",
      "Iter-440, train loss-2.1626, acc-0.4600, valid loss-2.1939, acc-0.3546, test loss-2.1902, acc-0.3690\n",
      "Iter-450, train loss-2.1748, acc-0.3800, valid loss-2.1912, acc-0.3588, test loss-2.1875, acc-0.3740\n",
      "Iter-460, train loss-2.1879, acc-0.3800, valid loss-2.1884, acc-0.3622, test loss-2.1847, acc-0.3771\n",
      "Iter-470, train loss-2.1591, acc-0.4200, valid loss-2.1856, acc-0.3664, test loss-2.1819, acc-0.3810\n",
      "Iter-480, train loss-2.1647, acc-0.3800, valid loss-2.1829, acc-0.3696, test loss-2.1792, acc-0.3843\n",
      "Iter-490, train loss-2.1980, acc-0.4000, valid loss-2.1801, acc-0.3728, test loss-2.1763, acc-0.3871\n",
      "Iter-500, train loss-2.1854, acc-0.3600, valid loss-2.1774, acc-0.3762, test loss-2.1737, acc-0.3913\n",
      "Iter-510, train loss-2.1489, acc-0.4600, valid loss-2.1747, acc-0.3790, test loss-2.1709, acc-0.3954\n",
      "Iter-520, train loss-2.1645, acc-0.4000, valid loss-2.1719, acc-0.3814, test loss-2.1682, acc-0.3983\n",
      "Iter-530, train loss-2.1714, acc-0.4400, valid loss-2.1691, acc-0.3848, test loss-2.1654, acc-0.4014\n",
      "Iter-540, train loss-2.2096, acc-0.2200, valid loss-2.1663, acc-0.3882, test loss-2.1626, acc-0.4041\n",
      "Iter-550, train loss-2.1319, acc-0.5800, valid loss-2.1636, acc-0.3910, test loss-2.1599, acc-0.4066\n",
      "Iter-560, train loss-2.1612, acc-0.4400, valid loss-2.1609, acc-0.3934, test loss-2.1572, acc-0.4089\n",
      "Iter-570, train loss-2.1536, acc-0.5000, valid loss-2.1581, acc-0.3962, test loss-2.1543, acc-0.4121\n",
      "Iter-580, train loss-2.1523, acc-0.4200, valid loss-2.1553, acc-0.3988, test loss-2.1516, acc-0.4164\n",
      "Iter-590, train loss-2.1436, acc-0.4200, valid loss-2.1527, acc-0.4016, test loss-2.1489, acc-0.4188\n",
      "Iter-600, train loss-2.1690, acc-0.3600, valid loss-2.1500, acc-0.4060, test loss-2.1462, acc-0.4232\n",
      "Iter-610, train loss-2.1334, acc-0.4800, valid loss-2.1472, acc-0.4096, test loss-2.1434, acc-0.4262\n",
      "Iter-620, train loss-2.1123, acc-0.4600, valid loss-2.1444, acc-0.4128, test loss-2.1406, acc-0.4300\n",
      "Iter-630, train loss-2.1703, acc-0.3800, valid loss-2.1417, acc-0.4150, test loss-2.1379, acc-0.4337\n",
      "Iter-640, train loss-2.1254, acc-0.4800, valid loss-2.1390, acc-0.4166, test loss-2.1352, acc-0.4358\n",
      "Iter-650, train loss-2.0766, acc-0.5400, valid loss-2.1361, acc-0.4192, test loss-2.1323, acc-0.4379\n",
      "Iter-660, train loss-2.1284, acc-0.4800, valid loss-2.1333, acc-0.4234, test loss-2.1295, acc-0.4418\n",
      "Iter-670, train loss-2.1289, acc-0.5000, valid loss-2.1305, acc-0.4272, test loss-2.1267, acc-0.4452\n",
      "Iter-680, train loss-2.1323, acc-0.3800, valid loss-2.1276, acc-0.4312, test loss-2.1238, acc-0.4484\n",
      "Iter-690, train loss-2.1423, acc-0.4000, valid loss-2.1250, acc-0.4328, test loss-2.1211, acc-0.4511\n",
      "Iter-700, train loss-2.1261, acc-0.4200, valid loss-2.1220, acc-0.4358, test loss-2.1182, acc-0.4536\n",
      "Iter-710, train loss-2.1077, acc-0.4600, valid loss-2.1193, acc-0.4378, test loss-2.1154, acc-0.4558\n",
      "Iter-720, train loss-2.1307, acc-0.4000, valid loss-2.1165, acc-0.4408, test loss-2.1127, acc-0.4587\n",
      "Iter-730, train loss-2.1326, acc-0.5000, valid loss-2.1138, acc-0.4428, test loss-2.1099, acc-0.4614\n",
      "Iter-740, train loss-2.0916, acc-0.4400, valid loss-2.1109, acc-0.4450, test loss-2.1071, acc-0.4626\n",
      "Iter-750, train loss-2.1150, acc-0.4400, valid loss-2.1080, acc-0.4472, test loss-2.1042, acc-0.4643\n",
      "Iter-760, train loss-2.1006, acc-0.4600, valid loss-2.1052, acc-0.4492, test loss-2.1014, acc-0.4664\n",
      "Iter-770, train loss-2.0620, acc-0.5200, valid loss-2.1023, acc-0.4520, test loss-2.0985, acc-0.4681\n",
      "Iter-780, train loss-2.0776, acc-0.5000, valid loss-2.0994, acc-0.4538, test loss-2.0956, acc-0.4705\n",
      "Iter-790, train loss-2.0908, acc-0.4800, valid loss-2.0967, acc-0.4556, test loss-2.0928, acc-0.4714\n",
      "Iter-800, train loss-2.0983, acc-0.5000, valid loss-2.0938, acc-0.4584, test loss-2.0900, acc-0.4731\n",
      "Iter-810, train loss-2.0439, acc-0.5200, valid loss-2.0910, acc-0.4604, test loss-2.0871, acc-0.4749\n",
      "Iter-820, train loss-2.0617, acc-0.5200, valid loss-2.0880, acc-0.4622, test loss-2.0842, acc-0.4768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.0594, acc-0.6000, valid loss-2.0853, acc-0.4642, test loss-2.0814, acc-0.4791\n",
      "Iter-840, train loss-2.0752, acc-0.5000, valid loss-2.0825, acc-0.4656, test loss-2.0786, acc-0.4808\n",
      "Iter-850, train loss-2.1120, acc-0.4800, valid loss-2.0797, acc-0.4674, test loss-2.0759, acc-0.4820\n",
      "Iter-860, train loss-2.0707, acc-0.4000, valid loss-2.0769, acc-0.4698, test loss-2.0729, acc-0.4834\n",
      "Iter-870, train loss-2.1215, acc-0.3800, valid loss-2.0740, acc-0.4724, test loss-2.0701, acc-0.4851\n",
      "Iter-880, train loss-2.0811, acc-0.5200, valid loss-2.0712, acc-0.4736, test loss-2.0673, acc-0.4869\n",
      "Iter-890, train loss-2.0442, acc-0.5000, valid loss-2.0683, acc-0.4744, test loss-2.0643, acc-0.4877\n",
      "Iter-900, train loss-2.0700, acc-0.4800, valid loss-2.0652, acc-0.4760, test loss-2.0613, acc-0.4895\n",
      "Iter-910, train loss-2.0732, acc-0.5000, valid loss-2.0624, acc-0.4762, test loss-2.0584, acc-0.4898\n",
      "Iter-920, train loss-2.0683, acc-0.4400, valid loss-2.0595, acc-0.4780, test loss-2.0555, acc-0.4911\n",
      "Iter-930, train loss-2.0434, acc-0.6000, valid loss-2.0566, acc-0.4794, test loss-2.0527, acc-0.4925\n",
      "Iter-940, train loss-2.0408, acc-0.4600, valid loss-2.0537, acc-0.4820, test loss-2.0497, acc-0.4944\n",
      "Iter-950, train loss-2.0978, acc-0.4200, valid loss-2.0509, acc-0.4852, test loss-2.0470, acc-0.4969\n",
      "Iter-960, train loss-2.0418, acc-0.5400, valid loss-2.0481, acc-0.4864, test loss-2.0441, acc-0.4978\n",
      "Iter-970, train loss-2.0402, acc-0.5200, valid loss-2.0452, acc-0.4868, test loss-2.0412, acc-0.4987\n",
      "Iter-980, train loss-2.0240, acc-0.5200, valid loss-2.0425, acc-0.4892, test loss-2.0385, acc-0.4998\n",
      "Iter-990, train loss-2.0367, acc-0.5400, valid loss-2.0396, acc-0.4904, test loss-2.0356, acc-0.5008\n",
      "Iter-1000, train loss-2.0580, acc-0.5200, valid loss-2.0367, acc-0.4910, test loss-2.0327, acc-0.5013\n",
      "Iter-1010, train loss-2.1015, acc-0.4200, valid loss-2.0339, acc-0.4922, test loss-2.0299, acc-0.5017\n",
      "Iter-1020, train loss-1.9952, acc-0.6400, valid loss-2.0309, acc-0.4928, test loss-2.0269, acc-0.5022\n",
      "Iter-1030, train loss-2.0415, acc-0.5400, valid loss-2.0278, acc-0.4932, test loss-2.0238, acc-0.5028\n",
      "Iter-1040, train loss-2.0497, acc-0.4400, valid loss-2.0249, acc-0.4950, test loss-2.0209, acc-0.5041\n",
      "Iter-1050, train loss-2.0599, acc-0.4000, valid loss-2.0220, acc-0.4964, test loss-2.0180, acc-0.5060\n",
      "Iter-1060, train loss-1.9740, acc-0.6400, valid loss-2.0191, acc-0.4980, test loss-2.0151, acc-0.5068\n",
      "Iter-1070, train loss-2.0357, acc-0.4600, valid loss-2.0163, acc-0.4986, test loss-2.0123, acc-0.5073\n",
      "Iter-1080, train loss-2.0259, acc-0.5200, valid loss-2.0135, acc-0.4996, test loss-2.0095, acc-0.5085\n",
      "Iter-1090, train loss-1.9931, acc-0.6000, valid loss-2.0105, acc-0.5006, test loss-2.0066, acc-0.5100\n",
      "Iter-1100, train loss-1.9463, acc-0.6600, valid loss-2.0076, acc-0.5008, test loss-2.0037, acc-0.5106\n",
      "Iter-1110, train loss-2.0034, acc-0.4800, valid loss-2.0046, acc-0.5018, test loss-2.0007, acc-0.5113\n",
      "Iter-1120, train loss-1.9913, acc-0.5200, valid loss-2.0017, acc-0.5024, test loss-1.9977, acc-0.5119\n",
      "Iter-1130, train loss-1.9063, acc-0.5400, valid loss-1.9988, acc-0.5028, test loss-1.9949, acc-0.5134\n",
      "Iter-1140, train loss-2.0478, acc-0.4800, valid loss-1.9960, acc-0.5040, test loss-1.9921, acc-0.5141\n",
      "Iter-1150, train loss-2.0244, acc-0.4800, valid loss-1.9932, acc-0.5046, test loss-1.9893, acc-0.5159\n",
      "Iter-1160, train loss-1.9865, acc-0.5000, valid loss-1.9903, acc-0.5054, test loss-1.9864, acc-0.5168\n",
      "Iter-1170, train loss-1.9868, acc-0.4400, valid loss-1.9875, acc-0.5058, test loss-1.9836, acc-0.5172\n",
      "Iter-1180, train loss-2.0255, acc-0.4600, valid loss-1.9846, acc-0.5084, test loss-1.9807, acc-0.5183\n",
      "Iter-1190, train loss-1.9963, acc-0.4000, valid loss-1.9816, acc-0.5088, test loss-1.9777, acc-0.5186\n",
      "Iter-1200, train loss-1.9895, acc-0.4600, valid loss-1.9786, acc-0.5100, test loss-1.9747, acc-0.5195\n",
      "Iter-1210, train loss-1.9312, acc-0.5000, valid loss-1.9758, acc-0.5104, test loss-1.9720, acc-0.5210\n",
      "Iter-1220, train loss-2.0194, acc-0.4400, valid loss-1.9730, acc-0.5118, test loss-1.9692, acc-0.5222\n",
      "Iter-1230, train loss-1.9642, acc-0.5000, valid loss-1.9702, acc-0.5124, test loss-1.9663, acc-0.5233\n",
      "Iter-1240, train loss-1.9940, acc-0.4400, valid loss-1.9674, acc-0.5136, test loss-1.9635, acc-0.5237\n",
      "Iter-1250, train loss-1.9414, acc-0.5000, valid loss-1.9645, acc-0.5138, test loss-1.9606, acc-0.5246\n",
      "Iter-1260, train loss-1.9946, acc-0.4800, valid loss-1.9615, acc-0.5136, test loss-1.9577, acc-0.5241\n",
      "Iter-1270, train loss-2.0057, acc-0.4600, valid loss-1.9586, acc-0.5142, test loss-1.9547, acc-0.5253\n",
      "Iter-1280, train loss-1.9439, acc-0.4800, valid loss-1.9556, acc-0.5154, test loss-1.9517, acc-0.5258\n",
      "Iter-1290, train loss-2.0038, acc-0.4400, valid loss-1.9527, acc-0.5160, test loss-1.9489, acc-0.5269\n",
      "Iter-1300, train loss-1.9260, acc-0.5200, valid loss-1.9498, acc-0.5174, test loss-1.9460, acc-0.5275\n",
      "Iter-1310, train loss-1.9796, acc-0.4400, valid loss-1.9471, acc-0.5184, test loss-1.9433, acc-0.5278\n",
      "Iter-1320, train loss-1.8747, acc-0.7400, valid loss-1.9442, acc-0.5188, test loss-1.9404, acc-0.5282\n",
      "Iter-1330, train loss-1.9133, acc-0.5000, valid loss-1.9412, acc-0.5196, test loss-1.9374, acc-0.5290\n",
      "Iter-1340, train loss-1.9011, acc-0.6000, valid loss-1.9382, acc-0.5188, test loss-1.9343, acc-0.5299\n",
      "Iter-1350, train loss-1.9331, acc-0.4200, valid loss-1.9353, acc-0.5200, test loss-1.9315, acc-0.5301\n",
      "Iter-1360, train loss-1.9571, acc-0.4200, valid loss-1.9323, acc-0.5206, test loss-1.9286, acc-0.5311\n",
      "Iter-1370, train loss-1.8840, acc-0.5800, valid loss-1.9294, acc-0.5216, test loss-1.9257, acc-0.5316\n",
      "Iter-1380, train loss-1.9443, acc-0.6000, valid loss-1.9264, acc-0.5210, test loss-1.9227, acc-0.5313\n",
      "Iter-1390, train loss-1.8972, acc-0.5600, valid loss-1.9234, acc-0.5224, test loss-1.9196, acc-0.5323\n",
      "Iter-1400, train loss-1.9554, acc-0.5400, valid loss-1.9205, acc-0.5244, test loss-1.9168, acc-0.5333\n",
      "Iter-1410, train loss-1.9026, acc-0.5800, valid loss-1.9176, acc-0.5236, test loss-1.9139, acc-0.5342\n",
      "Iter-1420, train loss-1.9350, acc-0.5600, valid loss-1.9147, acc-0.5240, test loss-1.9110, acc-0.5354\n",
      "Iter-1430, train loss-1.8776, acc-0.6200, valid loss-1.9118, acc-0.5242, test loss-1.9081, acc-0.5348\n",
      "Iter-1440, train loss-1.9188, acc-0.5800, valid loss-1.9088, acc-0.5252, test loss-1.9051, acc-0.5355\n",
      "Iter-1450, train loss-1.9334, acc-0.4000, valid loss-1.9059, acc-0.5256, test loss-1.9022, acc-0.5356\n",
      "Iter-1460, train loss-1.8716, acc-0.5000, valid loss-1.9030, acc-0.5258, test loss-1.8993, acc-0.5356\n",
      "Iter-1470, train loss-1.8008, acc-0.6400, valid loss-1.9000, acc-0.5256, test loss-1.8963, acc-0.5353\n",
      "Iter-1480, train loss-1.9283, acc-0.4200, valid loss-1.8972, acc-0.5276, test loss-1.8935, acc-0.5359\n",
      "Iter-1490, train loss-1.8868, acc-0.5600, valid loss-1.8942, acc-0.5296, test loss-1.8905, acc-0.5367\n",
      "Iter-1500, train loss-1.8961, acc-0.5200, valid loss-1.8913, acc-0.5304, test loss-1.8876, acc-0.5368\n",
      "Iter-1510, train loss-1.8846, acc-0.4600, valid loss-1.8885, acc-0.5314, test loss-1.8848, acc-0.5378\n",
      "Iter-1520, train loss-1.8000, acc-0.7000, valid loss-1.8855, acc-0.5320, test loss-1.8819, acc-0.5384\n",
      "Iter-1530, train loss-1.9184, acc-0.5600, valid loss-1.8827, acc-0.5326, test loss-1.8791, acc-0.5396\n",
      "Iter-1540, train loss-1.9070, acc-0.4600, valid loss-1.8799, acc-0.5334, test loss-1.8762, acc-0.5399\n",
      "Iter-1550, train loss-1.8623, acc-0.5000, valid loss-1.8770, acc-0.5338, test loss-1.8734, acc-0.5404\n",
      "Iter-1560, train loss-1.8698, acc-0.6200, valid loss-1.8742, acc-0.5350, test loss-1.8705, acc-0.5401\n",
      "Iter-1570, train loss-1.8700, acc-0.5400, valid loss-1.8712, acc-0.5364, test loss-1.8676, acc-0.5407\n",
      "Iter-1580, train loss-1.9006, acc-0.4600, valid loss-1.8682, acc-0.5368, test loss-1.8646, acc-0.5411\n",
      "Iter-1590, train loss-1.7892, acc-0.5000, valid loss-1.8654, acc-0.5376, test loss-1.8618, acc-0.5416\n",
      "Iter-1600, train loss-1.9168, acc-0.3800, valid loss-1.8626, acc-0.5384, test loss-1.8590, acc-0.5420\n",
      "Iter-1610, train loss-1.8304, acc-0.6200, valid loss-1.8596, acc-0.5378, test loss-1.8560, acc-0.5420\n",
      "Iter-1620, train loss-1.8742, acc-0.5200, valid loss-1.8568, acc-0.5384, test loss-1.8532, acc-0.5437\n",
      "Iter-1630, train loss-1.8967, acc-0.5400, valid loss-1.8539, acc-0.5384, test loss-1.8504, acc-0.5444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-1.8165, acc-0.6400, valid loss-1.8511, acc-0.5394, test loss-1.8476, acc-0.5453\n",
      "Iter-1650, train loss-1.8489, acc-0.5000, valid loss-1.8482, acc-0.5398, test loss-1.8447, acc-0.5461\n",
      "Iter-1660, train loss-1.7967, acc-0.6600, valid loss-1.8453, acc-0.5400, test loss-1.8418, acc-0.5465\n",
      "Iter-1670, train loss-1.8408, acc-0.4600, valid loss-1.8423, acc-0.5410, test loss-1.8389, acc-0.5476\n",
      "Iter-1680, train loss-1.8434, acc-0.5400, valid loss-1.8395, acc-0.5410, test loss-1.8360, acc-0.5477\n",
      "Iter-1690, train loss-1.8288, acc-0.6000, valid loss-1.8364, acc-0.5406, test loss-1.8330, acc-0.5476\n",
      "Iter-1700, train loss-1.9330, acc-0.4200, valid loss-1.8336, acc-0.5414, test loss-1.8302, acc-0.5476\n",
      "Iter-1710, train loss-1.8455, acc-0.4000, valid loss-1.8308, acc-0.5420, test loss-1.8274, acc-0.5476\n",
      "Iter-1720, train loss-1.8178, acc-0.5200, valid loss-1.8279, acc-0.5420, test loss-1.8245, acc-0.5477\n",
      "Iter-1730, train loss-1.8468, acc-0.5400, valid loss-1.8251, acc-0.5426, test loss-1.8217, acc-0.5471\n",
      "Iter-1740, train loss-1.8265, acc-0.5800, valid loss-1.8223, acc-0.5424, test loss-1.8189, acc-0.5481\n",
      "Iter-1750, train loss-1.8573, acc-0.4800, valid loss-1.8195, acc-0.5428, test loss-1.8161, acc-0.5489\n",
      "Iter-1760, train loss-1.8715, acc-0.5400, valid loss-1.8168, acc-0.5432, test loss-1.8134, acc-0.5492\n",
      "Iter-1770, train loss-1.8558, acc-0.5000, valid loss-1.8138, acc-0.5442, test loss-1.8104, acc-0.5499\n",
      "Iter-1780, train loss-1.7905, acc-0.6000, valid loss-1.8109, acc-0.5454, test loss-1.8075, acc-0.5502\n",
      "Iter-1790, train loss-1.8867, acc-0.4000, valid loss-1.8080, acc-0.5456, test loss-1.8047, acc-0.5505\n",
      "Iter-1800, train loss-1.8203, acc-0.5200, valid loss-1.8052, acc-0.5462, test loss-1.8019, acc-0.5514\n",
      "Iter-1810, train loss-1.8378, acc-0.4800, valid loss-1.8024, acc-0.5468, test loss-1.7991, acc-0.5513\n",
      "Iter-1820, train loss-1.7986, acc-0.5800, valid loss-1.7995, acc-0.5474, test loss-1.7962, acc-0.5517\n",
      "Iter-1830, train loss-1.8147, acc-0.5600, valid loss-1.7966, acc-0.5480, test loss-1.7933, acc-0.5523\n",
      "Iter-1840, train loss-1.8541, acc-0.4400, valid loss-1.7938, acc-0.5482, test loss-1.7905, acc-0.5528\n",
      "Iter-1850, train loss-1.7948, acc-0.6200, valid loss-1.7909, acc-0.5490, test loss-1.7876, acc-0.5527\n",
      "Iter-1860, train loss-1.7779, acc-0.5000, valid loss-1.7882, acc-0.5492, test loss-1.7849, acc-0.5532\n",
      "Iter-1870, train loss-1.8147, acc-0.5800, valid loss-1.7854, acc-0.5490, test loss-1.7821, acc-0.5534\n",
      "Iter-1880, train loss-1.8892, acc-0.4400, valid loss-1.7826, acc-0.5490, test loss-1.7793, acc-0.5533\n",
      "Iter-1890, train loss-1.8083, acc-0.5800, valid loss-1.7797, acc-0.5492, test loss-1.7764, acc-0.5536\n",
      "Iter-1900, train loss-1.8089, acc-0.5000, valid loss-1.7770, acc-0.5492, test loss-1.7737, acc-0.5537\n",
      "Iter-1910, train loss-1.7138, acc-0.6600, valid loss-1.7741, acc-0.5496, test loss-1.7709, acc-0.5546\n",
      "Iter-1920, train loss-1.8368, acc-0.4200, valid loss-1.7712, acc-0.5506, test loss-1.7680, acc-0.5546\n",
      "Iter-1930, train loss-1.7468, acc-0.6000, valid loss-1.7684, acc-0.5512, test loss-1.7652, acc-0.5553\n",
      "Iter-1940, train loss-1.6818, acc-0.6000, valid loss-1.7655, acc-0.5520, test loss-1.7623, acc-0.5562\n",
      "Iter-1950, train loss-1.8415, acc-0.6000, valid loss-1.7625, acc-0.5520, test loss-1.7594, acc-0.5564\n",
      "Iter-1960, train loss-1.7407, acc-0.5200, valid loss-1.7598, acc-0.5528, test loss-1.7567, acc-0.5572\n",
      "Iter-1970, train loss-1.7759, acc-0.4200, valid loss-1.7570, acc-0.5528, test loss-1.7539, acc-0.5572\n",
      "Iter-1980, train loss-1.8283, acc-0.4800, valid loss-1.7542, acc-0.5540, test loss-1.7511, acc-0.5572\n",
      "Iter-1990, train loss-1.7755, acc-0.5600, valid loss-1.7512, acc-0.5540, test loss-1.7482, acc-0.5573\n",
      "Iter-2000, train loss-1.6583, acc-0.6800, valid loss-1.7484, acc-0.5550, test loss-1.7454, acc-0.5575\n",
      "Iter-2010, train loss-1.7600, acc-0.5200, valid loss-1.7456, acc-0.5554, test loss-1.7426, acc-0.5579\n",
      "Iter-2020, train loss-1.7822, acc-0.5000, valid loss-1.7429, acc-0.5554, test loss-1.7398, acc-0.5578\n",
      "Iter-2030, train loss-1.9372, acc-0.4000, valid loss-1.7402, acc-0.5562, test loss-1.7372, acc-0.5588\n",
      "Iter-2040, train loss-1.7736, acc-0.5000, valid loss-1.7374, acc-0.5562, test loss-1.7344, acc-0.5591\n",
      "Iter-2050, train loss-1.7812, acc-0.5200, valid loss-1.7345, acc-0.5572, test loss-1.7316, acc-0.5598\n",
      "Iter-2060, train loss-1.6957, acc-0.6200, valid loss-1.7318, acc-0.5566, test loss-1.7289, acc-0.5601\n",
      "Iter-2070, train loss-1.7146, acc-0.5000, valid loss-1.7291, acc-0.5574, test loss-1.7262, acc-0.5606\n",
      "Iter-2080, train loss-1.8385, acc-0.4600, valid loss-1.7265, acc-0.5586, test loss-1.7236, acc-0.5617\n",
      "Iter-2090, train loss-1.7735, acc-0.5200, valid loss-1.7236, acc-0.5580, test loss-1.7207, acc-0.5622\n",
      "Iter-2100, train loss-1.5916, acc-0.6400, valid loss-1.7210, acc-0.5584, test loss-1.7181, acc-0.5626\n",
      "Iter-2110, train loss-1.7659, acc-0.6200, valid loss-1.7183, acc-0.5596, test loss-1.7154, acc-0.5636\n",
      "Iter-2120, train loss-1.6939, acc-0.6400, valid loss-1.7155, acc-0.5608, test loss-1.7127, acc-0.5651\n",
      "Iter-2130, train loss-1.7025, acc-0.6600, valid loss-1.7129, acc-0.5612, test loss-1.7101, acc-0.5653\n",
      "Iter-2140, train loss-1.8170, acc-0.5200, valid loss-1.7103, acc-0.5622, test loss-1.7075, acc-0.5666\n",
      "Iter-2150, train loss-1.6783, acc-0.4800, valid loss-1.7076, acc-0.5630, test loss-1.7048, acc-0.5671\n",
      "Iter-2160, train loss-1.6859, acc-0.6200, valid loss-1.7049, acc-0.5624, test loss-1.7021, acc-0.5672\n",
      "Iter-2170, train loss-1.7130, acc-0.5200, valid loss-1.7021, acc-0.5622, test loss-1.6994, acc-0.5671\n",
      "Iter-2180, train loss-1.7068, acc-0.5600, valid loss-1.6994, acc-0.5630, test loss-1.6967, acc-0.5675\n",
      "Iter-2190, train loss-1.6108, acc-0.6800, valid loss-1.6967, acc-0.5628, test loss-1.6940, acc-0.5674\n",
      "Iter-2200, train loss-1.6820, acc-0.5200, valid loss-1.6940, acc-0.5632, test loss-1.6913, acc-0.5681\n",
      "Iter-2210, train loss-1.7553, acc-0.4800, valid loss-1.6912, acc-0.5636, test loss-1.6885, acc-0.5681\n",
      "Iter-2220, train loss-1.7274, acc-0.5000, valid loss-1.6884, acc-0.5646, test loss-1.6858, acc-0.5681\n",
      "Iter-2230, train loss-1.6565, acc-0.6600, valid loss-1.6857, acc-0.5646, test loss-1.6831, acc-0.5685\n",
      "Iter-2240, train loss-1.6788, acc-0.5400, valid loss-1.6829, acc-0.5646, test loss-1.6804, acc-0.5686\n",
      "Iter-2250, train loss-1.7005, acc-0.5800, valid loss-1.6801, acc-0.5644, test loss-1.6776, acc-0.5683\n",
      "Iter-2260, train loss-1.7098, acc-0.5600, valid loss-1.6774, acc-0.5648, test loss-1.6749, acc-0.5684\n",
      "Iter-2270, train loss-1.6904, acc-0.5000, valid loss-1.6747, acc-0.5654, test loss-1.6722, acc-0.5694\n",
      "Iter-2280, train loss-1.6763, acc-0.5600, valid loss-1.6720, acc-0.5660, test loss-1.6695, acc-0.5700\n",
      "Iter-2290, train loss-1.7041, acc-0.4400, valid loss-1.6694, acc-0.5670, test loss-1.6669, acc-0.5704\n",
      "Iter-2300, train loss-1.6444, acc-0.6400, valid loss-1.6667, acc-0.5678, test loss-1.6643, acc-0.5710\n",
      "Iter-2310, train loss-1.6708, acc-0.5600, valid loss-1.6640, acc-0.5682, test loss-1.6616, acc-0.5713\n",
      "Iter-2320, train loss-1.6534, acc-0.5400, valid loss-1.6614, acc-0.5690, test loss-1.6590, acc-0.5720\n",
      "Iter-2330, train loss-1.7275, acc-0.5600, valid loss-1.6588, acc-0.5690, test loss-1.6564, acc-0.5720\n",
      "Iter-2340, train loss-1.6039, acc-0.6200, valid loss-1.6563, acc-0.5700, test loss-1.6539, acc-0.5729\n",
      "Iter-2350, train loss-1.7349, acc-0.5200, valid loss-1.6536, acc-0.5702, test loss-1.6512, acc-0.5725\n",
      "Iter-2360, train loss-1.5549, acc-0.6800, valid loss-1.6508, acc-0.5702, test loss-1.6485, acc-0.5728\n",
      "Iter-2370, train loss-1.6276, acc-0.6200, valid loss-1.6482, acc-0.5708, test loss-1.6459, acc-0.5734\n",
      "Iter-2380, train loss-1.6845, acc-0.5800, valid loss-1.6454, acc-0.5722, test loss-1.6432, acc-0.5744\n",
      "Iter-2390, train loss-1.6689, acc-0.5200, valid loss-1.6428, acc-0.5744, test loss-1.6407, acc-0.5766\n",
      "Iter-2400, train loss-1.6470, acc-0.6400, valid loss-1.6403, acc-0.5744, test loss-1.6382, acc-0.5762\n",
      "Iter-2410, train loss-1.6414, acc-0.6000, valid loss-1.6377, acc-0.5748, test loss-1.6355, acc-0.5768\n",
      "Iter-2420, train loss-1.6854, acc-0.5200, valid loss-1.6350, acc-0.5752, test loss-1.6329, acc-0.5774\n",
      "Iter-2430, train loss-1.6588, acc-0.5600, valid loss-1.6323, acc-0.5762, test loss-1.6302, acc-0.5780\n",
      "Iter-2440, train loss-1.6920, acc-0.5400, valid loss-1.6298, acc-0.5772, test loss-1.6277, acc-0.5794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-1.6248, acc-0.5800, valid loss-1.6272, acc-0.5776, test loss-1.6251, acc-0.5798\n",
      "Iter-2460, train loss-1.6633, acc-0.5200, valid loss-1.6245, acc-0.5784, test loss-1.6225, acc-0.5798\n",
      "Iter-2470, train loss-1.6286, acc-0.6200, valid loss-1.6218, acc-0.5804, test loss-1.6199, acc-0.5811\n",
      "Iter-2480, train loss-1.6731, acc-0.5400, valid loss-1.6193, acc-0.5810, test loss-1.6174, acc-0.5822\n",
      "Iter-2490, train loss-1.6172, acc-0.5600, valid loss-1.6167, acc-0.5820, test loss-1.6147, acc-0.5830\n",
      "Iter-2500, train loss-1.5846, acc-0.6200, valid loss-1.6140, acc-0.5814, test loss-1.6121, acc-0.5832\n",
      "Iter-2510, train loss-1.5944, acc-0.5800, valid loss-1.6113, acc-0.5826, test loss-1.6095, acc-0.5835\n",
      "Iter-2520, train loss-1.5935, acc-0.5200, valid loss-1.6088, acc-0.5834, test loss-1.6070, acc-0.5842\n",
      "Iter-2530, train loss-1.5191, acc-0.7000, valid loss-1.6062, acc-0.5834, test loss-1.6044, acc-0.5839\n",
      "Iter-2540, train loss-1.5113, acc-0.6600, valid loss-1.6035, acc-0.5834, test loss-1.6017, acc-0.5838\n",
      "Iter-2550, train loss-1.5897, acc-0.5800, valid loss-1.6010, acc-0.5834, test loss-1.5993, acc-0.5838\n",
      "Iter-2560, train loss-1.6443, acc-0.5600, valid loss-1.5985, acc-0.5840, test loss-1.5967, acc-0.5844\n",
      "Iter-2570, train loss-1.6923, acc-0.4800, valid loss-1.5960, acc-0.5840, test loss-1.5942, acc-0.5848\n",
      "Iter-2580, train loss-1.4633, acc-0.7000, valid loss-1.5933, acc-0.5840, test loss-1.5916, acc-0.5853\n",
      "Iter-2590, train loss-1.5502, acc-0.6000, valid loss-1.5907, acc-0.5854, test loss-1.5891, acc-0.5868\n",
      "Iter-2600, train loss-1.6189, acc-0.5400, valid loss-1.5882, acc-0.5860, test loss-1.5865, acc-0.5877\n",
      "Iter-2610, train loss-1.7084, acc-0.5000, valid loss-1.5858, acc-0.5860, test loss-1.5842, acc-0.5885\n",
      "Iter-2620, train loss-1.5927, acc-0.6200, valid loss-1.5834, acc-0.5864, test loss-1.5817, acc-0.5894\n",
      "Iter-2630, train loss-1.5903, acc-0.5400, valid loss-1.5807, acc-0.5864, test loss-1.5792, acc-0.5900\n",
      "Iter-2640, train loss-1.5683, acc-0.6600, valid loss-1.5783, acc-0.5878, test loss-1.5768, acc-0.5913\n",
      "Iter-2650, train loss-1.5171, acc-0.6200, valid loss-1.5758, acc-0.5886, test loss-1.5744, acc-0.5921\n",
      "Iter-2660, train loss-1.5582, acc-0.6000, valid loss-1.5733, acc-0.5886, test loss-1.5719, acc-0.5926\n",
      "Iter-2670, train loss-1.6022, acc-0.5200, valid loss-1.5708, acc-0.5896, test loss-1.5694, acc-0.5932\n",
      "Iter-2680, train loss-1.5946, acc-0.5800, valid loss-1.5684, acc-0.5894, test loss-1.5670, acc-0.5937\n",
      "Iter-2690, train loss-1.6117, acc-0.5600, valid loss-1.5659, acc-0.5904, test loss-1.5646, acc-0.5941\n",
      "Iter-2700, train loss-1.5392, acc-0.6200, valid loss-1.5634, acc-0.5914, test loss-1.5621, acc-0.5949\n",
      "Iter-2710, train loss-1.5893, acc-0.6000, valid loss-1.5609, acc-0.5926, test loss-1.5596, acc-0.5959\n",
      "Iter-2720, train loss-1.5131, acc-0.7000, valid loss-1.5585, acc-0.5926, test loss-1.5571, acc-0.5964\n",
      "Iter-2730, train loss-1.5150, acc-0.6000, valid loss-1.5561, acc-0.5928, test loss-1.5547, acc-0.5966\n",
      "Iter-2740, train loss-1.5902, acc-0.6200, valid loss-1.5536, acc-0.5928, test loss-1.5523, acc-0.5979\n",
      "Iter-2750, train loss-1.5441, acc-0.5400, valid loss-1.5511, acc-0.5928, test loss-1.5499, acc-0.5986\n",
      "Iter-2760, train loss-1.5683, acc-0.5400, valid loss-1.5488, acc-0.5930, test loss-1.5476, acc-0.5986\n",
      "Iter-2770, train loss-1.5190, acc-0.6600, valid loss-1.5462, acc-0.5938, test loss-1.5451, acc-0.5994\n",
      "Iter-2780, train loss-1.5289, acc-0.6200, valid loss-1.5438, acc-0.5936, test loss-1.5427, acc-0.6004\n",
      "Iter-2790, train loss-1.6047, acc-0.5200, valid loss-1.5414, acc-0.5952, test loss-1.5403, acc-0.6006\n",
      "Iter-2800, train loss-1.6110, acc-0.5200, valid loss-1.5390, acc-0.5950, test loss-1.5379, acc-0.6015\n",
      "Iter-2810, train loss-1.6588, acc-0.4600, valid loss-1.5365, acc-0.5968, test loss-1.5355, acc-0.6030\n",
      "Iter-2820, train loss-1.5269, acc-0.6600, valid loss-1.5341, acc-0.5970, test loss-1.5331, acc-0.6031\n",
      "Iter-2830, train loss-1.5176, acc-0.6200, valid loss-1.5317, acc-0.5982, test loss-1.5308, acc-0.6041\n",
      "Iter-2840, train loss-1.5965, acc-0.5200, valid loss-1.5294, acc-0.5998, test loss-1.5285, acc-0.6054\n",
      "Iter-2850, train loss-1.6550, acc-0.5600, valid loss-1.5269, acc-0.5998, test loss-1.5260, acc-0.6061\n",
      "Iter-2860, train loss-1.5736, acc-0.5400, valid loss-1.5246, acc-0.6000, test loss-1.5237, acc-0.6073\n",
      "Iter-2870, train loss-1.4741, acc-0.6400, valid loss-1.5222, acc-0.6008, test loss-1.5214, acc-0.6078\n",
      "Iter-2880, train loss-1.5364, acc-0.5800, valid loss-1.5198, acc-0.6018, test loss-1.5190, acc-0.6079\n",
      "Iter-2890, train loss-1.5300, acc-0.7400, valid loss-1.5176, acc-0.6020, test loss-1.5168, acc-0.6081\n",
      "Iter-2900, train loss-1.6121, acc-0.5200, valid loss-1.5151, acc-0.6034, test loss-1.5144, acc-0.6090\n",
      "Iter-2910, train loss-1.5969, acc-0.5400, valid loss-1.5129, acc-0.6042, test loss-1.5121, acc-0.6100\n",
      "Iter-2920, train loss-1.5090, acc-0.5800, valid loss-1.5106, acc-0.6054, test loss-1.5100, acc-0.6112\n",
      "Iter-2930, train loss-1.4528, acc-0.6000, valid loss-1.5083, acc-0.6056, test loss-1.5077, acc-0.6125\n",
      "Iter-2940, train loss-1.4855, acc-0.6400, valid loss-1.5060, acc-0.6060, test loss-1.5054, acc-0.6133\n",
      "Iter-2950, train loss-1.5695, acc-0.5600, valid loss-1.5037, acc-0.6076, test loss-1.5031, acc-0.6146\n",
      "Iter-2960, train loss-1.4184, acc-0.6200, valid loss-1.5014, acc-0.6074, test loss-1.5008, acc-0.6150\n",
      "Iter-2970, train loss-1.3732, acc-0.7000, valid loss-1.4990, acc-0.6072, test loss-1.4984, acc-0.6150\n",
      "Iter-2980, train loss-1.5750, acc-0.4600, valid loss-1.4966, acc-0.6072, test loss-1.4960, acc-0.6151\n",
      "Iter-2990, train loss-1.5150, acc-0.5400, valid loss-1.4942, acc-0.6072, test loss-1.4937, acc-0.6156\n",
      "Iter-3000, train loss-1.5837, acc-0.5600, valid loss-1.4919, acc-0.6088, test loss-1.4914, acc-0.6166\n",
      "Iter-3010, train loss-1.4114, acc-0.6600, valid loss-1.4896, acc-0.6092, test loss-1.4891, acc-0.6169\n",
      "Iter-3020, train loss-1.4773, acc-0.6600, valid loss-1.4873, acc-0.6094, test loss-1.4868, acc-0.6169\n",
      "Iter-3030, train loss-1.4748, acc-0.6200, valid loss-1.4850, acc-0.6104, test loss-1.4845, acc-0.6179\n",
      "Iter-3040, train loss-1.5691, acc-0.5600, valid loss-1.4827, acc-0.6120, test loss-1.4823, acc-0.6195\n",
      "Iter-3050, train loss-1.4690, acc-0.6000, valid loss-1.4805, acc-0.6142, test loss-1.4801, acc-0.6213\n",
      "Iter-3060, train loss-1.4360, acc-0.7000, valid loss-1.4781, acc-0.6150, test loss-1.4778, acc-0.6224\n",
      "Iter-3070, train loss-1.6384, acc-0.5600, valid loss-1.4759, acc-0.6146, test loss-1.4755, acc-0.6229\n",
      "Iter-3080, train loss-1.5188, acc-0.5800, valid loss-1.4735, acc-0.6158, test loss-1.4732, acc-0.6241\n",
      "Iter-3090, train loss-1.4328, acc-0.6600, valid loss-1.4713, acc-0.6158, test loss-1.4710, acc-0.6244\n",
      "Iter-3100, train loss-1.4329, acc-0.6800, valid loss-1.4690, acc-0.6166, test loss-1.4687, acc-0.6260\n",
      "Iter-3110, train loss-1.5081, acc-0.6000, valid loss-1.4667, acc-0.6186, test loss-1.4665, acc-0.6270\n",
      "Iter-3120, train loss-1.4722, acc-0.5600, valid loss-1.4644, acc-0.6190, test loss-1.4642, acc-0.6277\n",
      "Iter-3130, train loss-1.4678, acc-0.6600, valid loss-1.4621, acc-0.6200, test loss-1.4619, acc-0.6284\n",
      "Iter-3140, train loss-1.6563, acc-0.4600, valid loss-1.4600, acc-0.6214, test loss-1.4598, acc-0.6298\n",
      "Iter-3150, train loss-1.3768, acc-0.6800, valid loss-1.4578, acc-0.6216, test loss-1.4577, acc-0.6303\n",
      "Iter-3160, train loss-1.4247, acc-0.7000, valid loss-1.4555, acc-0.6232, test loss-1.4554, acc-0.6307\n",
      "Iter-3170, train loss-1.4386, acc-0.7600, valid loss-1.4534, acc-0.6242, test loss-1.4533, acc-0.6313\n",
      "Iter-3180, train loss-1.4538, acc-0.7000, valid loss-1.4512, acc-0.6248, test loss-1.4511, acc-0.6322\n",
      "Iter-3190, train loss-1.4298, acc-0.6800, valid loss-1.4489, acc-0.6262, test loss-1.4489, acc-0.6336\n",
      "Iter-3200, train loss-1.4556, acc-0.7400, valid loss-1.4468, acc-0.6272, test loss-1.4467, acc-0.6341\n",
      "Iter-3210, train loss-1.4504, acc-0.6600, valid loss-1.4446, acc-0.6276, test loss-1.4445, acc-0.6343\n",
      "Iter-3220, train loss-1.4447, acc-0.5600, valid loss-1.4423, acc-0.6278, test loss-1.4422, acc-0.6349\n",
      "Iter-3230, train loss-1.5718, acc-0.6200, valid loss-1.4401, acc-0.6286, test loss-1.4401, acc-0.6355\n",
      "Iter-3240, train loss-1.4535, acc-0.6600, valid loss-1.4379, acc-0.6286, test loss-1.4379, acc-0.6358\n",
      "Iter-3250, train loss-1.4446, acc-0.6600, valid loss-1.4359, acc-0.6298, test loss-1.4359, acc-0.6372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-1.4303, acc-0.6400, valid loss-1.4337, acc-0.6296, test loss-1.4337, acc-0.6371\n",
      "Iter-3270, train loss-1.4754, acc-0.6000, valid loss-1.4315, acc-0.6298, test loss-1.4316, acc-0.6381\n",
      "Iter-3280, train loss-1.4689, acc-0.5800, valid loss-1.4294, acc-0.6304, test loss-1.4295, acc-0.6385\n",
      "Iter-3290, train loss-1.4947, acc-0.6400, valid loss-1.4274, acc-0.6304, test loss-1.4274, acc-0.6393\n",
      "Iter-3300, train loss-1.3881, acc-0.6400, valid loss-1.4252, acc-0.6320, test loss-1.4253, acc-0.6391\n",
      "Iter-3310, train loss-1.5292, acc-0.5400, valid loss-1.4231, acc-0.6324, test loss-1.4232, acc-0.6395\n",
      "Iter-3320, train loss-1.3863, acc-0.6800, valid loss-1.4209, acc-0.6334, test loss-1.4211, acc-0.6404\n",
      "Iter-3330, train loss-1.5333, acc-0.5600, valid loss-1.4189, acc-0.6334, test loss-1.4190, acc-0.6411\n",
      "Iter-3340, train loss-1.3411, acc-0.6600, valid loss-1.4167, acc-0.6346, test loss-1.4169, acc-0.6418\n",
      "Iter-3350, train loss-1.4780, acc-0.5800, valid loss-1.4147, acc-0.6354, test loss-1.4148, acc-0.6429\n",
      "Iter-3360, train loss-1.3516, acc-0.6800, valid loss-1.4126, acc-0.6354, test loss-1.4127, acc-0.6430\n",
      "Iter-3370, train loss-1.4601, acc-0.5800, valid loss-1.4105, acc-0.6364, test loss-1.4106, acc-0.6432\n",
      "Iter-3380, train loss-1.4222, acc-0.6400, valid loss-1.4083, acc-0.6364, test loss-1.4085, acc-0.6436\n",
      "Iter-3390, train loss-1.4189, acc-0.6800, valid loss-1.4062, acc-0.6372, test loss-1.4065, acc-0.6454\n",
      "Iter-3400, train loss-1.3390, acc-0.7800, valid loss-1.4041, acc-0.6378, test loss-1.4044, acc-0.6454\n",
      "Iter-3410, train loss-1.5202, acc-0.5200, valid loss-1.4019, acc-0.6392, test loss-1.4023, acc-0.6465\n",
      "Iter-3420, train loss-1.4501, acc-0.5600, valid loss-1.3998, acc-0.6400, test loss-1.4001, acc-0.6475\n",
      "Iter-3430, train loss-1.3767, acc-0.6600, valid loss-1.3978, acc-0.6412, test loss-1.3981, acc-0.6483\n",
      "Iter-3440, train loss-1.3916, acc-0.6000, valid loss-1.3957, acc-0.6424, test loss-1.3961, acc-0.6491\n",
      "Iter-3450, train loss-1.3919, acc-0.6800, valid loss-1.3937, acc-0.6428, test loss-1.3941, acc-0.6501\n",
      "Iter-3460, train loss-1.4260, acc-0.6400, valid loss-1.3916, acc-0.6432, test loss-1.3921, acc-0.6512\n",
      "Iter-3470, train loss-1.4035, acc-0.6800, valid loss-1.3896, acc-0.6444, test loss-1.3901, acc-0.6526\n",
      "Iter-3480, train loss-1.3851, acc-0.6600, valid loss-1.3876, acc-0.6456, test loss-1.3881, acc-0.6532\n",
      "Iter-3490, train loss-1.4318, acc-0.6800, valid loss-1.3856, acc-0.6458, test loss-1.3861, acc-0.6539\n",
      "Iter-3500, train loss-1.3183, acc-0.6600, valid loss-1.3836, acc-0.6484, test loss-1.3841, acc-0.6552\n",
      "Iter-3510, train loss-1.3710, acc-0.6200, valid loss-1.3816, acc-0.6494, test loss-1.3821, acc-0.6562\n",
      "Iter-3520, train loss-1.3075, acc-0.7600, valid loss-1.3796, acc-0.6490, test loss-1.3801, acc-0.6563\n",
      "Iter-3530, train loss-1.2676, acc-0.7600, valid loss-1.3776, acc-0.6494, test loss-1.3782, acc-0.6569\n",
      "Iter-3540, train loss-1.5318, acc-0.5000, valid loss-1.3755, acc-0.6494, test loss-1.3761, acc-0.6575\n",
      "Iter-3550, train loss-1.2770, acc-0.7400, valid loss-1.3735, acc-0.6512, test loss-1.3742, acc-0.6585\n",
      "Iter-3560, train loss-1.3969, acc-0.6400, valid loss-1.3715, acc-0.6520, test loss-1.3722, acc-0.6590\n",
      "Iter-3570, train loss-1.4138, acc-0.5800, valid loss-1.3695, acc-0.6522, test loss-1.3702, acc-0.6598\n",
      "Iter-3580, train loss-1.2301, acc-0.7800, valid loss-1.3675, acc-0.6526, test loss-1.3682, acc-0.6607\n",
      "Iter-3590, train loss-1.3318, acc-0.6600, valid loss-1.3655, acc-0.6534, test loss-1.3662, acc-0.6616\n",
      "Iter-3600, train loss-1.3686, acc-0.6400, valid loss-1.3635, acc-0.6534, test loss-1.3643, acc-0.6625\n",
      "Iter-3610, train loss-1.3793, acc-0.6600, valid loss-1.3616, acc-0.6540, test loss-1.3624, acc-0.6630\n",
      "Iter-3620, train loss-1.3092, acc-0.7200, valid loss-1.3597, acc-0.6544, test loss-1.3604, acc-0.6633\n",
      "Iter-3630, train loss-1.4806, acc-0.5800, valid loss-1.3577, acc-0.6550, test loss-1.3585, acc-0.6643\n",
      "Iter-3640, train loss-1.3873, acc-0.6800, valid loss-1.3558, acc-0.6556, test loss-1.3566, acc-0.6653\n",
      "Iter-3650, train loss-1.2753, acc-0.6600, valid loss-1.3538, acc-0.6568, test loss-1.3546, acc-0.6659\n",
      "Iter-3660, train loss-1.4389, acc-0.6000, valid loss-1.3517, acc-0.6580, test loss-1.3526, acc-0.6664\n",
      "Iter-3670, train loss-1.2901, acc-0.6600, valid loss-1.3498, acc-0.6586, test loss-1.3507, acc-0.6670\n",
      "Iter-3680, train loss-1.3642, acc-0.7000, valid loss-1.3478, acc-0.6588, test loss-1.3488, acc-0.6682\n",
      "Iter-3690, train loss-1.3236, acc-0.6400, valid loss-1.3460, acc-0.6594, test loss-1.3470, acc-0.6683\n",
      "Iter-3700, train loss-1.3170, acc-0.6800, valid loss-1.3441, acc-0.6600, test loss-1.3451, acc-0.6694\n",
      "Iter-3710, train loss-1.3811, acc-0.6400, valid loss-1.3421, acc-0.6612, test loss-1.3431, acc-0.6698\n",
      "Iter-3720, train loss-1.3844, acc-0.6400, valid loss-1.3401, acc-0.6624, test loss-1.3411, acc-0.6709\n",
      "Iter-3730, train loss-1.2532, acc-0.6800, valid loss-1.3381, acc-0.6622, test loss-1.3392, acc-0.6722\n",
      "Iter-3740, train loss-1.3985, acc-0.6800, valid loss-1.3362, acc-0.6632, test loss-1.3373, acc-0.6722\n",
      "Iter-3750, train loss-1.3359, acc-0.7000, valid loss-1.3343, acc-0.6634, test loss-1.3354, acc-0.6721\n",
      "Iter-3760, train loss-1.3338, acc-0.6800, valid loss-1.3324, acc-0.6640, test loss-1.3336, acc-0.6728\n",
      "Iter-3770, train loss-1.3724, acc-0.6800, valid loss-1.3305, acc-0.6650, test loss-1.3317, acc-0.6738\n",
      "Iter-3780, train loss-1.2381, acc-0.7200, valid loss-1.3286, acc-0.6660, test loss-1.3299, acc-0.6747\n",
      "Iter-3790, train loss-1.4782, acc-0.5200, valid loss-1.3268, acc-0.6660, test loss-1.3281, acc-0.6744\n",
      "Iter-3800, train loss-1.3529, acc-0.6400, valid loss-1.3250, acc-0.6664, test loss-1.3262, acc-0.6750\n",
      "Iter-3810, train loss-1.2304, acc-0.7200, valid loss-1.3230, acc-0.6666, test loss-1.3243, acc-0.6760\n",
      "Iter-3820, train loss-1.4731, acc-0.5600, valid loss-1.3212, acc-0.6672, test loss-1.3225, acc-0.6772\n",
      "Iter-3830, train loss-1.3531, acc-0.6400, valid loss-1.3193, acc-0.6682, test loss-1.3206, acc-0.6778\n",
      "Iter-3840, train loss-1.3398, acc-0.5600, valid loss-1.3174, acc-0.6690, test loss-1.3188, acc-0.6782\n",
      "Iter-3850, train loss-1.3116, acc-0.6600, valid loss-1.3155, acc-0.6700, test loss-1.3169, acc-0.6794\n",
      "Iter-3860, train loss-1.2857, acc-0.7200, valid loss-1.3137, acc-0.6704, test loss-1.3151, acc-0.6794\n",
      "Iter-3870, train loss-1.2591, acc-0.7200, valid loss-1.3120, acc-0.6710, test loss-1.3134, acc-0.6796\n",
      "Iter-3880, train loss-1.2561, acc-0.7400, valid loss-1.3101, acc-0.6720, test loss-1.3117, acc-0.6798\n",
      "Iter-3890, train loss-1.5377, acc-0.5400, valid loss-1.3084, acc-0.6722, test loss-1.3099, acc-0.6801\n",
      "Iter-3900, train loss-1.3635, acc-0.6800, valid loss-1.3066, acc-0.6732, test loss-1.3081, acc-0.6804\n",
      "Iter-3910, train loss-1.4578, acc-0.5600, valid loss-1.3049, acc-0.6730, test loss-1.3064, acc-0.6809\n",
      "Iter-3920, train loss-1.3670, acc-0.7000, valid loss-1.3031, acc-0.6730, test loss-1.3047, acc-0.6815\n",
      "Iter-3930, train loss-1.2958, acc-0.7000, valid loss-1.3012, acc-0.6736, test loss-1.3029, acc-0.6822\n",
      "Iter-3940, train loss-1.4126, acc-0.6400, valid loss-1.2995, acc-0.6736, test loss-1.3011, acc-0.6819\n",
      "Iter-3950, train loss-1.2395, acc-0.7000, valid loss-1.2977, acc-0.6748, test loss-1.2994, acc-0.6830\n",
      "Iter-3960, train loss-1.2138, acc-0.7200, valid loss-1.2960, acc-0.6756, test loss-1.2977, acc-0.6826\n",
      "Iter-3970, train loss-1.2184, acc-0.7400, valid loss-1.2943, acc-0.6756, test loss-1.2959, acc-0.6835\n",
      "Iter-3980, train loss-1.2736, acc-0.7400, valid loss-1.2925, acc-0.6758, test loss-1.2942, acc-0.6837\n",
      "Iter-3990, train loss-1.3791, acc-0.6200, valid loss-1.2907, acc-0.6764, test loss-1.2924, acc-0.6847\n",
      "Iter-4000, train loss-1.3530, acc-0.6400, valid loss-1.2889, acc-0.6776, test loss-1.2906, acc-0.6858\n",
      "Iter-4010, train loss-1.2563, acc-0.7200, valid loss-1.2871, acc-0.6784, test loss-1.2888, acc-0.6868\n",
      "Iter-4020, train loss-1.4851, acc-0.5600, valid loss-1.2854, acc-0.6788, test loss-1.2871, acc-0.6873\n",
      "Iter-4030, train loss-1.3928, acc-0.6000, valid loss-1.2836, acc-0.6802, test loss-1.2854, acc-0.6882\n",
      "Iter-4040, train loss-1.2260, acc-0.7600, valid loss-1.2819, acc-0.6812, test loss-1.2837, acc-0.6884\n",
      "Iter-4050, train loss-1.3049, acc-0.6600, valid loss-1.2801, acc-0.6816, test loss-1.2819, acc-0.6895\n",
      "Iter-4060, train loss-1.1634, acc-0.7200, valid loss-1.2783, acc-0.6822, test loss-1.2801, acc-0.6892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-1.2609, acc-0.6600, valid loss-1.2766, acc-0.6822, test loss-1.2785, acc-0.6897\n",
      "Iter-4080, train loss-1.2548, acc-0.7400, valid loss-1.2748, acc-0.6834, test loss-1.2768, acc-0.6905\n",
      "Iter-4090, train loss-1.2641, acc-0.6600, valid loss-1.2730, acc-0.6838, test loss-1.2750, acc-0.6907\n",
      "Iter-4100, train loss-1.2650, acc-0.6800, valid loss-1.2713, acc-0.6838, test loss-1.2733, acc-0.6910\n",
      "Iter-4110, train loss-1.3622, acc-0.5800, valid loss-1.2697, acc-0.6836, test loss-1.2716, acc-0.6912\n",
      "Iter-4120, train loss-1.3256, acc-0.6200, valid loss-1.2681, acc-0.6840, test loss-1.2700, acc-0.6915\n",
      "Iter-4130, train loss-1.1496, acc-0.7800, valid loss-1.2664, acc-0.6844, test loss-1.2683, acc-0.6916\n",
      "Iter-4140, train loss-1.3261, acc-0.7000, valid loss-1.2647, acc-0.6848, test loss-1.2666, acc-0.6913\n",
      "Iter-4150, train loss-1.2670, acc-0.7000, valid loss-1.2630, acc-0.6856, test loss-1.2649, acc-0.6917\n",
      "Iter-4160, train loss-1.2510, acc-0.7000, valid loss-1.2612, acc-0.6862, test loss-1.2632, acc-0.6924\n",
      "Iter-4170, train loss-1.3267, acc-0.6600, valid loss-1.2596, acc-0.6864, test loss-1.2616, acc-0.6923\n",
      "Iter-4180, train loss-1.1307, acc-0.8200, valid loss-1.2579, acc-0.6860, test loss-1.2599, acc-0.6930\n",
      "Iter-4190, train loss-1.3312, acc-0.6200, valid loss-1.2562, acc-0.6864, test loss-1.2582, acc-0.6936\n",
      "Iter-4200, train loss-1.2551, acc-0.7200, valid loss-1.2546, acc-0.6876, test loss-1.2566, acc-0.6932\n",
      "Iter-4210, train loss-1.2560, acc-0.6800, valid loss-1.2529, acc-0.6878, test loss-1.2549, acc-0.6940\n",
      "Iter-4220, train loss-1.2810, acc-0.6400, valid loss-1.2513, acc-0.6882, test loss-1.2532, acc-0.6943\n",
      "Iter-4230, train loss-1.1233, acc-0.7400, valid loss-1.2496, acc-0.6884, test loss-1.2516, acc-0.6953\n",
      "Iter-4240, train loss-1.3799, acc-0.6600, valid loss-1.2480, acc-0.6888, test loss-1.2501, acc-0.6955\n",
      "Iter-4250, train loss-1.1589, acc-0.7600, valid loss-1.2464, acc-0.6888, test loss-1.2484, acc-0.6956\n",
      "Iter-4260, train loss-1.1870, acc-0.7800, valid loss-1.2447, acc-0.6892, test loss-1.2467, acc-0.6958\n",
      "Iter-4270, train loss-1.2509, acc-0.7000, valid loss-1.2430, acc-0.6894, test loss-1.2451, acc-0.6965\n",
      "Iter-4280, train loss-1.1989, acc-0.7400, valid loss-1.2414, acc-0.6896, test loss-1.2434, acc-0.6973\n",
      "Iter-4290, train loss-1.2739, acc-0.6000, valid loss-1.2398, acc-0.6894, test loss-1.2418, acc-0.6976\n",
      "Iter-4300, train loss-1.2622, acc-0.6400, valid loss-1.2382, acc-0.6900, test loss-1.2402, acc-0.6979\n",
      "Iter-4310, train loss-1.1886, acc-0.8200, valid loss-1.2365, acc-0.6908, test loss-1.2385, acc-0.6990\n",
      "Iter-4320, train loss-1.1538, acc-0.7600, valid loss-1.2348, acc-0.6910, test loss-1.2369, acc-0.6995\n",
      "Iter-4330, train loss-1.2524, acc-0.7200, valid loss-1.2333, acc-0.6918, test loss-1.2353, acc-0.6999\n",
      "Iter-4340, train loss-1.1488, acc-0.7200, valid loss-1.2317, acc-0.6918, test loss-1.2338, acc-0.7001\n",
      "Iter-4350, train loss-1.0988, acc-0.7600, valid loss-1.2300, acc-0.6912, test loss-1.2320, acc-0.7006\n",
      "Iter-4360, train loss-1.1674, acc-0.7400, valid loss-1.2283, acc-0.6924, test loss-1.2304, acc-0.7007\n",
      "Iter-4370, train loss-1.0801, acc-0.8000, valid loss-1.2267, acc-0.6934, test loss-1.2288, acc-0.7018\n",
      "Iter-4380, train loss-1.1352, acc-0.7600, valid loss-1.2251, acc-0.6946, test loss-1.2272, acc-0.7019\n",
      "Iter-4390, train loss-1.2669, acc-0.5800, valid loss-1.2234, acc-0.6960, test loss-1.2256, acc-0.7024\n",
      "Iter-4400, train loss-1.3089, acc-0.6000, valid loss-1.2219, acc-0.6962, test loss-1.2241, acc-0.7026\n",
      "Iter-4410, train loss-1.3402, acc-0.6800, valid loss-1.2204, acc-0.6970, test loss-1.2226, acc-0.7028\n",
      "Iter-4420, train loss-1.3654, acc-0.6800, valid loss-1.2189, acc-0.6968, test loss-1.2212, acc-0.7030\n",
      "Iter-4430, train loss-1.3128, acc-0.6600, valid loss-1.2174, acc-0.6970, test loss-1.2196, acc-0.7035\n",
      "Iter-4440, train loss-1.2420, acc-0.6800, valid loss-1.2157, acc-0.6980, test loss-1.2180, acc-0.7045\n",
      "Iter-4450, train loss-1.4898, acc-0.5600, valid loss-1.2142, acc-0.6986, test loss-1.2165, acc-0.7052\n",
      "Iter-4460, train loss-1.1948, acc-0.7400, valid loss-1.2125, acc-0.6990, test loss-1.2148, acc-0.7049\n",
      "Iter-4470, train loss-1.1767, acc-0.8000, valid loss-1.2110, acc-0.6998, test loss-1.2133, acc-0.7053\n",
      "Iter-4480, train loss-1.1879, acc-0.8000, valid loss-1.2094, acc-0.7000, test loss-1.2117, acc-0.7059\n",
      "Iter-4490, train loss-1.1262, acc-0.7800, valid loss-1.2079, acc-0.7010, test loss-1.2101, acc-0.7068\n",
      "Iter-4500, train loss-1.1558, acc-0.7600, valid loss-1.2063, acc-0.7012, test loss-1.2087, acc-0.7072\n",
      "Iter-4510, train loss-0.9491, acc-0.9200, valid loss-1.2048, acc-0.7014, test loss-1.2071, acc-0.7079\n",
      "Iter-4520, train loss-1.1621, acc-0.7800, valid loss-1.2032, acc-0.7018, test loss-1.2056, acc-0.7088\n",
      "Iter-4530, train loss-1.2109, acc-0.6800, valid loss-1.2017, acc-0.7020, test loss-1.2041, acc-0.7091\n",
      "Iter-4540, train loss-1.1632, acc-0.7600, valid loss-1.2002, acc-0.7026, test loss-1.2025, acc-0.7094\n",
      "Iter-4550, train loss-1.2151, acc-0.6600, valid loss-1.1987, acc-0.7032, test loss-1.2010, acc-0.7098\n",
      "Iter-4560, train loss-1.2193, acc-0.6800, valid loss-1.1972, acc-0.7040, test loss-1.1995, acc-0.7107\n",
      "Iter-4570, train loss-1.3060, acc-0.6800, valid loss-1.1957, acc-0.7048, test loss-1.1980, acc-0.7112\n",
      "Iter-4580, train loss-1.2418, acc-0.7000, valid loss-1.1942, acc-0.7060, test loss-1.1965, acc-0.7120\n",
      "Iter-4590, train loss-1.2382, acc-0.6800, valid loss-1.1927, acc-0.7066, test loss-1.1950, acc-0.7123\n",
      "Iter-4600, train loss-1.1368, acc-0.7200, valid loss-1.1912, acc-0.7074, test loss-1.1935, acc-0.7127\n",
      "Iter-4610, train loss-1.2274, acc-0.6600, valid loss-1.1897, acc-0.7074, test loss-1.1920, acc-0.7129\n",
      "Iter-4620, train loss-1.3695, acc-0.6000, valid loss-1.1882, acc-0.7074, test loss-1.1906, acc-0.7130\n",
      "Iter-4630, train loss-1.3892, acc-0.6200, valid loss-1.1866, acc-0.7080, test loss-1.1890, acc-0.7142\n",
      "Iter-4640, train loss-1.2155, acc-0.7000, valid loss-1.1852, acc-0.7082, test loss-1.1876, acc-0.7143\n",
      "Iter-4650, train loss-1.0077, acc-0.8200, valid loss-1.1836, acc-0.7090, test loss-1.1861, acc-0.7151\n",
      "Iter-4660, train loss-1.1529, acc-0.7000, valid loss-1.1822, acc-0.7090, test loss-1.1846, acc-0.7151\n",
      "Iter-4670, train loss-1.1714, acc-0.7800, valid loss-1.1806, acc-0.7100, test loss-1.1831, acc-0.7158\n",
      "Iter-4680, train loss-1.3096, acc-0.6000, valid loss-1.1792, acc-0.7100, test loss-1.1817, acc-0.7167\n",
      "Iter-4690, train loss-1.0063, acc-0.8200, valid loss-1.1778, acc-0.7106, test loss-1.1803, acc-0.7170\n",
      "Iter-4700, train loss-1.1093, acc-0.7200, valid loss-1.1764, acc-0.7116, test loss-1.1789, acc-0.7171\n",
      "Iter-4710, train loss-1.2829, acc-0.6200, valid loss-1.1750, acc-0.7116, test loss-1.1774, acc-0.7179\n",
      "Iter-4720, train loss-1.1645, acc-0.7000, valid loss-1.1735, acc-0.7124, test loss-1.1760, acc-0.7179\n",
      "Iter-4730, train loss-1.2445, acc-0.6600, valid loss-1.1721, acc-0.7126, test loss-1.1746, acc-0.7182\n",
      "Iter-4740, train loss-1.2994, acc-0.6800, valid loss-1.1706, acc-0.7130, test loss-1.1731, acc-0.7189\n",
      "Iter-4750, train loss-1.1874, acc-0.7000, valid loss-1.1691, acc-0.7138, test loss-1.1716, acc-0.7195\n",
      "Iter-4760, train loss-1.1584, acc-0.6600, valid loss-1.1677, acc-0.7140, test loss-1.1702, acc-0.7199\n",
      "Iter-4770, train loss-1.1119, acc-0.7400, valid loss-1.1663, acc-0.7140, test loss-1.1687, acc-0.7199\n",
      "Iter-4780, train loss-1.1499, acc-0.7000, valid loss-1.1647, acc-0.7146, test loss-1.1672, acc-0.7206\n",
      "Iter-4790, train loss-1.1982, acc-0.7200, valid loss-1.1633, acc-0.7146, test loss-1.1658, acc-0.7210\n",
      "Iter-4800, train loss-1.0651, acc-0.7400, valid loss-1.1618, acc-0.7148, test loss-1.1643, acc-0.7213\n",
      "Iter-4810, train loss-1.1110, acc-0.7800, valid loss-1.1605, acc-0.7142, test loss-1.1630, acc-0.7214\n",
      "Iter-4820, train loss-1.2178, acc-0.7200, valid loss-1.1591, acc-0.7148, test loss-1.1616, acc-0.7220\n",
      "Iter-4830, train loss-1.2381, acc-0.7200, valid loss-1.1575, acc-0.7146, test loss-1.1601, acc-0.7224\n",
      "Iter-4840, train loss-1.0505, acc-0.7600, valid loss-1.1562, acc-0.7150, test loss-1.1587, acc-0.7226\n",
      "Iter-4850, train loss-1.1885, acc-0.7200, valid loss-1.1548, acc-0.7146, test loss-1.1574, acc-0.7230\n",
      "Iter-4860, train loss-1.1697, acc-0.7800, valid loss-1.1533, acc-0.7148, test loss-1.1559, acc-0.7232\n",
      "Iter-4870, train loss-1.3285, acc-0.6200, valid loss-1.1519, acc-0.7162, test loss-1.1546, acc-0.7238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-1.3526, acc-0.6400, valid loss-1.1505, acc-0.7176, test loss-1.1532, acc-0.7248\n",
      "Iter-4890, train loss-1.0535, acc-0.7400, valid loss-1.1491, acc-0.7176, test loss-1.1518, acc-0.7251\n",
      "Iter-4900, train loss-1.0638, acc-0.7800, valid loss-1.1476, acc-0.7186, test loss-1.1504, acc-0.7258\n",
      "Iter-4910, train loss-1.1879, acc-0.7600, valid loss-1.1463, acc-0.7182, test loss-1.1490, acc-0.7260\n",
      "Iter-4920, train loss-1.1323, acc-0.7200, valid loss-1.1449, acc-0.7190, test loss-1.1476, acc-0.7267\n",
      "Iter-4930, train loss-1.1950, acc-0.6800, valid loss-1.1435, acc-0.7192, test loss-1.1462, acc-0.7270\n",
      "Iter-4940, train loss-1.1032, acc-0.6600, valid loss-1.1421, acc-0.7192, test loss-1.1448, acc-0.7277\n",
      "Iter-4950, train loss-1.2128, acc-0.7200, valid loss-1.1407, acc-0.7196, test loss-1.1435, acc-0.7280\n",
      "Iter-4960, train loss-1.0199, acc-0.7600, valid loss-1.1393, acc-0.7206, test loss-1.1421, acc-0.7285\n",
      "Iter-4970, train loss-1.1449, acc-0.7200, valid loss-1.1380, acc-0.7206, test loss-1.1407, acc-0.7286\n",
      "Iter-4980, train loss-1.2111, acc-0.7200, valid loss-1.1366, acc-0.7210, test loss-1.1394, acc-0.7292\n",
      "Iter-4990, train loss-1.2814, acc-0.6400, valid loss-1.1352, acc-0.7220, test loss-1.1381, acc-0.7298\n",
      "Iter-5000, train loss-1.1928, acc-0.7200, valid loss-1.1339, acc-0.7226, test loss-1.1367, acc-0.7299\n",
      "Iter-5010, train loss-1.0458, acc-0.8200, valid loss-1.1326, acc-0.7228, test loss-1.1355, acc-0.7305\n",
      "Iter-5020, train loss-1.4510, acc-0.5800, valid loss-1.1313, acc-0.7230, test loss-1.1341, acc-0.7307\n",
      "Iter-5030, train loss-0.9653, acc-0.7600, valid loss-1.1299, acc-0.7234, test loss-1.1328, acc-0.7312\n",
      "Iter-5040, train loss-1.1264, acc-0.7000, valid loss-1.1286, acc-0.7232, test loss-1.1315, acc-0.7310\n",
      "Iter-5050, train loss-1.2159, acc-0.6200, valid loss-1.1272, acc-0.7244, test loss-1.1301, acc-0.7317\n",
      "Iter-5060, train loss-1.0983, acc-0.7800, valid loss-1.1259, acc-0.7246, test loss-1.1288, acc-0.7325\n",
      "Iter-5070, train loss-1.0225, acc-0.8000, valid loss-1.1246, acc-0.7248, test loss-1.1275, acc-0.7329\n",
      "Iter-5080, train loss-1.1085, acc-0.7600, valid loss-1.1233, acc-0.7250, test loss-1.1262, acc-0.7329\n",
      "Iter-5090, train loss-1.1035, acc-0.8000, valid loss-1.1220, acc-0.7252, test loss-1.1249, acc-0.7334\n",
      "Iter-5100, train loss-1.1004, acc-0.7000, valid loss-1.1206, acc-0.7254, test loss-1.1235, acc-0.7335\n",
      "Iter-5110, train loss-1.0929, acc-0.7800, valid loss-1.1192, acc-0.7256, test loss-1.1221, acc-0.7336\n",
      "Iter-5120, train loss-1.0326, acc-0.7800, valid loss-1.1178, acc-0.7256, test loss-1.1208, acc-0.7335\n",
      "Iter-5130, train loss-1.1981, acc-0.6800, valid loss-1.1164, acc-0.7256, test loss-1.1195, acc-0.7339\n",
      "Iter-5140, train loss-1.1946, acc-0.6800, valid loss-1.1151, acc-0.7256, test loss-1.1182, acc-0.7339\n",
      "Iter-5150, train loss-1.1805, acc-0.6600, valid loss-1.1138, acc-0.7256, test loss-1.1170, acc-0.7342\n",
      "Iter-5160, train loss-1.4591, acc-0.5600, valid loss-1.1126, acc-0.7264, test loss-1.1158, acc-0.7347\n",
      "Iter-5170, train loss-1.1683, acc-0.6800, valid loss-1.1113, acc-0.7264, test loss-1.1145, acc-0.7345\n",
      "Iter-5180, train loss-1.1524, acc-0.7400, valid loss-1.1100, acc-0.7268, test loss-1.1132, acc-0.7357\n",
      "Iter-5190, train loss-1.1848, acc-0.6800, valid loss-1.1087, acc-0.7272, test loss-1.1119, acc-0.7356\n",
      "Iter-5200, train loss-1.1844, acc-0.6000, valid loss-1.1074, acc-0.7268, test loss-1.1107, acc-0.7358\n",
      "Iter-5210, train loss-1.2661, acc-0.6000, valid loss-1.1061, acc-0.7276, test loss-1.1094, acc-0.7356\n",
      "Iter-5220, train loss-1.0035, acc-0.7400, valid loss-1.1048, acc-0.7276, test loss-1.1081, acc-0.7360\n",
      "Iter-5230, train loss-1.1920, acc-0.7400, valid loss-1.1035, acc-0.7282, test loss-1.1068, acc-0.7365\n",
      "Iter-5240, train loss-1.1142, acc-0.7000, valid loss-1.1023, acc-0.7284, test loss-1.1056, acc-0.7368\n",
      "Iter-5250, train loss-1.2365, acc-0.6400, valid loss-1.1011, acc-0.7292, test loss-1.1044, acc-0.7378\n",
      "Iter-5260, train loss-1.0595, acc-0.7200, valid loss-1.0998, acc-0.7298, test loss-1.1032, acc-0.7380\n",
      "Iter-5270, train loss-1.2367, acc-0.7200, valid loss-1.0986, acc-0.7310, test loss-1.1019, acc-0.7381\n",
      "Iter-5280, train loss-1.0427, acc-0.7800, valid loss-1.0974, acc-0.7308, test loss-1.1007, acc-0.7380\n",
      "Iter-5290, train loss-1.0812, acc-0.7800, valid loss-1.0961, acc-0.7312, test loss-1.0994, acc-0.7383\n",
      "Iter-5300, train loss-1.1171, acc-0.6600, valid loss-1.0949, acc-0.7312, test loss-1.0982, acc-0.7389\n",
      "Iter-5310, train loss-1.1803, acc-0.6600, valid loss-1.0936, acc-0.7312, test loss-1.0969, acc-0.7394\n",
      "Iter-5320, train loss-1.2197, acc-0.7400, valid loss-1.0922, acc-0.7316, test loss-1.0956, acc-0.7405\n",
      "Iter-5330, train loss-1.0342, acc-0.8000, valid loss-1.0910, acc-0.7316, test loss-1.0944, acc-0.7413\n",
      "Iter-5340, train loss-1.0747, acc-0.7800, valid loss-1.0898, acc-0.7320, test loss-1.0932, acc-0.7415\n",
      "Iter-5350, train loss-1.1273, acc-0.7600, valid loss-1.0885, acc-0.7328, test loss-1.0919, acc-0.7416\n",
      "Iter-5360, train loss-1.0264, acc-0.7200, valid loss-1.0872, acc-0.7330, test loss-1.0907, acc-0.7414\n",
      "Iter-5370, train loss-1.0686, acc-0.7800, valid loss-1.0860, acc-0.7332, test loss-1.0895, acc-0.7412\n",
      "Iter-5380, train loss-1.2796, acc-0.6400, valid loss-1.0848, acc-0.7334, test loss-1.0884, acc-0.7415\n",
      "Iter-5390, train loss-1.0431, acc-0.7600, valid loss-1.0837, acc-0.7334, test loss-1.0872, acc-0.7418\n",
      "Iter-5400, train loss-1.0887, acc-0.7800, valid loss-1.0824, acc-0.7334, test loss-1.0860, acc-0.7416\n",
      "Iter-5410, train loss-1.1169, acc-0.7400, valid loss-1.0813, acc-0.7336, test loss-1.0848, acc-0.7422\n",
      "Iter-5420, train loss-0.9724, acc-0.8200, valid loss-1.0800, acc-0.7338, test loss-1.0836, acc-0.7432\n",
      "Iter-5430, train loss-0.9975, acc-0.8400, valid loss-1.0788, acc-0.7348, test loss-1.0824, acc-0.7434\n",
      "Iter-5440, train loss-0.9727, acc-0.7800, valid loss-1.0776, acc-0.7350, test loss-1.0812, acc-0.7434\n",
      "Iter-5450, train loss-1.2022, acc-0.6600, valid loss-1.0764, acc-0.7346, test loss-1.0801, acc-0.7435\n",
      "Iter-5460, train loss-1.1646, acc-0.7400, valid loss-1.0753, acc-0.7352, test loss-1.0789, acc-0.7443\n",
      "Iter-5470, train loss-1.1100, acc-0.7400, valid loss-1.0741, acc-0.7348, test loss-1.0778, acc-0.7441\n",
      "Iter-5480, train loss-1.0416, acc-0.7800, valid loss-1.0729, acc-0.7356, test loss-1.0766, acc-0.7445\n",
      "Iter-5490, train loss-1.1638, acc-0.7200, valid loss-1.0717, acc-0.7360, test loss-1.0754, acc-0.7448\n",
      "Iter-5500, train loss-1.0357, acc-0.8000, valid loss-1.0705, acc-0.7362, test loss-1.0742, acc-0.7452\n",
      "Iter-5510, train loss-0.9685, acc-0.8600, valid loss-1.0693, acc-0.7366, test loss-1.0730, acc-0.7451\n",
      "Iter-5520, train loss-1.1602, acc-0.6600, valid loss-1.0682, acc-0.7364, test loss-1.0718, acc-0.7452\n",
      "Iter-5530, train loss-1.0119, acc-0.8000, valid loss-1.0670, acc-0.7368, test loss-1.0706, acc-0.7453\n",
      "Iter-5540, train loss-1.0802, acc-0.7800, valid loss-1.0657, acc-0.7370, test loss-1.0694, acc-0.7455\n",
      "Iter-5550, train loss-1.0818, acc-0.7600, valid loss-1.0646, acc-0.7372, test loss-1.0683, acc-0.7456\n",
      "Iter-5560, train loss-1.1168, acc-0.7000, valid loss-1.0634, acc-0.7374, test loss-1.0671, acc-0.7458\n",
      "Iter-5570, train loss-0.9953, acc-0.7600, valid loss-1.0623, acc-0.7370, test loss-1.0660, acc-0.7458\n",
      "Iter-5580, train loss-0.8699, acc-0.8400, valid loss-1.0611, acc-0.7378, test loss-1.0648, acc-0.7462\n",
      "Iter-5590, train loss-1.0532, acc-0.7600, valid loss-1.0599, acc-0.7384, test loss-1.0636, acc-0.7466\n",
      "Iter-5600, train loss-1.0089, acc-0.7600, valid loss-1.0588, acc-0.7382, test loss-1.0625, acc-0.7470\n",
      "Iter-5610, train loss-0.9035, acc-0.8400, valid loss-1.0576, acc-0.7390, test loss-1.0613, acc-0.7469\n",
      "Iter-5620, train loss-1.1051, acc-0.7600, valid loss-1.0565, acc-0.7388, test loss-1.0603, acc-0.7470\n",
      "Iter-5630, train loss-1.1437, acc-0.7400, valid loss-1.0554, acc-0.7392, test loss-1.0592, acc-0.7474\n",
      "Iter-5640, train loss-1.0750, acc-0.6400, valid loss-1.0544, acc-0.7400, test loss-1.0581, acc-0.7475\n",
      "Iter-5650, train loss-0.9828, acc-0.7000, valid loss-1.0533, acc-0.7400, test loss-1.0570, acc-0.7481\n",
      "Iter-5660, train loss-0.9881, acc-0.8000, valid loss-1.0521, acc-0.7404, test loss-1.0559, acc-0.7478\n",
      "Iter-5670, train loss-1.0885, acc-0.7600, valid loss-1.0510, acc-0.7402, test loss-1.0547, acc-0.7478\n",
      "Iter-5680, train loss-1.0014, acc-0.7200, valid loss-1.0498, acc-0.7406, test loss-1.0536, acc-0.7483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-1.1004, acc-0.7000, valid loss-1.0487, acc-0.7412, test loss-1.0525, acc-0.7486\n",
      "Iter-5700, train loss-1.0044, acc-0.8400, valid loss-1.0476, acc-0.7412, test loss-1.0514, acc-0.7488\n",
      "Iter-5710, train loss-1.1254, acc-0.7400, valid loss-1.0465, acc-0.7414, test loss-1.0503, acc-0.7488\n",
      "Iter-5720, train loss-1.1770, acc-0.7600, valid loss-1.0454, acc-0.7418, test loss-1.0492, acc-0.7493\n",
      "Iter-5730, train loss-1.0881, acc-0.7200, valid loss-1.0443, acc-0.7416, test loss-1.0481, acc-0.7495\n",
      "Iter-5740, train loss-0.9649, acc-0.7800, valid loss-1.0432, acc-0.7416, test loss-1.0470, acc-0.7493\n",
      "Iter-5750, train loss-0.9826, acc-0.7800, valid loss-1.0422, acc-0.7422, test loss-1.0460, acc-0.7494\n",
      "Iter-5760, train loss-1.0518, acc-0.8000, valid loss-1.0412, acc-0.7422, test loss-1.0449, acc-0.7496\n",
      "Iter-5770, train loss-1.0837, acc-0.7000, valid loss-1.0401, acc-0.7420, test loss-1.0438, acc-0.7502\n",
      "Iter-5780, train loss-0.9625, acc-0.7600, valid loss-1.0390, acc-0.7422, test loss-1.0427, acc-0.7505\n",
      "Iter-5790, train loss-0.9186, acc-0.8200, valid loss-1.0379, acc-0.7428, test loss-1.0416, acc-0.7508\n",
      "Iter-5800, train loss-0.9762, acc-0.8400, valid loss-1.0368, acc-0.7430, test loss-1.0405, acc-0.7507\n",
      "Iter-5810, train loss-1.0035, acc-0.8200, valid loss-1.0356, acc-0.7428, test loss-1.0393, acc-0.7505\n",
      "Iter-5820, train loss-1.1874, acc-0.6600, valid loss-1.0345, acc-0.7430, test loss-1.0382, acc-0.7512\n",
      "Iter-5830, train loss-0.9459, acc-0.7600, valid loss-1.0335, acc-0.7438, test loss-1.0372, acc-0.7517\n",
      "Iter-5840, train loss-1.0415, acc-0.7200, valid loss-1.0323, acc-0.7448, test loss-1.0361, acc-0.7516\n",
      "Iter-5850, train loss-0.8866, acc-0.8600, valid loss-1.0311, acc-0.7442, test loss-1.0349, acc-0.7519\n",
      "Iter-5860, train loss-1.1600, acc-0.8000, valid loss-1.0301, acc-0.7448, test loss-1.0339, acc-0.7521\n",
      "Iter-5870, train loss-0.9327, acc-0.8200, valid loss-1.0289, acc-0.7444, test loss-1.0328, acc-0.7517\n",
      "Iter-5880, train loss-0.9390, acc-0.8000, valid loss-1.0279, acc-0.7446, test loss-1.0317, acc-0.7522\n",
      "Iter-5890, train loss-1.0185, acc-0.8000, valid loss-1.0268, acc-0.7452, test loss-1.0306, acc-0.7526\n",
      "Iter-5900, train loss-0.8803, acc-0.8200, valid loss-1.0257, acc-0.7450, test loss-1.0295, acc-0.7527\n",
      "Iter-5910, train loss-0.9291, acc-0.8200, valid loss-1.0246, acc-0.7452, test loss-1.0285, acc-0.7531\n",
      "Iter-5920, train loss-0.9869, acc-0.8000, valid loss-1.0235, acc-0.7454, test loss-1.0274, acc-0.7534\n",
      "Iter-5930, train loss-0.9750, acc-0.7800, valid loss-1.0224, acc-0.7454, test loss-1.0263, acc-0.7537\n",
      "Iter-5940, train loss-0.8971, acc-0.8600, valid loss-1.0214, acc-0.7456, test loss-1.0253, acc-0.7539\n",
      "Iter-5950, train loss-0.9013, acc-0.8400, valid loss-1.0204, acc-0.7458, test loss-1.0243, acc-0.7541\n",
      "Iter-5960, train loss-0.9835, acc-0.7800, valid loss-1.0193, acc-0.7468, test loss-1.0233, acc-0.7548\n",
      "Iter-5970, train loss-0.9929, acc-0.8000, valid loss-1.0183, acc-0.7466, test loss-1.0222, acc-0.7550\n",
      "Iter-5980, train loss-0.9905, acc-0.7400, valid loss-1.0172, acc-0.7468, test loss-1.0212, acc-0.7545\n",
      "Iter-5990, train loss-1.1413, acc-0.7200, valid loss-1.0162, acc-0.7474, test loss-1.0202, acc-0.7550\n",
      "Iter-6000, train loss-0.9886, acc-0.7600, valid loss-1.0152, acc-0.7468, test loss-1.0192, acc-0.7554\n",
      "Iter-6010, train loss-1.0241, acc-0.7800, valid loss-1.0141, acc-0.7472, test loss-1.0181, acc-0.7556\n",
      "Iter-6020, train loss-0.9625, acc-0.7800, valid loss-1.0130, acc-0.7474, test loss-1.0170, acc-0.7557\n",
      "Iter-6030, train loss-1.1168, acc-0.7000, valid loss-1.0120, acc-0.7474, test loss-1.0159, acc-0.7558\n",
      "Iter-6040, train loss-1.1291, acc-0.6800, valid loss-1.0109, acc-0.7482, test loss-1.0149, acc-0.7557\n",
      "Iter-6050, train loss-1.0300, acc-0.7000, valid loss-1.0098, acc-0.7482, test loss-1.0138, acc-0.7563\n",
      "Iter-6060, train loss-0.9107, acc-0.7600, valid loss-1.0088, acc-0.7488, test loss-1.0128, acc-0.7562\n",
      "Iter-6070, train loss-0.9232, acc-0.8400, valid loss-1.0078, acc-0.7490, test loss-1.0117, acc-0.7568\n",
      "Iter-6080, train loss-1.1697, acc-0.6600, valid loss-1.0067, acc-0.7488, test loss-1.0107, acc-0.7569\n",
      "Iter-6090, train loss-0.9524, acc-0.8200, valid loss-1.0057, acc-0.7488, test loss-1.0097, acc-0.7571\n",
      "Iter-6100, train loss-1.0499, acc-0.6600, valid loss-1.0047, acc-0.7494, test loss-1.0087, acc-0.7574\n",
      "Iter-6110, train loss-0.9705, acc-0.8400, valid loss-1.0037, acc-0.7490, test loss-1.0077, acc-0.7574\n",
      "Iter-6120, train loss-0.9786, acc-0.7200, valid loss-1.0028, acc-0.7488, test loss-1.0068, acc-0.7575\n",
      "Iter-6130, train loss-1.2173, acc-0.7200, valid loss-1.0018, acc-0.7494, test loss-1.0057, acc-0.7573\n",
      "Iter-6140, train loss-1.1881, acc-0.6600, valid loss-1.0007, acc-0.7504, test loss-1.0048, acc-0.7573\n",
      "Iter-6150, train loss-1.0021, acc-0.7600, valid loss-0.9997, acc-0.7506, test loss-1.0037, acc-0.7573\n",
      "Iter-6160, train loss-1.2650, acc-0.6000, valid loss-0.9986, acc-0.7504, test loss-1.0027, acc-0.7573\n",
      "Iter-6170, train loss-1.0740, acc-0.6800, valid loss-0.9977, acc-0.7512, test loss-1.0017, acc-0.7575\n",
      "Iter-6180, train loss-0.9596, acc-0.7800, valid loss-0.9967, acc-0.7512, test loss-1.0007, acc-0.7576\n",
      "Iter-6190, train loss-0.9641, acc-0.7000, valid loss-0.9957, acc-0.7516, test loss-0.9997, acc-0.7574\n",
      "Iter-6200, train loss-1.0013, acc-0.7200, valid loss-0.9948, acc-0.7514, test loss-0.9988, acc-0.7579\n",
      "Iter-6210, train loss-1.1025, acc-0.7200, valid loss-0.9938, acc-0.7518, test loss-0.9978, acc-0.7585\n",
      "Iter-6220, train loss-0.9480, acc-0.8200, valid loss-0.9928, acc-0.7518, test loss-0.9968, acc-0.7581\n",
      "Iter-6230, train loss-1.1589, acc-0.6000, valid loss-0.9918, acc-0.7522, test loss-0.9959, acc-0.7584\n",
      "Iter-6240, train loss-1.0905, acc-0.6800, valid loss-0.9909, acc-0.7518, test loss-0.9949, acc-0.7589\n",
      "Iter-6250, train loss-1.0239, acc-0.8000, valid loss-0.9898, acc-0.7518, test loss-0.9939, acc-0.7594\n",
      "Iter-6260, train loss-1.0575, acc-0.7400, valid loss-0.9889, acc-0.7514, test loss-0.9930, acc-0.7594\n",
      "Iter-6270, train loss-1.0281, acc-0.7800, valid loss-0.9879, acc-0.7516, test loss-0.9920, acc-0.7594\n",
      "Iter-6280, train loss-1.0948, acc-0.7000, valid loss-0.9870, acc-0.7520, test loss-0.9910, acc-0.7601\n",
      "Iter-6290, train loss-0.8456, acc-0.8800, valid loss-0.9860, acc-0.7514, test loss-0.9900, acc-0.7601\n",
      "Iter-6300, train loss-1.0729, acc-0.6600, valid loss-0.9851, acc-0.7516, test loss-0.9890, acc-0.7606\n",
      "Iter-6310, train loss-1.1475, acc-0.6400, valid loss-0.9842, acc-0.7520, test loss-0.9881, acc-0.7609\n",
      "Iter-6320, train loss-0.9097, acc-0.8000, valid loss-0.9832, acc-0.7524, test loss-0.9871, acc-0.7611\n",
      "Iter-6330, train loss-1.0163, acc-0.7600, valid loss-0.9823, acc-0.7524, test loss-0.9861, acc-0.7611\n",
      "Iter-6340, train loss-0.9207, acc-0.7800, valid loss-0.9813, acc-0.7530, test loss-0.9851, acc-0.7609\n",
      "Iter-6350, train loss-1.0676, acc-0.7200, valid loss-0.9803, acc-0.7530, test loss-0.9842, acc-0.7611\n",
      "Iter-6360, train loss-0.8324, acc-0.8000, valid loss-0.9793, acc-0.7530, test loss-0.9832, acc-0.7613\n",
      "Iter-6370, train loss-1.0579, acc-0.7400, valid loss-0.9783, acc-0.7536, test loss-0.9822, acc-0.7615\n",
      "Iter-6380, train loss-1.0889, acc-0.6800, valid loss-0.9774, acc-0.7530, test loss-0.9813, acc-0.7617\n",
      "Iter-6390, train loss-0.9548, acc-0.8200, valid loss-0.9764, acc-0.7534, test loss-0.9803, acc-0.7617\n",
      "Iter-6400, train loss-1.0564, acc-0.7000, valid loss-0.9754, acc-0.7540, test loss-0.9793, acc-0.7616\n",
      "Iter-6410, train loss-1.0226, acc-0.7800, valid loss-0.9745, acc-0.7540, test loss-0.9784, acc-0.7616\n",
      "Iter-6420, train loss-0.8577, acc-0.7400, valid loss-0.9736, acc-0.7544, test loss-0.9775, acc-0.7621\n",
      "Iter-6430, train loss-0.9739, acc-0.7200, valid loss-0.9726, acc-0.7544, test loss-0.9766, acc-0.7623\n",
      "Iter-6440, train loss-0.9895, acc-0.7200, valid loss-0.9717, acc-0.7552, test loss-0.9756, acc-0.7624\n",
      "Iter-6450, train loss-0.9722, acc-0.7600, valid loss-0.9707, acc-0.7552, test loss-0.9747, acc-0.7629\n",
      "Iter-6460, train loss-0.9987, acc-0.8000, valid loss-0.9698, acc-0.7554, test loss-0.9737, acc-0.7632\n",
      "Iter-6470, train loss-0.9413, acc-0.8200, valid loss-0.9689, acc-0.7554, test loss-0.9728, acc-0.7631\n",
      "Iter-6480, train loss-0.9385, acc-0.7400, valid loss-0.9680, acc-0.7560, test loss-0.9719, acc-0.7633\n",
      "Iter-6490, train loss-1.0822, acc-0.6800, valid loss-0.9671, acc-0.7560, test loss-0.9710, acc-0.7636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-1.1024, acc-0.6400, valid loss-0.9661, acc-0.7558, test loss-0.9700, acc-0.7646\n",
      "Iter-6510, train loss-0.9030, acc-0.8600, valid loss-0.9653, acc-0.7552, test loss-0.9691, acc-0.7647\n",
      "Iter-6520, train loss-0.9940, acc-0.7800, valid loss-0.9644, acc-0.7552, test loss-0.9682, acc-0.7648\n",
      "Iter-6530, train loss-1.2184, acc-0.6800, valid loss-0.9635, acc-0.7564, test loss-0.9673, acc-0.7654\n",
      "Iter-6540, train loss-1.1395, acc-0.7400, valid loss-0.9626, acc-0.7564, test loss-0.9664, acc-0.7662\n",
      "Iter-6550, train loss-0.8634, acc-0.8400, valid loss-0.9617, acc-0.7564, test loss-0.9655, acc-0.7664\n",
      "Iter-6560, train loss-0.8834, acc-0.8000, valid loss-0.9608, acc-0.7570, test loss-0.9646, acc-0.7668\n",
      "Iter-6570, train loss-1.0611, acc-0.7000, valid loss-0.9599, acc-0.7574, test loss-0.9637, acc-0.7670\n",
      "Iter-6580, train loss-1.0063, acc-0.7200, valid loss-0.9590, acc-0.7576, test loss-0.9628, acc-0.7675\n",
      "Iter-6590, train loss-0.9363, acc-0.7600, valid loss-0.9581, acc-0.7580, test loss-0.9619, acc-0.7677\n",
      "Iter-6600, train loss-1.1962, acc-0.6600, valid loss-0.9572, acc-0.7584, test loss-0.9610, acc-0.7684\n",
      "Iter-6610, train loss-0.8899, acc-0.8200, valid loss-0.9563, acc-0.7590, test loss-0.9601, acc-0.7682\n",
      "Iter-6620, train loss-0.8723, acc-0.7600, valid loss-0.9555, acc-0.7588, test loss-0.9593, acc-0.7689\n",
      "Iter-6630, train loss-1.0044, acc-0.7200, valid loss-0.9546, acc-0.7590, test loss-0.9584, acc-0.7692\n",
      "Iter-6640, train loss-0.8985, acc-0.7000, valid loss-0.9538, acc-0.7590, test loss-0.9576, acc-0.7690\n",
      "Iter-6650, train loss-1.0726, acc-0.7000, valid loss-0.9528, acc-0.7592, test loss-0.9567, acc-0.7689\n",
      "Iter-6660, train loss-0.8161, acc-0.9000, valid loss-0.9519, acc-0.7592, test loss-0.9558, acc-0.7692\n",
      "Iter-6670, train loss-0.9827, acc-0.6800, valid loss-0.9510, acc-0.7600, test loss-0.9549, acc-0.7699\n",
      "Iter-6680, train loss-0.8773, acc-0.8000, valid loss-0.9502, acc-0.7602, test loss-0.9540, acc-0.7699\n",
      "Iter-6690, train loss-0.9912, acc-0.7600, valid loss-0.9492, acc-0.7602, test loss-0.9531, acc-0.7704\n",
      "Iter-6700, train loss-1.1330, acc-0.6600, valid loss-0.9484, acc-0.7602, test loss-0.9522, acc-0.7708\n",
      "Iter-6710, train loss-0.8818, acc-0.7600, valid loss-0.9475, acc-0.7598, test loss-0.9513, acc-0.7709\n",
      "Iter-6720, train loss-1.0247, acc-0.7000, valid loss-0.9466, acc-0.7604, test loss-0.9505, acc-0.7708\n",
      "Iter-6730, train loss-0.8608, acc-0.8200, valid loss-0.9458, acc-0.7602, test loss-0.9496, acc-0.7707\n",
      "Iter-6740, train loss-1.0451, acc-0.7200, valid loss-0.9450, acc-0.7600, test loss-0.9487, acc-0.7711\n",
      "Iter-6750, train loss-0.8742, acc-0.7600, valid loss-0.9441, acc-0.7598, test loss-0.9478, acc-0.7713\n",
      "Iter-6760, train loss-0.8782, acc-0.8000, valid loss-0.9432, acc-0.7602, test loss-0.9469, acc-0.7721\n",
      "Iter-6770, train loss-1.1496, acc-0.6800, valid loss-0.9424, acc-0.7602, test loss-0.9461, acc-0.7730\n",
      "Iter-6780, train loss-1.0605, acc-0.7000, valid loss-0.9416, acc-0.7604, test loss-0.9453, acc-0.7730\n",
      "Iter-6790, train loss-1.1469, acc-0.6400, valid loss-0.9408, acc-0.7604, test loss-0.9445, acc-0.7725\n",
      "Iter-6800, train loss-0.8637, acc-0.8600, valid loss-0.9398, acc-0.7606, test loss-0.9435, acc-0.7728\n",
      "Iter-6810, train loss-1.0482, acc-0.7200, valid loss-0.9389, acc-0.7612, test loss-0.9426, acc-0.7731\n",
      "Iter-6820, train loss-0.8619, acc-0.7400, valid loss-0.9381, acc-0.7612, test loss-0.9418, acc-0.7733\n",
      "Iter-6830, train loss-0.9817, acc-0.7800, valid loss-0.9373, acc-0.7606, test loss-0.9410, acc-0.7736\n",
      "Iter-6840, train loss-0.9209, acc-0.7400, valid loss-0.9364, acc-0.7612, test loss-0.9401, acc-0.7735\n",
      "Iter-6850, train loss-1.0590, acc-0.7000, valid loss-0.9356, acc-0.7616, test loss-0.9393, acc-0.7734\n",
      "Iter-6860, train loss-0.9307, acc-0.7000, valid loss-0.9347, acc-0.7618, test loss-0.9384, acc-0.7736\n",
      "Iter-6870, train loss-0.9870, acc-0.8000, valid loss-0.9338, acc-0.7624, test loss-0.9375, acc-0.7735\n",
      "Iter-6880, train loss-0.8668, acc-0.8000, valid loss-0.9330, acc-0.7626, test loss-0.9366, acc-0.7739\n",
      "Iter-6890, train loss-1.1196, acc-0.7800, valid loss-0.9322, acc-0.7628, test loss-0.9358, acc-0.7739\n",
      "Iter-6900, train loss-0.9681, acc-0.7400, valid loss-0.9313, acc-0.7632, test loss-0.9350, acc-0.7741\n",
      "Iter-6910, train loss-1.1533, acc-0.7600, valid loss-0.9305, acc-0.7636, test loss-0.9342, acc-0.7740\n",
      "Iter-6920, train loss-1.0226, acc-0.6600, valid loss-0.9297, acc-0.7636, test loss-0.9334, acc-0.7741\n",
      "Iter-6930, train loss-0.8703, acc-0.8400, valid loss-0.9289, acc-0.7634, test loss-0.9326, acc-0.7747\n",
      "Iter-6940, train loss-0.9108, acc-0.7600, valid loss-0.9280, acc-0.7630, test loss-0.9317, acc-0.7752\n",
      "Iter-6950, train loss-1.0638, acc-0.7400, valid loss-0.9272, acc-0.7632, test loss-0.9309, acc-0.7756\n",
      "Iter-6960, train loss-0.8979, acc-0.7000, valid loss-0.9264, acc-0.7632, test loss-0.9301, acc-0.7756\n",
      "Iter-6970, train loss-0.9404, acc-0.7400, valid loss-0.9257, acc-0.7638, test loss-0.9293, acc-0.7758\n",
      "Iter-6980, train loss-0.8151, acc-0.8800, valid loss-0.9248, acc-0.7634, test loss-0.9285, acc-0.7758\n",
      "Iter-6990, train loss-0.8930, acc-0.8200, valid loss-0.9240, acc-0.7638, test loss-0.9277, acc-0.7755\n",
      "Iter-7000, train loss-0.7995, acc-0.8000, valid loss-0.9233, acc-0.7646, test loss-0.9269, acc-0.7757\n",
      "Iter-7010, train loss-0.7267, acc-0.8600, valid loss-0.9224, acc-0.7646, test loss-0.9260, acc-0.7759\n",
      "Iter-7020, train loss-0.9156, acc-0.8000, valid loss-0.9216, acc-0.7650, test loss-0.9252, acc-0.7759\n",
      "Iter-7030, train loss-0.9025, acc-0.9000, valid loss-0.9208, acc-0.7654, test loss-0.9243, acc-0.7760\n",
      "Iter-7040, train loss-0.9060, acc-0.7800, valid loss-0.9200, acc-0.7656, test loss-0.9235, acc-0.7762\n",
      "Iter-7050, train loss-1.0027, acc-0.8000, valid loss-0.9191, acc-0.7656, test loss-0.9227, acc-0.7765\n",
      "Iter-7060, train loss-1.0797, acc-0.7400, valid loss-0.9184, acc-0.7660, test loss-0.9219, acc-0.7766\n",
      "Iter-7070, train loss-0.8798, acc-0.8200, valid loss-0.9176, acc-0.7658, test loss-0.9211, acc-0.7763\n",
      "Iter-7080, train loss-0.8442, acc-0.7200, valid loss-0.9168, acc-0.7652, test loss-0.9203, acc-0.7764\n",
      "Iter-7090, train loss-0.8919, acc-0.8400, valid loss-0.9160, acc-0.7662, test loss-0.9195, acc-0.7764\n",
      "Iter-7100, train loss-0.9232, acc-0.7000, valid loss-0.9151, acc-0.7666, test loss-0.9187, acc-0.7768\n",
      "Iter-7110, train loss-0.7984, acc-0.8600, valid loss-0.9143, acc-0.7672, test loss-0.9180, acc-0.7768\n",
      "Iter-7120, train loss-0.8369, acc-0.8400, valid loss-0.9135, acc-0.7674, test loss-0.9171, acc-0.7767\n",
      "Iter-7130, train loss-0.8896, acc-0.8000, valid loss-0.9128, acc-0.7672, test loss-0.9164, acc-0.7768\n",
      "Iter-7140, train loss-0.9158, acc-0.7000, valid loss-0.9119, acc-0.7676, test loss-0.9155, acc-0.7767\n",
      "Iter-7150, train loss-0.9465, acc-0.8000, valid loss-0.9111, acc-0.7674, test loss-0.9148, acc-0.7767\n",
      "Iter-7160, train loss-1.0159, acc-0.7200, valid loss-0.9103, acc-0.7676, test loss-0.9140, acc-0.7772\n",
      "Iter-7170, train loss-0.9842, acc-0.7200, valid loss-0.9095, acc-0.7680, test loss-0.9132, acc-0.7775\n",
      "Iter-7180, train loss-0.7518, acc-0.9000, valid loss-0.9087, acc-0.7682, test loss-0.9124, acc-0.7773\n",
      "Iter-7190, train loss-1.1358, acc-0.7000, valid loss-0.9080, acc-0.7686, test loss-0.9117, acc-0.7775\n",
      "Iter-7200, train loss-1.0254, acc-0.6800, valid loss-0.9072, acc-0.7688, test loss-0.9108, acc-0.7775\n",
      "Iter-7210, train loss-0.9610, acc-0.7400, valid loss-0.9064, acc-0.7686, test loss-0.9101, acc-0.7779\n",
      "Iter-7220, train loss-0.9680, acc-0.7200, valid loss-0.9057, acc-0.7684, test loss-0.9093, acc-0.7782\n",
      "Iter-7230, train loss-0.8554, acc-0.7800, valid loss-0.9048, acc-0.7682, test loss-0.9084, acc-0.7783\n",
      "Iter-7240, train loss-0.9430, acc-0.8000, valid loss-0.9040, acc-0.7682, test loss-0.9076, acc-0.7790\n",
      "Iter-7250, train loss-0.9900, acc-0.7400, valid loss-0.9032, acc-0.7684, test loss-0.9068, acc-0.7789\n",
      "Iter-7260, train loss-0.8777, acc-0.7800, valid loss-0.9025, acc-0.7684, test loss-0.9060, acc-0.7791\n",
      "Iter-7270, train loss-0.8816, acc-0.7600, valid loss-0.9016, acc-0.7686, test loss-0.9052, acc-0.7793\n",
      "Iter-7280, train loss-1.0295, acc-0.6400, valid loss-0.9008, acc-0.7692, test loss-0.9044, acc-0.7802\n",
      "Iter-7290, train loss-0.9231, acc-0.7000, valid loss-0.9000, acc-0.7698, test loss-0.9036, acc-0.7802\n",
      "Iter-7300, train loss-0.9975, acc-0.6800, valid loss-0.8992, acc-0.7696, test loss-0.9028, acc-0.7803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-0.9197, acc-0.7800, valid loss-0.8984, acc-0.7706, test loss-0.9021, acc-0.7804\n",
      "Iter-7320, train loss-0.8854, acc-0.7600, valid loss-0.8976, acc-0.7708, test loss-0.9013, acc-0.7807\n",
      "Iter-7330, train loss-0.9988, acc-0.8200, valid loss-0.8968, acc-0.7706, test loss-0.9005, acc-0.7808\n",
      "Iter-7340, train loss-0.9621, acc-0.7400, valid loss-0.8961, acc-0.7710, test loss-0.8998, acc-0.7808\n",
      "Iter-7350, train loss-0.8947, acc-0.7400, valid loss-0.8954, acc-0.7712, test loss-0.8991, acc-0.7808\n",
      "Iter-7360, train loss-0.8439, acc-0.8000, valid loss-0.8945, acc-0.7714, test loss-0.8983, acc-0.7810\n",
      "Iter-7370, train loss-0.8808, acc-0.7200, valid loss-0.8938, acc-0.7722, test loss-0.8975, acc-0.7813\n",
      "Iter-7380, train loss-0.9282, acc-0.7800, valid loss-0.8930, acc-0.7720, test loss-0.8967, acc-0.7812\n",
      "Iter-7390, train loss-0.9665, acc-0.7200, valid loss-0.8922, acc-0.7724, test loss-0.8960, acc-0.7817\n",
      "Iter-7400, train loss-0.8983, acc-0.8000, valid loss-0.8914, acc-0.7726, test loss-0.8952, acc-0.7818\n",
      "Iter-7410, train loss-0.9956, acc-0.7400, valid loss-0.8907, acc-0.7724, test loss-0.8945, acc-0.7819\n",
      "Iter-7420, train loss-0.9501, acc-0.7400, valid loss-0.8899, acc-0.7730, test loss-0.8937, acc-0.7825\n",
      "Iter-7430, train loss-0.9038, acc-0.8600, valid loss-0.8893, acc-0.7722, test loss-0.8930, acc-0.7826\n",
      "Iter-7440, train loss-0.9580, acc-0.7400, valid loss-0.8885, acc-0.7732, test loss-0.8923, acc-0.7826\n",
      "Iter-7450, train loss-0.8115, acc-0.8400, valid loss-0.8877, acc-0.7734, test loss-0.8915, acc-0.7827\n",
      "Iter-7460, train loss-0.9120, acc-0.7400, valid loss-0.8870, acc-0.7738, test loss-0.8907, acc-0.7832\n",
      "Iter-7470, train loss-0.8681, acc-0.8000, valid loss-0.8862, acc-0.7732, test loss-0.8900, acc-0.7834\n",
      "Iter-7480, train loss-0.7998, acc-0.8400, valid loss-0.8855, acc-0.7736, test loss-0.8892, acc-0.7832\n",
      "Iter-7490, train loss-0.9286, acc-0.7400, valid loss-0.8847, acc-0.7738, test loss-0.8885, acc-0.7831\n",
      "Iter-7500, train loss-0.8943, acc-0.7000, valid loss-0.8840, acc-0.7752, test loss-0.8877, acc-0.7833\n",
      "Iter-7510, train loss-0.8966, acc-0.7800, valid loss-0.8833, acc-0.7748, test loss-0.8870, acc-0.7832\n",
      "Iter-7520, train loss-0.9324, acc-0.7400, valid loss-0.8826, acc-0.7754, test loss-0.8862, acc-0.7835\n",
      "Iter-7530, train loss-0.9275, acc-0.7600, valid loss-0.8818, acc-0.7760, test loss-0.8855, acc-0.7833\n",
      "Iter-7540, train loss-0.9833, acc-0.7400, valid loss-0.8811, acc-0.7766, test loss-0.8847, acc-0.7837\n",
      "Iter-7550, train loss-0.8002, acc-0.8000, valid loss-0.8803, acc-0.7770, test loss-0.8840, acc-0.7838\n",
      "Iter-7560, train loss-0.9298, acc-0.8000, valid loss-0.8795, acc-0.7772, test loss-0.8832, acc-0.7838\n",
      "Iter-7570, train loss-0.8038, acc-0.8200, valid loss-0.8787, acc-0.7782, test loss-0.8824, acc-0.7838\n",
      "Iter-7580, train loss-0.9374, acc-0.7000, valid loss-0.8780, acc-0.7778, test loss-0.8817, acc-0.7843\n",
      "Iter-7590, train loss-0.7481, acc-0.7800, valid loss-0.8773, acc-0.7788, test loss-0.8810, acc-0.7846\n",
      "Iter-7600, train loss-1.0278, acc-0.7200, valid loss-0.8765, acc-0.7788, test loss-0.8802, acc-0.7845\n",
      "Iter-7610, train loss-0.8635, acc-0.7800, valid loss-0.8757, acc-0.7790, test loss-0.8795, acc-0.7844\n",
      "Iter-7620, train loss-0.9221, acc-0.7600, valid loss-0.8750, acc-0.7794, test loss-0.8787, acc-0.7847\n",
      "Iter-7630, train loss-0.7923, acc-0.8200, valid loss-0.8742, acc-0.7796, test loss-0.8779, acc-0.7847\n",
      "Iter-7640, train loss-0.9575, acc-0.7800, valid loss-0.8735, acc-0.7800, test loss-0.8772, acc-0.7848\n",
      "Iter-7650, train loss-0.8221, acc-0.7800, valid loss-0.8728, acc-0.7812, test loss-0.8765, acc-0.7851\n",
      "Iter-7660, train loss-0.9694, acc-0.7000, valid loss-0.8721, acc-0.7802, test loss-0.8758, acc-0.7851\n",
      "Iter-7670, train loss-0.8852, acc-0.7600, valid loss-0.8714, acc-0.7808, test loss-0.8751, acc-0.7854\n",
      "Iter-7680, train loss-0.8844, acc-0.8600, valid loss-0.8706, acc-0.7808, test loss-0.8744, acc-0.7857\n",
      "Iter-7690, train loss-0.8543, acc-0.7800, valid loss-0.8699, acc-0.7810, test loss-0.8737, acc-0.7861\n",
      "Iter-7700, train loss-0.8812, acc-0.7600, valid loss-0.8692, acc-0.7808, test loss-0.8730, acc-0.7862\n",
      "Iter-7710, train loss-0.9759, acc-0.7200, valid loss-0.8685, acc-0.7812, test loss-0.8723, acc-0.7866\n",
      "Iter-7720, train loss-0.7686, acc-0.8400, valid loss-0.8678, acc-0.7812, test loss-0.8716, acc-0.7870\n",
      "Iter-7730, train loss-0.8314, acc-0.7600, valid loss-0.8671, acc-0.7818, test loss-0.8709, acc-0.7867\n",
      "Iter-7740, train loss-0.8840, acc-0.8200, valid loss-0.8663, acc-0.7820, test loss-0.8702, acc-0.7872\n",
      "Iter-7750, train loss-0.8798, acc-0.7400, valid loss-0.8656, acc-0.7822, test loss-0.8694, acc-0.7869\n",
      "Iter-7760, train loss-0.8203, acc-0.7800, valid loss-0.8649, acc-0.7818, test loss-0.8687, acc-0.7872\n",
      "Iter-7770, train loss-0.7554, acc-0.7800, valid loss-0.8642, acc-0.7818, test loss-0.8680, acc-0.7873\n",
      "Iter-7780, train loss-0.7139, acc-0.8600, valid loss-0.8635, acc-0.7818, test loss-0.8672, acc-0.7874\n",
      "Iter-7790, train loss-0.6928, acc-0.8800, valid loss-0.8627, acc-0.7812, test loss-0.8666, acc-0.7876\n",
      "Iter-7800, train loss-0.7207, acc-0.9000, valid loss-0.8620, acc-0.7818, test loss-0.8659, acc-0.7879\n",
      "Iter-7810, train loss-0.7457, acc-0.8400, valid loss-0.8613, acc-0.7820, test loss-0.8652, acc-0.7881\n",
      "Iter-7820, train loss-0.9269, acc-0.7400, valid loss-0.8606, acc-0.7818, test loss-0.8645, acc-0.7882\n",
      "Iter-7830, train loss-0.7785, acc-0.8400, valid loss-0.8598, acc-0.7824, test loss-0.8638, acc-0.7882\n",
      "Iter-7840, train loss-0.9781, acc-0.6800, valid loss-0.8591, acc-0.7822, test loss-0.8631, acc-0.7883\n",
      "Iter-7850, train loss-0.6780, acc-0.9200, valid loss-0.8584, acc-0.7824, test loss-0.8624, acc-0.7885\n",
      "Iter-7860, train loss-0.9898, acc-0.7200, valid loss-0.8577, acc-0.7824, test loss-0.8617, acc-0.7885\n",
      "Iter-7870, train loss-0.9734, acc-0.7600, valid loss-0.8570, acc-0.7824, test loss-0.8610, acc-0.7887\n",
      "Iter-7880, train loss-0.8761, acc-0.7600, valid loss-0.8563, acc-0.7826, test loss-0.8604, acc-0.7888\n",
      "Iter-7890, train loss-0.8866, acc-0.7200, valid loss-0.8557, acc-0.7826, test loss-0.8597, acc-0.7894\n",
      "Iter-7900, train loss-0.9155, acc-0.8000, valid loss-0.8550, acc-0.7832, test loss-0.8590, acc-0.7895\n",
      "Iter-7910, train loss-0.6887, acc-0.9200, valid loss-0.8542, acc-0.7834, test loss-0.8582, acc-0.7896\n",
      "Iter-7920, train loss-0.8414, acc-0.7200, valid loss-0.8535, acc-0.7840, test loss-0.8575, acc-0.7900\n",
      "Iter-7930, train loss-0.7980, acc-0.8400, valid loss-0.8529, acc-0.7844, test loss-0.8569, acc-0.7903\n",
      "Iter-7940, train loss-0.8822, acc-0.8000, valid loss-0.8522, acc-0.7842, test loss-0.8562, acc-0.7902\n",
      "Iter-7950, train loss-0.7776, acc-0.8600, valid loss-0.8515, acc-0.7846, test loss-0.8555, acc-0.7900\n",
      "Iter-7960, train loss-0.8212, acc-0.8000, valid loss-0.8508, acc-0.7850, test loss-0.8548, acc-0.7903\n",
      "Iter-7970, train loss-0.9106, acc-0.7400, valid loss-0.8502, acc-0.7852, test loss-0.8542, acc-0.7906\n",
      "Iter-7980, train loss-0.8280, acc-0.8200, valid loss-0.8495, acc-0.7848, test loss-0.8535, acc-0.7907\n",
      "Iter-7990, train loss-0.6688, acc-0.8200, valid loss-0.8488, acc-0.7850, test loss-0.8528, acc-0.7907\n",
      "Iter-8000, train loss-0.8947, acc-0.8200, valid loss-0.8482, acc-0.7856, test loss-0.8521, acc-0.7904\n",
      "Iter-8010, train loss-0.7503, acc-0.8200, valid loss-0.8475, acc-0.7856, test loss-0.8514, acc-0.7908\n",
      "Iter-8020, train loss-1.0128, acc-0.7200, valid loss-0.8468, acc-0.7860, test loss-0.8508, acc-0.7905\n",
      "Iter-8030, train loss-0.7998, acc-0.8600, valid loss-0.8461, acc-0.7864, test loss-0.8501, acc-0.7910\n",
      "Iter-8040, train loss-1.0654, acc-0.6600, valid loss-0.8455, acc-0.7868, test loss-0.8494, acc-0.7909\n",
      "Iter-8050, train loss-0.9714, acc-0.7800, valid loss-0.8448, acc-0.7870, test loss-0.8487, acc-0.7917\n",
      "Iter-8060, train loss-1.0564, acc-0.6600, valid loss-0.8442, acc-0.7868, test loss-0.8481, acc-0.7919\n",
      "Iter-8070, train loss-0.7422, acc-0.8800, valid loss-0.8435, acc-0.7868, test loss-0.8474, acc-0.7918\n",
      "Iter-8080, train loss-0.8985, acc-0.7000, valid loss-0.8429, acc-0.7874, test loss-0.8468, acc-0.7917\n",
      "Iter-8090, train loss-0.7847, acc-0.8200, valid loss-0.8422, acc-0.7874, test loss-0.8461, acc-0.7919\n",
      "Iter-8100, train loss-0.9294, acc-0.7000, valid loss-0.8416, acc-0.7872, test loss-0.8454, acc-0.7918\n",
      "Iter-8110, train loss-0.9710, acc-0.7400, valid loss-0.8409, acc-0.7876, test loss-0.8448, acc-0.7921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-0.8266, acc-0.7800, valid loss-0.8402, acc-0.7876, test loss-0.8441, acc-0.7924\n",
      "Iter-8130, train loss-0.7606, acc-0.8600, valid loss-0.8395, acc-0.7880, test loss-0.8434, acc-0.7924\n",
      "Iter-8140, train loss-0.8562, acc-0.7600, valid loss-0.8389, acc-0.7886, test loss-0.8428, acc-0.7929\n",
      "Iter-8150, train loss-0.8420, acc-0.8400, valid loss-0.8383, acc-0.7886, test loss-0.8422, acc-0.7928\n",
      "Iter-8160, train loss-0.9050, acc-0.6400, valid loss-0.8376, acc-0.7894, test loss-0.8415, acc-0.7935\n",
      "Iter-8170, train loss-0.7937, acc-0.8400, valid loss-0.8369, acc-0.7894, test loss-0.8409, acc-0.7937\n",
      "Iter-8180, train loss-0.7798, acc-0.7800, valid loss-0.8363, acc-0.7896, test loss-0.8402, acc-0.7938\n",
      "Iter-8190, train loss-0.9969, acc-0.7400, valid loss-0.8356, acc-0.7902, test loss-0.8395, acc-0.7942\n",
      "Iter-8200, train loss-0.9093, acc-0.7800, valid loss-0.8349, acc-0.7904, test loss-0.8389, acc-0.7942\n",
      "Iter-8210, train loss-0.9305, acc-0.7800, valid loss-0.8344, acc-0.7908, test loss-0.8383, acc-0.7944\n",
      "Iter-8220, train loss-0.8478, acc-0.6800, valid loss-0.8337, acc-0.7910, test loss-0.8377, acc-0.7946\n",
      "Iter-8230, train loss-0.7574, acc-0.8800, valid loss-0.8331, acc-0.7912, test loss-0.8371, acc-0.7944\n",
      "Iter-8240, train loss-0.8527, acc-0.7400, valid loss-0.8325, acc-0.7914, test loss-0.8364, acc-0.7947\n",
      "Iter-8250, train loss-1.1975, acc-0.6600, valid loss-0.8319, acc-0.7906, test loss-0.8358, acc-0.7953\n",
      "Iter-8260, train loss-0.7729, acc-0.7600, valid loss-0.8313, acc-0.7908, test loss-0.8352, acc-0.7956\n",
      "Iter-8270, train loss-0.9817, acc-0.7400, valid loss-0.8306, acc-0.7906, test loss-0.8345, acc-0.7956\n",
      "Iter-8280, train loss-0.8242, acc-0.8000, valid loss-0.8300, acc-0.7906, test loss-0.8339, acc-0.7958\n",
      "Iter-8290, train loss-0.9218, acc-0.7400, valid loss-0.8294, acc-0.7906, test loss-0.8333, acc-0.7957\n",
      "Iter-8300, train loss-0.8319, acc-0.7400, valid loss-0.8287, acc-0.7904, test loss-0.8327, acc-0.7960\n",
      "Iter-8310, train loss-0.8449, acc-0.8000, valid loss-0.8282, acc-0.7906, test loss-0.8321, acc-0.7960\n",
      "Iter-8320, train loss-0.9212, acc-0.7200, valid loss-0.8276, acc-0.7908, test loss-0.8314, acc-0.7957\n",
      "Iter-8330, train loss-0.8189, acc-0.8400, valid loss-0.8269, acc-0.7912, test loss-0.8308, acc-0.7961\n",
      "Iter-8340, train loss-0.8140, acc-0.8200, valid loss-0.8263, acc-0.7908, test loss-0.8302, acc-0.7963\n",
      "Iter-8350, train loss-0.9028, acc-0.7400, valid loss-0.8257, acc-0.7908, test loss-0.8296, acc-0.7964\n",
      "Iter-8360, train loss-0.8676, acc-0.7600, valid loss-0.8251, acc-0.7906, test loss-0.8290, acc-0.7964\n",
      "Iter-8370, train loss-0.8135, acc-0.8000, valid loss-0.8245, acc-0.7908, test loss-0.8283, acc-0.7964\n",
      "Iter-8380, train loss-0.8796, acc-0.7800, valid loss-0.8238, acc-0.7908, test loss-0.8276, acc-0.7965\n",
      "Iter-8390, train loss-0.7316, acc-0.8400, valid loss-0.8232, acc-0.7914, test loss-0.8270, acc-0.7967\n",
      "Iter-8400, train loss-0.9603, acc-0.7400, valid loss-0.8225, acc-0.7918, test loss-0.8263, acc-0.7968\n",
      "Iter-8410, train loss-0.7382, acc-0.8600, valid loss-0.8218, acc-0.7918, test loss-0.8257, acc-0.7969\n",
      "Iter-8420, train loss-0.8152, acc-0.7800, valid loss-0.8212, acc-0.7918, test loss-0.8250, acc-0.7973\n",
      "Iter-8430, train loss-0.8030, acc-0.7800, valid loss-0.8206, acc-0.7918, test loss-0.8244, acc-0.7974\n",
      "Iter-8440, train loss-0.8261, acc-0.7800, valid loss-0.8200, acc-0.7920, test loss-0.8238, acc-0.7976\n",
      "Iter-8450, train loss-0.8193, acc-0.8200, valid loss-0.8193, acc-0.7918, test loss-0.8232, acc-0.7982\n",
      "Iter-8460, train loss-0.7499, acc-0.8800, valid loss-0.8187, acc-0.7920, test loss-0.8226, acc-0.7985\n",
      "Iter-8470, train loss-0.7826, acc-0.8200, valid loss-0.8180, acc-0.7920, test loss-0.8220, acc-0.7983\n",
      "Iter-8480, train loss-0.8430, acc-0.7800, valid loss-0.8174, acc-0.7926, test loss-0.8213, acc-0.7987\n",
      "Iter-8490, train loss-0.8943, acc-0.7200, valid loss-0.8168, acc-0.7926, test loss-0.8207, acc-0.7986\n",
      "Iter-8500, train loss-0.9416, acc-0.7200, valid loss-0.8162, acc-0.7922, test loss-0.8201, acc-0.7989\n",
      "Iter-8510, train loss-0.7790, acc-0.8400, valid loss-0.8156, acc-0.7926, test loss-0.8194, acc-0.7991\n",
      "Iter-8520, train loss-0.8552, acc-0.8000, valid loss-0.8150, acc-0.7926, test loss-0.8188, acc-0.7988\n",
      "Iter-8530, train loss-0.9608, acc-0.7600, valid loss-0.8144, acc-0.7932, test loss-0.8182, acc-0.7993\n",
      "Iter-8540, train loss-0.6814, acc-0.8600, valid loss-0.8138, acc-0.7932, test loss-0.8176, acc-0.7994\n",
      "Iter-8550, train loss-0.7448, acc-0.8400, valid loss-0.8131, acc-0.7936, test loss-0.8170, acc-0.7993\n",
      "Iter-8560, train loss-0.8890, acc-0.7600, valid loss-0.8126, acc-0.7938, test loss-0.8164, acc-0.7997\n",
      "Iter-8570, train loss-0.8335, acc-0.8200, valid loss-0.8120, acc-0.7936, test loss-0.8158, acc-0.8000\n",
      "Iter-8580, train loss-0.8497, acc-0.7800, valid loss-0.8114, acc-0.7936, test loss-0.8152, acc-0.7996\n",
      "Iter-8590, train loss-0.8249, acc-0.8000, valid loss-0.8108, acc-0.7946, test loss-0.8146, acc-0.7997\n",
      "Iter-8600, train loss-0.9217, acc-0.6600, valid loss-0.8101, acc-0.7950, test loss-0.8139, acc-0.7998\n",
      "Iter-8610, train loss-0.7981, acc-0.8000, valid loss-0.8095, acc-0.7950, test loss-0.8133, acc-0.8001\n",
      "Iter-8620, train loss-0.8125, acc-0.8400, valid loss-0.8089, acc-0.7948, test loss-0.8127, acc-0.8004\n",
      "Iter-8630, train loss-0.6607, acc-0.9200, valid loss-0.8083, acc-0.7950, test loss-0.8121, acc-0.8003\n",
      "Iter-8640, train loss-0.8690, acc-0.7400, valid loss-0.8077, acc-0.7952, test loss-0.8115, acc-0.8002\n",
      "Iter-8650, train loss-0.8840, acc-0.8000, valid loss-0.8071, acc-0.7958, test loss-0.8109, acc-0.8003\n",
      "Iter-8660, train loss-0.7601, acc-0.8200, valid loss-0.8064, acc-0.7966, test loss-0.8102, acc-0.8005\n",
      "Iter-8670, train loss-0.7734, acc-0.8000, valid loss-0.8058, acc-0.7958, test loss-0.8096, acc-0.8010\n",
      "Iter-8680, train loss-0.7748, acc-0.8200, valid loss-0.8052, acc-0.7964, test loss-0.8090, acc-0.8012\n",
      "Iter-8690, train loss-0.8495, acc-0.7800, valid loss-0.8047, acc-0.7964, test loss-0.8084, acc-0.8011\n",
      "Iter-8700, train loss-0.7230, acc-0.8600, valid loss-0.8041, acc-0.7968, test loss-0.8078, acc-0.8012\n",
      "Iter-8710, train loss-0.8537, acc-0.7600, valid loss-0.8034, acc-0.7968, test loss-0.8072, acc-0.8013\n",
      "Iter-8720, train loss-0.7984, acc-0.7800, valid loss-0.8029, acc-0.7964, test loss-0.8066, acc-0.8014\n",
      "Iter-8730, train loss-0.9854, acc-0.6600, valid loss-0.8023, acc-0.7966, test loss-0.8060, acc-0.8018\n",
      "Iter-8740, train loss-0.8405, acc-0.7200, valid loss-0.8017, acc-0.7974, test loss-0.8054, acc-0.8019\n",
      "Iter-8750, train loss-1.0221, acc-0.7400, valid loss-0.8011, acc-0.7974, test loss-0.8047, acc-0.8022\n",
      "Iter-8760, train loss-0.9107, acc-0.7400, valid loss-0.8006, acc-0.7970, test loss-0.8042, acc-0.8023\n",
      "Iter-8770, train loss-0.9330, acc-0.7000, valid loss-0.8000, acc-0.7972, test loss-0.8036, acc-0.8023\n",
      "Iter-8780, train loss-0.8054, acc-0.7400, valid loss-0.7994, acc-0.7988, test loss-0.8030, acc-0.8026\n",
      "Iter-8790, train loss-0.8844, acc-0.8200, valid loss-0.7988, acc-0.7986, test loss-0.8024, acc-0.8027\n",
      "Iter-8800, train loss-0.9159, acc-0.7800, valid loss-0.7982, acc-0.7990, test loss-0.8018, acc-0.8030\n",
      "Iter-8810, train loss-0.7676, acc-0.8800, valid loss-0.7976, acc-0.7988, test loss-0.8012, acc-0.8032\n",
      "Iter-8820, train loss-0.7710, acc-0.8200, valid loss-0.7970, acc-0.7988, test loss-0.8006, acc-0.8032\n",
      "Iter-8830, train loss-0.7973, acc-0.7000, valid loss-0.7964, acc-0.7990, test loss-0.8000, acc-0.8037\n",
      "Iter-8840, train loss-0.6872, acc-0.8000, valid loss-0.7959, acc-0.7988, test loss-0.7995, acc-0.8042\n",
      "Iter-8850, train loss-0.8369, acc-0.8200, valid loss-0.7953, acc-0.7982, test loss-0.7989, acc-0.8043\n",
      "Iter-8860, train loss-0.6361, acc-0.8200, valid loss-0.7948, acc-0.7986, test loss-0.7984, acc-0.8046\n",
      "Iter-8870, train loss-0.8495, acc-0.7800, valid loss-0.7942, acc-0.7986, test loss-0.7978, acc-0.8047\n",
      "Iter-8880, train loss-1.0472, acc-0.7600, valid loss-0.7936, acc-0.7986, test loss-0.7972, acc-0.8051\n",
      "Iter-8890, train loss-0.7759, acc-0.8000, valid loss-0.7931, acc-0.7994, test loss-0.7966, acc-0.8051\n",
      "Iter-8900, train loss-0.8178, acc-0.7800, valid loss-0.7925, acc-0.7996, test loss-0.7960, acc-0.8051\n",
      "Iter-8910, train loss-0.7690, acc-0.8000, valid loss-0.7919, acc-0.7992, test loss-0.7955, acc-0.8054\n",
      "Iter-8920, train loss-0.6872, acc-0.8200, valid loss-0.7913, acc-0.7998, test loss-0.7949, acc-0.8051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-0.7909, acc-0.7800, valid loss-0.7908, acc-0.7994, test loss-0.7943, acc-0.8057\n",
      "Iter-8940, train loss-0.6907, acc-0.9000, valid loss-0.7902, acc-0.7998, test loss-0.7937, acc-0.8054\n",
      "Iter-8950, train loss-0.8370, acc-0.8000, valid loss-0.7896, acc-0.7996, test loss-0.7932, acc-0.8057\n",
      "Iter-8960, train loss-0.7007, acc-0.8600, valid loss-0.7891, acc-0.7996, test loss-0.7926, acc-0.8059\n",
      "Iter-8970, train loss-0.7781, acc-0.8400, valid loss-0.7885, acc-0.7996, test loss-0.7921, acc-0.8060\n",
      "Iter-8980, train loss-0.8662, acc-0.8200, valid loss-0.7880, acc-0.8002, test loss-0.7915, acc-0.8066\n",
      "Iter-8990, train loss-0.7905, acc-0.7600, valid loss-0.7874, acc-0.8000, test loss-0.7909, acc-0.8062\n",
      "Iter-9000, train loss-0.7781, acc-0.8200, valid loss-0.7868, acc-0.8004, test loss-0.7903, acc-0.8062\n",
      "Iter-9010, train loss-0.8444, acc-0.7400, valid loss-0.7862, acc-0.8004, test loss-0.7898, acc-0.8067\n",
      "Iter-9020, train loss-0.7388, acc-0.8200, valid loss-0.7856, acc-0.8006, test loss-0.7892, acc-0.8066\n",
      "Iter-9030, train loss-0.7035, acc-0.8400, valid loss-0.7851, acc-0.8008, test loss-0.7886, acc-0.8067\n",
      "Iter-9040, train loss-0.6643, acc-0.9000, valid loss-0.7845, acc-0.8012, test loss-0.7880, acc-0.8067\n",
      "Iter-9050, train loss-0.8354, acc-0.7800, valid loss-0.7839, acc-0.8016, test loss-0.7874, acc-0.8070\n",
      "Iter-9060, train loss-0.8135, acc-0.8400, valid loss-0.7833, acc-0.8020, test loss-0.7868, acc-0.8071\n",
      "Iter-9070, train loss-0.7614, acc-0.9200, valid loss-0.7827, acc-0.8026, test loss-0.7863, acc-0.8069\n",
      "Iter-9080, train loss-0.8871, acc-0.7800, valid loss-0.7822, acc-0.8020, test loss-0.7857, acc-0.8072\n",
      "Iter-9090, train loss-0.8691, acc-0.7400, valid loss-0.7816, acc-0.8024, test loss-0.7851, acc-0.8078\n",
      "Iter-9100, train loss-0.8621, acc-0.7200, valid loss-0.7811, acc-0.8024, test loss-0.7846, acc-0.8079\n",
      "Iter-9110, train loss-0.7340, acc-0.8800, valid loss-0.7805, acc-0.8024, test loss-0.7840, acc-0.8077\n",
      "Iter-9120, train loss-0.8399, acc-0.7400, valid loss-0.7800, acc-0.8020, test loss-0.7834, acc-0.8081\n",
      "Iter-9130, train loss-0.7801, acc-0.7600, valid loss-0.7794, acc-0.8026, test loss-0.7829, acc-0.8085\n",
      "Iter-9140, train loss-0.8156, acc-0.7800, valid loss-0.7789, acc-0.8024, test loss-0.7824, acc-0.8088\n",
      "Iter-9150, train loss-0.8172, acc-0.7400, valid loss-0.7784, acc-0.8026, test loss-0.7819, acc-0.8088\n",
      "Iter-9160, train loss-0.6431, acc-0.9000, valid loss-0.7779, acc-0.8038, test loss-0.7814, acc-0.8089\n",
      "Iter-9170, train loss-0.6674, acc-0.8600, valid loss-0.7773, acc-0.8032, test loss-0.7808, acc-0.8093\n",
      "Iter-9180, train loss-0.7430, acc-0.8000, valid loss-0.7767, acc-0.8034, test loss-0.7803, acc-0.8093\n",
      "Iter-9190, train loss-0.8415, acc-0.8000, valid loss-0.7762, acc-0.8038, test loss-0.7797, acc-0.8089\n",
      "Iter-9200, train loss-0.7413, acc-0.8800, valid loss-0.7756, acc-0.8048, test loss-0.7792, acc-0.8091\n",
      "Iter-9210, train loss-0.8473, acc-0.7400, valid loss-0.7751, acc-0.8044, test loss-0.7786, acc-0.8091\n",
      "Iter-9220, train loss-0.7342, acc-0.7800, valid loss-0.7746, acc-0.8044, test loss-0.7781, acc-0.8093\n",
      "Iter-9230, train loss-0.8816, acc-0.8000, valid loss-0.7741, acc-0.8048, test loss-0.7775, acc-0.8102\n",
      "Iter-9240, train loss-0.7176, acc-0.8400, valid loss-0.7736, acc-0.8050, test loss-0.7769, acc-0.8105\n",
      "Iter-9250, train loss-0.7790, acc-0.7800, valid loss-0.7730, acc-0.8052, test loss-0.7764, acc-0.8105\n",
      "Iter-9260, train loss-0.8858, acc-0.6600, valid loss-0.7725, acc-0.8052, test loss-0.7758, acc-0.8109\n",
      "Iter-9270, train loss-0.8354, acc-0.8200, valid loss-0.7719, acc-0.8054, test loss-0.7752, acc-0.8108\n",
      "Iter-9280, train loss-0.9629, acc-0.8000, valid loss-0.7714, acc-0.8054, test loss-0.7747, acc-0.8113\n",
      "Iter-9290, train loss-0.6635, acc-0.8600, valid loss-0.7709, acc-0.8054, test loss-0.7741, acc-0.8116\n",
      "Iter-9300, train loss-0.7299, acc-0.8200, valid loss-0.7703, acc-0.8054, test loss-0.7736, acc-0.8120\n",
      "Iter-9310, train loss-0.7588, acc-0.7400, valid loss-0.7698, acc-0.8056, test loss-0.7731, acc-0.8121\n",
      "Iter-9320, train loss-0.6175, acc-0.9000, valid loss-0.7693, acc-0.8060, test loss-0.7726, acc-0.8122\n",
      "Iter-9330, train loss-0.9470, acc-0.7600, valid loss-0.7687, acc-0.8056, test loss-0.7720, acc-0.8119\n",
      "Iter-9340, train loss-0.9555, acc-0.7400, valid loss-0.7682, acc-0.8054, test loss-0.7714, acc-0.8122\n",
      "Iter-9350, train loss-0.9069, acc-0.6800, valid loss-0.7676, acc-0.8054, test loss-0.7709, acc-0.8125\n",
      "Iter-9360, train loss-0.7405, acc-0.8000, valid loss-0.7670, acc-0.8058, test loss-0.7704, acc-0.8122\n",
      "Iter-9370, train loss-0.7160, acc-0.8400, valid loss-0.7665, acc-0.8062, test loss-0.7699, acc-0.8126\n",
      "Iter-9380, train loss-0.7799, acc-0.7800, valid loss-0.7659, acc-0.8060, test loss-0.7694, acc-0.8127\n",
      "Iter-9390, train loss-0.7147, acc-0.8400, valid loss-0.7654, acc-0.8062, test loss-0.7688, acc-0.8131\n",
      "Iter-9400, train loss-0.6430, acc-0.8000, valid loss-0.7649, acc-0.8068, test loss-0.7683, acc-0.8129\n",
      "Iter-9410, train loss-0.6709, acc-0.8000, valid loss-0.7644, acc-0.8068, test loss-0.7678, acc-0.8133\n",
      "Iter-9420, train loss-0.7023, acc-0.9200, valid loss-0.7638, acc-0.8070, test loss-0.7672, acc-0.8135\n",
      "Iter-9430, train loss-0.7284, acc-0.7600, valid loss-0.7633, acc-0.8072, test loss-0.7667, acc-0.8137\n",
      "Iter-9440, train loss-0.8629, acc-0.7400, valid loss-0.7628, acc-0.8072, test loss-0.7662, acc-0.8137\n",
      "Iter-9450, train loss-0.6108, acc-0.8800, valid loss-0.7623, acc-0.8070, test loss-0.7657, acc-0.8137\n",
      "Iter-9460, train loss-0.7126, acc-0.8600, valid loss-0.7618, acc-0.8076, test loss-0.7651, acc-0.8142\n",
      "Iter-9470, train loss-0.6634, acc-0.8400, valid loss-0.7612, acc-0.8076, test loss-0.7646, acc-0.8145\n",
      "Iter-9480, train loss-0.7636, acc-0.7600, valid loss-0.7607, acc-0.8082, test loss-0.7640, acc-0.8142\n",
      "Iter-9490, train loss-0.6665, acc-0.8600, valid loss-0.7602, acc-0.8078, test loss-0.7635, acc-0.8142\n",
      "Iter-9500, train loss-0.7612, acc-0.8000, valid loss-0.7596, acc-0.8078, test loss-0.7629, acc-0.8141\n",
      "Iter-9510, train loss-0.7974, acc-0.8400, valid loss-0.7591, acc-0.8080, test loss-0.7624, acc-0.8142\n",
      "Iter-9520, train loss-0.8455, acc-0.7200, valid loss-0.7586, acc-0.8086, test loss-0.7618, acc-0.8144\n",
      "Iter-9530, train loss-0.8379, acc-0.7000, valid loss-0.7581, acc-0.8084, test loss-0.7613, acc-0.8146\n",
      "Iter-9540, train loss-0.9388, acc-0.6800, valid loss-0.7575, acc-0.8086, test loss-0.7608, acc-0.8143\n",
      "Iter-9550, train loss-0.7186, acc-0.9000, valid loss-0.7570, acc-0.8090, test loss-0.7603, acc-0.8145\n",
      "Iter-9560, train loss-0.8034, acc-0.7600, valid loss-0.7565, acc-0.8094, test loss-0.7598, acc-0.8148\n",
      "Iter-9570, train loss-0.6165, acc-0.8800, valid loss-0.7560, acc-0.8094, test loss-0.7593, acc-0.8149\n",
      "Iter-9580, train loss-0.8442, acc-0.8000, valid loss-0.7554, acc-0.8092, test loss-0.7588, acc-0.8146\n",
      "Iter-9590, train loss-0.8136, acc-0.7800, valid loss-0.7548, acc-0.8098, test loss-0.7583, acc-0.8143\n",
      "Iter-9600, train loss-0.7802, acc-0.8000, valid loss-0.7543, acc-0.8102, test loss-0.7577, acc-0.8148\n",
      "Iter-9610, train loss-0.5229, acc-0.9200, valid loss-0.7538, acc-0.8112, test loss-0.7572, acc-0.8151\n",
      "Iter-9620, train loss-0.6964, acc-0.8400, valid loss-0.7533, acc-0.8114, test loss-0.7567, acc-0.8152\n",
      "Iter-9630, train loss-0.7773, acc-0.8200, valid loss-0.7528, acc-0.8118, test loss-0.7561, acc-0.8156\n",
      "Iter-9640, train loss-0.8426, acc-0.8000, valid loss-0.7522, acc-0.8118, test loss-0.7556, acc-0.8158\n",
      "Iter-9650, train loss-0.7098, acc-0.8400, valid loss-0.7517, acc-0.8126, test loss-0.7551, acc-0.8162\n",
      "Iter-9660, train loss-0.6041, acc-0.9000, valid loss-0.7512, acc-0.8130, test loss-0.7546, acc-0.8161\n",
      "Iter-9670, train loss-0.7711, acc-0.8000, valid loss-0.7507, acc-0.8126, test loss-0.7541, acc-0.8162\n",
      "Iter-9680, train loss-0.7451, acc-0.8000, valid loss-0.7502, acc-0.8124, test loss-0.7536, acc-0.8164\n",
      "Iter-9690, train loss-0.7356, acc-0.9000, valid loss-0.7497, acc-0.8128, test loss-0.7531, acc-0.8161\n",
      "Iter-9700, train loss-0.8038, acc-0.7600, valid loss-0.7491, acc-0.8128, test loss-0.7525, acc-0.8161\n",
      "Iter-9710, train loss-0.8488, acc-0.8200, valid loss-0.7487, acc-0.8132, test loss-0.7520, acc-0.8163\n",
      "Iter-9720, train loss-0.7808, acc-0.7600, valid loss-0.7482, acc-0.8130, test loss-0.7515, acc-0.8166\n",
      "Iter-9730, train loss-0.8394, acc-0.8000, valid loss-0.7477, acc-0.8132, test loss-0.7510, acc-0.8169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-0.7930, acc-0.8000, valid loss-0.7472, acc-0.8132, test loss-0.7505, acc-0.8168\n",
      "Iter-9750, train loss-0.8757, acc-0.6800, valid loss-0.7467, acc-0.8138, test loss-0.7500, acc-0.8167\n",
      "Iter-9760, train loss-0.6576, acc-0.8200, valid loss-0.7461, acc-0.8138, test loss-0.7495, acc-0.8168\n",
      "Iter-9770, train loss-0.9665, acc-0.7200, valid loss-0.7457, acc-0.8136, test loss-0.7490, acc-0.8171\n",
      "Iter-9780, train loss-0.8037, acc-0.8000, valid loss-0.7452, acc-0.8136, test loss-0.7485, acc-0.8171\n",
      "Iter-9790, train loss-0.7932, acc-0.8000, valid loss-0.7448, acc-0.8138, test loss-0.7481, acc-0.8170\n",
      "Iter-9800, train loss-0.9688, acc-0.7600, valid loss-0.7443, acc-0.8138, test loss-0.7476, acc-0.8171\n",
      "Iter-9810, train loss-0.8784, acc-0.7800, valid loss-0.7438, acc-0.8140, test loss-0.7471, acc-0.8174\n",
      "Iter-9820, train loss-0.7472, acc-0.8600, valid loss-0.7433, acc-0.8144, test loss-0.7466, acc-0.8175\n",
      "Iter-9830, train loss-0.7089, acc-0.8400, valid loss-0.7428, acc-0.8146, test loss-0.7462, acc-0.8176\n",
      "Iter-9840, train loss-0.6341, acc-0.8800, valid loss-0.7423, acc-0.8148, test loss-0.7456, acc-0.8174\n",
      "Iter-9850, train loss-0.6543, acc-0.8200, valid loss-0.7417, acc-0.8150, test loss-0.7451, acc-0.8175\n",
      "Iter-9860, train loss-0.8322, acc-0.7000, valid loss-0.7412, acc-0.8154, test loss-0.7446, acc-0.8178\n",
      "Iter-9870, train loss-0.6888, acc-0.8200, valid loss-0.7407, acc-0.8156, test loss-0.7441, acc-0.8180\n",
      "Iter-9880, train loss-0.8359, acc-0.7000, valid loss-0.7402, acc-0.8160, test loss-0.7436, acc-0.8180\n",
      "Iter-9890, train loss-0.7036, acc-0.8200, valid loss-0.7397, acc-0.8166, test loss-0.7431, acc-0.8182\n",
      "Iter-9900, train loss-0.8258, acc-0.7000, valid loss-0.7392, acc-0.8166, test loss-0.7427, acc-0.8186\n",
      "Iter-9910, train loss-0.6285, acc-0.8600, valid loss-0.7387, acc-0.8168, test loss-0.7422, acc-0.8187\n",
      "Iter-9920, train loss-0.7049, acc-0.8400, valid loss-0.7382, acc-0.8172, test loss-0.7417, acc-0.8189\n",
      "Iter-9930, train loss-0.6885, acc-0.8400, valid loss-0.7377, acc-0.8174, test loss-0.7411, acc-0.8187\n",
      "Iter-9940, train loss-0.7470, acc-0.8200, valid loss-0.7372, acc-0.8178, test loss-0.7406, acc-0.8188\n",
      "Iter-9950, train loss-0.7063, acc-0.8000, valid loss-0.7367, acc-0.8180, test loss-0.7401, acc-0.8191\n",
      "Iter-9960, train loss-0.7617, acc-0.8000, valid loss-0.7362, acc-0.8178, test loss-0.7396, acc-0.8194\n",
      "Iter-9970, train loss-0.8704, acc-0.7800, valid loss-0.7358, acc-0.8184, test loss-0.7391, acc-0.8191\n",
      "Iter-9980, train loss-0.7599, acc-0.7600, valid loss-0.7353, acc-0.8178, test loss-0.7386, acc-0.8195\n",
      "Iter-9990, train loss-0.7466, acc-0.8000, valid loss-0.7348, acc-0.8180, test loss-0.7381, acc-0.8193\n",
      "Iter-10000, train loss-0.6326, acc-0.8600, valid loss-0.7343, acc-0.8180, test loss-0.7376, acc-0.8196\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsP8H9jJ66dgFfLEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11902a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJaAoilKIUSWgKIpSiPl/Ht7KX6SgsJoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1194ef5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

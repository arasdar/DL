{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, C, L, p_dropout):\n",
    "        self.L = L\n",
    "        self.H = H\n",
    "        self.D = D\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Input layer\n",
    "        m_in = dict(\n",
    "            Wx=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            bx=np.zeros((1, H))\n",
    "            )\n",
    "\n",
    "        # Hidden layers\n",
    "        m_h = dict(\n",
    "            Wxh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Wx=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Wh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            bxh=np.zeros((1, H)),\n",
    "            bx=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H))\n",
    "            )\n",
    "            \n",
    "        # Output layer\n",
    "        m_out = dict(\n",
    "            Wy=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "            by=np.zeros((1, C))\n",
    "            )\n",
    "        \n",
    "        # Model parameters\n",
    "        self.model = []\n",
    "        self.model.append(m_in) # input layer: layer == 0\n",
    "        for _ in range(self.L): # hidden layer: layer == 1:self.L\n",
    "            self.model.append(m_h)\n",
    "        self.model.append(m_out) # output layer: layer == self.L\n",
    "                \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "        a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = (a * dropped) + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, bxh = m['Wxh'], m['Whh'], m['bxh']\n",
    "        Xh = (X @ Wxh) + (h @ Whh) + bxh\n",
    "        Xh, Xh_nl_cache = l.tanh_forward(Xh)\n",
    "        # Xh, Xh_nl_cache = self.selu_forward(Xh)\n",
    "        \n",
    "        Wx, bx = m['Wx'], m['bx']\n",
    "        X_in = X.copy()\n",
    "        # X = (X @ Wx) + bx\n",
    "        X, X_fc_cache = l.fc_forward(X, Wx, bx)\n",
    "        X, X_nl_cache = l.tanh_forward(X)\n",
    "        # X, X_nl_cache = self.selu_forward(X)\n",
    "        X += X_in\n",
    "        \n",
    "        Wh, bh = m['Wh'], m['bh']\n",
    "        h_in = h.copy()\n",
    "        # h = (h @ Wh) + bh\n",
    "        h, h_fc_cache = l.fc_forward(h, Wh, bh)\n",
    "        h, h_nl_cache = l.tanh_forward(h)\n",
    "        # h, h_nl_cache = self.selu_forward(h)\n",
    "        h += h_in\n",
    "        \n",
    "        h = Xh + h\n",
    "        y = Xh + X\n",
    "\n",
    "        h, h_selu_nl_cache = self.selu_forward(h)\n",
    "        h, h_selu_do_cache = self.alpha_dropout_fwd(h, self.p_dropout)\n",
    "        \n",
    "        y, y_selu_nl_cache = self.selu_forward(y)\n",
    "        y, y_selu_do_cache = self.alpha_dropout_fwd(y, self.p_dropout)\n",
    "        \n",
    "        cache = (X_in, h_in, Wxh, Whh, Xh_nl_cache, X_fc_cache, X_nl_cache, h_fc_cache, h_nl_cache, h_selu_nl_cache, h_selu_do_cache, y_selu_nl_cache, y_selu_do_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        (X_in, h_in, Wxh, Whh, Xh_nl_cache, X_fc_cache, X_nl_cache, h_fc_cache, h_nl_cache, h_selu_nl_cache, h_selu_do_cache, y_selu_nl_cache, y_selu_do_cache) = cache\n",
    "         \n",
    "        dh = self.alpha_dropout_bwd(dh, h_selu_do_cache)\n",
    "        dh = self.selu_backward(dh, h_selu_nl_cache)\n",
    "        \n",
    "        dy = self.alpha_dropout_bwd(dy, y_selu_do_cache)\n",
    "        dy = self.selu_backward(dy, y_selu_nl_cache)\n",
    "\n",
    "        dh_out = dh.copy()\n",
    "        # dh = self.selu_backward(dh, h_nl_cache)\n",
    "        dh = l.tanh_backward(dh, h_nl_cache)\n",
    "        dh, dWh, dbh = l.fc_backward(dh, h_fc_cache)\n",
    "        dh += dh_out\n",
    "        #         dWh = h_in.T @ dh # nxh = nx1 @ 1xh\n",
    "        #         dbh = dh * 1.0\n",
    "        #         dh = dh @ Wh.T # 1xn = 1xh @ hxn\n",
    "\n",
    "        dX = dy.copy()\n",
    "        # dX = self.selu_backward(dX, X_nl_cache)\n",
    "        dX = l.tanh_backward(dX, X_nl_cache)\n",
    "        dX, dWx, dbx = l.fc_backward(dX, X_fc_cache)\n",
    "        dX += dy\n",
    "        #         dWx = X_in.T @ dX # nxh = nx1 @ 1xh\n",
    "        #         dbx = dX * 1.0\n",
    "        #         dX = dX @ Wx.T # 1xn = 1xh @ hxn\n",
    "\n",
    "        dXh = dy + dh        \n",
    "        # dXh = self.selu_backward(dXh, Xh_nl_cache)\n",
    "        dXh = l.tanh_backward(dXh, Xh_nl_cache)\n",
    "        dbxh = dXh * 1.0\n",
    "        dWhh = h_in.T @ dXh\n",
    "        dWxh = X_in.T @ dXh\n",
    "        dX += dXh @ Wxh.T # 1xn = 1xh @ hxn\n",
    "        dh += dXh @ Whh.T # 1xn = 1xh @ hxn\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, bxh=dbxh, Wx=dWx, bx=dbx, Wh=dWh, bh=dbh)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        # Input (1), hidden (L), and output layers (1)\n",
    "        caches = []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        # Hidden layers and cells connections\n",
    "        for _ in range(self.L + 2):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        # Hidden layers    \n",
    "        for layer in range(self.L + 2):\n",
    "            if layer == 0: # input layer\n",
    "                ys = []\n",
    "                for X in X_train:\n",
    "                    X_one_hot = np.zeros(self.D)\n",
    "                    X_one_hot[X] = 1.\n",
    "                    X = X_one_hot.reshape(1, -1)\n",
    "                    y, cache = l.fc_forward(X, self.model[layer]['Wx'], self.model[layer]['bx'])\n",
    "                    # print(y.shape, X.shape)\n",
    "                    caches[layer].append(cache)\n",
    "                    ys.append(y)\n",
    "            if (layer > 0) and (layer < self.L + 1): # hidden layers\n",
    "                Xs = ys.copy()\n",
    "                ys = []\n",
    "                for X in Xs:\n",
    "                    y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                    caches[layer].append(cache)\n",
    "                    ys.append(y)\n",
    "            if layer == self.L + 1: # output layer\n",
    "                Xs = ys.copy()\n",
    "                ys = []\n",
    "                for X in Xs:\n",
    "                    y, cache = l.fc_forward(X, self.model[layer]['Wy'], self.model[layer]['by'])\n",
    "                    caches[layer].append(cache)\n",
    "                    ys.append(y)\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L + 2):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "         \n",
    "        for layer in reversed(range(self.L + 2)):\n",
    "            if layer == (self.L + 1):  # Output layer\n",
    "                dXs = []\n",
    "                for t in reversed(range(len(dys))):\n",
    "                    dy = dys[t]\n",
    "                    dX, dWy, dby = l.fc_backward(dy, caches[layer][t])\n",
    "                    grads[layer]['Wy'] += dWy\n",
    "                    grads[layer]['by'] += dby\n",
    "                    dXs.append(dX)\n",
    "            if (layer > 0) and (layer < (self.L + 1)): # Middle layers\n",
    "                dys = dXs.copy()\n",
    "                dXs = []\n",
    "                for t in reversed(range(len(dys))):\n",
    "                    dy = dys[t]\n",
    "                    dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                    for key in grad[layer].keys():\n",
    "                        grads[layer][key] += grad[layer][key]\n",
    "                    dXs.append(dX)\n",
    "            if layer == 0: # Input-Output layer\n",
    "                dys = dXs.copy()\n",
    "                dXs = []\n",
    "                for t in reversed(range(len(dys))):\n",
    "                    dy = dys[t]\n",
    "                    dX, dWx, dbx = l.fc_backward(dy, caches[layer][t])\n",
    "                    grads[layer]['Wx'] += dWx\n",
    "                    grads[layer]['bx'] += dbx\n",
    "                    dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    #     def test(self, X_seed, h, size):\n",
    "    #         chars = [self.idx2char[X_seed]]\n",
    "    #         idx_list = list(range(self.vocab_size))\n",
    "    #         X = X_seed\n",
    "\n",
    "    #         h_init = h.copy()\n",
    "    #         h = []\n",
    "    #         for _ in range(self.L):\n",
    "    #             h.append(h_init.copy())\n",
    "\n",
    "    #         # Test is different than train since y[t+1] is related to y[t] \n",
    "    #         for _ in range(size):\n",
    "    #             X_one_hot = np.zeros(self.D)\n",
    "    #             X_one_hot[X] = 1.\n",
    "    #             X = X_one_hot.reshape(1, -1)\n",
    "    #             for layer in range(self.L): # start, stop, step\n",
    "    #                 y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "    #                 if layer == self.L-1: # this is the last layer\n",
    "    #                     y_logit = y\n",
    "    #                 else: \n",
    "    #                     X = y # y: output for this layer, X: input for this layer\n",
    "    #             y_prob = l.softmax(y_logit)\n",
    "    #             idx = np.random.choice(idx_list, p=y_prob.ravel())\n",
    "    #             chars.append(self.idx2char[idx])\n",
    "    #             X = idx\n",
    "\n",
    "    #         return ''.join(chars)\n",
    "\n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        # for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam_rnn(self, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "        M, R = [], []\n",
    "         \n",
    "        # Hidden layers\n",
    "        for layer in range(nn.L + 2):\n",
    "            M.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            R.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "         \n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8\n",
    "        state = self.initial_state()\n",
    "        smooth_loss = 1.0\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibacthes\n",
    "            for idx in range(len(minibatches)):\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                ys, caches = self.train_forward(X_mini, state)\n",
    "                loss, dys = self.loss_function(y_train=y_mini, ys=ys)\n",
    "                _, grads = self.train_backward(dys, caches)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "                for layer in range(nn.L + 2):\n",
    "                    for key in grads[layer].keys(): #key, value: items\n",
    "                        M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                        R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Print loss and test sample\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "                #                 sample = self.test(X_mini[0], state, size=100)\n",
    "                #                 print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.5/site-packages/ipykernel_launcher.py:334: RuntimeWarning: overflow encountered in square\n",
      "/Users/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.5/site-packages/ipykernel_launcher.py:337: RuntimeWarning: overflow encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 186.5414\n",
      "Iter-2 loss: 174.6118\n",
      "Iter-3 loss: 161.5729\n",
      "Iter-4 loss: 124.1154\n",
      "Iter-5 loss: 159.1228\n",
      "Iter-6 loss: 158.5768\n",
      "Iter-7 loss: 136.5101\n",
      "Iter-8 loss: 146.6200\n",
      "Iter-9 loss: 163.5492\n",
      "Iter-10 loss: 142.3116\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAoIgCJUYUQKCIAiVGFECgiAIlZj/D47lwAX1WtSJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bf99438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 20 # depth\n",
    "n_iter = 10 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "num_output_units = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "nn = RNN(D=num_input_units, H=num_hidden_units, C=num_output_units, L=num_layers, p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "nn.adam_rnn(X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "    \n",
    "        h = (X @ Wxh) + (hprev @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        #         y, nl_cache = self.selu_forward(y)\n",
    "        #         y, do_cache = self.alpha_dropout_fwd(h=y, q=1.0-self.p_dropout) # q=1-p, 1=keep_prob\n",
    "        #         y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "\n",
    "        #         cache = (X, hprev, Wxh, Whh, h_cache, y_cache, nl_cache, do_cache)\n",
    "        cache = (X, hprev, Wxh, Whh, h_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        #         X, hprev, Wxh, Whh, h_cache, y_cache, nl_cache, do_cache = cache\n",
    "        X, hprev, Wxh, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        #         dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        #         dy = self.alpha_dropout_bwd(dout=dy, cache=do_cache)\n",
    "        #         dy = self.selu_backward(dy, nl_cache)\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "        dbh = dh * 1.0\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "    \n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            \n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-130 training loss: 2.2327\n",
      "ereinccast ic Semwergend tidaige fimeclino-minric ineky fos tuntise pel toreiof in hary. Norlt an sgo\n",
      "Iter-260 training loss: 2.2368\n",
      "e fy-khilht lerbis a tron roulith oud the cit agoly in iry a pouniand un the Mithe 12rowir tund of 19\n",
      "Iter-390 training loss: 2.0913\n",
      "ed wade nalale, fitec Noviis inlth is. Loveg carol leriziciric or, e cimeleacuracel the lbil if is in\n",
      "Iter-520 training loss: 2.0849\n",
      "ereins ef. iust the weseet cal conn a 45 Mesol iscy city ged rocaltuttuun (Japan od corlt the Sichint\n",
      "Iter-650 training loss: 2.1343\n",
      "e wonsto. Nd Japan lend ag bul. Japan, is extas peded aed tar The land lertul embes wary. The 1 Nn bo\n",
      "Iter-780 training loss: 2.0464\n",
      "ereser JapEtbowarg in Ast thir rind prankareg and the Usind is k8 Japan, on mertabiand tumpond il opo\n",
      "Iter-910 training loss: 2.0473\n",
      "e nosest the foctiridest fhhky. Japan rom ibed a treg Aly is Aujmarow wan in e fion is O-9and the fom\n",
      "Iter-1040 training loss: 2.0275\n",
      "e wovt aed in wal in To Senlslth lopperth-last ren, whaon the fertod. In of Jap41papania ren's it aod\n",
      "Iter-1170 training loss: 1.9979\n",
      "er and Eato, molt intoky' thesed, enj warged. The noligo witan isiorcetomighiod. ofy Dhino-Janduds th\n",
      "Iter-1300 training loss: 2.7135\n",
      "ered asen shotutry nrecoscinsy Gared is com ek marty. Asd ppand olt 1945-eakialky thampeadern cild's \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7f20e0458c18>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 1300 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.10 # keep_prob=1.0-p_dropout, q=1-p, 5% to 10% noise is recommanded for p_dropout\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEKMQYCIKMQYBIiIQoxBgIgoxP4fPXYSBCvzdekAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20e0516630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

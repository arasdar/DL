{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18720, 205, 16) float64 (5400, 205, 16) float64 (13320, 205, 16) float64\n",
      "(13320, 205, 16) float64 (9540, 205, 16) float64 (3780, 205, 16) float64\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "\n",
    "# Reading the data for the Face from all the subjects\n",
    "BahramFace = spio.loadmat(file_name='../data/bci-project-data-RAW/BahramFace.mat')\n",
    "DJFace = spio.loadmat(file_name='../data/bci-project-data-RAW/DJFace.mat')\n",
    "NickFace = spio.loadmat(file_name='../data/bci-project-data-RAW/NickFace.mat')\n",
    "RoohiFace = spio.loadmat(file_name='../data/bci-project-data-RAW/RoohiFace.mat')\n",
    "SarahFace = spio.loadmat(file_name='../data/bci-project-data-RAW/SarahFace.mat')\n",
    "\n",
    "# Deviding the input data into train and validation\n",
    "# For creating the training and testing set, \n",
    "# 30% percent of each subject is considered as test and\n",
    "# 70% of each subject is conidered as training.\n",
    "length = int(BahramFace['Intensification_Data'].shape[0] * 0.30)\n",
    "# length\n",
    "\n",
    "FacesDataAll = np.vstack(tup=(BahramFace['Intensification_Data'][:], \n",
    "                       DJFace['Intensification_Data'][:], \n",
    "                       NickFace['Intensification_Data'][:],\n",
    "                      RoohiFace['Intensification_Data'][:],\n",
    "                      SarahFace['Intensification_Data'][:]))\n",
    "\n",
    "FacesDataTrainAll = np.vstack(tup=(BahramFace['Intensification_Data'][:-length], \n",
    "                       DJFace['Intensification_Data'][:-length], \n",
    "                       NickFace['Intensification_Data'][:-length],\n",
    "                      RoohiFace['Intensification_Data'][:-length],\n",
    "                      SarahFace['Intensification_Data'][:-length]))\n",
    "\n",
    "FacesDataTest = np.vstack(tup=(BahramFace['Intensification_Data'][-length:], \n",
    "                       DJFace['Intensification_Data'][-length:], \n",
    "                       NickFace['Intensification_Data'][-length:],\n",
    "                      RoohiFace['Intensification_Data'][-length:],\n",
    "                      SarahFace['Intensification_Data'][-length:]))\n",
    "\n",
    "print(FacesDataAll.shape, FacesDataAll.dtype, \n",
    " FacesDataTest.shape, FacesDataTest.dtype, \n",
    " FacesDataTrainAll.shape, FacesDataTrainAll.dtype)\n",
    "\n",
    "BahramFaceDataTrain = BahramFace['Intensification_Data'][:-length]\n",
    "DJFaceDataTrain = DJFace['Intensification_Data'][:-length]\n",
    "NickFaceDataTrain = NickFace['Intensification_Data'][:-length]\n",
    "RoohiFaceDataTrain = RoohiFace['Intensification_Data'][:-length]\n",
    "SarahFaceDataTrain = SarahFace['Intensification_Data'][:-length]\n",
    "\n",
    "# 30% of the total training data is validation,\n",
    "# 70% of the total training data is training\n",
    "# This is applied to every single subject data.\n",
    "length2 = int(BahramFaceDataTrain.shape[0] * 0.30)\n",
    "# length2\n",
    "\n",
    "FacesDataTrain = np.vstack(tup=(BahramFaceDataTrain[:-length2], \n",
    "                       DJFaceDataTrain[:-length2], \n",
    "                       NickFaceDataTrain[:-length2],\n",
    "                      RoohiFaceDataTrain[:-length2],\n",
    "                      SarahFaceDataTrain[:-length2]))\n",
    "\n",
    "FacesDataValid = np.vstack(tup=(BahramFaceDataTrain[-length2:], \n",
    "                       DJFaceDataTrain[-length2:], \n",
    "                       NickFaceDataTrain[-length2:],\n",
    "                      RoohiFaceDataTrain[-length2:],\n",
    "                      SarahFaceDataTrain[-length2:]))\n",
    "\n",
    "print(FacesDataTrainAll.shape, FacesDataTrainAll.dtype, \n",
    " FacesDataTrain.shape, FacesDataTrain.dtype, \n",
    " FacesDataValid.shape, FacesDataValid.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18720, 1) uint8 (5400, 1) uint8 (13320, 1) uint8\n"
     ]
    }
   ],
   "source": [
    "FacesLabelAll = np.vstack(tup=(BahramFace['Intensification_Label'][:], \n",
    "                       DJFace['Intensification_Label'][:], \n",
    "                       NickFace['Intensification_Label'][:],\n",
    "                      RoohiFace['Intensification_Label'][:],\n",
    "                      SarahFace['Intensification_Label'][:]))\n",
    "\n",
    "FacesLabelTrainAll = np.vstack(tup=(BahramFace['Intensification_Label'][:-length], \n",
    "                       DJFace['Intensification_Label'][:-length], \n",
    "                       NickFace['Intensification_Label'][:-length],\n",
    "                      RoohiFace['Intensification_Label'][:-length],\n",
    "                      SarahFace['Intensification_Label'][:-length]))\n",
    "\n",
    "FacesLabelTest = np.vstack(tup=(BahramFace['Intensification_Label'][-length:], \n",
    "                       DJFace['Intensification_Label'][-length:], \n",
    "                       NickFace['Intensification_Label'][-length:],\n",
    "                      RoohiFace['Intensification_Label'][-length:],\n",
    "                      SarahFace['Intensification_Label'][-length:]))\n",
    "\n",
    "print(FacesLabelAll.shape, FacesLabelAll.dtype,\n",
    " FacesLabelTest.shape, FacesLabelTest.dtype, \n",
    " FacesLabelTrainAll.shape, FacesLabelTrainAll.dtype)\n",
    "\n",
    "BahramFaceLabelTrain = BahramFace['Intensification_Label'][:-length]\n",
    "DJFaceLabelTrain = DJFace['Intensification_Label'][:-length]\n",
    "NickFaceLabelTrain = NickFace['Intensification_Label'][:-length]\n",
    "RoohiFaceLabelTrain = RoohiFace['Intensification_Label'][:-length]\n",
    "SarahFaceLabelTrain = SarahFace['Intensification_Label'][:-length]\n",
    "\n",
    "FacesLabelTrain = np.vstack(tup=(BahramFaceLabelTrain[:-length2], \n",
    "                       DJFaceLabelTrain[:-length2], \n",
    "                       NickFaceLabelTrain[:-length2],\n",
    "                      RoohiFaceLabelTrain[:-length2],\n",
    "                      SarahFaceLabelTrain[:-length2]))\n",
    "\n",
    "FacesLabelValid = np.vstack(tup=(BahramFaceLabelTrain[-length2:], \n",
    "                       DJFaceLabelTrain[-length2:], \n",
    "                       NickFaceLabelTrain[-length2:],\n",
    "                      RoohiFaceLabelTrain[-length2:],\n",
    "                      SarahFaceLabelTrain[-length2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9540, 205, 16) float64 (3780, 205, 16) float64 (3780, 205, 16) float64\n"
     ]
    }
   ],
   "source": [
    "# # Normalizing input data\n",
    "# def normalize(inputs, inputs_all):\n",
    "#     return (inputs - inputs_all.mean(axis=0)[None,:,:]) / inputs_all.std(axis=0)[None,:,:]\n",
    "# Yalda suggested this normalization.\n",
    "def normalize(inputs):\n",
    "    return (inputs - inputs.mean(axis=0)[None,:,:]) / inputs.std(axis=0)[None,:,:]\n",
    "\n",
    "# onehot vectorizing output labels\n",
    "def one_hot(labels, n_class):\n",
    "    \"\"\" One-hot encoding \"\"\"\n",
    "    expansion = np.eye(n_class)\n",
    "    y = expansion[:, labels-1].T\n",
    "    assert y.shape[1] == n_class, \"Wrong number of labels!\"\n",
    "\n",
    "    return y\n",
    "\n",
    "# get minibatches for learning\n",
    "def get_batches(X, y, batch_size):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], y[b:b+batch_size]\n",
    "\n",
    "# Standardize/normalize train and test\n",
    "# X_train_norm_all = normalize(inputs=FacesDataTrain, inputs_all=FacesDataAll)\n",
    "X_train_norm = normalize(inputs=FacesDataTrain)\n",
    "X_valid_norm = normalize(inputs=FacesDataValid)\n",
    "X_test_norm = normalize(inputs=FacesDataValid)\n",
    "\n",
    "print(X_train_norm.shape, X_train_norm.dtype, \n",
    "X_valid_norm.shape, X_valid_norm.dtype,\n",
    "X_test_norm.shape, X_test_norm.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len, n_channels 95 205 16\n",
      "np.mean(FacesLabelAll == 0), np.mean(FacesLabelAll==1) 0.833333333333 0.166666666667\n",
      "n_classes 2\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "# Input data\n",
    "batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = int(FacesLabelAll.max(axis=0)+1)\n",
    "print('np.mean(FacesLabelAll == 0), np.mean(FacesLabelAll==1)', \n",
    "     np.mean(FacesLabelAll == 0), np.mean(FacesLabelAll==1))\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.001 #1e-3\n",
    "epochs = 10 # num iterations for updating model\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9540, 2) (3780, 2) (5400, 2) (9540, 205, 16) (3780, 205, 16) (3780, 205, 16)\n",
      "float64 float64 float64 float64 float64 float64\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.array(FacesLabelTrain, dtype=int).reshape(-1)\n",
    "Y_valid = np.array(FacesLabelValid, dtype=int).reshape(-1)\n",
    "Y_test = np.array(FacesLabelTest, dtype=int).reshape(-1)\n",
    "\n",
    "Y_train_onehot = one_hot(labels=Y_train, n_class=n_classes)\n",
    "Y_valid_onehot = one_hot(labels=Y_valid, n_class=n_classes)\n",
    "Y_test_onehot = one_hot(labels=Y_test, n_class=n_classes)\n",
    "\n",
    "print(Y_train_onehot.shape, Y_valid_onehot.shape, Y_test_onehot.shape, \n",
    " X_train_norm.shape, X_valid_norm.shape, X_test_norm.shape)\n",
    "\n",
    "print(Y_train_onehot.dtype, Y_valid_onehot.dtype, Y_test_onehot.dtype,\n",
    " X_train_norm.dtype, X_valid_norm.dtype, X_test_norm.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Buffering/ placeholders to transfer the data from py to tf\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs_')\n",
    "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_')\n",
    "keep_prob_ = tf.placeholder(tf.float32, name = 'keep_prob_')\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate_')# Construct the LSTM inputs and LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_conv.shape, out_conv.shape (?, 205, 16) (?, 101, 32)\n",
      "in_conv.shape, out_conv.shape (?, 101, 32) (?, 49, 64)\n",
      "out_conv.shape, in_fc.shape, out_fc.shape (?, 49, 64) (?, 3136) (?, 6272)\n",
      "out_fc.shape, logits.shape (?, 6272) (?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass: Convolutional Layers, FC Layer, and Output layer\n",
    "# tf.layers.conv2d vs tf.nn.convolution:\n",
    "# tf.layers.conv2d (actually _Conv) uses tf.nn.convolution as the backend. \n",
    "# I would use tf.nn.conv2d when loading a pretrained model, \n",
    "# and tf.layers.conv2d for a model trained from scratch.\n",
    "# Pooling: To reduce the size for memory efficiency & equivariancy/invariency/ minicolumns\n",
    "# Max pooling is also considered some sort of conv+non-linearity\n",
    "# FC: Global feature/ 1st dense/fully connected\n",
    "# Output: pred 2nd densely/fully connected\n",
    "\n",
    "# (batch, 205, 16) --> (batch, 101, 32)\n",
    "# (205 - 5 + 0)/2 + 1 = (200/2)+1= 100 +1 = 101\n",
    "# 2/5 == strides/kernel_size is \n",
    "# 40% non-overlap \n",
    "# 60% overlapp\n",
    "in_conv = inputs_\n",
    "out_conv = tf.layers.conv1d(inputs=in_conv, filters=32, kernel_size=5, strides=2, padding='valid')\n",
    "out_conv = tf.layers.batch_normalization(inputs=out_conv)\n",
    "out_conv = tf.nn.relu(features=out_conv)\n",
    "out_conv = tf.nn.dropout(x=out_conv, keep_prob=keep_prob_)\n",
    "print('in_conv.shape, out_conv.shape', in_conv.shape, out_conv.shape)\n",
    "\n",
    "# # (batch, 201, 32) --> (batch, 99, 32)\n",
    "# # (201 - 5 + 0)/2 + 1 = (196/2)+1= 98 +1 = 99\n",
    "# # 2/5 == strides/kernel_size is \n",
    "# # 40% non-overlap \n",
    "# # 60% overlapp\n",
    "# in_pool = out_conv\n",
    "# out_pool = tf.layers.max_pooling1d(inputs=in_pool, pool_size=5, strides=2, padding='valid')\n",
    "# out_pool = tf.nn.dropout(x=out_pool, keep_prob=keep_prob_)\n",
    "# print('in_pool.shape, out_pool.shape', in_pool.shape, out_pool.shape)\n",
    "\n",
    "# (batch, 101, 32) --> (batch, 49, 64)\n",
    "# (101 - 5 + 0)/2 + 1 = (96/2)+1= 48 +1 = 49\n",
    "# 2/5 == strides/kernel_size is \n",
    "# 40% non-overlap \n",
    "# 60% overlapp\n",
    "in_conv = out_conv\n",
    "out_conv = tf.layers.conv1d(inputs=in_conv, filters=64, kernel_size=5, strides=2, padding='valid')\n",
    "out_conv = tf.layers.batch_normalization(inputs=out_conv)\n",
    "out_conv = tf.nn.relu(features=out_conv)\n",
    "out_conv = tf.nn.dropout(x=out_conv, keep_prob=keep_prob_)\n",
    "print('in_conv.shape, out_conv.shape', in_conv.shape, out_conv.shape)\n",
    "\n",
    "# # (batch, 95, 64) --> (batch, 46, 64)\n",
    "# # (95 - 5 + 0)/2 + 1 = (90/2)+1= 45 +1 = 46\n",
    "# # 2/5 == strides/kernel_size is \n",
    "# # 40% non-overlap \n",
    "# # 60% overlapp\n",
    "# in_pool = out_conv\n",
    "# out_pool = tf.layers.max_pooling1d(inputs=in_pool, pool_size=5, strides=2, padding='valid')\n",
    "# out_pool = tf.nn.dropout(x=out_pool, keep_prob=keep_prob_)\n",
    "# print('in_pool.shape, out_pool.shape', in_pool.shape, out_pool.shape)\n",
    "\n",
    "# (batch, 49, 64) --> (batch, 49*64) --> (batch, 49*64*2)\n",
    "in_fc = tf.reshape(tensor=out_conv, shape=(-1, 49*64))\n",
    "out_fc = tf.layers.dense(inputs=in_fc, units=49*64*2)\n",
    "out_fc = tf.layers.batch_normalization(inputs=out_fc)\n",
    "out_fc = tf.nn.relu(features=out_fc)\n",
    "out_fc = tf.nn.dropout(x=out_fc, keep_prob=keep_prob_)\n",
    "print('out_conv.shape, in_fc.shape, out_fc.shape', out_conv.shape, in_fc.shape, out_fc.shape)\n",
    "\n",
    "# (batch, 49*64*2) --> (batch, 2) \n",
    "logits = tf.layers.dense(inputs=out_fc, units=n_classes)\n",
    "print('out_fc.shape, logits.shape', out_fc.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_1:0\", shape=(?,), dtype=bool) Tensor(\"accuracy_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "print(correct_pred, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Train loss: 0.547055 Valid loss: 0.627991 Train acc: 0.821053 Valid acc: 0.799657\n",
      "Epoch: 2/10 Train loss: 0.470396 Valid loss: 0.593883 Train acc: 0.835579 Valid acc: 0.787560\n",
      "Epoch: 3/10 Train loss: 0.428875 Valid loss: 0.588322 Train acc: 0.845684 Valid acc: 0.779752\n",
      "Epoch: 4/10 Train loss: 0.404340 Valid loss: 0.589585 Train acc: 0.851500 Valid acc: 0.776690\n",
      "Epoch: 5/10 Train loss: 0.386425 Valid loss: 0.591347 Train acc: 0.856400 Valid acc: 0.774213\n",
      "Epoch: 6/10 Train loss: 0.373878 Valid loss: 0.598118 Train acc: 0.859790 Valid acc: 0.771453\n",
      "Epoch: 7/10 Train loss: 0.364088 Valid loss: 0.598368 Train acc: 0.862511 Valid acc: 0.771737\n",
      "Epoch: 8/10 Train loss: 0.356170 Valid loss: 0.601279 Train acc: 0.865040 Valid acc: 0.772188\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "# Plotting the acc and loss curve\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initalize session global variables just in the case they are initialized.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(epochs):\n",
    "       \n",
    "        # Loop over batches\n",
    "        for x, y in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_:x, labels_:y, keep_prob_:keep_prob, learning_rate_:learning_rate}\n",
    "            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)\n",
    "            \n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            ################################ Validation\n",
    "            # Loop over batches\n",
    "            acc_batch, loss_batch = [], []\n",
    "            for x, y in get_batches(X_valid_norm, Y_train_onehot, batch_size):\n",
    "\n",
    "                # Feed dictionary\n",
    "                feed = {inputs_:x, labels_:y, keep_prob_:1.0}\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "                \n",
    "                # list of accuracy and loss for validation batch\n",
    "                acc_batch.append(acc)\n",
    "                loss_batch.append(loss)\n",
    "\n",
    "            valid_acc.append(np.mean(acc_batch))\n",
    "            valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "        # Print info in every iteration/epochs\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(valid_loss)),\n",
    "              \"Train acc: {:6f}\".format(np.mean(train_acc)),\n",
    "              \"Valid acc: {:.6f}\".format(np.mean(valid_acc)))\n",
    "        \n",
    "    ################################ Test\n",
    "    # Loop over batches\n",
    "    acc_batch, loss_batch = [], []\n",
    "    for x, y in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed = {inputs_:x, labels_:y, keep_prob_:1.0}\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "\n",
    "        # Initialize \n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Print info\n",
    "    print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "          \"Test loss: {:6f}\".format(np.mean(loss_batch)),\n",
    "          \"Test acc: {:6f}\".format(np.mean(acc_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_loss, label='train_loss')\n",
    "mplot.plot(valid_loss, label='valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_acc, label='train_acc')\n",
    "mplot.plot(valid_acc, label='valid_acc')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/pandas/io/parsers.py:709: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n",
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_all(name, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result\n",
    "\n",
    "allpaths = find_all(name='fNIR_data.txt', path='/home/arasdar/datasets/fNIRs_data/')\n",
    "\n",
    "# df: data frame object\n",
    "df = []\n",
    "for each_idx in range(len(allpaths)):\n",
    "    df.append(pd.read_csv(filepath_or_buffer=allpaths[each_idx], names=['time', 'sample', \n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel']))\n",
    "\n",
    "for each in range(len(df)):\n",
    "    df[each]['sample'][1:] = df[each]['sample'][1:].astype(str).str[2:]\n",
    "    df[each]['channel.39'][1:] = df[each]['channel.39'][1:].astype(str).str[1:-1]\n",
    "    \n",
    "matrices = []\n",
    "for each in range(len(df)):\n",
    "    matrices.append(df[each][1:].as_matrix().astype(float))\n",
    "\n",
    "data = []\n",
    "for mat in matrices:\n",
    "#     print(mat[:, 2:].shape, mat[:, 2:].dtype)\n",
    "    data.append(mat[:, 2:])\n",
    "\n",
    "X = data[0] # initialize the stack\n",
    "X.shape, X.dtype\n",
    "for each in range(1, len(data), 1): # start, stop, step\n",
    "    X = np.vstack(tup=(X, data[each]))\n",
    "    \n",
    "Xnorm = (X - X.mean(axis=0))/ X.std(axis=0)\n",
    "# print('Xnorm.shape, Xnorm.dtype, Xnorm.mean(axis=0), Xnorm.std(axis=0):', \n",
    "#      Xnorm.shape, Xnorm.dtype, Xnorm.mean(axis=0), Xnorm.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(Xnt), len(Xnt_mb), len(Xnv): 44 2202 44\n",
      "Xnv[0].shape, Xnt_mb[0].shape: (250, 40) (250, 40)\n",
      "Xnv[0].dtype, Xnt_mb[0].dtype: float64 float64\n"
     ]
    }
   ],
   "source": [
    "# sr = 0.129 # 111933.573-111933.504 is the difference between each two samples or sampling rate\n",
    "# each_trial=30 # 30seconds=20sec+10sec\n",
    "# width = np.ceil(30/0.129)\n",
    "# num_mb = mat.shape[0] - width +1\n",
    "width, l, h, Xnt, Xnv = 250, 0, 0, [], []\n",
    "for idx in range(0, len(data), 1): # start, stop, step\n",
    "    l = h\n",
    "    h += data[idx].shape[0]\n",
    "    #     print(idx, l, h, h-width, Xnorm.shape, data[idx].shape, Xnorm[l:h].shape, width)\n",
    "    Xnt.append(Xnorm[l:h-width])\n",
    "    Xnv.append(Xnorm[h-width:h])\n",
    "#     print(Xnt[idx].shape, Xnv[idx].shape, data[idx].shape, width)    \n",
    "#     print(Xnt[idx].dtype, Xnv[idx].dtype, data[idx].dtype, width)\n",
    "    \n",
    "mb = []\n",
    "stride = 50\n",
    "for eachXnt in range(len(Xnt)):\n",
    "    num_mb = ((Xnt[eachXnt].shape[0]-width)//stride) + 1\n",
    "    # each step = (num_mb-1)*stride\n",
    "    for each in range(num_mb-1):\n",
    "        each *= stride\n",
    "        mb.append(Xnt[eachXnt][each:each+width, :])\n",
    "\n",
    "Xnt_mb = mb\n",
    "print('len(Xnt), len(Xnt_mb), len(Xnv):', len(Xnt), len(Xnt_mb), len(Xnv))\n",
    "print('Xnv[0].shape, Xnt_mb[0].shape:', Xnv[0].shape, Xnt_mb[0].shape)\n",
    "print('Xnv[0].dtype, Xnt_mb[0].dtype:', Xnv[0].dtype, Xnt_mb[0].dtype)\n",
    "# # len(mb)\n",
    "# for eachXnv, eachXnt in zip(Xnv, Xnt):\n",
    "#     print(eachXnv.shape, eachXnt.shape, eachXnv.dtype, eachXnt.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N, W, C 22 250 40\n",
      "batch_size, seq_len, n_channels 22 250 40\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "# NWC for signal: N is batch size, W is the width/sequence length, and C is the number of channels\n",
    "# NHWC for images: This is the same as signals and H stands for height\n",
    "N, W, C = len(Xnt_mb)//100, Xnt_mb[0].shape[0], Xnt_mb[0].shape[1]\n",
    "print('N, W, C', N, W, C)\n",
    "batch_size, seq_len, n_channels = N, W, C\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.001 #1e-3\n",
    "epochs = 10 # num iterations for updating model\n",
    "keep_prob = 0.90 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output and hyperpaprameters NP tensors to feed into TF tensors: NP2TF tensors for computation\n",
    "# Input and output and hyperpaprameters tensors to feed into the tensor flow framwork\n",
    "Xin = tf.placeholder(dtype=tf.float32, shape=[None, seq_len, n_channels], name=None)\n",
    "keep_prob_= tf.placeholder(dtype=tf.float32, name=None, shape=None)\n",
    "learning_rate_ = tf.placeholder(dtype=tf.float32, name=None, shape=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 250, 40) <dtype: 'float32'>\n",
      "(62, 40, 80) <dtype: 'float32_ref'>\n",
      "(?, 125, 80) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xin.shape, Xin.dtype)\n",
    "# shape = [kernel_width, input_depth, output_depth] \n",
    "W, Cin, Cout = Xin.shape[1].value//4, Xin.shape[2].value, Xin.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W1 = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W1.shape, W1.dtype)\n",
    "Xconv1 = tf.nn.conv1d(data_format='NWC', filters=W1, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=Xin)\n",
    "Xconv1 = tf.nn.relu(features=Xconv1, name=None)\n",
    "print(Xconv1.shape, Xconv1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 125, 80) <dtype: 'float32'>\n",
      "(31, 80, 160) <dtype: 'float32_ref'>\n",
      "(?, 63, 160) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv1.shape, Xconv1.dtype)\n",
    "W, Cin, Cout = Xconv1.shape[1].value//4, Xconv1.shape[2].value, Xconv1.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W2 = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W2.shape, W2.dtype)\n",
    "Xconv2 = tf.nn.conv1d(data_format='NWC', filters=W2, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=Xconv1)\n",
    "Xconv2 = tf.nn.relu(features=Xconv2, name=None)\n",
    "print(Xconv2.shape, Xconv2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 63, 160) <dtype: 'float32'>\n",
      "(15, 160, 320) <dtype: 'float32_ref'>\n",
      "(?, 32, 320) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv2.shape, Xconv2.dtype)\n",
    "W, Cin, Cout = Xconv2.shape[1].value//4, Xconv2.shape[2].value, Xconv2.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W3 = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W3.shape, W3.dtype)\n",
    "Xconv3 = tf.nn.conv1d(data_format='NWC', filters=W3, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=Xconv2)\n",
    "Xconv3 = tf.nn.relu(features=Xconv3, name=None)\n",
    "print(Xconv3.shape, Xconv3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 320) <dtype: 'float32'>\n",
      "(15, 160, 320) <dtype: 'float32_ref'>\n",
      "[22, 63, 160]\n",
      "WARNING:tensorflow:From /home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "(22, 63, 160) <dtype: 'float32'>\n",
      "(?, 63, 160) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv3.shape, Xconv3.dtype)\n",
    "W, Cin, Cout = Xconv2.shape[1].value//4, Xconv2.shape[2].value, Xconv2.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W3T = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W3T.shape, W3T.dtype)\n",
    "# output shape should be NWC equal to Xconv2\n",
    "out_shape = [batch_size, Xconv2.shape[1].value, Xconv2.shape[2].value] # NWC, only N should be set (experimental)\n",
    "print(out_shape)\n",
    "Xconv2_ = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=W3T, name=None, padding='SAME', stride=2, \n",
    "                                         value=Xconv3, output_shape=out_shape)\n",
    "Xconv2_ = tf.nn.relu(features=Xconv2_, name=None)\n",
    "print(Xconv2_.shape, Xconv2_.dtype)\n",
    "print(Xconv2.shape, Xconv2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 63, 160) <dtype: 'float32'>\n",
      "(31, 80, 160) <dtype: 'float32_ref'>\n",
      "[22, 125, 80]\n",
      "(22, 125, 80) <dtype: 'float32'>\n",
      "(?, 125, 80) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv2_.shape, Xconv2_.dtype)\n",
    "W, Cin, Cout = Xconv1.shape[1].value//4, Xconv1.shape[2].value, Xconv1.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W2T = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W2T.shape, W2T.dtype)\n",
    "# output shape should be NWC equal to Xconv1\n",
    "out_shape = [batch_size, Xconv1.shape[1].value, Xconv1.shape[2].value] # NWC, only N should be set (experimental)\n",
    "print(out_shape)\n",
    "Xconv1_ = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=W2T, name=None, padding='SAME', stride=2, \n",
    "                                         value=Xconv2_, output_shape=out_shape)\n",
    "Xconv1_ = tf.nn.relu(features=Xconv1_, name=None)\n",
    "print(Xconv1_.shape, Xconv1_.dtype)\n",
    "print(Xconv1.shape, Xconv1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 125, 80) <dtype: 'float32'>\n",
      "(62, 40, 80) <dtype: 'float32_ref'>\n",
      "[22, 250, 40]\n",
      "(22, 250, 40) <dtype: 'float32'>\n",
      "(?, 250, 40) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv1_.shape, Xconv1_.dtype)\n",
    "W, Cin, Cout = Xin.shape[1].value//4, Xin.shape[2].value, Xin.shape[2].value*2 \n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W1T = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W1T.shape, W1T.dtype)\n",
    "# output shape should be NWC equal to Xin\n",
    "out_shape = [batch_size, Xin.shape[1].value, Xin.shape[2].value] # NWC, only N should be set (experimental)\n",
    "print(out_shape)\n",
    "Xout = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=W1T, name=None, padding='SAME', stride=2, \n",
    "                                         value=Xconv1_, output_shape=out_shape)\n",
    "# Xin_ = tf.nn.relu(features=Xin_, name=None)\n",
    "print(Xout.shape, Xout.dtype)\n",
    "print(Xin.shape, Xin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 250, 40) <dtype: 'float32'>\n",
      "() <dtype: 'float32'>\n",
      "optimizer name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_Variable/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_1/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_2/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_3/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_4/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_5/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "# cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost_tensor = tf.nn.sigmoid_cross_entropy_with_logits(labels=Xin, logits=Xout, name=None)\n",
    "print(cost_tensor.shape, cost_tensor.dtype)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "# The cost has to be positive since it is a distance between two vectors in the hyperspace\n",
    "# Eucleadian dist or length, angle or entropy are all these kinds of distance\n",
    "cost = tf.abs(name=None, x=cost)\n",
    "print(cost.shape, cost.dtype)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "print('optimizer', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, batch_size):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X = X[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n"
     ]
    }
   ],
   "source": [
    "# get_batches(X=Xnt_mb, batch_size=batch_size)\n",
    "# Loop over batches\n",
    "for each in get_batches(X=Xnt_mb, batch_size=batch_size):\n",
    "    print(len(each), each[0].shape, each[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 (250, 40) float64\n",
      "22 (250, 40) float64\n"
     ]
    }
   ],
   "source": [
    "for each in get_batches(X=Xnv, batch_size=batch_size):\n",
    "    print(len(each), each[0].shape, each[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Train loss: 240055872.000000 Valid loss: 279964864.000000\n",
      "Epoch: 2/10 Train loss: 138505376.000000 Valid loss: 160027584.000000\n",
      "Epoch: 3/10 Train loss: 100330072.000000 Valid loss: 116028952.000000\n",
      "Epoch: 4/10 Train loss: 81924712.000000 Valid loss: 95336728.000000\n",
      "Epoch: 5/10 Train loss: 68981376.000000 Valid loss: 78715264.000000\n",
      "Epoch: 6/10 Train loss: 59525788.000000 Valid loss: 67471280.000000\n",
      "Epoch: 7/10 Train loss: 52373224.000000 Valid loss: 59546268.000000\n",
      "Epoch: 8/10 Train loss: 47348000.000000 Valid loss: 54902176.000000\n",
      "Epoch: 9/10 Train loss: 43242848.000000 Valid loss: 49328020.000000\n",
      "Epoch: 10/10 Train loss: 39359896.000000 Valid loss: 44633184.000000\n"
     ]
    }
   ],
   "source": [
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "train_loss, valid_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # my assumption is the local variables are the parameters and hyperparameters\n",
    "    # the globala variables are the one needed/required by the seession/graph to run\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Loop over batches\n",
    "        for X_train_norm_batch in get_batches(X=Xnt_mb, batch_size=batch_size):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {Xin : X_train_norm_batch, keep_prob_ : keep_prob, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ = sess.run(fetches=[cost, optimizer], feed_dict = feed)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "            ################## Validation\n",
    "            loss_batch = []    \n",
    "            # Loop over batches\n",
    "            for X_valid_norm_batch in get_batches(X=Xnv, batch_size=batch_size):\n",
    "\n",
    "                # Feed dictionary\n",
    "                feed = {Xin : X_valid_norm_batch, keep_prob_ : 1.0} \n",
    "                # no learning is needed therefore no learning rate is needed.\n",
    "\n",
    "                # Loss\n",
    "                loss = sess.run(fetches=[cost], feed_dict = feed)\n",
    "                # no learning is needed therefore no learning rate is needed.\n",
    "                # Therefore no optimization approach or backprop is needed either.\n",
    "                loss_batch.append(loss)\n",
    "\n",
    "            # Store\n",
    "            valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "        # Print info for every iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(valid_loss)))\n",
    "    \n",
    "    # At the end of training and validation\n",
    "    saver.save(sess,\"checkpoints/cnn-fnirs-autoencoder.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "%matplotlib inline\n",
    "\n",
    "mplot.plot(train_loss, label='har train_loss')\n",
    "mplot.plot(valid_loss, label='har valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize the saver which has already been initialized.\n",
    "test_loss = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    ################## Test\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for X_test_norm_batch, _ in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        # No learning/training is needed at this step\n",
    "        feed = {Xin: X_test_norm_batch, keep_prob_ : 1.0}\n",
    "\n",
    "        # Loss\n",
    "        # Only the computation of the cost is needed\n",
    "        loss = sess.run(fetches=[cost], feed_dict = feed)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Store\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.16675734494 0.145946681175 0.134113166485 0.174918389554 0.186887921654 0.191376496192 0.0\n"
     ]
    }
   ],
   "source": [
    "# test and train\n",
    "# input and output\n",
    "X_train_all, Y_train_all, list_ch_train = read_data(data_path=\"../data/har-data/\", split=\"train\") # train\n",
    "X_test, Y_test, list_ch_test = read_data(data_path=\"../data/har-data/\", split=\"test\") # test\n",
    "assert list_ch_train == list_ch_test, \"Mistmatch in channels!\"\n",
    "assert Y_train_all.max(axis=0) == Y_test.max(axis=0)\n",
    "print(np.mean(Y_train_all==0), np.mean(Y_train_all==1), np.mean(Y_train_all==2), np.mean(Y_train_all==3),\n",
    "     np.mean(Y_train_all==4), np.mean(Y_train_all==5), np.mean(Y_train_all==6), np.mean(Y_train_all==7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5146, 128, 9) (2206, 128, 9) (5146,) (2206,)\n",
      "float64 float64 int64 int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_all,\n",
    "                                                      Y_train_all,\n",
    "                                                      stratify = Y_train_all,\n",
    "                                                      random_state = 123,\n",
    "                                                      test_size=0.30)\n",
    "print(X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape)\n",
    "print(X_train.dtype, X_valid.dtype, Y_train.dtype, Y_valid.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5146, 128, 9) float64 (2206, 128, 9) float64 (2947, 128, 9) float64\n"
     ]
    }
   ],
   "source": [
    "# Standardize/normalize train and test\n",
    "X_train_norm = (X_train - X_train.mean(axis=0, keepdims=3))/ X_train.std(axis=0, keepdims=3)\n",
    "X_valid_norm = (X_valid - X_train.mean(axis=0, keepdims=3))/ X_train.std(axis=0, keepdims=3)\n",
    "X_test_norm = (X_test - X_train.mean(axis=0, keepdims=3))/ X_train.std(axis=0, keepdims=3)\n",
    "\n",
    "print(X_train_norm.shape, X_train_norm.dtype,\n",
    "      X_valid_norm.shape, X_valid_norm.dtype,\n",
    "      X_test_norm.shape, X_test_norm.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5146, 6) (2206, 6) (5146, 128, 9) (2206, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "Y_train_onehot = one_hot(labels=Y_train.reshape(-1), n_class=6)\n",
    "Y_valid_onehot = one_hot(labels=Y_valid.reshape(-1), n_class=6)\n",
    "Y_test_onehot = one_hot(labels=Y_test.reshape(-1), n_class=6)\n",
    "\n",
    "print(Y_train_onehot.shape, Y_valid_onehot.shape, \n",
    " X_train_norm.shape, X_valid_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len, n_channels 51 128 9\n",
      "n_classes 6\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "# Input data\n",
    "batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = int(Y_train_all.max(axis=0))\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.0001 #1e-4\n",
    "epochs = 100 # num iterations for updating model\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Buffering/ placeholders to transfer the data from py to tf\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs_')\n",
    "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_')\n",
    "keep_prob_ = tf.placeholder(tf.float32, name = 'keep_prob_')\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate_')# Construct the LSTM inputs and LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 26) (?, 64, 26)\n",
      "(?, 64, 36) (?, 32, 36)\n",
      "(?, 32, 72) (?, 16, 72)\n",
      "(?, 16, 144) (?, 8, 144)\n",
      "(?, 8, 144) (?, 1152) (?, 6)\n"
     ]
    }
   ],
   "source": [
    "# with graph.as_default():\n",
    "# (5146, 128, 9) (2206, 128, 9) (5146,) (2206,)\n",
    "# (batch, 128, 9) --> (batch, 64, 18)\n",
    "conv1 = tf.layers.conv1d(inputs=inputs_, filters=26, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_1 = tf.nn.dropout(max_pool_1, keep_prob=keep_prob_)\n",
    "print(conv1.shape, max_pool_1.shape)\n",
    "\n",
    "# (batch, 64, 18) --> (batch, 32, 36)\n",
    "conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=36, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_2 = tf.nn.dropout(max_pool_2, keep_prob=keep_prob_)\n",
    "print(conv2.shape, max_pool_2.shape)\n",
    "\n",
    "# (batch, 32, 36) --> (batch, 16, 72)\n",
    "conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=72, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_3 = tf.nn.dropout(max_pool_3, keep_prob=keep_prob_)\n",
    "print(conv3.shape, max_pool_3.shape)\n",
    "\n",
    "# (batch, 16, 72) --> (batch, 8, 144)\n",
    "conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=144, kernel_size=2, strides=1, \n",
    "                         padding='same', activation = tf.nn.relu)\n",
    "max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_4 = tf.nn.dropout(max_pool_4, keep_prob=keep_prob_)\n",
    "print(conv4.shape, max_pool_4.shape)\n",
    "\n",
    "# Flatten and add dropout + predicted output\n",
    "flat = tf.reshape(max_pool_4, (-1, 8*144))\n",
    "flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "logits = tf.layers.dense(flat, n_classes)\n",
    "print(max_pool_4.shape, flat.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool) Tensor(\"accuracy:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "print(correct_pred, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train loss: 1.641564 Valid loss: 1.605675 Train acc: 0.223529 Valid acc: 0.254182\n",
      "Epoch: 2/100 Train loss: 1.485839 Valid loss: 1.447329 Train acc: 0.348628 Valid acc: 0.386938\n",
      "Epoch: 3/100 Train loss: 1.335508 Valid loss: 1.292867 Train acc: 0.449412 Valid acc: 0.499991\n",
      "Epoch: 4/100 Train loss: 1.182494 Valid loss: 1.136548 Train acc: 0.525147 Valid acc: 0.582839\n",
      "Epoch: 5/100 Train loss: 1.044514 Valid loss: 0.996981 Train acc: 0.585922 Valid acc: 0.642026\n",
      "Epoch: 6/100 Train loss: 0.931177 Valid loss: 0.884307 Train acc: 0.633301 Valid acc: 0.686240\n",
      "Epoch: 7/100 Train loss: 0.839905 Valid loss: 0.794558 Train acc: 0.670980 Valid acc: 0.719571\n",
      "Epoch: 8/100 Train loss: 0.766116 Valid loss: 0.722345 Train acc: 0.701250 Valid acc: 0.745851\n",
      "Epoch: 9/100 Train loss: 0.705446 Valid loss: 0.663075 Train acc: 0.725926 Valid acc: 0.767015\n",
      "Epoch: 10/100 Train loss: 0.654369 Valid loss: 0.613831 Train acc: 0.746608 Valid acc: 0.784370\n",
      "Epoch: 11/100 Train loss: 0.610830 Valid loss: 0.572251 Train acc: 0.764064 Valid acc: 0.799048\n",
      "Epoch: 12/100 Train loss: 0.573711 Valid loss: 0.536706 Train acc: 0.778889 Valid acc: 0.811502\n",
      "Epoch: 13/100 Train loss: 0.541169 Valid loss: 0.505930 Train acc: 0.791961 Valid acc: 0.822246\n",
      "Epoch: 14/100 Train loss: 0.512795 Valid loss: 0.479154 Train acc: 0.803095 Valid acc: 0.831601\n",
      "Epoch: 15/100 Train loss: 0.487918 Valid loss: 0.455584 Train acc: 0.812889 Valid acc: 0.839753\n",
      "Epoch: 16/100 Train loss: 0.465807 Valid loss: 0.434670 Train acc: 0.821434 Valid acc: 0.846992\n",
      "Epoch: 17/100 Train loss: 0.445856 Valid loss: 0.416042 Train acc: 0.829100 Valid acc: 0.853384\n",
      "Epoch: 18/100 Train loss: 0.428050 Valid loss: 0.399353 Train acc: 0.835937 Valid acc: 0.859075\n",
      "Epoch: 19/100 Train loss: 0.411934 Valid loss: 0.384343 Train acc: 0.842085 Valid acc: 0.864170\n",
      "Epoch: 20/100 Train loss: 0.397357 Valid loss: 0.370705 Train acc: 0.847667 Valid acc: 0.868776\n",
      "Epoch: 21/100 Train loss: 0.384125 Valid loss: 0.358289 Train acc: 0.852680 Valid acc: 0.872970\n",
      "Epoch: 22/100 Train loss: 0.371818 Valid loss: 0.346920 Train acc: 0.857335 Valid acc: 0.876807\n",
      "Epoch: 23/100 Train loss: 0.360528 Valid loss: 0.336494 Train acc: 0.861645 Valid acc: 0.880310\n",
      "Epoch: 24/100 Train loss: 0.350164 Valid loss: 0.326881 Train acc: 0.865572 Valid acc: 0.883536\n",
      "Epoch: 25/100 Train loss: 0.340470 Valid loss: 0.317982 Train acc: 0.869145 Valid acc: 0.886500\n",
      "Epoch: 26/100 Train loss: 0.331506 Valid loss: 0.309732 Train acc: 0.872459 Valid acc: 0.889244\n",
      "Epoch: 27/100 Train loss: 0.323149 Valid loss: 0.302076 Train acc: 0.875621 Valid acc: 0.891779\n",
      "Epoch: 28/100 Train loss: 0.315360 Valid loss: 0.294926 Train acc: 0.878558 Valid acc: 0.894128\n",
      "Epoch: 29/100 Train loss: 0.308002 Valid loss: 0.288245 Train acc: 0.881237 Valid acc: 0.896319\n",
      "Epoch: 30/100 Train loss: 0.301148 Valid loss: 0.281980 Train acc: 0.883758 Valid acc: 0.898371\n",
      "Epoch: 31/100 Train loss: 0.294662 Valid loss: 0.276113 Train acc: 0.886173 Valid acc: 0.900292\n",
      "Epoch: 32/100 Train loss: 0.288556 Valid loss: 0.270578 Train acc: 0.888413 Valid acc: 0.902082\n",
      "Epoch: 33/100 Train loss: 0.282822 Valid loss: 0.265357 Train acc: 0.890523 Valid acc: 0.903756\n",
      "Epoch: 34/100 Train loss: 0.277396 Valid loss: 0.260425 Train acc: 0.892480 Valid acc: 0.905335\n",
      "Epoch: 35/100 Train loss: 0.272273 Valid loss: 0.255759 Train acc: 0.894319 Valid acc: 0.906839\n",
      "Epoch: 36/100 Train loss: 0.267402 Valid loss: 0.251325 Train acc: 0.896160 Valid acc: 0.908250\n",
      "Epoch: 37/100 Train loss: 0.262715 Valid loss: 0.247118 Train acc: 0.897901 Valid acc: 0.909575\n",
      "Epoch: 38/100 Train loss: 0.258227 Valid loss: 0.243107 Train acc: 0.899598 Valid acc: 0.910842\n",
      "Epoch: 39/100 Train loss: 0.253945 Valid loss: 0.239286 Train acc: 0.901197 Valid acc: 0.912048\n",
      "Epoch: 40/100 Train loss: 0.249886 Valid loss: 0.235643 Train acc: 0.902691 Valid acc: 0.913197\n",
      "Epoch: 41/100 Train loss: 0.246018 Valid loss: 0.232165 Train acc: 0.904094 Valid acc: 0.914296\n",
      "Epoch: 42/100 Train loss: 0.242303 Valid loss: 0.228833 Train acc: 0.905430 Valid acc: 0.915352\n",
      "Epoch: 43/100 Train loss: 0.238729 Valid loss: 0.225647 Train acc: 0.906712 Valid acc: 0.916357\n",
      "Epoch: 44/100 Train loss: 0.235290 Valid loss: 0.222580 Train acc: 0.907928 Valid acc: 0.917323\n",
      "Epoch: 45/100 Train loss: 0.232018 Valid loss: 0.219638 Train acc: 0.909116 Valid acc: 0.918247\n",
      "Epoch: 46/100 Train loss: 0.228869 Valid loss: 0.216803 Train acc: 0.910235 Valid acc: 0.919132\n",
      "Epoch: 47/100 Train loss: 0.225770 Valid loss: 0.214074 Train acc: 0.911377 Valid acc: 0.919988\n",
      "Epoch: 48/100 Train loss: 0.222846 Valid loss: 0.211448 Train acc: 0.912431 Valid acc: 0.920810\n",
      "Epoch: 49/100 Train loss: 0.220034 Valid loss: 0.208908 Train acc: 0.913445 Valid acc: 0.921600\n",
      "Epoch: 50/100 Train loss: 0.217292 Valid loss: 0.206456 Train acc: 0.914447 Valid acc: 0.922366\n",
      "Epoch: 51/100 Train loss: 0.214643 Valid loss: 0.204087 Train acc: 0.915390 Valid acc: 0.923100\n",
      "Epoch: 52/100 Train loss: 0.212072 Valid loss: 0.201791 Train acc: 0.916286 Valid acc: 0.923809\n",
      "Epoch: 53/100 Train loss: 0.209564 Valid loss: 0.199568 Train acc: 0.917185 Valid acc: 0.924499\n",
      "Epoch: 54/100 Train loss: 0.207158 Valid loss: 0.197408 Train acc: 0.918083 Valid acc: 0.925166\n",
      "Epoch: 55/100 Train loss: 0.204816 Valid loss: 0.195309 Train acc: 0.918945 Valid acc: 0.925827\n",
      "Epoch: 56/100 Train loss: 0.202506 Valid loss: 0.193264 Train acc: 0.919797 Valid acc: 0.926472\n",
      "Epoch: 57/100 Train loss: 0.200287 Valid loss: 0.191283 Train acc: 0.920619 Valid acc: 0.927106\n",
      "Epoch: 58/100 Train loss: 0.198114 Valid loss: 0.189351 Train acc: 0.921420 Valid acc: 0.927727\n",
      "Epoch: 59/100 Train loss: 0.195989 Valid loss: 0.187467 Train acc: 0.922187 Valid acc: 0.928337\n",
      "Epoch: 60/100 Train loss: 0.193937 Valid loss: 0.185639 Train acc: 0.922895 Valid acc: 0.928930\n",
      "Epoch: 61/100 Train loss: 0.191931 Valid loss: 0.183870 Train acc: 0.923603 Valid acc: 0.929504\n",
      "Epoch: 62/100 Train loss: 0.189986 Valid loss: 0.182141 Train acc: 0.924314 Valid acc: 0.930069\n",
      "Epoch: 63/100 Train loss: 0.188060 Valid loss: 0.180450 Train acc: 0.924998 Valid acc: 0.930622\n",
      "Epoch: 64/100 Train loss: 0.186200 Valid loss: 0.178796 Train acc: 0.925668 Valid acc: 0.931162\n",
      "Epoch: 65/100 Train loss: 0.184411 Valid loss: 0.177186 Train acc: 0.926299 Valid acc: 0.931686\n",
      "Epoch: 66/100 Train loss: 0.182630 Valid loss: 0.175621 Train acc: 0.926934 Valid acc: 0.932199\n",
      "Epoch: 67/100 Train loss: 0.180912 Valid loss: 0.174088 Train acc: 0.927548 Valid acc: 0.932701\n",
      "Epoch: 68/100 Train loss: 0.179247 Valid loss: 0.172590 Train acc: 0.928172 Valid acc: 0.933193\n",
      "Epoch: 69/100 Train loss: 0.177585 Valid loss: 0.171123 Train acc: 0.928741 Valid acc: 0.933676\n",
      "Epoch: 70/100 Train loss: 0.175963 Valid loss: 0.169689 Train acc: 0.929328 Valid acc: 0.934146\n",
      "Epoch: 71/100 Train loss: 0.174380 Valid loss: 0.168289 Train acc: 0.929903 Valid acc: 0.934608\n",
      "Epoch: 72/100 Train loss: 0.172847 Valid loss: 0.166913 Train acc: 0.930447 Valid acc: 0.935062\n",
      "Epoch: 73/100 Train loss: 0.171335 Valid loss: 0.165570 Train acc: 0.930996 Valid acc: 0.935510\n",
      "Epoch: 74/100 Train loss: 0.169845 Valid loss: 0.164249 Train acc: 0.931539 Valid acc: 0.935950\n",
      "Epoch: 75/100 Train loss: 0.168379 Valid loss: 0.162951 Train acc: 0.932055 Valid acc: 0.936386\n",
      "Epoch: 76/100 Train loss: 0.166965 Valid loss: 0.161678 Train acc: 0.932570 Valid acc: 0.936814\n",
      "Epoch: 77/100 Train loss: 0.165566 Valid loss: 0.160433 Train acc: 0.933086 Valid acc: 0.937237\n",
      "Epoch: 78/100 Train loss: 0.164199 Valid loss: 0.159212 Train acc: 0.933585 Valid acc: 0.937648\n",
      "Epoch: 79/100 Train loss: 0.162834 Valid loss: 0.158015 Train acc: 0.934095 Valid acc: 0.938052\n",
      "Epoch: 80/100 Train loss: 0.161523 Valid loss: 0.156839 Train acc: 0.934559 Valid acc: 0.938449\n",
      "Epoch: 81/100 Train loss: 0.160222 Valid loss: 0.155691 Train acc: 0.935038 Valid acc: 0.938842\n",
      "Epoch: 82/100 Train loss: 0.158945 Valid loss: 0.154560 Train acc: 0.935485 Valid acc: 0.939226\n",
      "Epoch: 83/100 Train loss: 0.157698 Valid loss: 0.153450 Train acc: 0.935941 Valid acc: 0.939603\n",
      "Epoch: 84/100 Train loss: 0.156482 Valid loss: 0.152360 Train acc: 0.936370 Valid acc: 0.939978\n",
      "Epoch: 85/100 Train loss: 0.155280 Valid loss: 0.151294 Train acc: 0.936828 Valid acc: 0.940352\n",
      "Epoch: 86/100 Train loss: 0.154091 Valid loss: 0.150245 Train acc: 0.937255 Valid acc: 0.940717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87/100 Train loss: 0.152936 Valid loss: 0.149210 Train acc: 0.937674 Valid acc: 0.941077\n",
      "Epoch: 88/100 Train loss: 0.151802 Valid loss: 0.148207 Train acc: 0.938091 Valid acc: 0.941431\n",
      "Epoch: 89/100 Train loss: 0.150704 Valid loss: 0.147215 Train acc: 0.938500 Valid acc: 0.941777\n",
      "Epoch: 90/100 Train loss: 0.149604 Valid loss: 0.146238 Train acc: 0.938904 Valid acc: 0.942118\n",
      "Epoch: 91/100 Train loss: 0.148534 Valid loss: 0.145286 Train acc: 0.939298 Valid acc: 0.942452\n",
      "Epoch: 92/100 Train loss: 0.147475 Valid loss: 0.144343 Train acc: 0.939678 Valid acc: 0.942781\n",
      "Epoch: 93/100 Train loss: 0.146429 Valid loss: 0.143421 Train acc: 0.940082 Valid acc: 0.943101\n",
      "Epoch: 94/100 Train loss: 0.145416 Valid loss: 0.142509 Train acc: 0.940467 Valid acc: 0.943422\n",
      "Epoch: 95/100 Train loss: 0.144392 Valid loss: 0.141619 Train acc: 0.940855 Valid acc: 0.943736\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "\n",
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x, y in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : keep_prob, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            ################## Validation\n",
    "            acc_batch = []\n",
    "            loss_batch = []    \n",
    "            # Loop over batches\n",
    "            for x, y in get_batches(X_valid_norm, Y_valid_onehot, batch_size):\n",
    "\n",
    "                # Feed dictionary\n",
    "                feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "                # Loss\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "                acc_batch.append(acc)\n",
    "                loss_batch.append(loss)\n",
    "\n",
    "            # Store\n",
    "            valid_acc.append(np.mean(acc_batch))\n",
    "            valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "        # Print info for every iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(valid_loss)),\n",
    "              \"Train acc: {:6f}\".format(np.mean(train_acc)),\n",
    "              \"Valid acc: {:.6f}\".format(np.mean(valid_acc)))\n",
    "                \n",
    "    saver.save(sess,\"checkpoints-cnn/har.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_loss, label='train_loss')\n",
    "mplot.plot(valid_loss, label='valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplot.plot(train_acc, label='train_acc')\n",
    "mplot.plot(valid_acc, label='valid_acc')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-cnn'))\n",
    "    \n",
    "    ################## Test\n",
    "    acc_batch = []\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for x, y in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "        # Loss\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Store\n",
    "    test_acc.append(np.mean(acc_batch))\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)),\n",
    "          \"Test acc: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

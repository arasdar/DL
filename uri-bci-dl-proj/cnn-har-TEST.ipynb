{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "# test and train read\n",
    "X_train_valid, Y_train_valid, list_ch_train_valid = read_data(data_path=\"../../datasets/har/har-data/\", split=\"train\")\n",
    "X_test, Y_test, list_ch_test = read_data(data_path=\"../../datasets/har/har-data/\", split=\"test\")\n",
    "\n",
    "assert list_ch_train_valid == list_ch_test, \"Mistmatch in channels!\"\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "\n",
    "# print(np.mean(Y_train_valid==0), np.mean(Y_train_valid==1), np.mean(Y_train_valid==2), \n",
    "#       np.mean(Y_train_valid==3), np.mean(Y_train_valid==4), np.mean(Y_train_valid==5),\n",
    "#       np.mean(Y_train_valid==6), np.mean(Y_train_valid==7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing input and output data\n",
    "# from utilities import *\n",
    "\n",
    "# Normalizing/standardizing the input data features\n",
    "X_train_valid_norm, X_test_norm = standardize(test=X_test, train=X_train_valid)\n",
    "\n",
    "# # Onehot encoding/vectorizing the output data labels\n",
    "# print(np.mean((Y_train_valid).reshape(-1)==0), np.mean((Y_train_valid).reshape(-1)==1),\n",
    "#      np.mean((Y_train_valid).reshape(-1)==2), np.mean((Y_train_valid).reshape(-1)==3),\n",
    "#      np.mean((Y_train_valid).reshape(-1)==4), np.mean((Y_train_valid).reshape(-1)==5),\n",
    "#      np.mean((Y_train_valid).reshape(-1)==6), np.mean((Y_train_valid).reshape(-1)==7))\n",
    "\n",
    "Y_train_valid_onehot = one_hot(labels=Y_train_valid.reshape(-1), n_class=6) \n",
    "Y_test_onehot = one_hot(labels=Y_test.reshape(-1), n_class=6) \n",
    "\n",
    "# print(Y_train_valid_onehot.shape, Y_train_valid_onehot.dtype, \n",
    "#       Y_test_onehot.shape, Y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_norm, X_valid_norm, Y_train_onehot, Y_valid_onehot = train_test_split(X_train_valid_norm, \n",
    "                                                                              Y_train_valid_onehot,\n",
    "                                                                              test_size=0.30)\n",
    "\n",
    "# print(X_train_norm.shape, X_valid_norm.shape, Y_train_onehot.shape, Y_valid_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len, n_channels 51 128 9\n",
      "n_classes 6\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = Y_train_valid.max(axis=0)\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.0001 #1e-4\n",
    "epochs = 1000 # num iterations for updating model\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the data from python/numpy to tensorflow framework\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs_')\n",
    "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_')\n",
    "keep_prob_ = tf.placeholder(tf.float32, name = 'keep_prob_')\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate_')# Construct the LSTM inputs and LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_.shape, conv1.shape, max_pool_1.shape (?, 128, 9) (?, 128, 18) (?, 64, 18)\n",
      "max_pool_1.shape, conv2.shape, max_pool_2.shape (?, 64, 18) (?, 64, 36) (?, 32, 36)\n",
      "max_pool_2.shape, conv3.shape, max_pool_3.shape (?, 32, 36) (?, 32, 72) (?, 16, 72)\n",
      "max_pool_3.shape, conv4.shape, max_pool_4.shape (?, 16, 72) (?, 16, 144) (?, 8, 144)\n"
     ]
    }
   ],
   "source": [
    "# batch_size, seq_len, n_channels: 51 128 9; n_classes: 6\n",
    "# (batch, 128, 9) --> (batch, 256, 18)\n",
    "# conv same\n",
    "# pool same: (128-2+0)/2 + 1 = (126/2)+1 = 63 + 1=64\n",
    "conv1 = tf.layers.conv1d(inputs=inputs_, filters=18, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_1 = tf.nn.dropout(max_pool_1, keep_prob=keep_prob_)\n",
    "print('inputs_.shape, conv1.shape, max_pool_1.shape', inputs_.shape, conv1.shape, max_pool_1.shape)\n",
    "\n",
    "# (batch, 64, 18) --> (batch, 32, 36)\n",
    "# conv same\n",
    "# pool same: (64-2+0)/2 + 1 = (62/2)+1 = 31 + 1=32\n",
    "conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=36, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_2 = tf.nn.dropout(max_pool_2, keep_prob=keep_prob_)\n",
    "print('max_pool_1.shape, conv2.shape, max_pool_2.shape', max_pool_1.shape, conv2.shape, max_pool_2.shape)\n",
    "\n",
    "# (batch, 32, 36) --> (batch, 16, 72)\n",
    "# conv same\n",
    "# pool same: (32-2+0)/2 + 1 = (30/2)+1 = 15 + 1=16\n",
    "conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=72, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_3 = tf.nn.dropout(max_pool_3, keep_prob=keep_prob_)\n",
    "print('max_pool_2.shape, conv3.shape, max_pool_3.shape', max_pool_2.shape, conv3.shape, max_pool_3.shape)\n",
    "\n",
    "# (batch, 16, 72) --> (batch, 8, 144)\n",
    "# conv same\n",
    "# pool same: (16-2+0)/2 + 1 = (14/2)+1 = 7 + 1=8\n",
    "conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=144, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_4 = tf.nn.dropout(max_pool_4, keep_prob=keep_prob_)\n",
    "print('max_pool_3.shape, conv4.shape, max_pool_4.shape', max_pool_3.shape, conv4.shape, max_pool_4.shape)\n",
    "\n",
    "# # Flatten and add dropout + predicted output\n",
    "# flat = tf.reshape(max_pool_4, (-1, 8*144))\n",
    "# flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "# logits = tf.layers.dense(flat, n_classes)\n",
    "# print('max_pool_4.shape, flat.shape, logits.shape', max_pool_4.shape, flat.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_pool_4.shape, flat.shape, logits.shape (?, 8, 144) (?, 1152) (?, 6)\n"
     ]
    }
   ],
   "source": [
    "# print(max_pool_4.shape, max_pool_4.dtype)\n",
    "# # max_pool_4_vec = tf.reshape(name=None, shape=[None, 8, 144], tensor=max_pool_4)\n",
    "# flat = tf.layers.flatten(inputs=max_pool_4, name=None)\n",
    "# print(flat.shape, max_pool_4.shape[0], max_pool_4.shape[1]*max_pool_4.shape[2], n_classes.shape, n_classes.dtype)\n",
    "\n",
    "# # We only need the last output tensor to pass into a classifier\n",
    "# # logits = tf.layers.dense(flat, n_classes, name='logits', reuse=True)\n",
    "# # print(logits.shape, logits.dtype, n_classes.shape)\n",
    "\n",
    "# Flatten and add dropout + predicted output\n",
    "flat = tf.reshape(tensor=max_pool_4, shape=(-1, 8*144))\n",
    "# flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "logits = tf.layers.dense(inputs=flat, units=n_classes)\n",
    "print('max_pool_4.shape, flat.shape, logits.shape', max_pool_4.shape, flat.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_tensor, cost Tensor(\"softmax_cross_entropy_with_logits_sg_2/Reshape_2:0\", shape=(?,), dtype=float32) Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "optimizer name: \"Adam_2\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam_2/update_conv1d_8/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_8/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_9/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_9/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_10/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_10/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_11/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_11/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/update_dense_2/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_dense_2/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/Assign\"\n",
      "input: \"^Adam_2/Assign_1\"\n",
      "\n",
      "correct_pred, accuracy Tensor(\"Equal_2:0\", shape=(?,), dtype=bool) Tensor(\"accuracy_2:0\", shape=(), dtype=float32)\n",
      "confusion_matrix Tensor(\"confusion_matrix_2/SparseTensorDenseAdd:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "print('cost_tensor, cost', cost_tensor, cost)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "print('optimizer', optimizer)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "print('correct_pred, accuracy', correct_pred, accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = tf.confusion_matrix(predictions=tf.argmax(logits, 1),\n",
    "                                       labels=tf.argmax(labels_, 1))\n",
    "print('confusion_matrix', confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000 Train loss: 1.548371\n",
      "Epoch: 2/1000 Train loss: 1.322101\n",
      "Epoch: 3/1000 Train loss: 1.126480\n",
      "Epoch: 4/1000 Train loss: 0.964911\n",
      "Epoch: 5/1000 Train loss: 0.842287\n",
      "Epoch: 6/1000 Train loss: 0.748052\n",
      "Epoch: 7/1000 Train loss: 0.673803\n",
      "Epoch: 8/1000 Train loss: 0.613995\n",
      "Epoch: 9/1000 Train loss: 0.564960\n",
      "Epoch: 10/1000 Train loss: 0.524171\n",
      "Epoch: 11/1000 Train loss: 0.489799\n",
      "Epoch: 12/1000 Train loss: 0.460481\n",
      "Epoch: 13/1000 Train loss: 0.435198\n",
      "Epoch: 14/1000 Train loss: 0.413179\n",
      "Epoch: 15/1000 Train loss: 0.393831\n",
      "Epoch: 16/1000 Train loss: 0.376692\n",
      "Epoch: 17/1000 Train loss: 0.361400\n",
      "Epoch: 18/1000 Train loss: 0.347668\n",
      "Epoch: 19/1000 Train loss: 0.335267\n",
      "Epoch: 20/1000 Train loss: 0.324010\n",
      "Epoch: 21/1000 Train loss: 0.313743\n",
      "Epoch: 22/1000 Train loss: 0.304339\n",
      "Epoch: 23/1000 Train loss: 0.295690\n",
      "Epoch: 24/1000 Train loss: 0.287707\n",
      "Epoch: 25/1000 Train loss: 0.280314\n",
      "Epoch: 26/1000 Train loss: 0.273445\n",
      "Epoch: 27/1000 Train loss: 0.267044\n",
      "Epoch: 28/1000 Train loss: 0.261063\n",
      "Epoch: 29/1000 Train loss: 0.255460\n",
      "Epoch: 30/1000 Train loss: 0.250196\n",
      "Epoch: 31/1000 Train loss: 0.245242\n",
      "Epoch: 32/1000 Train loss: 0.240567\n",
      "Epoch: 33/1000 Train loss: 0.236147\n",
      "Epoch: 34/1000 Train loss: 0.231959\n",
      "Epoch: 35/1000 Train loss: 0.227984\n",
      "Epoch: 36/1000 Train loss: 0.224203\n",
      "Epoch: 37/1000 Train loss: 0.220601\n",
      "Epoch: 38/1000 Train loss: 0.217163\n",
      "Epoch: 39/1000 Train loss: 0.213878\n",
      "Epoch: 40/1000 Train loss: 0.210734\n",
      "Epoch: 41/1000 Train loss: 0.207721\n",
      "Epoch: 42/1000 Train loss: 0.204829\n",
      "Epoch: 43/1000 Train loss: 0.202049\n",
      "Epoch: 44/1000 Train loss: 0.199375\n",
      "Epoch: 45/1000 Train loss: 0.196799\n",
      "Epoch: 46/1000 Train loss: 0.194315\n",
      "Epoch: 47/1000 Train loss: 0.191916\n",
      "Epoch: 48/1000 Train loss: 0.189597\n",
      "Epoch: 49/1000 Train loss: 0.187353\n",
      "Epoch: 50/1000 Train loss: 0.185179\n",
      "Epoch: 51/1000 Train loss: 0.183072\n",
      "Epoch: 52/1000 Train loss: 0.181027\n",
      "Epoch: 53/1000 Train loss: 0.179041\n",
      "Epoch: 54/1000 Train loss: 0.177111\n",
      "Epoch: 55/1000 Train loss: 0.175233\n",
      "Epoch: 56/1000 Train loss: 0.173404\n",
      "Epoch: 57/1000 Train loss: 0.171623\n",
      "Epoch: 58/1000 Train loss: 0.169886\n",
      "Epoch: 59/1000 Train loss: 0.168193\n",
      "Epoch: 60/1000 Train loss: 0.166540\n",
      "Epoch: 61/1000 Train loss: 0.164926\n",
      "Epoch: 62/1000 Train loss: 0.163350\n",
      "Epoch: 63/1000 Train loss: 0.161809\n",
      "Epoch: 64/1000 Train loss: 0.160302\n",
      "Epoch: 65/1000 Train loss: 0.158827\n",
      "Epoch: 66/1000 Train loss: 0.157384\n",
      "Epoch: 67/1000 Train loss: 0.155972\n",
      "Epoch: 68/1000 Train loss: 0.154589\n",
      "Epoch: 69/1000 Train loss: 0.153234\n",
      "Epoch: 70/1000 Train loss: 0.151906\n",
      "Epoch: 71/1000 Train loss: 0.150605\n",
      "Epoch: 72/1000 Train loss: 0.149330\n",
      "Epoch: 73/1000 Train loss: 0.148080\n",
      "Epoch: 74/1000 Train loss: 0.146854\n",
      "Epoch: 75/1000 Train loss: 0.145651\n",
      "Epoch: 76/1000 Train loss: 0.144471\n",
      "Epoch: 77/1000 Train loss: 0.143314\n",
      "Epoch: 78/1000 Train loss: 0.142177\n",
      "Epoch: 79/1000 Train loss: 0.141061\n",
      "Epoch: 80/1000 Train loss: 0.139966\n",
      "Epoch: 81/1000 Train loss: 0.138890\n",
      "Epoch: 82/1000 Train loss: 0.137833\n",
      "Epoch: 83/1000 Train loss: 0.136795\n",
      "Epoch: 84/1000 Train loss: 0.135774\n",
      "Epoch: 85/1000 Train loss: 0.134771\n",
      "Epoch: 86/1000 Train loss: 0.133785\n",
      "Epoch: 87/1000 Train loss: 0.132815\n",
      "Epoch: 88/1000 Train loss: 0.131860\n",
      "Epoch: 89/1000 Train loss: 0.130922\n",
      "Epoch: 90/1000 Train loss: 0.129999\n",
      "Epoch: 91/1000 Train loss: 0.129090\n",
      "Epoch: 92/1000 Train loss: 0.128196\n",
      "Epoch: 93/1000 Train loss: 0.127316\n",
      "Epoch: 94/1000 Train loss: 0.126450\n",
      "Epoch: 95/1000 Train loss: 0.125597\n",
      "Epoch: 96/1000 Train loss: 0.124757\n",
      "Epoch: 97/1000 Train loss: 0.123930\n",
      "Epoch: 98/1000 Train loss: 0.123115\n",
      "Epoch: 99/1000 Train loss: 0.122313\n",
      "Epoch: 100/1000 Train loss: 0.121522\n",
      "Epoch: 101/1000 Train loss: 0.120742\n",
      "Epoch: 102/1000 Train loss: 0.119974\n",
      "Epoch: 103/1000 Train loss: 0.119216\n",
      "Epoch: 104/1000 Train loss: 0.118470\n",
      "Epoch: 105/1000 Train loss: 0.117734\n",
      "Epoch: 106/1000 Train loss: 0.117008\n",
      "Epoch: 107/1000 Train loss: 0.116292\n",
      "Epoch: 108/1000 Train loss: 0.115585\n",
      "Epoch: 109/1000 Train loss: 0.114888\n",
      "Epoch: 110/1000 Train loss: 0.114201\n",
      "Epoch: 111/1000 Train loss: 0.113522\n",
      "Epoch: 112/1000 Train loss: 0.112853\n",
      "Epoch: 113/1000 Train loss: 0.112192\n",
      "Epoch: 114/1000 Train loss: 0.111540\n",
      "Epoch: 115/1000 Train loss: 0.110896\n",
      "Epoch: 116/1000 Train loss: 0.110260\n",
      "Epoch: 117/1000 Train loss: 0.109632\n",
      "Epoch: 118/1000 Train loss: 0.109012\n",
      "Epoch: 119/1000 Train loss: 0.108400\n",
      "Epoch: 120/1000 Train loss: 0.107795\n",
      "Epoch: 121/1000 Train loss: 0.107197\n",
      "Epoch: 122/1000 Train loss: 0.106607\n",
      "Epoch: 123/1000 Train loss: 0.106024\n",
      "Epoch: 124/1000 Train loss: 0.105447\n",
      "Epoch: 125/1000 Train loss: 0.104878\n",
      "Epoch: 126/1000 Train loss: 0.104315\n",
      "Epoch: 127/1000 Train loss: 0.103758\n",
      "Epoch: 128/1000 Train loss: 0.103208\n",
      "Epoch: 129/1000 Train loss: 0.102664\n",
      "Epoch: 130/1000 Train loss: 0.102126\n",
      "Epoch: 131/1000 Train loss: 0.101594\n",
      "Epoch: 132/1000 Train loss: 0.101068\n",
      "Epoch: 133/1000 Train loss: 0.100548\n",
      "Epoch: 134/1000 Train loss: 0.100033\n",
      "Epoch: 135/1000 Train loss: 0.099524\n",
      "Epoch: 136/1000 Train loss: 0.099020\n",
      "Epoch: 137/1000 Train loss: 0.098521\n",
      "Epoch: 138/1000 Train loss: 0.098028\n",
      "Epoch: 139/1000 Train loss: 0.097540\n",
      "Epoch: 140/1000 Train loss: 0.097056\n",
      "Epoch: 141/1000 Train loss: 0.096578\n",
      "Epoch: 142/1000 Train loss: 0.096104\n",
      "Epoch: 143/1000 Train loss: 0.095635\n",
      "Epoch: 144/1000 Train loss: 0.095170\n",
      "Epoch: 145/1000 Train loss: 0.094710\n",
      "Epoch: 146/1000 Train loss: 0.094255\n",
      "Epoch: 147/1000 Train loss: 0.093803\n",
      "Epoch: 148/1000 Train loss: 0.093356\n",
      "Epoch: 149/1000 Train loss: 0.092913\n",
      "Epoch: 150/1000 Train loss: 0.092474\n",
      "Epoch: 151/1000 Train loss: 0.092040\n",
      "Epoch: 152/1000 Train loss: 0.091609\n",
      "Epoch: 153/1000 Train loss: 0.091182\n",
      "Epoch: 154/1000 Train loss: 0.090758\n",
      "Epoch: 155/1000 Train loss: 0.090339\n",
      "Epoch: 156/1000 Train loss: 0.089923\n",
      "Epoch: 157/1000 Train loss: 0.089511\n",
      "Epoch: 158/1000 Train loss: 0.089102\n",
      "Epoch: 159/1000 Train loss: 0.088696\n",
      "Epoch: 160/1000 Train loss: 0.088294\n",
      "Epoch: 161/1000 Train loss: 0.087896\n",
      "Epoch: 162/1000 Train loss: 0.087500\n",
      "Epoch: 163/1000 Train loss: 0.087108\n",
      "Epoch: 164/1000 Train loss: 0.086719\n",
      "Epoch: 165/1000 Train loss: 0.086333\n",
      "Epoch: 166/1000 Train loss: 0.085950\n",
      "Epoch: 167/1000 Train loss: 0.085571\n",
      "Epoch: 168/1000 Train loss: 0.085194\n",
      "Epoch: 169/1000 Train loss: 0.084820\n",
      "Epoch: 170/1000 Train loss: 0.084449\n",
      "Epoch: 171/1000 Train loss: 0.084081\n",
      "Epoch: 172/1000 Train loss: 0.083716\n",
      "Epoch: 173/1000 Train loss: 0.083353\n",
      "Epoch: 174/1000 Train loss: 0.082993\n",
      "Epoch: 175/1000 Train loss: 0.082636\n",
      "Epoch: 176/1000 Train loss: 0.082282\n",
      "Epoch: 177/1000 Train loss: 0.081930\n",
      "Epoch: 178/1000 Train loss: 0.081580\n",
      "Epoch: 179/1000 Train loss: 0.081233\n",
      "Epoch: 180/1000 Train loss: 0.080888\n",
      "Epoch: 181/1000 Train loss: 0.080546\n",
      "Epoch: 182/1000 Train loss: 0.080206\n",
      "Epoch: 183/1000 Train loss: 0.079868\n",
      "Epoch: 184/1000 Train loss: 0.079532\n",
      "Epoch: 185/1000 Train loss: 0.079199\n",
      "Epoch: 186/1000 Train loss: 0.078868\n",
      "Epoch: 187/1000 Train loss: 0.078539\n",
      "Epoch: 188/1000 Train loss: 0.078212\n",
      "Epoch: 189/1000 Train loss: 0.077887\n",
      "Epoch: 190/1000 Train loss: 0.077565\n",
      "Epoch: 191/1000 Train loss: 0.077244\n",
      "Epoch: 192/1000 Train loss: 0.076926\n",
      "Epoch: 193/1000 Train loss: 0.076610\n",
      "Epoch: 194/1000 Train loss: 0.076296\n",
      "Epoch: 195/1000 Train loss: 0.075984\n",
      "Epoch: 196/1000 Train loss: 0.075674\n",
      "Epoch: 197/1000 Train loss: 0.075366\n",
      "Epoch: 198/1000 Train loss: 0.075060\n",
      "Epoch: 199/1000 Train loss: 0.074756\n",
      "Epoch: 200/1000 Train loss: 0.074454\n",
      "Epoch: 201/1000 Train loss: 0.074154\n",
      "Epoch: 202/1000 Train loss: 0.073855\n",
      "Epoch: 203/1000 Train loss: 0.073559\n",
      "Epoch: 204/1000 Train loss: 0.073265\n",
      "Epoch: 205/1000 Train loss: 0.072972\n",
      "Epoch: 206/1000 Train loss: 0.072681\n",
      "Epoch: 207/1000 Train loss: 0.072393\n",
      "Epoch: 208/1000 Train loss: 0.072106\n",
      "Epoch: 209/1000 Train loss: 0.071821\n",
      "Epoch: 210/1000 Train loss: 0.071537\n",
      "Epoch: 211/1000 Train loss: 0.071256\n",
      "Epoch: 212/1000 Train loss: 0.070976\n",
      "Epoch: 213/1000 Train loss: 0.070698\n",
      "Epoch: 214/1000 Train loss: 0.070423\n",
      "Epoch: 215/1000 Train loss: 0.070148\n",
      "Epoch: 216/1000 Train loss: 0.069876\n",
      "Epoch: 217/1000 Train loss: 0.069605\n",
      "Epoch: 218/1000 Train loss: 0.069336\n",
      "Epoch: 219/1000 Train loss: 0.069069\n",
      "Epoch: 220/1000 Train loss: 0.068804\n",
      "Epoch: 221/1000 Train loss: 0.068540\n",
      "Epoch: 222/1000 Train loss: 0.068278\n",
      "Epoch: 223/1000 Train loss: 0.068017\n",
      "Epoch: 224/1000 Train loss: 0.067759\n",
      "Epoch: 225/1000 Train loss: 0.067501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 226/1000 Train loss: 0.067246\n",
      "Epoch: 227/1000 Train loss: 0.066992\n",
      "Epoch: 228/1000 Train loss: 0.066739\n",
      "Epoch: 229/1000 Train loss: 0.066488\n",
      "Epoch: 230/1000 Train loss: 0.066239\n",
      "Epoch: 231/1000 Train loss: 0.065991\n",
      "Epoch: 232/1000 Train loss: 0.065745\n",
      "Epoch: 233/1000 Train loss: 0.065501\n",
      "Epoch: 234/1000 Train loss: 0.065258\n",
      "Epoch: 235/1000 Train loss: 0.065016\n",
      "Epoch: 236/1000 Train loss: 0.064776\n",
      "Epoch: 237/1000 Train loss: 0.064537\n",
      "Epoch: 238/1000 Train loss: 0.064300\n",
      "Epoch: 239/1000 Train loss: 0.064064\n",
      "Epoch: 240/1000 Train loss: 0.063830\n",
      "Epoch: 241/1000 Train loss: 0.063597\n",
      "Epoch: 242/1000 Train loss: 0.063366\n",
      "Epoch: 243/1000 Train loss: 0.063136\n",
      "Epoch: 244/1000 Train loss: 0.062907\n",
      "Epoch: 245/1000 Train loss: 0.062680\n",
      "Epoch: 246/1000 Train loss: 0.062454\n",
      "Epoch: 247/1000 Train loss: 0.062230\n",
      "Epoch: 248/1000 Train loss: 0.062007\n",
      "Epoch: 249/1000 Train loss: 0.061785\n",
      "Epoch: 250/1000 Train loss: 0.061565\n",
      "Epoch: 251/1000 Train loss: 0.061527\n",
      "Epoch: 252/1000 Train loss: 0.061354\n",
      "Epoch: 253/1000 Train loss: 0.061137\n",
      "Epoch: 254/1000 Train loss: 0.060921\n",
      "Epoch: 255/1000 Train loss: 0.060706\n",
      "Epoch: 256/1000 Train loss: 0.060493\n",
      "Epoch: 257/1000 Train loss: 0.060281\n",
      "Epoch: 258/1000 Train loss: 0.060070\n",
      "Epoch: 259/1000 Train loss: 0.059861\n",
      "Epoch: 260/1000 Train loss: 0.059654\n",
      "Epoch: 261/1000 Train loss: 0.059448\n",
      "Epoch: 262/1000 Train loss: 0.059243\n",
      "Epoch: 263/1000 Train loss: 0.059040\n",
      "Epoch: 264/1000 Train loss: 0.058838\n",
      "Epoch: 265/1000 Train loss: 0.058638\n",
      "Epoch: 266/1000 Train loss: 0.058439\n",
      "Epoch: 267/1000 Train loss: 0.058241\n",
      "Epoch: 268/1000 Train loss: 0.058045\n",
      "Epoch: 269/1000 Train loss: 0.057850\n",
      "Epoch: 270/1000 Train loss: 0.057656\n",
      "Epoch: 271/1000 Train loss: 0.057464\n",
      "Epoch: 272/1000 Train loss: 0.057273\n",
      "Epoch: 273/1000 Train loss: 0.057083\n",
      "Epoch: 274/1000 Train loss: 0.056894\n",
      "Epoch: 275/1000 Train loss: 0.056707\n",
      "Epoch: 276/1000 Train loss: 0.056521\n",
      "Epoch: 277/1000 Train loss: 0.056336\n",
      "Epoch: 278/1000 Train loss: 0.056152\n",
      "Epoch: 279/1000 Train loss: 0.055970\n",
      "Epoch: 280/1000 Train loss: 0.055788\n",
      "Epoch: 281/1000 Train loss: 0.055608\n",
      "Epoch: 282/1000 Train loss: 0.055429\n",
      "Epoch: 283/1000 Train loss: 0.055251\n",
      "Epoch: 284/1000 Train loss: 0.055074\n",
      "Epoch: 285/1000 Train loss: 0.054898\n",
      "Epoch: 286/1000 Train loss: 0.054723\n",
      "Epoch: 287/1000 Train loss: 0.054549\n",
      "Epoch: 288/1000 Train loss: 0.054376\n",
      "Epoch: 289/1000 Train loss: 0.054204\n",
      "Epoch: 290/1000 Train loss: 0.054033\n",
      "Epoch: 291/1000 Train loss: 0.053863\n",
      "Epoch: 292/1000 Train loss: 0.053694\n",
      "Epoch: 293/1000 Train loss: 0.053526\n",
      "Epoch: 294/1000 Train loss: 0.053359\n",
      "Epoch: 295/1000 Train loss: 0.053193\n",
      "Epoch: 296/1000 Train loss: 0.053028\n",
      "Epoch: 297/1000 Train loss: 0.052864\n",
      "Epoch: 298/1000 Train loss: 0.052700\n",
      "Epoch: 299/1000 Train loss: 0.052538\n",
      "Epoch: 300/1000 Train loss: 0.052376\n",
      "Epoch: 301/1000 Train loss: 0.052216\n",
      "Epoch: 302/1000 Train loss: 0.052056\n",
      "Epoch: 303/1000 Train loss: 0.051897\n",
      "Epoch: 304/1000 Train loss: 0.051739\n",
      "Epoch: 305/1000 Train loss: 0.051582\n",
      "Epoch: 306/1000 Train loss: 0.051426\n",
      "Epoch: 307/1000 Train loss: 0.051271\n",
      "Epoch: 308/1000 Train loss: 0.051116\n",
      "Epoch: 309/1000 Train loss: 0.050963\n",
      "Epoch: 310/1000 Train loss: 0.050810\n",
      "Epoch: 311/1000 Train loss: 0.050658\n",
      "Epoch: 312/1000 Train loss: 0.050507\n",
      "Epoch: 313/1000 Train loss: 0.050356\n",
      "Epoch: 314/1000 Train loss: 0.050207\n",
      "Epoch: 315/1000 Train loss: 0.050058\n",
      "Epoch: 316/1000 Train loss: 0.049910\n",
      "Epoch: 317/1000 Train loss: 0.049763\n",
      "Epoch: 318/1000 Train loss: 0.049617\n",
      "Epoch: 319/1000 Train loss: 0.049471\n",
      "Epoch: 320/1000 Train loss: 0.049326\n",
      "Epoch: 321/1000 Train loss: 0.049182\n",
      "Epoch: 322/1000 Train loss: 0.049039\n",
      "Epoch: 323/1000 Train loss: 0.048896\n",
      "Epoch: 324/1000 Train loss: 0.048754\n",
      "Epoch: 325/1000 Train loss: 0.048613\n",
      "Epoch: 326/1000 Train loss: 0.048473\n",
      "Epoch: 327/1000 Train loss: 0.048333\n",
      "Epoch: 328/1000 Train loss: 0.048194\n",
      "Epoch: 329/1000 Train loss: 0.048056\n",
      "Epoch: 330/1000 Train loss: 0.047919\n",
      "Epoch: 331/1000 Train loss: 0.047782\n",
      "Epoch: 332/1000 Train loss: 0.047646\n",
      "Epoch: 333/1000 Train loss: 0.047510\n",
      "Epoch: 334/1000 Train loss: 0.047376\n",
      "Epoch: 335/1000 Train loss: 0.047242\n",
      "Epoch: 336/1000 Train loss: 0.047108\n",
      "Epoch: 337/1000 Train loss: 0.046975\n",
      "Epoch: 338/1000 Train loss: 0.046843\n",
      "Epoch: 339/1000 Train loss: 0.046713\n",
      "Epoch: 340/1000 Train loss: 0.046765\n",
      "Epoch: 341/1000 Train loss: 0.046652\n",
      "Epoch: 342/1000 Train loss: 0.046531\n",
      "Epoch: 343/1000 Train loss: 0.046401\n",
      "Epoch: 344/1000 Train loss: 0.046273\n",
      "Epoch: 345/1000 Train loss: 0.046145\n",
      "Epoch: 346/1000 Train loss: 0.046017\n",
      "Epoch: 347/1000 Train loss: 0.045891\n",
      "Epoch: 348/1000 Train loss: 0.045765\n",
      "Epoch: 349/1000 Train loss: 0.045639\n",
      "Epoch: 350/1000 Train loss: 0.045515\n",
      "Epoch: 351/1000 Train loss: 0.045391\n",
      "Epoch: 352/1000 Train loss: 0.045267\n",
      "Epoch: 353/1000 Train loss: 0.045145\n",
      "Epoch: 354/1000 Train loss: 0.045023\n",
      "Epoch: 355/1000 Train loss: 0.044902\n",
      "Epoch: 356/1000 Train loss: 0.044781\n",
      "Epoch: 357/1000 Train loss: 0.044661\n",
      "Epoch: 358/1000 Train loss: 0.044542\n",
      "Epoch: 359/1000 Train loss: 0.044423\n",
      "Epoch: 360/1000 Train loss: 0.044305\n",
      "Epoch: 361/1000 Train loss: 0.044188\n",
      "Epoch: 362/1000 Train loss: 0.044071\n",
      "Epoch: 363/1000 Train loss: 0.043955\n",
      "Epoch: 364/1000 Train loss: 0.043839\n",
      "Epoch: 365/1000 Train loss: 0.043724\n",
      "Epoch: 366/1000 Train loss: 0.043610\n",
      "Epoch: 367/1000 Train loss: 0.043496\n",
      "Epoch: 368/1000 Train loss: 0.043383\n",
      "Epoch: 369/1000 Train loss: 0.043271\n",
      "Epoch: 370/1000 Train loss: 0.043159\n",
      "Epoch: 371/1000 Train loss: 0.043048\n",
      "Epoch: 372/1000 Train loss: 0.042937\n",
      "Epoch: 373/1000 Train loss: 0.042827\n",
      "Epoch: 374/1000 Train loss: 0.042717\n",
      "Epoch: 375/1000 Train loss: 0.042608\n",
      "Epoch: 376/1000 Train loss: 0.042500\n",
      "Epoch: 377/1000 Train loss: 0.042392\n",
      "Epoch: 378/1000 Train loss: 0.042285\n",
      "Epoch: 379/1000 Train loss: 0.042178\n",
      "Epoch: 380/1000 Train loss: 0.042072\n",
      "Epoch: 381/1000 Train loss: 0.041966\n",
      "Epoch: 382/1000 Train loss: 0.041861\n",
      "Epoch: 383/1000 Train loss: 0.041756\n",
      "Epoch: 384/1000 Train loss: 0.041652\n",
      "Epoch: 385/1000 Train loss: 0.041549\n",
      "Epoch: 386/1000 Train loss: 0.041446\n",
      "Epoch: 387/1000 Train loss: 0.041343\n",
      "Epoch: 388/1000 Train loss: 0.041241\n",
      "Epoch: 389/1000 Train loss: 0.041139\n",
      "Epoch: 390/1000 Train loss: 0.041038\n",
      "Epoch: 391/1000 Train loss: 0.040938\n",
      "Epoch: 392/1000 Train loss: 0.040837\n",
      "Epoch: 393/1000 Train loss: 0.040738\n",
      "Epoch: 394/1000 Train loss: 0.040639\n",
      "Epoch: 395/1000 Train loss: 0.040540\n",
      "Epoch: 396/1000 Train loss: 0.040441\n",
      "Epoch: 397/1000 Train loss: 0.040343\n",
      "Epoch: 398/1000 Train loss: 0.040246\n",
      "Epoch: 399/1000 Train loss: 0.040149\n",
      "Epoch: 400/1000 Train loss: 0.040052\n",
      "Epoch: 401/1000 Train loss: 0.039956\n",
      "Epoch: 402/1000 Train loss: 0.039860\n",
      "Epoch: 403/1000 Train loss: 0.039765\n",
      "Epoch: 404/1000 Train loss: 0.039670\n",
      "Epoch: 405/1000 Train loss: 0.039576\n",
      "Epoch: 406/1000 Train loss: 0.039482\n",
      "Epoch: 407/1000 Train loss: 0.039388\n",
      "Epoch: 408/1000 Train loss: 0.039295\n",
      "Epoch: 409/1000 Train loss: 0.039202\n",
      "Epoch: 410/1000 Train loss: 0.039109\n",
      "Epoch: 411/1000 Train loss: 0.039017\n",
      "Epoch: 412/1000 Train loss: 0.038925\n",
      "Epoch: 413/1000 Train loss: 0.038834\n",
      "Epoch: 414/1000 Train loss: 0.038743\n",
      "Epoch: 415/1000 Train loss: 0.038653\n",
      "Epoch: 416/1000 Train loss: 0.038563\n",
      "Epoch: 417/1000 Train loss: 0.038474\n",
      "Epoch: 418/1000 Train loss: 0.038385\n",
      "Epoch: 419/1000 Train loss: 0.038297\n",
      "Epoch: 420/1000 Train loss: 0.038208\n",
      "Epoch: 421/1000 Train loss: 0.038121\n",
      "Epoch: 422/1000 Train loss: 0.038033\n",
      "Epoch: 423/1000 Train loss: 0.037946\n",
      "Epoch: 424/1000 Train loss: 0.037859\n",
      "Epoch: 425/1000 Train loss: 0.037772\n",
      "Epoch: 426/1000 Train loss: 0.037685\n",
      "Epoch: 427/1000 Train loss: 0.037599\n",
      "Epoch: 428/1000 Train loss: 0.037514\n",
      "Epoch: 429/1000 Train loss: 0.037429\n",
      "Epoch: 430/1000 Train loss: 0.037344\n",
      "Epoch: 431/1000 Train loss: 0.037259\n",
      "Epoch: 432/1000 Train loss: 0.037175\n",
      "Epoch: 433/1000 Train loss: 0.037091\n",
      "Epoch: 434/1000 Train loss: 0.037008\n",
      "Epoch: 435/1000 Train loss: 0.036924\n",
      "Epoch: 436/1000 Train loss: 0.036842\n",
      "Epoch: 437/1000 Train loss: 0.036759\n",
      "Epoch: 438/1000 Train loss: 0.036678\n",
      "Epoch: 439/1000 Train loss: 0.036596\n",
      "Epoch: 440/1000 Train loss: 0.036515\n",
      "Epoch: 441/1000 Train loss: 0.036597\n",
      "Epoch: 442/1000 Train loss: 0.036547\n",
      "Epoch: 443/1000 Train loss: 0.036468\n",
      "Epoch: 444/1000 Train loss: 0.036388\n",
      "Epoch: 445/1000 Train loss: 0.036309\n",
      "Epoch: 446/1000 Train loss: 0.036229\n",
      "Epoch: 447/1000 Train loss: 0.036150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 448/1000 Train loss: 0.036072\n",
      "Epoch: 449/1000 Train loss: 0.035993\n",
      "Epoch: 450/1000 Train loss: 0.035915\n",
      "Epoch: 451/1000 Train loss: 0.035837\n",
      "Epoch: 452/1000 Train loss: 0.035759\n",
      "Epoch: 453/1000 Train loss: 0.035682\n",
      "Epoch: 454/1000 Train loss: 0.035605\n",
      "Epoch: 455/1000 Train loss: 0.035529\n",
      "Epoch: 456/1000 Train loss: 0.035452\n",
      "Epoch: 457/1000 Train loss: 0.035376\n",
      "Epoch: 458/1000 Train loss: 0.035301\n",
      "Epoch: 459/1000 Train loss: 0.035225\n",
      "Epoch: 460/1000 Train loss: 0.035150\n",
      "Epoch: 461/1000 Train loss: 0.035075\n",
      "Epoch: 462/1000 Train loss: 0.035001\n",
      "Epoch: 463/1000 Train loss: 0.034927\n",
      "Epoch: 464/1000 Train loss: 0.034853\n",
      "Epoch: 465/1000 Train loss: 0.034779\n",
      "Epoch: 466/1000 Train loss: 0.034706\n",
      "Epoch: 467/1000 Train loss: 0.034633\n",
      "Epoch: 468/1000 Train loss: 0.034561\n",
      "Epoch: 469/1000 Train loss: 0.034489\n",
      "Epoch: 470/1000 Train loss: 0.034417\n",
      "Epoch: 471/1000 Train loss: 0.034345\n",
      "Epoch: 472/1000 Train loss: 0.034274\n",
      "Epoch: 473/1000 Train loss: 0.034203\n",
      "Epoch: 474/1000 Train loss: 0.034132\n",
      "Epoch: 475/1000 Train loss: 0.034062\n",
      "Epoch: 476/1000 Train loss: 0.033992\n",
      "Epoch: 477/1000 Train loss: 0.033922\n",
      "Epoch: 478/1000 Train loss: 0.033853\n",
      "Epoch: 479/1000 Train loss: 0.033783\n",
      "Epoch: 480/1000 Train loss: 0.033714\n",
      "Epoch: 481/1000 Train loss: 0.033646\n",
      "Epoch: 482/1000 Train loss: 0.033577\n",
      "Epoch: 483/1000 Train loss: 0.033509\n",
      "Epoch: 484/1000 Train loss: 0.033441\n",
      "Epoch: 485/1000 Train loss: 0.033374\n",
      "Epoch: 486/1000 Train loss: 0.033306\n",
      "Epoch: 487/1000 Train loss: 0.033239\n",
      "Epoch: 488/1000 Train loss: 0.033172\n",
      "Epoch: 489/1000 Train loss: 0.033106\n",
      "Epoch: 490/1000 Train loss: 0.033041\n",
      "Epoch: 491/1000 Train loss: 0.033000\n",
      "Epoch: 492/1000 Train loss: 0.032990\n",
      "Epoch: 493/1000 Train loss: 0.032926\n",
      "Epoch: 494/1000 Train loss: 0.032862\n",
      "Epoch: 495/1000 Train loss: 0.032797\n",
      "Epoch: 496/1000 Train loss: 0.032732\n",
      "Epoch: 497/1000 Train loss: 0.032667\n",
      "Epoch: 498/1000 Train loss: 0.032603\n",
      "Epoch: 499/1000 Train loss: 0.032539\n",
      "Epoch: 500/1000 Train loss: 0.032475\n",
      "Epoch: 501/1000 Train loss: 0.032411\n",
      "Epoch: 502/1000 Train loss: 0.032348\n",
      "Epoch: 503/1000 Train loss: 0.032285\n",
      "Epoch: 504/1000 Train loss: 0.032222\n",
      "Epoch: 505/1000 Train loss: 0.032159\n",
      "Epoch: 506/1000 Train loss: 0.032097\n",
      "Epoch: 507/1000 Train loss: 0.032035\n",
      "Epoch: 508/1000 Train loss: 0.031973\n",
      "Epoch: 509/1000 Train loss: 0.031911\n",
      "Epoch: 510/1000 Train loss: 0.031849\n",
      "Epoch: 511/1000 Train loss: 0.031788\n",
      "Epoch: 512/1000 Train loss: 0.031727\n",
      "Epoch: 513/1000 Train loss: 0.031666\n",
      "Epoch: 514/1000 Train loss: 0.031606\n",
      "Epoch: 515/1000 Train loss: 0.031545\n",
      "Epoch: 516/1000 Train loss: 0.031485\n",
      "Epoch: 517/1000 Train loss: 0.031425\n",
      "Epoch: 518/1000 Train loss: 0.031366\n",
      "Epoch: 519/1000 Train loss: 0.031306\n",
      "Epoch: 520/1000 Train loss: 0.031247\n",
      "Epoch: 521/1000 Train loss: 0.031188\n",
      "Epoch: 522/1000 Train loss: 0.031130\n",
      "Epoch: 523/1000 Train loss: 0.031071\n",
      "Epoch: 524/1000 Train loss: 0.031013\n",
      "Epoch: 525/1000 Train loss: 0.030955\n",
      "Epoch: 526/1000 Train loss: 0.030897\n",
      "Epoch: 527/1000 Train loss: 0.030839\n",
      "Epoch: 528/1000 Train loss: 0.030782\n",
      "Epoch: 529/1000 Train loss: 0.030725\n",
      "Epoch: 530/1000 Train loss: 0.030668\n",
      "Epoch: 531/1000 Train loss: 0.030611\n",
      "Epoch: 532/1000 Train loss: 0.030554\n",
      "Epoch: 533/1000 Train loss: 0.030499\n",
      "Epoch: 534/1000 Train loss: 0.030536\n",
      "Epoch: 535/1000 Train loss: 0.030498\n",
      "Epoch: 536/1000 Train loss: 0.030445\n",
      "Epoch: 537/1000 Train loss: 0.030390\n",
      "Epoch: 538/1000 Train loss: 0.030334\n",
      "Epoch: 539/1000 Train loss: 0.030279\n",
      "Epoch: 540/1000 Train loss: 0.030224\n",
      "Epoch: 541/1000 Train loss: 0.030169\n",
      "Epoch: 542/1000 Train loss: 0.030115\n",
      "Epoch: 543/1000 Train loss: 0.030060\n",
      "Epoch: 544/1000 Train loss: 0.030006\n",
      "Epoch: 545/1000 Train loss: 0.029952\n",
      "Epoch: 546/1000 Train loss: 0.029898\n",
      "Epoch: 547/1000 Train loss: 0.029844\n",
      "Epoch: 548/1000 Train loss: 0.029791\n",
      "Epoch: 549/1000 Train loss: 0.029738\n",
      "Epoch: 550/1000 Train loss: 0.029684\n",
      "Epoch: 551/1000 Train loss: 0.029632\n",
      "Epoch: 552/1000 Train loss: 0.029579\n",
      "Epoch: 553/1000 Train loss: 0.029526\n",
      "Epoch: 554/1000 Train loss: 0.029474\n",
      "Epoch: 555/1000 Train loss: 0.029421\n",
      "Epoch: 556/1000 Train loss: 0.029369\n",
      "Epoch: 557/1000 Train loss: 0.029317\n",
      "Epoch: 558/1000 Train loss: 0.029266\n",
      "Epoch: 559/1000 Train loss: 0.029214\n",
      "Epoch: 560/1000 Train loss: 0.029163\n",
      "Epoch: 561/1000 Train loss: 0.029112\n",
      "Epoch: 562/1000 Train loss: 0.029060\n",
      "Epoch: 563/1000 Train loss: 0.029010\n",
      "Epoch: 564/1000 Train loss: 0.028959\n",
      "Epoch: 565/1000 Train loss: 0.028908\n",
      "Epoch: 566/1000 Train loss: 0.028858\n",
      "Epoch: 567/1000 Train loss: 0.028808\n",
      "Epoch: 568/1000 Train loss: 0.028758\n",
      "Epoch: 569/1000 Train loss: 0.028708\n",
      "Epoch: 570/1000 Train loss: 0.028659\n",
      "Epoch: 571/1000 Train loss: 0.028609\n",
      "Epoch: 572/1000 Train loss: 0.028560\n",
      "Epoch: 573/1000 Train loss: 0.028511\n",
      "Epoch: 574/1000 Train loss: 0.028462\n",
      "Epoch: 575/1000 Train loss: 0.028413\n",
      "Epoch: 576/1000 Train loss: 0.028364\n",
      "Epoch: 577/1000 Train loss: 0.028316\n",
      "Epoch: 578/1000 Train loss: 0.028268\n",
      "Epoch: 579/1000 Train loss: 0.028220\n",
      "Epoch: 580/1000 Train loss: 0.028172\n",
      "Epoch: 581/1000 Train loss: 0.028124\n",
      "Epoch: 582/1000 Train loss: 0.028076\n",
      "Epoch: 583/1000 Train loss: 0.028029\n",
      "Epoch: 584/1000 Train loss: 0.028040\n",
      "Epoch: 585/1000 Train loss: 0.028039\n",
      "Epoch: 586/1000 Train loss: 0.027994\n",
      "Epoch: 587/1000 Train loss: 0.027947\n",
      "Epoch: 588/1000 Train loss: 0.027900\n",
      "Epoch: 589/1000 Train loss: 0.027854\n",
      "Epoch: 590/1000 Train loss: 0.027807\n",
      "Epoch: 591/1000 Train loss: 0.027761\n",
      "Epoch: 592/1000 Train loss: 0.027715\n",
      "Epoch: 593/1000 Train loss: 0.027669\n",
      "Epoch: 594/1000 Train loss: 0.027623\n",
      "Epoch: 595/1000 Train loss: 0.027578\n",
      "Epoch: 596/1000 Train loss: 0.027532\n",
      "Epoch: 597/1000 Train loss: 0.027487\n",
      "Epoch: 598/1000 Train loss: 0.027442\n",
      "Epoch: 599/1000 Train loss: 0.027396\n",
      "Epoch: 600/1000 Train loss: 0.027351\n",
      "Epoch: 601/1000 Train loss: 0.027307\n",
      "Epoch: 602/1000 Train loss: 0.027262\n",
      "Epoch: 603/1000 Train loss: 0.027218\n",
      "Epoch: 604/1000 Train loss: 0.027173\n",
      "Epoch: 605/1000 Train loss: 0.027129\n",
      "Epoch: 606/1000 Train loss: 0.027085\n",
      "Epoch: 607/1000 Train loss: 0.027041\n",
      "Epoch: 608/1000 Train loss: 0.026997\n",
      "Epoch: 609/1000 Train loss: 0.026953\n",
      "Epoch: 610/1000 Train loss: 0.026910\n",
      "Epoch: 611/1000 Train loss: 0.026866\n",
      "Epoch: 612/1000 Train loss: 0.026823\n",
      "Epoch: 613/1000 Train loss: 0.026780\n",
      "Epoch: 614/1000 Train loss: 0.026737\n",
      "Epoch: 615/1000 Train loss: 0.026694\n",
      "Epoch: 616/1000 Train loss: 0.026651\n",
      "Epoch: 617/1000 Train loss: 0.026609\n",
      "Epoch: 618/1000 Train loss: 0.026566\n",
      "Epoch: 619/1000 Train loss: 0.026524\n",
      "Epoch: 620/1000 Train loss: 0.026482\n",
      "Epoch: 621/1000 Train loss: 0.026440\n",
      "Epoch: 622/1000 Train loss: 0.026398\n",
      "Epoch: 623/1000 Train loss: 0.026356\n",
      "Epoch: 624/1000 Train loss: 0.026314\n",
      "Epoch: 625/1000 Train loss: 0.026273\n",
      "Epoch: 626/1000 Train loss: 0.026232\n",
      "Epoch: 627/1000 Train loss: 0.026190\n",
      "Epoch: 628/1000 Train loss: 0.026165\n",
      "Epoch: 629/1000 Train loss: 0.026150\n",
      "Epoch: 630/1000 Train loss: 0.026120\n",
      "Epoch: 631/1000 Train loss: 0.026079\n",
      "Epoch: 632/1000 Train loss: 0.026039\n",
      "Epoch: 633/1000 Train loss: 0.025998\n",
      "Epoch: 634/1000 Train loss: 0.025958\n",
      "Epoch: 635/1000 Train loss: 0.025918\n",
      "Epoch: 636/1000 Train loss: 0.025878\n",
      "Epoch: 637/1000 Train loss: 0.025838\n",
      "Epoch: 638/1000 Train loss: 0.025798\n",
      "Epoch: 639/1000 Train loss: 0.025758\n",
      "Epoch: 640/1000 Train loss: 0.025718\n",
      "Epoch: 641/1000 Train loss: 0.025679\n",
      "Epoch: 642/1000 Train loss: 0.025639\n",
      "Epoch: 643/1000 Train loss: 0.025600\n",
      "Epoch: 644/1000 Train loss: 0.025561\n",
      "Epoch: 645/1000 Train loss: 0.025522\n",
      "Epoch: 646/1000 Train loss: 0.025483\n",
      "Epoch: 647/1000 Train loss: 0.025444\n",
      "Epoch: 648/1000 Train loss: 0.025405\n",
      "Epoch: 649/1000 Train loss: 0.025367\n",
      "Epoch: 650/1000 Train loss: 0.025328\n",
      "Epoch: 651/1000 Train loss: 0.025290\n",
      "Epoch: 652/1000 Train loss: 0.025252\n",
      "Epoch: 653/1000 Train loss: 0.025213\n",
      "Epoch: 654/1000 Train loss: 0.025175\n",
      "Epoch: 655/1000 Train loss: 0.025138\n",
      "Epoch: 656/1000 Train loss: 0.025100\n",
      "Epoch: 657/1000 Train loss: 0.025062\n",
      "Epoch: 658/1000 Train loss: 0.025024\n",
      "Epoch: 659/1000 Train loss: 0.024987\n",
      "Epoch: 660/1000 Train loss: 0.024950\n",
      "Epoch: 661/1000 Train loss: 0.024913\n",
      "Epoch: 662/1000 Train loss: 0.024876\n",
      "Epoch: 663/1000 Train loss: 0.024839\n",
      "Epoch: 664/1000 Train loss: 0.024802\n",
      "Epoch: 665/1000 Train loss: 0.024770\n",
      "Epoch: 666/1000 Train loss: 0.024844\n",
      "Epoch: 667/1000 Train loss: 0.024808\n",
      "Epoch: 668/1000 Train loss: 0.024772\n",
      "Epoch: 669/1000 Train loss: 0.024735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 670/1000 Train loss: 0.024699\n",
      "Epoch: 671/1000 Train loss: 0.024663\n",
      "Epoch: 672/1000 Train loss: 0.024627\n",
      "Epoch: 673/1000 Train loss: 0.024590\n",
      "Epoch: 674/1000 Train loss: 0.024554\n",
      "Epoch: 675/1000 Train loss: 0.024519\n",
      "Epoch: 676/1000 Train loss: 0.024483\n",
      "Epoch: 677/1000 Train loss: 0.024447\n",
      "Epoch: 678/1000 Train loss: 0.024411\n",
      "Epoch: 679/1000 Train loss: 0.024376\n",
      "Epoch: 680/1000 Train loss: 0.024341\n",
      "Epoch: 681/1000 Train loss: 0.024305\n",
      "Epoch: 682/1000 Train loss: 0.024270\n",
      "Epoch: 683/1000 Train loss: 0.024235\n",
      "Epoch: 684/1000 Train loss: 0.024200\n",
      "Epoch: 685/1000 Train loss: 0.024165\n",
      "Epoch: 686/1000 Train loss: 0.024130\n",
      "Epoch: 687/1000 Train loss: 0.024096\n",
      "Epoch: 688/1000 Train loss: 0.024061\n",
      "Epoch: 689/1000 Train loss: 0.024027\n",
      "Epoch: 690/1000 Train loss: 0.023992\n",
      "Epoch: 691/1000 Train loss: 0.023958\n",
      "Epoch: 692/1000 Train loss: 0.023924\n",
      "Epoch: 693/1000 Train loss: 0.023890\n",
      "Epoch: 694/1000 Train loss: 0.023856\n",
      "Epoch: 695/1000 Train loss: 0.023822\n",
      "Epoch: 696/1000 Train loss: 0.023788\n",
      "Epoch: 697/1000 Train loss: 0.023754\n",
      "Epoch: 698/1000 Train loss: 0.023721\n",
      "Epoch: 699/1000 Train loss: 0.023687\n",
      "Epoch: 700/1000 Train loss: 0.023654\n",
      "Epoch: 701/1000 Train loss: 0.023621\n",
      "Epoch: 702/1000 Train loss: 0.023587\n",
      "Epoch: 703/1000 Train loss: 0.023554\n",
      "Epoch: 704/1000 Train loss: 0.023521\n",
      "Epoch: 705/1000 Train loss: 0.023488\n",
      "Epoch: 706/1000 Train loss: 0.023456\n",
      "Epoch: 707/1000 Train loss: 0.023423\n",
      "Epoch: 708/1000 Train loss: 0.023391\n",
      "Epoch: 709/1000 Train loss: 0.023417\n",
      "Epoch: 710/1000 Train loss: 0.023404\n",
      "Epoch: 711/1000 Train loss: 0.023372\n",
      "Epoch: 712/1000 Train loss: 0.023340\n",
      "Epoch: 713/1000 Train loss: 0.023308\n",
      "Epoch: 714/1000 Train loss: 0.023276\n",
      "Epoch: 715/1000 Train loss: 0.023244\n",
      "Epoch: 716/1000 Train loss: 0.023212\n",
      "Epoch: 717/1000 Train loss: 0.023180\n",
      "Epoch: 718/1000 Train loss: 0.023148\n",
      "Epoch: 719/1000 Train loss: 0.023116\n",
      "Epoch: 720/1000 Train loss: 0.023084\n",
      "Epoch: 721/1000 Train loss: 0.023053\n",
      "Epoch: 722/1000 Train loss: 0.023021\n",
      "Epoch: 723/1000 Train loss: 0.022990\n",
      "Epoch: 724/1000 Train loss: 0.022959\n",
      "Epoch: 725/1000 Train loss: 0.022927\n",
      "Epoch: 726/1000 Train loss: 0.022896\n",
      "Epoch: 727/1000 Train loss: 0.022865\n",
      "Epoch: 728/1000 Train loss: 0.022834\n",
      "Epoch: 729/1000 Train loss: 0.022803\n",
      "Epoch: 730/1000 Train loss: 0.022772\n",
      "Epoch: 731/1000 Train loss: 0.022742\n",
      "Epoch: 732/1000 Train loss: 0.022711\n",
      "Epoch: 733/1000 Train loss: 0.022680\n",
      "Epoch: 734/1000 Train loss: 0.022650\n",
      "Epoch: 735/1000 Train loss: 0.022620\n",
      "Epoch: 736/1000 Train loss: 0.022589\n",
      "Epoch: 737/1000 Train loss: 0.022559\n",
      "Epoch: 738/1000 Train loss: 0.022529\n",
      "Epoch: 739/1000 Train loss: 0.022499\n",
      "Epoch: 740/1000 Train loss: 0.022469\n",
      "Epoch: 741/1000 Train loss: 0.022439\n",
      "Epoch: 742/1000 Train loss: 0.022409\n",
      "Epoch: 743/1000 Train loss: 0.022379\n",
      "Epoch: 744/1000 Train loss: 0.022349\n",
      "Epoch: 745/1000 Train loss: 0.022320\n",
      "Epoch: 746/1000 Train loss: 0.022290\n",
      "Epoch: 747/1000 Train loss: 0.022261\n",
      "Epoch: 748/1000 Train loss: 0.022231\n",
      "Epoch: 749/1000 Train loss: 0.022202\n",
      "Epoch: 750/1000 Train loss: 0.022173\n",
      "Epoch: 751/1000 Train loss: 0.022144\n",
      "Epoch: 752/1000 Train loss: 0.022115\n",
      "Epoch: 753/1000 Train loss: 0.022086\n",
      "Epoch: 754/1000 Train loss: 0.022057\n",
      "Epoch: 755/1000 Train loss: 0.022028\n",
      "Epoch: 756/1000 Train loss: 0.022000\n",
      "Epoch: 757/1000 Train loss: 0.021971\n",
      "Epoch: 758/1000 Train loss: 0.021942\n",
      "Epoch: 759/1000 Train loss: 0.021914\n",
      "Epoch: 760/1000 Train loss: 0.021886\n",
      "Epoch: 761/1000 Train loss: 0.021883\n",
      "Epoch: 762/1000 Train loss: 0.021862\n",
      "Epoch: 763/1000 Train loss: 0.021834\n",
      "Epoch: 764/1000 Train loss: 0.021806\n",
      "Epoch: 765/1000 Train loss: 0.021778\n",
      "Epoch: 766/1000 Train loss: 0.021750\n",
      "Epoch: 767/1000 Train loss: 0.021722\n",
      "Epoch: 768/1000 Train loss: 0.021694\n",
      "Epoch: 769/1000 Train loss: 0.021666\n",
      "Epoch: 770/1000 Train loss: 0.021639\n",
      "Epoch: 771/1000 Train loss: 0.021611\n",
      "Epoch: 772/1000 Train loss: 0.021583\n",
      "Epoch: 773/1000 Train loss: 0.021556\n",
      "Epoch: 774/1000 Train loss: 0.021528\n",
      "Epoch: 775/1000 Train loss: 0.021501\n",
      "Epoch: 776/1000 Train loss: 0.021473\n",
      "Epoch: 777/1000 Train loss: 0.021446\n",
      "Epoch: 778/1000 Train loss: 0.021419\n",
      "Epoch: 779/1000 Train loss: 0.021391\n",
      "Epoch: 780/1000 Train loss: 0.021364\n",
      "Epoch: 781/1000 Train loss: 0.021337\n",
      "Epoch: 782/1000 Train loss: 0.021310\n",
      "Epoch: 783/1000 Train loss: 0.021283\n",
      "Epoch: 784/1000 Train loss: 0.021257\n",
      "Epoch: 785/1000 Train loss: 0.021230\n",
      "Epoch: 786/1000 Train loss: 0.021203\n",
      "Epoch: 787/1000 Train loss: 0.021176\n",
      "Epoch: 788/1000 Train loss: 0.021150\n",
      "Epoch: 789/1000 Train loss: 0.021123\n",
      "Epoch: 790/1000 Train loss: 0.021097\n",
      "Epoch: 791/1000 Train loss: 0.021070\n",
      "Epoch: 792/1000 Train loss: 0.021044\n",
      "Epoch: 793/1000 Train loss: 0.021018\n",
      "Epoch: 794/1000 Train loss: 0.020992\n",
      "Epoch: 795/1000 Train loss: 0.020965\n",
      "Epoch: 796/1000 Train loss: 0.020939\n",
      "Epoch: 797/1000 Train loss: 0.020913\n",
      "Epoch: 798/1000 Train loss: 0.020887\n",
      "Epoch: 799/1000 Train loss: 0.020862\n",
      "Epoch: 800/1000 Train loss: 0.020836\n",
      "Epoch: 801/1000 Train loss: 0.020810\n",
      "Epoch: 802/1000 Train loss: 0.020785\n",
      "Epoch: 803/1000 Train loss: 0.020759\n",
      "Epoch: 804/1000 Train loss: 0.020733\n",
      "Epoch: 805/1000 Train loss: 0.020708\n",
      "Epoch: 806/1000 Train loss: 0.020683\n",
      "Epoch: 807/1000 Train loss: 0.020657\n",
      "Epoch: 808/1000 Train loss: 0.020632\n",
      "Epoch: 809/1000 Train loss: 0.020607\n",
      "Epoch: 810/1000 Train loss: 0.020581\n",
      "Epoch: 811/1000 Train loss: 0.020556\n",
      "Epoch: 812/1000 Train loss: 0.020534\n",
      "Epoch: 813/1000 Train loss: 0.020528\n",
      "Epoch: 814/1000 Train loss: 0.020517\n",
      "Epoch: 815/1000 Train loss: 0.020511\n",
      "Epoch: 816/1000 Train loss: 0.020487\n",
      "Epoch: 817/1000 Train loss: 0.020462\n",
      "Epoch: 818/1000 Train loss: 0.020438\n",
      "Epoch: 819/1000 Train loss: 0.020413\n",
      "Epoch: 820/1000 Train loss: 0.020388\n",
      "Epoch: 821/1000 Train loss: 0.020364\n",
      "Epoch: 822/1000 Train loss: 0.020339\n",
      "Epoch: 823/1000 Train loss: 0.020315\n",
      "Epoch: 824/1000 Train loss: 0.020290\n",
      "Epoch: 825/1000 Train loss: 0.020266\n",
      "Epoch: 826/1000 Train loss: 0.020242\n",
      "Epoch: 827/1000 Train loss: 0.020218\n",
      "Epoch: 828/1000 Train loss: 0.020193\n",
      "Epoch: 829/1000 Train loss: 0.020169\n",
      "Epoch: 830/1000 Train loss: 0.020145\n",
      "Epoch: 831/1000 Train loss: 0.020121\n",
      "Epoch: 832/1000 Train loss: 0.020097\n",
      "Epoch: 833/1000 Train loss: 0.020073\n",
      "Epoch: 834/1000 Train loss: 0.020050\n",
      "Epoch: 835/1000 Train loss: 0.020026\n",
      "Epoch: 836/1000 Train loss: 0.020002\n",
      "Epoch: 837/1000 Train loss: 0.019979\n",
      "Epoch: 838/1000 Train loss: 0.019955\n",
      "Epoch: 839/1000 Train loss: 0.019931\n",
      "Epoch: 840/1000 Train loss: 0.019908\n",
      "Epoch: 841/1000 Train loss: 0.019885\n",
      "Epoch: 842/1000 Train loss: 0.019861\n",
      "Epoch: 843/1000 Train loss: 0.019838\n",
      "Epoch: 844/1000 Train loss: 0.019815\n",
      "Epoch: 845/1000 Train loss: 0.019791\n",
      "Epoch: 846/1000 Train loss: 0.019768\n",
      "Epoch: 847/1000 Train loss: 0.019745\n",
      "Epoch: 848/1000 Train loss: 0.019722\n",
      "Epoch: 849/1000 Train loss: 0.019699\n",
      "Epoch: 850/1000 Train loss: 0.019676\n",
      "Epoch: 851/1000 Train loss: 0.019653\n",
      "Epoch: 852/1000 Train loss: 0.019630\n",
      "Epoch: 853/1000 Train loss: 0.019607\n",
      "Epoch: 854/1000 Train loss: 0.019585\n",
      "Epoch: 855/1000 Train loss: 0.019562\n",
      "Epoch: 856/1000 Train loss: 0.019539\n",
      "Epoch: 857/1000 Train loss: 0.019517\n",
      "Epoch: 858/1000 Train loss: 0.019494\n",
      "Epoch: 859/1000 Train loss: 0.019472\n",
      "Epoch: 860/1000 Train loss: 0.019449\n",
      "Epoch: 861/1000 Train loss: 0.019427\n",
      "Epoch: 862/1000 Train loss: 0.019405\n",
      "Epoch: 863/1000 Train loss: 0.019382\n",
      "Epoch: 864/1000 Train loss: 0.019360\n",
      "Epoch: 865/1000 Train loss: 0.019338\n",
      "Epoch: 866/1000 Train loss: 0.019316\n",
      "Epoch: 867/1000 Train loss: 0.019294\n",
      "Epoch: 868/1000 Train loss: 0.019272\n",
      "Epoch: 869/1000 Train loss: 0.019250\n",
      "Epoch: 870/1000 Train loss: 0.019228\n",
      "Epoch: 871/1000 Train loss: 0.019206\n",
      "Epoch: 872/1000 Train loss: 0.019184\n",
      "Epoch: 873/1000 Train loss: 0.019163\n",
      "Epoch: 874/1000 Train loss: 0.019141\n",
      "Epoch: 875/1000 Train loss: 0.019119\n",
      "Epoch: 876/1000 Train loss: 0.019098\n",
      "Epoch: 877/1000 Train loss: 0.019076\n",
      "Epoch: 878/1000 Train loss: 0.019055\n",
      "Epoch: 879/1000 Train loss: 0.019033\n",
      "Epoch: 880/1000 Train loss: 0.019012\n",
      "Epoch: 881/1000 Train loss: 0.018990\n",
      "Epoch: 882/1000 Train loss: 0.018969\n",
      "Epoch: 883/1000 Train loss: 0.018948\n",
      "Epoch: 884/1000 Train loss: 0.018926\n",
      "Epoch: 885/1000 Train loss: 0.018905\n",
      "Epoch: 886/1000 Train loss: 0.018884\n",
      "Epoch: 887/1000 Train loss: 0.018863\n",
      "Epoch: 888/1000 Train loss: 0.018849\n",
      "Epoch: 889/1000 Train loss: 0.018860\n",
      "Epoch: 890/1000 Train loss: 0.018844\n",
      "Epoch: 891/1000 Train loss: 0.018824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 892/1000 Train loss: 0.018803\n",
      "Epoch: 893/1000 Train loss: 0.018782\n",
      "Epoch: 894/1000 Train loss: 0.018762\n",
      "Epoch: 895/1000 Train loss: 0.018741\n",
      "Epoch: 896/1000 Train loss: 0.018720\n",
      "Epoch: 897/1000 Train loss: 0.018699\n",
      "Epoch: 898/1000 Train loss: 0.018679\n",
      "Epoch: 899/1000 Train loss: 0.018658\n",
      "Epoch: 900/1000 Train loss: 0.018637\n",
      "Epoch: 901/1000 Train loss: 0.018617\n",
      "Epoch: 902/1000 Train loss: 0.018596\n",
      "Epoch: 903/1000 Train loss: 0.018576\n",
      "Epoch: 904/1000 Train loss: 0.018555\n",
      "Epoch: 905/1000 Train loss: 0.018535\n",
      "Epoch: 906/1000 Train loss: 0.018515\n",
      "Epoch: 907/1000 Train loss: 0.018494\n",
      "Epoch: 908/1000 Train loss: 0.018474\n",
      "Epoch: 909/1000 Train loss: 0.018454\n",
      "Epoch: 910/1000 Train loss: 0.018434\n",
      "Epoch: 911/1000 Train loss: 0.018414\n",
      "Epoch: 912/1000 Train loss: 0.018394\n",
      "Epoch: 913/1000 Train loss: 0.018374\n",
      "Epoch: 914/1000 Train loss: 0.018354\n",
      "Epoch: 915/1000 Train loss: 0.018334\n",
      "Epoch: 916/1000 Train loss: 0.018314\n",
      "Epoch: 917/1000 Train loss: 0.018294\n",
      "Epoch: 918/1000 Train loss: 0.018274\n",
      "Epoch: 919/1000 Train loss: 0.018255\n",
      "Epoch: 920/1000 Train loss: 0.018235\n",
      "Epoch: 921/1000 Train loss: 0.018215\n",
      "Epoch: 922/1000 Train loss: 0.018196\n",
      "Epoch: 923/1000 Train loss: 0.018176\n",
      "Epoch: 924/1000 Train loss: 0.018156\n",
      "Epoch: 925/1000 Train loss: 0.018137\n",
      "Epoch: 926/1000 Train loss: 0.018117\n",
      "Epoch: 927/1000 Train loss: 0.018098\n",
      "Epoch: 928/1000 Train loss: 0.018079\n",
      "Epoch: 929/1000 Train loss: 0.018059\n",
      "Epoch: 930/1000 Train loss: 0.018040\n",
      "Epoch: 931/1000 Train loss: 0.018021\n",
      "Epoch: 932/1000 Train loss: 0.018002\n",
      "Epoch: 933/1000 Train loss: 0.017982\n",
      "Epoch: 934/1000 Train loss: 0.017963\n",
      "Epoch: 935/1000 Train loss: 0.017944\n",
      "Epoch: 936/1000 Train loss: 0.017925\n",
      "Epoch: 937/1000 Train loss: 0.017906\n",
      "Epoch: 938/1000 Train loss: 0.017887\n",
      "Epoch: 939/1000 Train loss: 0.017868\n",
      "Epoch: 940/1000 Train loss: 0.017849\n",
      "Epoch: 941/1000 Train loss: 0.017830\n",
      "Epoch: 942/1000 Train loss: 0.017812\n",
      "Epoch: 943/1000 Train loss: 0.017793\n",
      "Epoch: 944/1000 Train loss: 0.017774\n",
      "Epoch: 945/1000 Train loss: 0.017755\n",
      "Epoch: 946/1000 Train loss: 0.017737\n",
      "Epoch: 947/1000 Train loss: 0.017718\n",
      "Epoch: 948/1000 Train loss: 0.017699\n",
      "Epoch: 949/1000 Train loss: 0.017681\n",
      "Epoch: 950/1000 Train loss: 0.017662\n",
      "Epoch: 951/1000 Train loss: 0.017644\n",
      "Epoch: 952/1000 Train loss: 0.017626\n",
      "Epoch: 953/1000 Train loss: 0.017607\n",
      "Epoch: 954/1000 Train loss: 0.017589\n",
      "Epoch: 955/1000 Train loss: 0.017571\n",
      "Epoch: 956/1000 Train loss: 0.017590\n",
      "Epoch: 957/1000 Train loss: 0.017579\n",
      "Epoch: 958/1000 Train loss: 0.017560\n",
      "Epoch: 959/1000 Train loss: 0.017542\n",
      "Epoch: 960/1000 Train loss: 0.017524\n",
      "Epoch: 961/1000 Train loss: 0.017506\n",
      "Epoch: 962/1000 Train loss: 0.017488\n",
      "Epoch: 963/1000 Train loss: 0.017470\n",
      "Epoch: 964/1000 Train loss: 0.017452\n",
      "Epoch: 965/1000 Train loss: 0.017434\n",
      "Epoch: 966/1000 Train loss: 0.017416\n",
      "Epoch: 967/1000 Train loss: 0.017398\n",
      "Epoch: 968/1000 Train loss: 0.017380\n",
      "Epoch: 969/1000 Train loss: 0.017362\n",
      "Epoch: 970/1000 Train loss: 0.017345\n",
      "Epoch: 971/1000 Train loss: 0.017327\n",
      "Epoch: 972/1000 Train loss: 0.017309\n",
      "Epoch: 973/1000 Train loss: 0.017291\n",
      "Epoch: 974/1000 Train loss: 0.017274\n",
      "Epoch: 975/1000 Train loss: 0.017256\n",
      "Epoch: 976/1000 Train loss: 0.017239\n",
      "Epoch: 977/1000 Train loss: 0.017221\n",
      "Epoch: 978/1000 Train loss: 0.017204\n",
      "Epoch: 979/1000 Train loss: 0.017186\n",
      "Epoch: 980/1000 Train loss: 0.017169\n",
      "Epoch: 981/1000 Train loss: 0.017151\n",
      "Epoch: 982/1000 Train loss: 0.017134\n",
      "Epoch: 983/1000 Train loss: 0.017117\n",
      "Epoch: 984/1000 Train loss: 0.017099\n",
      "Epoch: 985/1000 Train loss: 0.017082\n",
      "Epoch: 986/1000 Train loss: 0.017065\n",
      "Epoch: 987/1000 Train loss: 0.017048\n",
      "Epoch: 988/1000 Train loss: 0.017030\n",
      "Epoch: 989/1000 Train loss: 0.017013\n",
      "Epoch: 990/1000 Train loss: 0.016996\n",
      "Epoch: 991/1000 Train loss: 0.016979\n",
      "Epoch: 992/1000 Train loss: 0.016962\n",
      "Epoch: 993/1000 Train loss: 0.016945\n",
      "Epoch: 994/1000 Train loss: 0.016928\n",
      "Epoch: 995/1000 Train loss: 0.016911\n",
      "Epoch: 996/1000 Train loss: 0.016894\n",
      "Epoch: 997/1000 Train loss: 0.016878\n",
      "Epoch: 998/1000 Train loss: 0.016861\n",
      "Epoch: 999/1000 Train loss: 0.016844\n",
      "Epoch: 1000/1000 Train loss: 0.016827\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "\n",
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "#         state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x, y in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "        # Print info for every/each iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)))\n",
    "        \n",
    "#             ################## Validation\n",
    "#             state_valid = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "#             acc_batch = []\n",
    "#             loss_batch = []    \n",
    "#             # Loop over batches\n",
    "#             for x, y in get_batches(X_valid_norm, Y_valid_onehot, batch_size):\n",
    "\n",
    "#                 # Feed dictionary\n",
    "#                 feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0, initial_state : state_valid}\n",
    "\n",
    "#                 # Loss\n",
    "#                 loss, state_valid, acc = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "#                 acc_batch.append(acc)\n",
    "#                 loss_batch.append(loss)\n",
    "\n",
    "#             # Store\n",
    "#             valid_acc.append(np.mean(acc_batch))\n",
    "#             valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "#         # Print info for every iter/epoch\n",
    "#         print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "#               \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "#               \"Valid loss: {:.6f}\".format(np.mean(valid_loss)),\n",
    "#               \"Train acc: {:6f}\".format(np.mean(train_acc)),\n",
    "#               \"Valid acc: {:.6f}\".format(np.mean(valid_acc)))\n",
    "                \n",
    "#     saver.save(sess,\"checkpoints/dcnn-lstm-har.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_loss, label='har train_loss')\n",
    "mplot.plot(valid_loss, label='har valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n61rwnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f964801abe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as mplot\n",
    "mplot.plot(train_acc, label='har train_acc')\n",
    "mplot.plot(valid_acc, label='har valid_acc')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dcnn-lstm-har.ckpt\n",
      "Test loss: 1.733549 Test acc: 0.251806\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    ################## Test\n",
    "    state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    acc_batch = []\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for x, y in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0, initial_state : state}\n",
    "\n",
    "        # Loss\n",
    "        loss, state, acc = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Store\n",
    "    test_acc.append(np.mean(acc_batch))\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)),\n",
    "          \"Test acc: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

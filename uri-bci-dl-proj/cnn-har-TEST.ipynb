{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "# test and train read\n",
    "X_train_valid, Y_train_valid, _ = read_data(data_path=\"../../datasets/har/har-data/\", split=\"train\")\n",
    "X_test, Y_test, _ = read_data(data_path=\"../../datasets/har/har-data/\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing/standardizing the input data features\n",
    "X_train_valid_norm, X_test_norm = standardize(test=X_test, train=X_train_valid)\n",
    "Y_train_valid_onehot = one_hot(labels=Y_train_valid.reshape(-1), n_class=6) \n",
    "Y_test_onehot = one_hot(labels=Y_test.reshape(-1), n_class=6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_norm, X_valid_norm, Y_train_onehot, Y_valid_onehot = train_test_split(X_train_valid_norm, \n",
    "                                                                              Y_train_valid_onehot,\n",
    "                                                                              test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len, n_channels 66 128 9\n",
      "n_classes 6\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "# NWC for signal: N is batch size, W is the width/sequence length, and C is the number of channels\n",
    "# NHWC for images: This is the same as signals and H stands for height\n",
    "N, W, C = X_train_norm.shape[0]//100, X_train_norm.shape[1], X_train_norm.shape[2]\n",
    "batch_size, seq_len, n_channels = N, W, C\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = Y_train_valid.max(axis=0)\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.0001 #1e-4\n",
    "epochs = 100 # num iterations for updating model\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output and hyperpaprameters NP tensors to feed into TF tensors: NP2TF tensors for computation\n",
    "# Input and output and hyperpaprameters tensors to feed into the tensor flow framwork\n",
    "Xin = tf.placeholder(dtype=tf.float32, shape=[None, seq_len, n_channels], name = 'Xin')\n",
    "keep_prob_= tf.placeholder(dtype=tf.float32, name = 'keep_prob_', shape=None)\n",
    "learning_rate_ = tf.placeholder(dtype=tf.float32, name = 'learning_rate_', shape=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Successfully implemented!\n",
    "# # Simple example of running it;\n",
    "# # This is a simple example of using placeholder to define a tensor and feed it into the session\n",
    "# # Also doing this, we can see how it might work using the session\n",
    "# # tensor, varilable, and operation for generating an output for high performance computation\n",
    "# Xexample = tf.placeholder(dtype=tf.float32, shape=(1024, 1024), name=None)\n",
    "# Yexample = tf.matmul(a=Xexample, b=Xexample, name=None)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "# #   print(sess.run(y))  # ERROR: will fail because x was not fed.\n",
    "\n",
    "# #     rand_array = np.random.rand(1024, 1024)\n",
    "# #     Xin = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=Xexample.shape, stddev=1.)    \n",
    "#     Xin = np.random.normal(loc=0., scale=1., size=Xexample.shape)\n",
    "#     print(Xexample.shape)\n",
    "#     print(Xin.shape, Xin.dtype)\n",
    "#     Yout = sess.run(fetches=Yexample, feed_dict={Xexample: Xin})\n",
    "#     print(Yout.shape, Yout.dtype, Yexample.shape, Yexample.dtype)  # Will succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successfull!\n",
    "# print(inputs_.shape, inputs_.dtype, inputs_.shape[0], inputs_.shape[1], inputs_.shape[2])\n",
    "# # NWC=?, 128, 9\n",
    "# width, in_ch, out_ch = inputs_.shape[1]//4, inputs_.shape[2], inputs_.shape[2]*2\n",
    "# # print(width, in_ch, out_ch)\n",
    "# filter_shape=[width.value, in_ch.value, out_ch.value]\n",
    "# print(filter_shape)\n",
    "# init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, stddev=1., shape=filter_shape)\n",
    "# # print(init_val.shape, init_val.dtype)\n",
    "# # filters = tf.get_variable(shape=[width, in_ch, out_ch], name='conv1d1', dtype=tf.float32, trainable=True)\n",
    "# filter1 = tf.Variable(initial_value=init_val, dtype=tf.float32, name=None, trainable=True)\n",
    "# conv1d = tf.nn.conv1d(data_format='NWC', filters=filter1, name=None, padding='SAME', stride=2,\n",
    "#                       use_cudnn_on_gpu=True, value=inputs_)\n",
    "# print(conv1d.shape, conv1d.dtype)\n",
    "# filterT = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "# print(filterT.shape, filterT.dtype)\n",
    "# output_shape = [batch_size, inputs_.shape[1].value, inputs_.shape[2].value]\n",
    "# print(output_shape)\n",
    "# conv1dT = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=filterT, name=None, output_shape=output_shape, \n",
    "#                                          padding='SAME', stride=2, value=conv1d)\n",
    "# print(conv1dT.shape, conv1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successfull!\n",
    "# # This is explaining a single operation on input tensors and varilables to generate the output tensor\n",
    "# # The pull request is open as of this moment, so the API and behavior can and probably will change. \n",
    "# # Some feature that one might expect from conv1d_transpose aren't supported:\n",
    "\n",
    "# # output_shape requires batch size to be known statically, can't pass -1;\n",
    "# # on the other hand, output shape is dynamic (this explains None dimension).\n",
    "# # Also, the kernel_width=7 expects in_width=255, not 256. \n",
    "# # Should make kernel_width less than 4 to match in_width=256. \n",
    "# # The result is this demo code:\n",
    "# Xexample = tf.placeholder(shape=[None, 256, 16], dtype=tf.float32, name=None)\n",
    "# print(Xexample.shape, Xexample.dtype)\n",
    "# [kernel_width, output_depth, input_depth] = [Xexample.shape[1].value//4, Xexample.shape[2].value//2, \n",
    "#                                              Xexample.shape[2].value]\n",
    "# shape = [kernel_width, output_depth, input_depth] \n",
    "# print(shape)\n",
    "# init_val = tf.random_normal(shape=shape, dtype=tf.float32, mean=0., name=None, stddev=1.)\n",
    "# Wexample = tf.Variable(initial_value=init_val, dtype=tf.float32, name=None, trainable=True)\n",
    "# print(Wexample.shape, Wexample.dtype)\n",
    "# # output shape is based on NWC: batch size, width, and Channels\n",
    "# stride = 4\n",
    "# shape = [1, Xexample.shape[1].value*stride, Wexample.shape[1].value] \n",
    "# print(shape)\n",
    "# Yexample = tf.contrib.nn.conv1d_transpose(value=Xexample, filter=Wexample, output_shape=shape, data_format='NWC',\n",
    "#                                      name=None, stride=4, padding='SAME')\n",
    "# # both paddings 'SAME' or 'VALID' work # any difference?\n",
    "# print(Yexample.shape, Yexample.dtype)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(fetches=tf.global_variables_initializer())\n",
    "#     print(Xexample.shape)\n",
    "#     shape = [Yexample.shape[0].value, Xexample.shape[1].value, Xexample.shape[2].value]\n",
    "#     print(shape)\n",
    "#     Xin = np.random.normal(loc=0., scale=1., size=shape) \n",
    "#     Yout = sess.run(fetches=Yexample, feed_dict={Xexample: Xin})\n",
    "#     print(Yout.shape, Yout.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 9) <dtype: 'float32'>\n",
      "(32, 9, 18) <dtype: 'float32_ref'>\n",
      "(?, 64, 18) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xin.shape, Xin.dtype)\n",
    "# shape = [kernel_width, input_depth, output_depth] \n",
    "shape = [Xin.shape[1].value//4, Xin.shape[2].value, Xin.shape[2].value*2]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W1 = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W1.shape, W1.dtype)\n",
    "Xconv1 = tf.nn.conv1d(data_format='NWC', filters=W1, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=Xin)\n",
    "Xconv1 = tf.nn.relu(features=Xconv1, name=None)\n",
    "print(Xconv1.shape, Xconv1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 64, 18) <dtype: 'float32'>\n",
      "(16, 18, 36) <dtype: 'float32_ref'>\n",
      "(?, 32, 36) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv1.shape, Xconv1.dtype)\n",
    "shape = [Xconv1.shape[1].value//4, Xconv1.shape[2].value, Xconv1.shape[2].value*2]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W2 = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W2.shape, W2.dtype)\n",
    "Xconv2 = tf.nn.conv1d(data_format='NWC', filters=W2, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=Xconv1)\n",
    "Xconv2 = tf.nn.relu(features=Xconv2, name=None)\n",
    "print(Xconv2.shape, Xconv2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 36) <dtype: 'float32'>\n",
      "(8, 36, 72) <dtype: 'float32_ref'>\n",
      "(?, 16, 72) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv2.shape, Xconv2.dtype)\n",
    "shape = [Xconv2.shape[1].value//4, Xconv2.shape[2].value, Xconv2.shape[2].value*2]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W3 = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W3.shape, W3.dtype)\n",
    "Xconv3 = tf.nn.conv1d(data_format='NWC', filters=W3, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=Xconv2)\n",
    "Xconv3 = tf.nn.relu(features=Xconv3, name=None)\n",
    "print(Xconv3.shape, Xconv3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 72) <dtype: 'float32'>\n",
      "(8, 36, 72) <dtype: 'float32_ref'>\n",
      "[66, 32, 36]\n",
      "(66, 32, 36) <dtype: 'float32'>\n",
      "(?, 32, 36) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv3.shape, Xconv3.dtype)\n",
    "shape = [Xconv2.shape[1].value//4, Xconv2.shape[2].value, Xconv2.shape[2].value*2]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W3T = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W3T.shape, W3T.dtype)\n",
    "# output shape should be NWC equal to Xconv2\n",
    "out_shape = [batch_size, Xconv2.shape[1].value, Xconv2.shape[2].value] # NWC, only N should be set (experimental)\n",
    "print(out_shape)\n",
    "Xconv2_ = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=W3T, name=None, padding='SAME', stride=2, \n",
    "                                         value=Xconv3, output_shape=out_shape)\n",
    "Xconv2_ = tf.nn.relu(features=Xconv2_, name=None)\n",
    "print(Xconv2_.shape, Xconv2_.dtype)\n",
    "print(Xconv2.shape, Xconv2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 32, 36) <dtype: 'float32'>\n",
      "(16, 18, 36) <dtype: 'float32_ref'>\n",
      "[66, 64, 18]\n",
      "(66, 64, 18) <dtype: 'float32'>\n",
      "(?, 64, 18) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv2_.shape, Xconv2_.dtype)\n",
    "shape = [Xconv1.shape[1].value//4, Xconv1.shape[2].value, Xconv1.shape[2].value*2]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W2T = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W2T.shape, W2T.dtype)\n",
    "# output shape should be NWC equal to Xconv1\n",
    "out_shape = [batch_size, Xconv1.shape[1].value, Xconv1.shape[2].value] # NWC, only N should be set (experimental)\n",
    "print(out_shape)\n",
    "Xconv1_ = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=W2T, name=None, padding='SAME', stride=2, \n",
    "                                         value=Xconv2_, output_shape=out_shape)\n",
    "Xconv1_ = tf.nn.relu(features=Xconv1_, name=None)\n",
    "print(Xconv1_.shape, Xconv1_.dtype)\n",
    "print(Xconv1.shape, Xconv1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 64, 18) <dtype: 'float32'>\n",
      "(32, 9, 18) <dtype: 'float32_ref'>\n",
      "[66, 128, 9]\n",
      "(66, 128, 9) <dtype: 'float32'>\n",
      "(?, 128, 9) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv1_.shape, Xconv1_.dtype)\n",
    "shape = [Xin.shape[1].value//4, Xin.shape[2].value, Xin.shape[2].value*2]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W1T = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W1T.shape, W1T.dtype)\n",
    "# output shape should be NWC equal to Xin\n",
    "out_shape = [batch_size, Xin.shape[1].value, Xin.shape[2].value] # NWC, only N should be set (experimental)\n",
    "print(out_shape)\n",
    "Xout = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=W1T, name=None, padding='SAME', stride=2, \n",
    "                                         value=Xconv1_, output_shape=out_shape)\n",
    "# Xin_ = tf.nn.relu(features=Xin_, name=None)\n",
    "print(Xout.shape, Xout.dtype)\n",
    "print(Xin.shape, Xin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 128, 9) <dtype: 'float32'>\n",
      "() <dtype: 'float32'>\n",
      "optimizer name: \"Adam_3\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam_3/update_Variable_12/ApplyAdam\"\n",
      "input: \"^Adam_3/update_Variable_13/ApplyAdam\"\n",
      "input: \"^Adam_3/update_Variable_14/ApplyAdam\"\n",
      "input: \"^Adam_3/update_Variable_15/ApplyAdam\"\n",
      "input: \"^Adam_3/update_Variable_16/ApplyAdam\"\n",
      "input: \"^Adam_3/update_Variable_19/ApplyAdam\"\n",
      "input: \"^Adam_3/Assign\"\n",
      "input: \"^Adam_3/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "# cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost_tensor = tf.nn.sigmoid_cross_entropy_with_logits(labels=Xin, logits=Xout, name=None)\n",
    "print(cost_tensor.shape, cost_tensor.dtype)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "cost = tf.abs(name=None, x=cost)\n",
    "print(cost.shape, cost.dtype)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "print('optimizer', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train loss: 922495.687500 Valid loss: 932820.875000\n",
      "Epoch: 2/100 Train loss: 707970.500000 Valid loss: 715640.312500\n",
      "Epoch: 3/100 Train loss: 563612.000000 Valid loss: 569524.375000\n",
      "Epoch: 4/100 Train loss: 455472.312500 Valid loss: 460006.312500\n",
      "Epoch: 5/100 Train loss: 371090.187500 Valid loss: 376383.593750\n",
      "Epoch: 6/100 Train loss: 314066.031250 Valid loss: 319681.812500\n",
      "Epoch: 7/100 Train loss: 273076.656250 Valid loss: 278923.125000\n",
      "Epoch: 8/100 Train loss: 242141.343750 Valid loss: 248159.500000\n",
      "Epoch: 9/100 Train loss: 217934.109375 Valid loss: 224071.437500\n",
      "Epoch: 10/100 Train loss: 198436.984375 Valid loss: 204666.265625\n",
      "Epoch: 11/100 Train loss: 182402.796875 Valid loss: 188689.078125\n",
      "Epoch: 12/100 Train loss: 168927.296875 Valid loss: 175269.046875\n",
      "Epoch: 13/100 Train loss: 157446.078125 Valid loss: 163834.453125\n",
      "Epoch: 14/100 Train loss: 147528.531250 Valid loss: 153950.625000\n",
      "Epoch: 15/100 Train loss: 138875.500000 Valid loss: 145326.171875\n",
      "Epoch: 16/100 Train loss: 131249.062500 Valid loss: 137725.625000\n",
      "Epoch: 17/100 Train loss: 124469.656250 Valid loss: 130968.156250\n",
      "Epoch: 18/100 Train loss: 118400.726562 Valid loss: 124921.757812\n",
      "Epoch: 19/100 Train loss: 112930.406250 Valid loss: 119469.335938\n",
      "Epoch: 20/100 Train loss: 107980.593750 Valid loss: 114533.820312\n",
      "Epoch: 21/100 Train loss: 103479.515625 Valid loss: 110035.023438\n",
      "Epoch: 22/100 Train loss: 99354.570312 Valid loss: 105915.390625\n",
      "Epoch: 23/100 Train loss: 95560.679688 Valid loss: 102124.773438\n",
      "Epoch: 24/100 Train loss: 92058.625000 Valid loss: 98624.335938\n",
      "Epoch: 25/100 Train loss: 88817.289062 Valid loss: 95384.835938\n",
      "Epoch: 26/100 Train loss: 85802.085938 Valid loss: 92375.304688\n",
      "Epoch: 27/100 Train loss: 82996.351562 Valid loss: 89569.875000\n",
      "Epoch: 28/100 Train loss: 80371.000000 Valid loss: 86947.664062\n",
      "Epoch: 29/100 Train loss: 77912.023438 Valid loss: 84491.203125\n",
      "Epoch: 30/100 Train loss: 75602.359375 Valid loss: 82184.617188\n",
      "Epoch: 31/100 Train loss: 73438.093750 Valid loss: 80018.437500\n",
      "Epoch: 32/100 Train loss: 71397.031250 Valid loss: 77976.609375\n",
      "Epoch: 33/100 Train loss: 69467.882812 Valid loss: 76048.750000\n",
      "Epoch: 34/100 Train loss: 67644.984375 Valid loss: 74223.281250\n",
      "Epoch: 35/100 Train loss: 65916.984375 Valid loss: 72493.367188\n",
      "Epoch: 36/100 Train loss: 64275.511719 Valid loss: 70853.070312\n",
      "Epoch: 37/100 Train loss: 62714.984375 Valid loss: 69296.742188\n",
      "Epoch: 38/100 Train loss: 61234.164062 Valid loss: 67815.890625\n",
      "Epoch: 39/100 Train loss: 59824.210938 Valid loss: 66402.406250\n",
      "Epoch: 40/100 Train loss: 58477.261719 Valid loss: 65054.246094\n",
      "Epoch: 41/100 Train loss: 57186.425781 Valid loss: 63766.351562\n",
      "Epoch: 42/100 Train loss: 55956.382812 Valid loss: 62534.644531\n",
      "Epoch: 43/100 Train loss: 54785.621094 Valid loss: 61356.093750\n",
      "Epoch: 44/100 Train loss: 53660.792969 Valid loss: 60225.332031\n",
      "Epoch: 45/100 Train loss: 52579.648438 Valid loss: 59140.691406\n",
      "Epoch: 46/100 Train loss: 51540.773438 Valid loss: 58099.000000\n",
      "Epoch: 47/100 Train loss: 50542.726562 Valid loss: 57097.277344\n",
      "Epoch: 48/100 Train loss: 49583.839844 Valid loss: 56133.601562\n",
      "Epoch: 49/100 Train loss: 48662.269531 Valid loss: 55208.000000\n",
      "Epoch: 50/100 Train loss: 47773.347656 Valid loss: 54316.222656\n",
      "Epoch: 51/100 Train loss: 46915.292969 Valid loss: 53455.210938\n",
      "Epoch: 52/100 Train loss: 46087.515625 Valid loss: 52624.136719\n",
      "Epoch: 53/100 Train loss: 45289.843750 Valid loss: 51822.460938\n",
      "Epoch: 54/100 Train loss: 44519.367188 Valid loss: 51048.925781\n",
      "Epoch: 55/100 Train loss: 43771.503906 Valid loss: 50300.566406\n",
      "Epoch: 56/100 Train loss: 43048.097656 Valid loss: 49577.085938\n",
      "Epoch: 57/100 Train loss: 42349.726562 Valid loss: 48877.050781\n",
      "Epoch: 58/100 Train loss: 41676.011719 Valid loss: 48199.140625\n",
      "Epoch: 59/100 Train loss: 41020.703125 Valid loss: 47541.238281\n",
      "Epoch: 60/100 Train loss: 40384.964844 Valid loss: 46904.476562\n",
      "Epoch: 61/100 Train loss: 39779.218750 Valid loss: 46286.886719\n",
      "Epoch: 62/100 Train loss: 39197.593750 Valid loss: 45689.250000\n",
      "Epoch: 63/100 Train loss: 38619.671875 Valid loss: 45107.601562\n",
      "Epoch: 64/100 Train loss: 38071.917969 Valid loss: 44544.019531\n",
      "Epoch: 65/100 Train loss: 37528.832031 Valid loss: 43995.875000\n",
      "Epoch: 66/100 Train loss: 37003.503906 Valid loss: 43464.136719\n",
      "Epoch: 67/100 Train loss: 36487.117188 Valid loss: 42946.210938\n",
      "Epoch: 68/100 Train loss: 35984.031250 Valid loss: 42441.910156\n",
      "Epoch: 69/100 Train loss: 35497.687500 Valid loss: 41951.511719\n",
      "Epoch: 70/100 Train loss: 35020.558594 Valid loss: 41473.847656\n",
      "Epoch: 71/100 Train loss: 34556.007812 Valid loss: 41009.207031\n",
      "Epoch: 72/100 Train loss: 34103.972656 Valid loss: 40556.285156\n",
      "Epoch: 73/100 Train loss: 33661.347656 Valid loss: 40115.058594\n",
      "Epoch: 74/100 Train loss: 33231.472656 Valid loss: 39684.519531\n",
      "Epoch: 75/100 Train loss: 32814.261719 Valid loss: 39265.253906\n",
      "Epoch: 76/100 Train loss: 32407.021484 Valid loss: 38857.789062\n",
      "Epoch: 77/100 Train loss: 32008.996094 Valid loss: 38460.562500\n",
      "Epoch: 78/100 Train loss: 31618.423828 Valid loss: 38072.664062\n",
      "Epoch: 79/100 Train loss: 31238.779297 Valid loss: 37695.257812\n",
      "Epoch: 80/100 Train loss: 30870.101562 Valid loss: 37326.910156\n",
      "Epoch: 81/100 Train loss: 30511.388672 Valid loss: 36965.539062\n",
      "Epoch: 82/100 Train loss: 30167.193359 Valid loss: 36613.308594\n",
      "Epoch: 83/100 Train loss: 29834.332031 Valid loss: 36270.140625\n",
      "Epoch: 84/100 Train loss: 29499.394531 Valid loss: 35932.667969\n",
      "Epoch: 85/100 Train loss: 29174.673828 Valid loss: 35603.640625\n",
      "Epoch: 86/100 Train loss: 28854.554688 Valid loss: 35282.355469\n",
      "Epoch: 87/100 Train loss: 28542.427734 Valid loss: 34967.585938\n",
      "Epoch: 88/100 Train loss: 28231.558594 Valid loss: 34659.335938\n",
      "Epoch: 89/100 Train loss: 27938.361328 Valid loss: 34360.375000\n",
      "Epoch: 90/100 Train loss: 27648.925781 Valid loss: 34067.570312\n",
      "Epoch: 91/100 Train loss: 27368.271484 Valid loss: 33780.757812\n",
      "Epoch: 92/100 Train loss: 27092.007812 Valid loss: 33500.406250\n",
      "Epoch: 93/100 Train loss: 26815.177734 Valid loss: 33223.816406\n",
      "Epoch: 94/100 Train loss: 26550.515625 Valid loss: 32954.421875\n",
      "Epoch: 95/100 Train loss: 26293.578125 Valid loss: 32690.068359\n",
      "Epoch: 96/100 Train loss: 26044.335938 Valid loss: 32430.939453\n",
      "Epoch: 97/100 Train loss: 25791.378906 Valid loss: 32175.966797\n",
      "Epoch: 98/100 Train loss: 25538.443359 Valid loss: 31925.880859\n",
      "Epoch: 99/100 Train loss: 25290.222656 Valid loss: 31680.236328\n",
      "Epoch: 100/100 Train loss: 25054.058594 Valid loss: 31439.818359\n"
     ]
    }
   ],
   "source": [
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "train_loss, valid_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # my assumption is the local variables are the parameters and hyperparameters\n",
    "    # the globala variables are the one needed/required by the seession/graph to run\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Loop over batches\n",
    "        for X_train_norm_batch, _ in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {Xin : X_train_norm_batch, keep_prob_ : 0.95, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ = sess.run(fetches=[cost, optimizer], feed_dict = feed)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "            ################## Validation\n",
    "            loss_batch = []    \n",
    "            # Loop over batches\n",
    "            for X_valid_norm_batch, _ in get_batches(X_valid_norm, Y_valid_onehot, batch_size):\n",
    "\n",
    "                # Feed dictionary\n",
    "                feed = {Xin : X_valid_norm_batch, keep_prob_ : 1.0} \n",
    "                # no learning is needed therefore no learning rate is needed.\n",
    "\n",
    "                # Loss\n",
    "                loss = sess.run(fetches=[cost], feed_dict = feed)\n",
    "                # no learning is needed therefore no learning rate is needed.\n",
    "                # Therefore no optimization approach or backprop is needed either.\n",
    "                loss_batch.append(loss)\n",
    "\n",
    "            # Store\n",
    "            valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "        # Print info for every iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(valid_loss)))\n",
    "                \n",
    "    saver.save(sess,\"checkpoints/cnn-har-TEST.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "%matplotlib inline\n",
    "\n",
    "mplot.plot(train_loss, label='har train_loss')\n",
    "mplot.plot(valid_loss, label='har valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cnn-har-TEST.ckpt\n",
      "Test loss: 32326.398438\n"
     ]
    }
   ],
   "source": [
    "# initilize the saver which has already been initialized.\n",
    "test_loss = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    ################## Test\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for X_test_norm_batch, _ in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        # No learning/training is needed at this step\n",
    "        feed = {Xin: X_test_norm_batch, keep_prob_ : 1.0}\n",
    "\n",
    "        # Loss\n",
    "        # Only the computation of the cost is needed\n",
    "        loss = sess.run(fetches=[cost], feed_dict = feed)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Store\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

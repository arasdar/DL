{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'Intensification_Data'])\n",
      "(6480, 192, 64) float64\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "x_mat = spio.loadmat(file_name='../data/bci-sample-data/x.mat')\n",
    "print(x_mat.keys())\n",
    "x_data = x_mat['Intensification_Data']\n",
    "print(x_data.shape, x_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'Intensification_SType']) (6480, 192)\n",
      "(6480,) int64\n",
      "0.833333333333 0.166666666667\n"
     ]
    }
   ],
   "source": [
    "# Output data: class labels\n",
    "y_mat = spio.loadmat(file_name='../data/bci-sample-data/y.mat')\n",
    "print(y_mat.keys(), y_mat['Intensification_SType'].shape)\n",
    "y_labels = np.mean(y_mat['Intensification_SType'], axis=1, dtype=int)\n",
    "print(y_labels.shape, y_labels.dtype)\n",
    "print(np.mean(y_labels==0), np.mean(y_labels==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4536, 192, 64) (1944, 192, 64) (4536,) (1944,)\n",
      "float64 float64 int64 int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(x_data,\n",
    "                                                      y_labels,\n",
    "                                                      stratify = y_labels,\n",
    "                                                      random_state = 123,\n",
    "                                                      test_size=0.30)\n",
    "print(X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape)\n",
    "print(X_train.dtype, X_valid.dtype, Y_train.dtype, Y_valid.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4536, 192, 64) float64 (1944, 192, 64) float64\n"
     ]
    }
   ],
   "source": [
    "# Standardize/normalize train and test\n",
    "X_train_norm, X_valid_norm = standardize(train=X_train, test=X_valid)\n",
    "\n",
    "print(X_train_norm.shape, X_train_norm.dtype, \n",
    "X_valid_norm.shape, X_valid_norm.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len, n_channels 45 192 64\n",
      "n_classes 2\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "# Input data\n",
    "batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = int(y_labels[0].max(axis=0)+1)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.0001 #1e-4\n",
    "epochs = 100 # num iterations for updating model\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4536, 2) (1944, 2) (4536, 192, 64) (1944, 192, 64)\n"
     ]
    }
   ],
   "source": [
    "Y_train_onehot = one_hot(labels=Y_train, n_class=n_classes)\n",
    "Y_valid_onehot = one_hot(labels=Y_valid, n_class=n_classes)\n",
    "\n",
    "print(Y_train_onehot.shape, Y_valid_onehot.shape, \n",
    " X_train_norm.shape, X_valid_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Buffering/ placeholders to transfer the data from py to tf\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs_')\n",
    "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_')\n",
    "keep_prob_ = tf.placeholder(tf.float32, name = 'keep_prob_')\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate_')# Construct the LSTM inputs and LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 192, 128) (?, 96, 128)\n",
      "(?, 96, 256) (?, 48, 256)\n",
      "(?, 48, 512) (?, 24, 512)\n",
      "(?, 24, 1024) (?, 12, 1024)\n"
     ]
    }
   ],
   "source": [
    "# with graph.as_default():\n",
    "# (451, 3072, 13) float64 (193, 3072, 13) float64 (276, 3072, 13) float64\n",
    "# (4536, 192, 64) (1944, 192, 64) (4536,) (1944,)\n",
    "# (batch, 192, 64) --> (batch, 96, 128)\n",
    "conv1 = tf.layers.conv1d(inputs=inputs_, filters=128, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_1 = tf.nn.dropout(max_pool_1, keep_prob=keep_prob_)\n",
    "print(conv1.shape, max_pool_1.shape)\n",
    "\n",
    "# (batch, 96, 128) --> (batch, 48, 256)\n",
    "conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=256, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_2 = tf.nn.dropout(max_pool_2, keep_prob=keep_prob_)\n",
    "print(conv2.shape, max_pool_2.shape)\n",
    "\n",
    "# (batch, 48, 256) --> (batch, 24, 512)\n",
    "conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=512, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_3 = tf.nn.dropout(max_pool_3, keep_prob=keep_prob_)\n",
    "print(conv3.shape, max_pool_3.shape)\n",
    "\n",
    "# (batch, 24, 512) --> (batch, 12, 1024)\n",
    "conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=1024, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_4 = tf.nn.dropout(max_pool_4, keep_prob=keep_prob_)\n",
    "print(conv4.shape, max_pool_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 12, 1024) (?, 12288) (?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Flatten and add dropout + predicted output\n",
    "flat = tf.reshape(max_pool_4, (-1, 12*1024))\n",
    "flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "logits = tf.layers.dense(flat, n_classes)\n",
    "print(max_pool_4.shape, flat.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool) Tensor(\"accuracy:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "print(correct_pred, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train loss: 0.471488 Valid loss: 0.473677 Train acc: 0.829778 Valid acc: 0.833075\n",
      "Epoch: 2/100 Train loss: 0.456434 Valid loss: 0.466009 Train acc: 0.831333 Valid acc: 0.833070\n",
      "Epoch: 3/100 Train loss: 0.441879 Valid loss: 0.456761 Train acc: 0.832074 Valid acc: 0.833116\n",
      "Epoch: 4/100 Train loss: 0.424683 Valid loss: 0.444778 Train acc: 0.836556 Valid acc: 0.833836\n",
      "Epoch: 5/100 Train loss: 0.408135 Valid loss: 0.433275 Train acc: 0.841467 Valid acc: 0.835527\n",
      "Epoch: 6/100 Train loss: 0.391972 Valid loss: 0.423798 Train acc: 0.847407 Valid acc: 0.837553\n",
      "Epoch: 7/100 Train loss: 0.376409 Valid loss: 0.415873 Train acc: 0.853270 Valid acc: 0.839561\n",
      "Epoch: 8/100 Train loss: 0.361936 Valid loss: 0.409821 Train acc: 0.858528 Valid acc: 0.841142\n",
      "Epoch: 9/100 Train loss: 0.347894 Valid loss: 0.405094 Train acc: 0.864025 Valid acc: 0.842585\n",
      "Epoch: 10/100 Train loss: 0.334930 Valid loss: 0.401202 Train acc: 0.869156 Valid acc: 0.843800\n",
      "Epoch: 11/100 Train loss: 0.322238 Valid loss: 0.398595 Train acc: 0.874040 Valid acc: 0.844723\n",
      "Epoch: 12/100 Train loss: 0.310177 Valid loss: 0.397019 Train acc: 0.878982 Valid acc: 0.845407\n",
      "Epoch: 13/100 Train loss: 0.298583 Valid loss: 0.396515 Train acc: 0.883727 Valid acc: 0.845882\n",
      "Epoch: 14/100 Train loss: 0.287491 Valid loss: 0.396667 Train acc: 0.888349 Valid acc: 0.846215\n",
      "Epoch: 15/100 Train loss: 0.276853 Valid loss: 0.397306 Train acc: 0.892667 Valid acc: 0.846558\n",
      "Epoch: 16/100 Train loss: 0.266649 Valid loss: 0.398896 Train acc: 0.896903 Valid acc: 0.846724\n",
      "Epoch: 17/100 Train loss: 0.256949 Valid loss: 0.401026 Train acc: 0.900967 Valid acc: 0.846876\n",
      "Epoch: 18/100 Train loss: 0.247211 Valid loss: 0.403560 Train acc: 0.905037 Valid acc: 0.847125\n",
      "Epoch: 19/100 Train loss: 0.238024 Valid loss: 0.406503 Train acc: 0.909076 Valid acc: 0.847261\n",
      "Epoch: 20/100 Train loss: 0.229198 Valid loss: 0.410556 Train acc: 0.912778 Valid acc: 0.847371\n",
      "Epoch: 21/100 Train loss: 0.220682 Valid loss: 0.414778 Train acc: 0.916381 Valid acc: 0.847462\n",
      "Epoch: 22/100 Train loss: 0.212817 Valid loss: 0.419730 Train acc: 0.919586 Valid acc: 0.847537\n",
      "Epoch: 23/100 Train loss: 0.205444 Valid loss: 0.424701 Train acc: 0.922628 Valid acc: 0.847515\n",
      "Epoch: 24/100 Train loss: 0.198284 Valid loss: 0.429962 Train acc: 0.925546 Valid acc: 0.847570\n",
      "Epoch: 25/100 Train loss: 0.191752 Valid loss: 0.435412 Train acc: 0.928222 Valid acc: 0.847654\n",
      "Epoch: 26/100 Train loss: 0.185408 Valid loss: 0.441218 Train acc: 0.930803 Valid acc: 0.847701\n",
      "Epoch: 27/100 Train loss: 0.179596 Valid loss: 0.446631 Train acc: 0.933128 Valid acc: 0.847853\n",
      "Epoch: 28/100 Train loss: 0.174013 Valid loss: 0.452441 Train acc: 0.935325 Valid acc: 0.847947\n",
      "Epoch: 29/100 Train loss: 0.168622 Valid loss: 0.458176 Train acc: 0.937456 Valid acc: 0.847990\n",
      "Epoch: 30/100 Train loss: 0.163488 Valid loss: 0.463692 Train acc: 0.939459 Valid acc: 0.848132\n",
      "Epoch: 31/100 Train loss: 0.158654 Valid loss: 0.469327 Train acc: 0.941355 Valid acc: 0.848292\n",
      "Epoch: 32/100 Train loss: 0.154135 Valid loss: 0.475273 Train acc: 0.943125 Valid acc: 0.848412\n",
      "Epoch: 33/100 Train loss: 0.149762 Valid loss: 0.481289 Train acc: 0.944822 Valid acc: 0.848478\n",
      "Epoch: 34/100 Train loss: 0.145591 Valid loss: 0.487546 Train acc: 0.946425 Valid acc: 0.848566\n",
      "Epoch: 35/100 Train loss: 0.141647 Valid loss: 0.493875 Train acc: 0.947930 Valid acc: 0.848543\n"
     ]
    }
   ],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x, y in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : keep_prob, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            ################## Validation\n",
    "            val_acc_ = []\n",
    "            val_loss_ = []    \n",
    "            for x_v, y_v in get_batches(X_valid_norm, Y_valid_onehot, batch_size):\n",
    "                \n",
    "                # Feed\n",
    "                feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0}  \n",
    "\n",
    "                # Loss\n",
    "                loss_v, acc_v = sess.run([cost, accuracy], feed_dict = feed)                    \n",
    "                val_acc_.append(acc_v)\n",
    "                val_loss_.append(loss_v)\n",
    "\n",
    "            # Store\n",
    "            validation_acc.append(np.mean(val_acc_))\n",
    "            validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "        # Print info for every iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(validation_loss)),\n",
    "              \"Train acc: {:6f}\".format(np.mean(train_acc)),\n",
    "              \"Valid acc: {:.6f}\".format(np.mean(validation_acc)))\n",
    "                \n",
    "    saver.save(sess,\"checkpoints-cnn/har.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_loss, label='train_loss')\n",
    "mplot.plot(validation_loss, label='valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_acc, label='train_acc')\n",
    "mplot.plot(validation_acc, label='valid_acc')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_acc = []\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     # Restore\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints-cnn'))\n",
    "    \n",
    "#     for x_t, y_t in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "#         feed = {inputs_: x_t,\n",
    "#                 labels_: y_t,\n",
    "#                 keep_prob_: 1}\n",
    "        \n",
    "#         batch_acc = sess.run(accuracy, feed_dict=feed)\n",
    "#         test_acc.append(batch_acc)\n",
    "#     print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18720, 205, 16) float64 (18720, 1) uint8\n",
      "0.833333333333 0.166666666667 0.0 0.0\n",
      "[2]\n",
      "[[[ 6.87143564  6.26277733]]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "\n",
    "BahramFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/BahramFace.mat')\n",
    "DJFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/DJFace.mat')\n",
    "NickFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/NickFace.mat')\n",
    "RoohiFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/RoohiFace.mat')\n",
    "SarahFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/SarahFace.mat')\n",
    "\n",
    "AllData = np.concatenate((BahramFace['Intensification_Data'],\n",
    "                            DJFace['Intensification_Data'],\n",
    "                            NickFace['Intensification_Data'],\n",
    "                            RoohiFace['Intensification_Data'],\n",
    "                            SarahFace['Intensification_Data']), axis=0)\n",
    "\n",
    "AllLabels = np.concatenate((BahramFace['Intensification_Label'],\n",
    "                            DJFace['Intensification_Label'],\n",
    "                            NickFace['Intensification_Label'],\n",
    "                            RoohiFace['Intensification_Label'],\n",
    "                            SarahFace['Intensification_Label']), axis=0)\n",
    "\n",
    "print(AllData.shape, AllData.dtype, AllLabels.shape, AllLabels.dtype)\n",
    "print(np.mean(AllLabels==0), np.mean(AllLabels==1), np.mean(AllLabels==2), np.mean(AllLabels==3))\n",
    "print((AllLabels +  1).max(axis=0))\n",
    "print(AllData[:1, :1, :2], AllLabels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18720, 205, 16) float64 (18720, 1) uint8\n",
      "0.833333333333 0.166666666667 0.0 0.0\n",
      "[2]\n",
      "[[[-1.14882886  1.12322485]]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data before anything\n",
    "np.random.shuffle(AllData)\n",
    "np.random.shuffle(AllLabels)\n",
    "\n",
    "print(AllData.shape, AllData.dtype, AllLabels.shape, AllLabels.dtype)\n",
    "print(np.mean(AllLabels==0), np.mean(AllLabels==1), np.mean(AllLabels==2), np.mean(AllLabels==3))\n",
    "print((AllLabels +  1).max(axis=0))\n",
    "print(AllData[:1, :1, :2], AllLabels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166666666667 0.833333333333 0.833333333333 0.166666666667\n",
      "(3120, 205, 16) (15600, 205, 16) (15600, 1) (3120, 1)\n",
      "1.0 0.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Solving the overfitting problem by adding linearly transformed and separable data\n",
    "# linearly transformed and separable data augmentation\n",
    "AllLabelOne = (AllLabels==1).reshape(-1)\n",
    "AllData_LabelOne = AllData[AllLabelOne]\n",
    "AllLabels_LabelOne = AllLabels[AllLabelOne]\n",
    "\n",
    "AllLabelZero = (AllLabels==0).reshape(-1)\n",
    "AllData_LabelZero = AllData[AllLabelZero]\n",
    "AllLabels_LabelZero = AllLabels[AllLabelZero]\n",
    "\n",
    "# print(AllData_LabelOne.shape, AllData_LabelZero.shape, AllLabelZero.shape, AllLabelOne.shape)\n",
    "print(np.mean(AllLabelZero==0), np.mean(AllLabelZero==1), \n",
    "      np.mean(AllLabelOne==0), np.mean(AllLabelOne==1))\n",
    "# print(AllLabelOne[:10])\n",
    "print(AllData_LabelOne.shape, AllData_LabelZero.shape, AllLabels_LabelZero.shape, AllLabels_LabelOne.shape)\n",
    "print(np.mean(AllLabels_LabelZero==0), np.mean(AllLabels_LabelZero==1), \n",
    "      np.mean(AllLabels_LabelOne==0), np.mean(AllLabels_LabelOne==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2808 18720\n",
      "(12792, 205, 16) (2808, 205, 16) (12792, 1) (2808, 1)\n",
      "(12792, 205, 16) (2808, 205, 16) (12792, 1) (2808, 1)\n",
      "(13104, 205, 16) (5616, 205, 16) (13104, 1) (5616, 1)\n",
      "0.97619047619 0.0238095238095\n",
      "np.mean(Y_test==0), np.mean(Y_test==1) 0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "# Train and test split\n",
    "# test data size 30% of all data\n",
    "test_size = int((AllData.shape[0] * 0.3)//2)  #is the test data size for each class tgt or non tgt\n",
    "print(test_size, AllData.shape[0])\n",
    "\n",
    "X_train_valid1, Y_train_valid1 = AllData_LabelZero[:-test_size], AllLabels_LabelZero[:-test_size]\n",
    "X_test1, Y_test1 = AllData_LabelZero[-test_size:], AllLabels_LabelZero[-test_size:]\n",
    "X_train_valid2, Y_train_valid2 = AllData_LabelOne[:-test_size], AllLabels_LabelOne[:-test_size]\n",
    "X_test2, Y_test2 = AllData_LabelOne[-test_size:], AllLabels_LabelOne[-test_size:]\n",
    "\n",
    "print(X_train_valid1.shape, X_test1.shape, Y_train_valid1.shape, Y_test1.shape)\n",
    "print(X_train_valid1.shape, X_test1.shape, Y_train_valid1.shape, Y_test1.shape)\n",
    "\n",
    "X_train_valid = np.concatenate((X_train_valid1, X_train_valid2), axis=0)\n",
    "Y_train_valid = np.concatenate((Y_train_valid1, Y_train_valid2), axis=0)\n",
    "X_test = np.concatenate((X_test1, X_test2), axis=0)\n",
    "Y_test = np.concatenate((Y_test1, Y_test2), axis=0)\n",
    "\n",
    "print(X_train_valid.shape, X_test.shape, Y_train_valid.shape, Y_test.shape)\n",
    "print(np.mean(Y_train_valid==0), np.mean(Y_train_valid==1)) \n",
    "print('np.mean(Y_test==0), np.mean(Y_test==1)', np.mean(Y_test==0), np.mean(Y_test==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing for data augmentation\n",
    "AllData, AllLabels = X_train_valid, Y_train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 12792\n",
      "41\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# Solving the overfitting problem by adding linearly transformed and separable data\n",
    "# linearly transformed and separable data augmentation\n",
    "AllLabelOne = (AllLabels==1).reshape(-1)\n",
    "AllData_LabelOne = AllData[AllLabelOne]\n",
    "AllLabels_LabelOne = AllLabels[AllLabelOne]\n",
    "\n",
    "AllLabelZero = (AllLabels==0).reshape(-1)\n",
    "AllData_LabelZero = AllData[AllLabelZero]\n",
    "AllLabels_LabelZero = AllLabels[AllLabelZero]\n",
    "\n",
    "print(AllData_LabelOne.shape[0], AllData_LabelZero.shape[0])\n",
    "\n",
    "# Number of times, we need to generate target data for uniform distribution\n",
    "print(int(AllData_LabelZero.shape[0]/ AllData_LabelOne.shape[0]))\n",
    "num_DataLabelOneNew = int(AllData_LabelZero.shape[0]/ AllData_LabelOne.shape[0])\n",
    "print(num_DataLabelOneNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12792, 205, 16) (12792,)\n",
      "float64 uint8\n",
      "(12792, 205, 16) (12792, 205, 16)\n",
      "(12792,) (12792, 1)\n",
      "(25584, 205, 16) float64 (25584,) uint8\n",
      "0.5 0.5 0.0 0.0\n",
      "2\n",
      "np.mean(AllLabelsNew==0), np.mean(AllLabelsNew==1) 0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "# Linear synthetic data augmentatiion or creation or generation\n",
    "AllData_LabelOneNew_list, AllLabels_LabelOneNew_list = [], []\n",
    "\n",
    "# w*data+b: translation, rotation, and scaling\n",
    "# linear transformation of the target data\n",
    "w, b = 1.0, 0.0\n",
    "for _ in range(num_DataLabelOneNew):\n",
    "    AllData_LabelOneNew = (w * AllData_LabelOne) + b\n",
    "    AllData_LabelOneNew_list.append(AllData_LabelOneNew)\n",
    "    w *= 0.9 # 1.0, 0.9, 0.81, 0.729, 0.6561\n",
    "    b += 0.1 # 0.0, 0.1, 0.2, 0.3, 0.4\n",
    "    AllLabels_LabelOneNew_list.append(AllLabels_LabelOne)\n",
    "\n",
    "AllData_LabelOneNew_total = np.array(AllData_LabelOneNew_list, \n",
    "                                      dtype=AllData_LabelOne.dtype).reshape(-1, 205, 16)\n",
    "AllLabels_LabelOneNew_total = np.array(AllLabels_LabelOneNew_list, \n",
    "                                      dtype=AllLabels_LabelOne.dtype).reshape(-1)\n",
    "print(AllData_LabelOneNew_total.shape, AllLabels_LabelOneNew_total.shape)\n",
    "print(AllData_LabelOneNew_total.dtype, AllLabels_LabelOneNew_total.dtype)\n",
    "\n",
    "\n",
    "AllDataNew = np.concatenate((AllData_LabelOneNew_total, AllData_LabelZero), axis=0)\n",
    "print(AllData_LabelOneNew_total.shape, AllData_LabelZero.shape)\n",
    "\n",
    "print(AllLabels_LabelOneNew_total.shape, AllLabels_LabelZero.shape)\n",
    "AllLabelsNew = np.concatenate((AllLabels_LabelOneNew_total, AllLabels_LabelZero.reshape(-1)), axis=0)\n",
    "\n",
    "print(AllDataNew.shape, AllDataNew.dtype, AllLabelsNew.shape, AllLabelsNew.dtype)\n",
    "print(np.mean(AllLabelsNew==0), np.mean(AllLabelsNew==1), np.mean(AllLabelsNew==2), np.mean(AllLabelsNew==3))\n",
    "print((AllLabelsNew +  1).max(axis=0))\n",
    "print('np.mean(AllLabelsNew==0), np.mean(AllLabelsNew==1)', np.mean(AllLabelsNew==0), np.mean(AllLabelsNew==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25584, 205, 16) float64 (25584,) uint8\n",
      "0.5 0.5 0.0 0.0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Implementing Yalda's seggestion\n",
    "#  AllData and Alllabels are limited to train-valid data only\n",
    "X_train_valid, Y_train_valid = AllDataNew, AllLabelsNew\n",
    "print(AllDataNew.shape, AllDataNew.dtype, AllLabelsNew.shape, AllLabelsNew.dtype)\n",
    "print(np.mean(AllLabelsNew==0), np.mean(AllLabelsNew==1), np.mean(AllLabelsNew==2), np.mean(AllLabelsNew==3))\n",
    "print((AllLabelsNew +  1).max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.5 0.5 0.0\n",
      "0.0 0.5 0.5 0.0\n",
      "(25584, 2) float64 (5616, 2) float64\n"
     ]
    }
   ],
   "source": [
    "# Preparing input and output data\n",
    "from utilities import *\n",
    "\n",
    "# Normalizing/standardizing the input data features\n",
    "X_train_valid_norm, X_test_norm = standardize(test=X_test, train=X_train_valid)\n",
    "\n",
    "# Onehot encoding/vectorizing the output data labels\n",
    "print(np.mean((Y_train_valid+1).reshape(-1)==0), np.mean((Y_train_valid+1).reshape(-1)==1),\n",
    "     np.mean((Y_train_valid+1).reshape(-1)==2), np.mean((Y_train_valid+1).reshape(-1)==3))\n",
    "\n",
    "print(np.mean((Y_test+1).reshape(-1)==0), np.mean((Y_test+1).reshape(-1)==1),\n",
    "     np.mean((Y_test+1).reshape(-1)==2), np.mean((Y_test+1).reshape(-1)==3))\n",
    "\n",
    "Y_train_valid_onehot = one_hot(labels=(Y_train_valid+1).reshape(-1), n_class=2) \n",
    "Y_test_onehot = one_hot(labels=(Y_test+1).reshape(-1), n_class=2) \n",
    "\n",
    "print(Y_train_valid_onehot.shape, Y_train_valid_onehot.dtype, \n",
    "      Y_test_onehot.shape, Y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17908, 205, 16) (7676, 205, 16) (17908, 2) (7676, 2)\n"
     ]
    }
   ],
   "source": [
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_norm, X_valid_norm, Y_train_onehot, Y_valid_onehot = train_test_split(X_train_valid_norm, \n",
    "                                                                              Y_train_valid_onehot,\n",
    "                                                                              test_size=0.30)\n",
    "\n",
    "print(X_train_norm.shape, X_valid_norm.shape, Y_train_onehot.shape, Y_valid_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len, n_channels 179 205 16\n",
      "n_classes 2\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = Y_train_valid.max(axis=0)+1\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.0001 #1e-4\n",
    "epochs = 10 # num iterations for updating model\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the data from python/numpy to tensorflow framework\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs_')\n",
    "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_')\n",
    "keep_prob_ = tf.placeholder(tf.float32, name = 'keep_prob_')\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_.shape, conv1.shape, max_pool_1.shape (?, 205, 16) (?, 204, 32) (?, 102, 32)\n",
      "max_pool_1.shape, conv2.shape, max_pool_2.shape (?, 102, 32) (?, 102, 64) (?, 51, 64)\n",
      "max_pool_2.shape, conv3.shape, max_pool_3.shape (?, 51, 64) (?, 50, 128) (?, 25, 128)\n",
      "max_pool_3.shape, conv4.shape, max_pool_4.shape (?, 25, 128) (?, 24, 256) (?, 12, 256)\n",
      "max_pool_4.shape, flat.shape, logits.shape (?, 12, 256) (?, 3072) (?, 2)\n"
     ]
    }
   ],
   "source": [
    "# batch_size, seq_len, n_channels 179 205 16\n",
    "# (batch, 205, 16) --> (batch, 102, 32)\n",
    "# conv valid: (205-2+0)/1 + 1 = (203/1)+1 = 203 + 1=204\n",
    "# pool same: (204-2+0)/2 + 1 = (202/2)+1 = 101 + 1=102\n",
    "conv1 = tf.layers.conv1d(inputs=inputs_, filters=32, kernel_size=2, strides=1, padding='valid', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_1 = tf.nn.dropout(max_pool_1, keep_prob=keep_prob_)\n",
    "print('inputs_.shape, conv1.shape, max_pool_1.shape', inputs_.shape, conv1.shape, max_pool_1.shape)\n",
    "\n",
    "# (batch, 102, 32) --> (batch, 51, 64)\n",
    "# conv same\n",
    "# pool same: (102-2+0)/2 + 1 = (100/2)+1 = 50 + 1=51\n",
    "conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_2 = tf.nn.dropout(max_pool_2, keep_prob=keep_prob_)\n",
    "print('max_pool_1.shape, conv2.shape, max_pool_2.shape', max_pool_1.shape, conv2.shape, max_pool_2.shape)\n",
    "\n",
    "# (batch, 51, 64) --> (batch, 25, 128)\n",
    "# conv valid: (51-2+0)/1 + 1 = (49/1)+1 = 49 + 1=50\n",
    "# pool same: (50-2+0)/2 + 1 = (48/2)+1 = 24 + 1=25\n",
    "conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=128, kernel_size=2, strides=1, padding='valid', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_3 = tf.nn.dropout(max_pool_3, keep_prob=keep_prob_)\n",
    "print('max_pool_2.shape, conv3.shape, max_pool_3.shape', max_pool_2.shape, conv3.shape, max_pool_3.shape)\n",
    "\n",
    "# (batch, 25, 128) --> (batch, 12, 256)\n",
    "# conv valid: (25-2+0)/1 + 1 = (23/1)+1 = 23 + 1=24\n",
    "# pool same: (24-2+0)/2 + 1 = (22/2)+1 = 11 + 1=12\n",
    "conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=256, kernel_size=2, strides=1, padding='valid', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_4 = tf.nn.dropout(max_pool_4, keep_prob=keep_prob_)\n",
    "print('max_pool_3.shape, conv4.shape, max_pool_4.shape', max_pool_3.shape, conv4.shape, max_pool_4.shape)\n",
    "\n",
    "# Flatten and add dropout + predicted output\n",
    "flat = tf.reshape(max_pool_4, (-1, 12*256))\n",
    "flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "logits = tf.layers.dense(flat, n_classes)\n",
    "print('max_pool_4.shape, flat.shape, logits.shape', max_pool_4.shape, flat.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_tensor, cost Tensor(\"Reshape_3:0\", shape=(?,), dtype=float32) Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "optimizer name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_conv1d/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_1/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_1/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_2/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_2/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_3/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_3/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_dense/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_dense/bias/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n",
      "correct_pred, accuracy Tensor(\"Equal:0\", shape=(?,), dtype=bool) Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "confusion_matrix Tensor(\"confusion_matrix/SparseTensorDenseAdd:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "print('cost_tensor, cost', cost_tensor, cost)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "print('optimizer', optimizer)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "print('correct_pred, accuracy', correct_pred, accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = tf.confusion_matrix(predictions=tf.argmax(logits, 1),\n",
    "                                       labels=tf.argmax(labels_, 1))\n",
    "print('confusion_matrix', confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Train loss: 0.481470 Valid loss: 0.471453 Train acc: 0.762793 Valid acc: 0.766024\n",
      "Epoch: 2/10 Train loss: 0.371572 Valid loss: 0.358994 Train acc: 0.837123 Valid acc: 0.839918\n",
      "Epoch: 3/10 Train loss: 0.316479 Valid loss: 0.303622 Train acc: 0.868492 Valid acc: 0.871753\n",
      "Epoch: 4/10 Train loss: 0.283021 Valid loss: 0.269910 Train acc: 0.886634 Valid acc: 0.890437\n",
      "Epoch: 5/10 Train loss: 0.259822 Valid loss: 0.247278 Train acc: 0.898659 Valid acc: 0.902812\n",
      "Epoch: 6/10 Train loss: 0.242894 Valid loss: 0.230777 Train acc: 0.906862 Valid acc: 0.911622\n",
      "Epoch: 7/10 Train loss: 0.229474 Valid loss: 0.218175 Train acc: 0.913384 Valid acc: 0.918269\n",
      "Epoch: 8/10 Train loss: 0.218514 Valid loss: 0.208136 Train acc: 0.918666 Valid acc: 0.923456\n",
      "Epoch: 9/10 Train loss: 0.209592 Valid loss: 0.199877 Train acc: 0.922924 Valid acc: 0.927629\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "\n",
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x, y in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : keep_prob, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            ################## Validation\n",
    "            acc_batch = []\n",
    "            loss_batch = []    \n",
    "            # Loop over batches\n",
    "            for x, y in get_batches(X_valid_norm, Y_valid_onehot, batch_size):\n",
    "\n",
    "                # Feed dictionary\n",
    "                feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "                # Loss\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "                acc_batch.append(acc)\n",
    "                loss_batch.append(loss)\n",
    "\n",
    "            # Store\n",
    "            valid_acc.append(np.mean(acc_batch))\n",
    "            valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "        # Print info for every iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(valid_loss)),\n",
    "              \"Train acc: {:6f}\".format(np.mean(train_acc)),\n",
    "              \"Valid acc: {:.6f}\".format(np.mean(valid_acc)))\n",
    "                \n",
    "    saver.save(sess,\"checkpoints_/dcnn-face-yalda.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_loss, label='Face train_loss')\n",
    "mplot.plot(valid_loss, label='Face valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as mplot\n",
    "mplot.plot(train_acc, label='Face train_acc')\n",
    "mplot.plot(valid_acc, label='Face valid_acc')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints_/'))\n",
    "    \n",
    "    ################## Test\n",
    "    acc_batch = []\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for x, y in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "        # Loss\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Store\n",
    "    test_acc.append(np.mean(acc_batch))\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)),\n",
    "          \"Test acc: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

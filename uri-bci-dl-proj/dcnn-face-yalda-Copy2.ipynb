{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18720, 205, 16) float64 (18720, 1) uint8\n",
      "0.833333333333 0.166666666667 0.0 0.0\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "\n",
    "BahramFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/BahramFace.mat')\n",
    "DJFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/DJFace.mat')\n",
    "NickFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/NickFace.mat')\n",
    "RoohiFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/RoohiFace.mat')\n",
    "SarahFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/SarahFace.mat')\n",
    "\n",
    "AllData = np.concatenate((BahramFace['Intensification_Data'],\n",
    "                            DJFace['Intensification_Data'],\n",
    "                            NickFace['Intensification_Data'],\n",
    "                            RoohiFace['Intensification_Data'],\n",
    "                            SarahFace['Intensification_Data']), axis=0)\n",
    "\n",
    "AllLabels = np.concatenate((BahramFace['Intensification_Label'],\n",
    "                            DJFace['Intensification_Label'],\n",
    "                            NickFace['Intensification_Label'],\n",
    "                            RoohiFace['Intensification_Label'],\n",
    "                            SarahFace['Intensification_Label']), axis=0)\n",
    "\n",
    "print(AllData.shape, AllData.dtype, AllLabels.shape, AllLabels.dtype)\n",
    "print(np.mean(AllLabels==0), np.mean(AllLabels==1), np.mean(AllLabels==2), np.mean(AllLabels==3))\n",
    "print((AllLabels +  1).max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13104, 205, 16) (5616, 205, 16) (13104, 1) (5616, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_valid, X_test, Y_train_valid, Y_test = train_test_split(AllData, AllLabels, test_size=0.30)\n",
    "\n",
    "print(X_train_valid.shape, X_test.shape, Y_train_valid.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.834325396825 0.165674603175 0.0\n",
      "0.0 0.831018518519 0.168981481481 0.0\n",
      "(5616, 2) float64\n"
     ]
    }
   ],
   "source": [
    "from utilities import *\n",
    "\n",
    "# Normalizing/standardizing the input data features\n",
    "X_train_valid_norm, X_test_norm = standardize(test=X_test, train=X_train_valid)\n",
    "\n",
    "# Onehot encoding/vectorizing the output data labels\n",
    "print(np.mean((Y_train_valid+1).reshape(-1)==0), np.mean((Y_train_valid+1).reshape(-1)==1),\n",
    "     np.mean((Y_train_valid+1).reshape(-1)==2), np.mean((Y_train_valid+1).reshape(-1)==3))\n",
    "\n",
    "print(np.mean((Y_test+1).reshape(-1)==0), np.mean((Y_test+1).reshape(-1)==1),\n",
    "     np.mean((Y_test+1).reshape(-1)==2), np.mean((Y_test+1).reshape(-1)==3))\n",
    "\n",
    "# Y_train_valid_onehot = one_hot(labels=(Y_train_valid+1).reshape(-1), n_class=2) \n",
    "# print(Y_train_valid_onehot.shape, Y_train_valid_onehot.dtype, \n",
    "Y_test_onehot = one_hot(labels=(Y_test+1).reshape(-1), n_class=2) \n",
    "print(Y_test_onehot.shape, Y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% train 30 % valid\n",
    "# get_batches from each train and valid the same\n",
    "# still it will be 70% to 30%\n",
    "# get_batches 83% vs 16%\n",
    "# get_batch 16% vs 16% each time\n",
    "# X_train_valid, X_test, Y_train_valid, Y_test = train_test_split(AllData, AllLabels, test_size=0.30)\n",
    "X_train_norm, X_valid_norm, Y_train, Y_valid = train_test_split(X_train_valid_norm, Y_train_valid, \n",
    "                                                                              test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batches(X, Y, batch_size = 100):\n",
    "#     \"\"\" Return a generator for batches \"\"\"\n",
    "#     n_batches = len(X) // batch_size\n",
    "#     X, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "#     # Loop over batches and yield\n",
    "#     for b in range(0, len(X), batch_size):\n",
    "#         yield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches2(X_norm, Y_labels):\n",
    "    # Input train or valid\n",
    "    # This is not-applicable to test\n",
    "    X, Y = X_norm, Y_labels #_onehot\n",
    "    AllLabels = Y_labels # 100%\n",
    "\n",
    "    # non = 0 is 87%  AllLabelZero\n",
    "    # tgt = 1 is 13%  AllLabelOne\n",
    "    AllLabelZero = (AllLabels==0).reshape(-1) # 87%\n",
    "    AllLabelOne = (AllLabels==1).reshape(-1) # 13%\n",
    "\n",
    "    X_non, Y_non = X[AllLabelZero], Y[AllLabelZero] # 87%\n",
    "    X_tgt, Y_tgt = X[AllLabelOne], Y[AllLabelOne] # 13%\n",
    "#     print('X_non.shape, Y_non.shape', X_non.shape, Y_non.shape)\n",
    "#     print('X_tgt.shape, Y_tgt.shape', X_tgt.shape, Y_tgt.shape)\n",
    "\n",
    "    # Non-target batch size for get_batches from non-target data\n",
    "    batch_size = X_tgt.shape[0] # 13% -> tgt = 1 is 13%  AllLabelOne\n",
    "    assert X_tgt.shape[0] == Y_tgt.shape[0]\n",
    "#     print(batch_size)\n",
    "\n",
    "    # # 87% - 13% +1: non - tgt + 1\n",
    "    # n_batches = X_non.shape[0] - tgt_batch_size + 1 # stride=1 \n",
    "    # # max overlap for non-tgt 87% - 13% +1\n",
    "\n",
    "#     # 87% // 13%: non/ tgt\n",
    "#     # max overlap for non-tgt 87%// 13%\n",
    "#     num_non_batches = X_non.shape[0]// tgt_batch_size # stride= tgt_batch_size \n",
    "# #     print(num_non_batches)\n",
    "#     # # n_batches = len(X) // batch_size # stride=batch_size # min overlap\n",
    "#     X_non_, Y_non_ = X_non[:num_non_batches*tgt_batch_size], Y_non[:num_non_batches*tgt_batch_size]\n",
    "\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = X_non.shape[0] // batch_size\n",
    "    X_non, Y_non = X_non[:n_batches*batch_size], Y_non[:n_batches*batch_size]\n",
    "    \n",
    "#     \"\"\" Return a generator for batches \"\"\"\n",
    "#     n_batches = len(X) // batch_size\n",
    "#     X, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over target batches: start, stop, step\n",
    "    for i in range(0, X_non.shape[0], batch_size):\n",
    "#         print(i)\n",
    "        # each_train_valid_batch\n",
    "        each_X_norm = np.concatenate((X_non[i:i+batch_size], X_tgt), axis=0)\n",
    "        each_Y = np.concatenate((Y_non[i:i+batch_size], Y_tgt), axis=0)\n",
    "        each_Y_onehot = one_hot(labels=(each_Y+1).reshape(-1), n_class=2)\n",
    "#         print('each_X_norm.shape, each_Y_onehot.shape', each_X_norm.shape, each_Y_onehot.shape)\n",
    "#         print('each_X_norm.dtype, each_Y_onehot.dtype', each_X_norm.dtype, each_Y_onehot.dtype)\n",
    "        yield each_X_norm, each_Y_onehot\n",
    "        \n",
    "    #     # Loop over batches and yield\n",
    "    #     for b in range(0, len(X), batch_size):\n",
    "    #         yield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_batches2 at 0x7f6a29b88830>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batches2(X_norm=X_valid_norm, Y_labels=Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_batches2 at 0x7f6a29be5728>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batches2(X_norm=X_train_norm, Y_labels=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len, n_channels 205 16\n",
      "n_classes [2]\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "# batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "# print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "print('seq_len, n_channels', seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = Y_train_valid.max(axis=0)+1\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.0001 #1e-4\n",
    "epochs = 1000 # num iterations for updating model\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_.shape, labels_.shape (?, 205, 16) (?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Feed the data from python/numpy to tensorflow framework\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs_')\n",
    "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_')\n",
    "keep_prob_ = tf.placeholder(tf.float32, name = 'keep_prob_')\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate_')\n",
    "print('inputs_.shape, labels_.shape', inputs_.shape, labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_.shape, conv1.shape, max_pool_1.shape (?, 205, 16) (?, 204, 32) (?, 102, 32)\n",
      "max_pool_1.shape, conv2.shape, max_pool_2.shape (?, 102, 32) (?, 102, 64) (?, 51, 64)\n",
      "max_pool_2.shape, conv3.shape, max_pool_3.shape (?, 51, 64) (?, 50, 128) (?, 25, 128)\n",
      "max_pool_3.shape, conv4.shape, max_pool_4.shape (?, 25, 128) (?, 24, 256) (?, 12, 256)\n",
      "max_pool_4.shape, flat.shape, logits.shape (?, 12, 256) (?, 3072) (?, 2)\n"
     ]
    }
   ],
   "source": [
    "# inputs_.shape, labels_.shape (?, 205, 16) (?, 2)\n",
    "# (batch, 205, 16) --> (batch, 102, 32)\n",
    "# conv valid: (205-2+0)/1 + 1 = (203/1)+1 = 203 + 1=204\n",
    "# pool same: (204-2+0)/2 + 1 = (202/2)+1 = 101 + 1=102\n",
    "conv1 = tf.layers.conv1d(inputs=inputs_, filters=32, kernel_size=2, strides=1, padding='valid', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_1 = tf.nn.dropout(max_pool_1, keep_prob=keep_prob_)\n",
    "print('inputs_.shape, conv1.shape, max_pool_1.shape', inputs_.shape, conv1.shape, max_pool_1.shape)\n",
    "\n",
    "# (batch, 102, 32) --> (batch, 51, 64)\n",
    "# conv same\n",
    "# pool same: (102-2+0)/2 + 1 = (100/2)+1 = 50 + 1=51\n",
    "conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_2 = tf.nn.dropout(max_pool_2, keep_prob=keep_prob_)\n",
    "print('max_pool_1.shape, conv2.shape, max_pool_2.shape', max_pool_1.shape, conv2.shape, max_pool_2.shape)\n",
    "\n",
    "# (batch, 51, 64) --> (batch, 25, 128)\n",
    "# conv valid: (51-2+0)/1 + 1 = (49/1)+1 = 49 + 1=50\n",
    "# pool same: (50-2+0)/2 + 1 = (48/2)+1 = 24 + 1=25\n",
    "conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=128, kernel_size=2, strides=1, padding='valid', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_3 = tf.nn.dropout(max_pool_3, keep_prob=keep_prob_)\n",
    "print('max_pool_2.shape, conv3.shape, max_pool_3.shape', max_pool_2.shape, conv3.shape, max_pool_3.shape)\n",
    "\n",
    "# (batch, 25, 128) --> (batch, 12, 256)\n",
    "# conv valid: (25-2+0)/1 + 1 = (23/1)+1 = 23 + 1=24\n",
    "# pool same: (24-2+0)/2 + 1 = (22/2)+1 = 11 + 1=12\n",
    "conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=256, kernel_size=2, strides=1, padding='valid', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_4 = tf.nn.dropout(max_pool_4, keep_prob=keep_prob_)\n",
    "print('max_pool_3.shape, conv4.shape, max_pool_4.shape', max_pool_3.shape, conv4.shape, max_pool_4.shape)\n",
    "\n",
    "# Flatten and add dropout + predicted output\n",
    "flat = tf.reshape(max_pool_4, (-1, 12*256))\n",
    "flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "logits = tf.layers.dense(flat, n_classes)\n",
    "print('max_pool_4.shape, flat.shape, logits.shape', max_pool_4.shape, flat.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_tensor, cost Tensor(\"Reshape_3:0\", shape=(?,), dtype=float32) Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "optimizer name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_conv1d/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_1/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_1/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_2/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_2/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_3/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_3/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_dense/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_dense/bias/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n",
      "correct_pred, accuracy Tensor(\"Equal:0\", shape=(?,), dtype=bool) Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "confusion_matrix Tensor(\"confusion_matrix/SparseTensorDenseAdd:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "print('cost_tensor, cost', cost_tensor, cost)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "print('optimizer', optimizer)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "print('correct_pred, accuracy', correct_pred, accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = tf.confusion_matrix(predictions=tf.argmax(logits, 1),\n",
    "                                       labels=tf.argmax(labels_, 1))\n",
    "print('confusion_matrix', confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000 Train loss: 0.702099 Valid loss: 0.691981 Train acc: 0.505180 Valid acc: 0.532724\n",
      "Epoch: 2/1000 Train loss: 0.697455 Valid loss: 0.688519 Train acc: 0.517738 Valid acc: 0.555588\n",
      "Epoch: 3/1000 Train loss: 0.692908 Valid loss: 0.685635 Train acc: 0.533224 Valid acc: 0.573478\n",
      "Epoch: 4/1000 Train loss: 0.688956 Valid loss: 0.682754 Train acc: 0.546590 Valid acc: 0.588228\n",
      "Epoch: 5/1000 Train loss: 0.684962 Valid loss: 0.679713 Train acc: 0.559462 Valid acc: 0.601752\n",
      "Epoch: 6/1000 Train loss: 0.681186 Valid loss: 0.676569 Train acc: 0.571268 Valid acc: 0.613963\n",
      "Epoch: 7/1000 Train loss: 0.677308 Valid loss: 0.673274 Train acc: 0.581911 Valid acc: 0.624295\n",
      "Epoch: 8/1000 Train loss: 0.673514 Valid loss: 0.669750 Train acc: 0.591811 Valid acc: 0.632705\n",
      "Epoch: 9/1000 Train loss: 0.669010 Valid loss: 0.665937 Train acc: 0.602018 Valid acc: 0.640385\n",
      "Epoch: 10/1000 Train loss: 0.664639 Valid loss: 0.661804 Train acc: 0.610984 Valid acc: 0.647471\n",
      "Epoch: 11/1000 Train loss: 0.659811 Valid loss: 0.657331 Train acc: 0.619440 Valid acc: 0.654109\n",
      "Epoch: 12/1000 Train loss: 0.654721 Valid loss: 0.652514 Train acc: 0.627322 Valid acc: 0.660312\n",
      "Epoch: 13/1000 Train loss: 0.649484 Valid loss: 0.647391 Train acc: 0.634673 Valid acc: 0.666009\n",
      "Epoch: 14/1000 Train loss: 0.643840 Valid loss: 0.642021 Train acc: 0.641719 Valid acc: 0.671201\n",
      "Epoch: 15/1000 Train loss: 0.638202 Valid loss: 0.636483 Train acc: 0.648122 Valid acc: 0.675952\n",
      "Epoch: 16/1000 Train loss: 0.632293 Valid loss: 0.630870 Train acc: 0.654115 Valid acc: 0.680219\n",
      "Epoch: 17/1000 Train loss: 0.626550 Valid loss: 0.625269 Train acc: 0.659537 Valid acc: 0.684227\n",
      "Epoch: 18/1000 Train loss: 0.620742 Valid loss: 0.619743 Train acc: 0.664984 Valid acc: 0.688020\n",
      "Epoch: 19/1000 Train loss: 0.615139 Valid loss: 0.614338 Train acc: 0.669871 Valid acc: 0.691628\n",
      "Epoch: 20/1000 Train loss: 0.609592 Valid loss: 0.609084 Train acc: 0.674620 Valid acc: 0.695189\n",
      "Epoch: 21/1000 Train loss: 0.604152 Valid loss: 0.603986 Train acc: 0.679013 Valid acc: 0.698635\n",
      "Epoch: 22/1000 Train loss: 0.598843 Valid loss: 0.599046 Train acc: 0.683371 Valid acc: 0.701898\n",
      "Epoch: 23/1000 Train loss: 0.593629 Valid loss: 0.594264 Train acc: 0.687390 Valid acc: 0.704870\n",
      "Epoch: 24/1000 Train loss: 0.588578 Valid loss: 0.589637 Train acc: 0.691399 Valid acc: 0.707664\n",
      "Epoch: 25/1000 Train loss: 0.583737 Valid loss: 0.585169 Train acc: 0.695142 Valid acc: 0.710311\n",
      "Epoch: 26/1000 Train loss: 0.578916 Valid loss: 0.580854 Train acc: 0.698852 Valid acc: 0.712964\n",
      "Epoch: 27/1000 Train loss: 0.574318 Valid loss: 0.576686 Train acc: 0.702346 Valid acc: 0.715452\n",
      "Epoch: 28/1000 Train loss: 0.569883 Valid loss: 0.572661 Train acc: 0.705656 Valid acc: 0.717826\n",
      "Epoch: 29/1000 Train loss: 0.565715 Valid loss: 0.568775 Train acc: 0.708685 Valid acc: 0.720117\n",
      "Epoch: 30/1000 Train loss: 0.561559 Valid loss: 0.565020 Train acc: 0.711683 Valid acc: 0.722316\n",
      "Epoch: 31/1000 Train loss: 0.557468 Valid loss: 0.561392 Train acc: 0.714665 Valid acc: 0.724382\n",
      "Epoch: 32/1000 Train loss: 0.553541 Valid loss: 0.557889 Train acc: 0.717434 Valid acc: 0.726377\n",
      "Epoch: 33/1000 Train loss: 0.549809 Valid loss: 0.554509 Train acc: 0.720139 Valid acc: 0.728332\n",
      "Epoch: 34/1000 Train loss: 0.546048 Valid loss: 0.551249 Train acc: 0.722752 Valid acc: 0.730218\n",
      "Epoch: 35/1000 Train loss: 0.542462 Valid loss: 0.548109 Train acc: 0.725227 Valid acc: 0.732067\n",
      "Epoch: 36/1000 Train loss: 0.539040 Valid loss: 0.545084 Train acc: 0.727696 Valid acc: 0.733879\n",
      "Epoch: 37/1000 Train loss: 0.535662 Valid loss: 0.542170 Train acc: 0.730019 Valid acc: 0.735649\n",
      "Epoch: 38/1000 Train loss: 0.532422 Valid loss: 0.539362 Train acc: 0.732228 Valid acc: 0.737385\n",
      "Epoch: 39/1000 Train loss: 0.529228 Valid loss: 0.536655 Train acc: 0.734367 Valid acc: 0.739031\n",
      "Epoch: 40/1000 Train loss: 0.526061 Valid loss: 0.534046 Train acc: 0.736531 Valid acc: 0.740595\n",
      "Epoch: 41/1000 Train loss: 0.523016 Valid loss: 0.531529 Train acc: 0.738631 Valid acc: 0.742159\n",
      "Epoch: 42/1000 Train loss: 0.520103 Valid loss: 0.529099 Train acc: 0.740614 Valid acc: 0.743639\n",
      "Epoch: 43/1000 Train loss: 0.517266 Valid loss: 0.526751 Train acc: 0.742530 Valid acc: 0.745051\n",
      "Epoch: 44/1000 Train loss: 0.514506 Valid loss: 0.524485 Train acc: 0.744444 Valid acc: 0.746416\n",
      "Epoch: 45/1000 Train loss: 0.511818 Valid loss: 0.522297 Train acc: 0.746276 Valid acc: 0.747772\n",
      "Epoch: 46/1000 Train loss: 0.509164 Valid loss: 0.520182 Train acc: 0.748043 Valid acc: 0.749070\n",
      "Epoch: 47/1000 Train loss: 0.506626 Valid loss: 0.518145 Train acc: 0.749748 Valid acc: 0.750313\n",
      "Epoch: 48/1000 Train loss: 0.504054 Valid loss: 0.516176 Train acc: 0.751400 Valid acc: 0.751515\n",
      "Epoch: 49/1000 Train loss: 0.501571 Valid loss: 0.514271 Train acc: 0.753064 Valid acc: 0.752681\n",
      "Epoch: 50/1000 Train loss: 0.499208 Valid loss: 0.512430 Train acc: 0.754624 Valid acc: 0.753780\n",
      "Epoch: 51/1000 Train loss: 0.496868 Valid loss: 0.510645 Train acc: 0.756197 Valid acc: 0.754827\n",
      "Epoch: 52/1000 Train loss: 0.494601 Valid loss: 0.508920 Train acc: 0.757671 Valid acc: 0.755844\n",
      "Epoch: 53/1000 Train loss: 0.492360 Valid loss: 0.507242 Train acc: 0.759128 Valid acc: 0.756840\n",
      "Epoch: 54/1000 Train loss: 0.490153 Valid loss: 0.505616 Train acc: 0.760560 Valid acc: 0.757801\n",
      "Epoch: 55/1000 Train loss: 0.488009 Valid loss: 0.504034 Train acc: 0.761947 Valid acc: 0.758744\n",
      "Epoch: 56/1000 Train loss: 0.485922 Valid loss: 0.502501 Train acc: 0.763263 Valid acc: 0.759646\n",
      "Epoch: 57/1000 Train loss: 0.483855 Valid loss: 0.501010 Train acc: 0.764610 Valid acc: 0.760511\n",
      "Epoch: 58/1000 Train loss: 0.481795 Valid loss: 0.499558 Train acc: 0.765949 Valid acc: 0.761355\n",
      "Epoch: 59/1000 Train loss: 0.479823 Valid loss: 0.498147 Train acc: 0.767239 Valid acc: 0.762156\n",
      "Epoch: 60/1000 Train loss: 0.477872 Valid loss: 0.496773 Train acc: 0.768502 Valid acc: 0.762926\n",
      "Epoch: 61/1000 Train loss: 0.475948 Valid loss: 0.495445 Train acc: 0.769759 Valid acc: 0.763677\n",
      "Epoch: 62/1000 Train loss: 0.474065 Valid loss: 0.494154 Train acc: 0.770979 Valid acc: 0.764415\n",
      "Epoch: 63/1000 Train loss: 0.472195 Valid loss: 0.492898 Train acc: 0.772154 Valid acc: 0.765140\n",
      "Epoch: 64/1000 Train loss: 0.470334 Valid loss: 0.491669 Train acc: 0.773333 Valid acc: 0.765848\n",
      "Epoch: 65/1000 Train loss: 0.468528 Valid loss: 0.490466 Train acc: 0.774468 Valid acc: 0.766550\n",
      "Epoch: 66/1000 Train loss: 0.466717 Valid loss: 0.489294 Train acc: 0.775647 Valid acc: 0.767215\n",
      "Epoch: 67/1000 Train loss: 0.464970 Valid loss: 0.488154 Train acc: 0.776764 Valid acc: 0.767862\n",
      "Epoch: 68/1000 Train loss: 0.463249 Valid loss: 0.487036 Train acc: 0.777862 Valid acc: 0.768499\n",
      "Epoch: 69/1000 Train loss: 0.461565 Valid loss: 0.485958 Train acc: 0.778937 Valid acc: 0.769093\n",
      "Epoch: 70/1000 Train loss: 0.459904 Valid loss: 0.484906 Train acc: 0.779996 Valid acc: 0.769664\n",
      "Epoch: 71/1000 Train loss: 0.458233 Valid loss: 0.483883 Train acc: 0.781059 Valid acc: 0.770222\n",
      "Epoch: 72/1000 Train loss: 0.456554 Valid loss: 0.482882 Train acc: 0.782125 Valid acc: 0.770768\n",
      "Epoch: 73/1000 Train loss: 0.454904 Valid loss: 0.481904 Train acc: 0.783168 Valid acc: 0.771307\n",
      "Epoch: 74/1000 Train loss: 0.453275 Valid loss: 0.480951 Train acc: 0.784209 Valid acc: 0.771820\n",
      "Epoch: 75/1000 Train loss: 0.451678 Valid loss: 0.480023 Train acc: 0.785254 Valid acc: 0.772307\n",
      "Epoch: 76/1000 Train loss: 0.450111 Valid loss: 0.479124 Train acc: 0.786236 Valid acc: 0.772766\n",
      "Epoch: 77/1000 Train loss: 0.448554 Valid loss: 0.478248 Train acc: 0.787213 Valid acc: 0.773219\n",
      "Epoch: 78/1000 Train loss: 0.447027 Valid loss: 0.477393 Train acc: 0.788172 Valid acc: 0.773673\n",
      "Epoch: 79/1000 Train loss: 0.445465 Valid loss: 0.476560 Train acc: 0.789150 Valid acc: 0.774121\n",
      "Epoch: 80/1000 Train loss: 0.443943 Valid loss: 0.475747 Train acc: 0.790075 Valid acc: 0.774564\n",
      "Epoch: 81/1000 Train loss: 0.442467 Valid loss: 0.474953 Train acc: 0.790958 Valid acc: 0.774996\n",
      "Epoch: 82/1000 Train loss: 0.440957 Valid loss: 0.474168 Train acc: 0.791894 Valid acc: 0.775413\n",
      "Epoch: 83/1000 Train loss: 0.439498 Valid loss: 0.473403 Train acc: 0.792798 Valid acc: 0.775826\n",
      "Epoch: 84/1000 Train loss: 0.438077 Valid loss: 0.472654 Train acc: 0.793674 Valid acc: 0.776225\n",
      "Epoch: 85/1000 Train loss: 0.436633 Valid loss: 0.471938 Train acc: 0.794547 Valid acc: 0.776598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/1000 Train loss: 0.435247 Valid loss: 0.471230 Train acc: 0.795412 Valid acc: 0.776970\n",
      "Epoch: 87/1000 Train loss: 0.433855 Valid loss: 0.470535 Train acc: 0.796260 Valid acc: 0.777329\n",
      "Epoch: 88/1000 Train loss: 0.432478 Valid loss: 0.469855 Train acc: 0.797111 Valid acc: 0.777676\n",
      "Epoch: 89/1000 Train loss: 0.431117 Valid loss: 0.469187 Train acc: 0.797955 Valid acc: 0.778011\n",
      "Epoch: 90/1000 Train loss: 0.429759 Valid loss: 0.468543 Train acc: 0.798781 Valid acc: 0.778333\n",
      "Epoch: 91/1000 Train loss: 0.428426 Valid loss: 0.467916 Train acc: 0.799609 Valid acc: 0.778650\n",
      "Epoch: 92/1000 Train loss: 0.427074 Valid loss: 0.467299 Train acc: 0.800425 Valid acc: 0.778971\n",
      "Epoch: 93/1000 Train loss: 0.425744 Valid loss: 0.466695 Train acc: 0.801235 Valid acc: 0.779283\n",
      "Epoch: 94/1000 Train loss: 0.424439 Valid loss: 0.466112 Train acc: 0.802029 Valid acc: 0.779596\n",
      "Epoch: 95/1000 Train loss: 0.423165 Valid loss: 0.465541 Train acc: 0.802795 Valid acc: 0.779908\n",
      "Epoch: 96/1000 Train loss: 0.421847 Valid loss: 0.464983 Train acc: 0.803601 Valid acc: 0.780219\n",
      "Epoch: 97/1000 Train loss: 0.420563 Valid loss: 0.464449 Train acc: 0.804388 Valid acc: 0.780516\n",
      "Epoch: 98/1000 Train loss: 0.419296 Valid loss: 0.463914 Train acc: 0.805159 Valid acc: 0.780805\n",
      "Epoch: 99/1000 Train loss: 0.418014 Valid loss: 0.463409 Train acc: 0.805957 Valid acc: 0.781081\n",
      "Epoch: 100/1000 Train loss: 0.416739 Valid loss: 0.462911 Train acc: 0.806738 Valid acc: 0.781354\n",
      "Epoch: 101/1000 Train loss: 0.415489 Valid loss: 0.462432 Train acc: 0.807450 Valid acc: 0.781619\n",
      "Epoch: 102/1000 Train loss: 0.414216 Valid loss: 0.461963 Train acc: 0.808232 Valid acc: 0.781892\n",
      "Epoch: 103/1000 Train loss: 0.413005 Valid loss: 0.461501 Train acc: 0.808953 Valid acc: 0.782160\n",
      "Epoch: 104/1000 Train loss: 0.411783 Valid loss: 0.461051 Train acc: 0.809675 Valid acc: 0.782415\n",
      "Epoch: 105/1000 Train loss: 0.410550 Valid loss: 0.460614 Train acc: 0.810425 Valid acc: 0.782647\n",
      "Epoch: 106/1000 Train loss: 0.409330 Valid loss: 0.460178 Train acc: 0.811171 Valid acc: 0.782902\n",
      "Epoch: 107/1000 Train loss: 0.408107 Valid loss: 0.459769 Train acc: 0.811904 Valid acc: 0.783151\n",
      "Epoch: 108/1000 Train loss: 0.406919 Valid loss: 0.459362 Train acc: 0.812591 Valid acc: 0.783398\n",
      "Epoch: 109/1000 Train loss: 0.405730 Valid loss: 0.458958 Train acc: 0.813302 Valid acc: 0.783639\n",
      "Epoch: 110/1000 Train loss: 0.404560 Valid loss: 0.458563 Train acc: 0.813993 Valid acc: 0.783875\n",
      "Epoch: 111/1000 Train loss: 0.403365 Valid loss: 0.458176 Train acc: 0.814704 Valid acc: 0.784100\n",
      "Epoch: 112/1000 Train loss: 0.402187 Valid loss: 0.457801 Train acc: 0.815399 Valid acc: 0.784319\n",
      "Epoch: 113/1000 Train loss: 0.401028 Valid loss: 0.457441 Train acc: 0.816059 Valid acc: 0.784535\n",
      "Epoch: 114/1000 Train loss: 0.399879 Valid loss: 0.457090 Train acc: 0.816745 Valid acc: 0.784751\n",
      "Epoch: 115/1000 Train loss: 0.398726 Valid loss: 0.456744 Train acc: 0.817406 Valid acc: 0.784964\n",
      "Epoch: 116/1000 Train loss: 0.397584 Valid loss: 0.456417 Train acc: 0.818085 Valid acc: 0.785170\n",
      "Epoch: 117/1000 Train loss: 0.396447 Valid loss: 0.456098 Train acc: 0.818751 Valid acc: 0.785370\n",
      "Epoch: 118/1000 Train loss: 0.395321 Valid loss: 0.455796 Train acc: 0.819403 Valid acc: 0.785571\n",
      "Epoch: 119/1000 Train loss: 0.394189 Valid loss: 0.455487 Train acc: 0.820040 Valid acc: 0.785761\n",
      "Epoch: 120/1000 Train loss: 0.393076 Valid loss: 0.455186 Train acc: 0.820687 Valid acc: 0.785950\n",
      "Epoch: 121/1000 Train loss: 0.391935 Valid loss: 0.454916 Train acc: 0.821351 Valid acc: 0.786146\n",
      "Epoch: 122/1000 Train loss: 0.390842 Valid loss: 0.454640 Train acc: 0.822016 Valid acc: 0.786337\n",
      "Epoch: 123/1000 Train loss: 0.389726 Valid loss: 0.454383 Train acc: 0.822676 Valid acc: 0.786521\n",
      "Epoch: 124/1000 Train loss: 0.388612 Valid loss: 0.454125 Train acc: 0.823347 Valid acc: 0.786699\n",
      "Epoch: 125/1000 Train loss: 0.387526 Valid loss: 0.453881 Train acc: 0.823967 Valid acc: 0.786878\n",
      "Epoch: 126/1000 Train loss: 0.386441 Valid loss: 0.453647 Train acc: 0.824579 Valid acc: 0.787052\n",
      "Epoch: 127/1000 Train loss: 0.385376 Valid loss: 0.453436 Train acc: 0.825197 Valid acc: 0.787232\n",
      "Epoch: 128/1000 Train loss: 0.384290 Valid loss: 0.453217 Train acc: 0.825829 Valid acc: 0.787401\n",
      "Epoch: 129/1000 Train loss: 0.383201 Valid loss: 0.453007 Train acc: 0.826458 Valid acc: 0.787570\n",
      "Epoch: 130/1000 Train loss: 0.382126 Valid loss: 0.452810 Train acc: 0.827068 Valid acc: 0.787741\n",
      "Epoch: 131/1000 Train loss: 0.381082 Valid loss: 0.452606 Train acc: 0.827683 Valid acc: 0.787903\n",
      "Epoch: 132/1000 Train loss: 0.380009 Valid loss: 0.452431 Train acc: 0.828314 Valid acc: 0.788073\n",
      "Epoch: 133/1000 Train loss: 0.378947 Valid loss: 0.452240 Train acc: 0.828926 Valid acc: 0.788235\n",
      "Epoch: 134/1000 Train loss: 0.377898 Valid loss: 0.452070 Train acc: 0.829531 Valid acc: 0.788399\n",
      "Epoch: 135/1000 Train loss: 0.376860 Valid loss: 0.451911 Train acc: 0.830114 Valid acc: 0.788559\n",
      "Epoch: 136/1000 Train loss: 0.375811 Valid loss: 0.451749 Train acc: 0.830719 Valid acc: 0.788723\n",
      "Epoch: 137/1000 Train loss: 0.374765 Valid loss: 0.451612 Train acc: 0.831317 Valid acc: 0.788874\n",
      "Epoch: 138/1000 Train loss: 0.373732 Valid loss: 0.451469 Train acc: 0.831897 Valid acc: 0.789032\n",
      "Epoch: 139/1000 Train loss: 0.372685 Valid loss: 0.451345 Train acc: 0.832497 Valid acc: 0.789188\n",
      "Epoch: 140/1000 Train loss: 0.371658 Valid loss: 0.451213 Train acc: 0.833095 Valid acc: 0.789341\n",
      "Epoch: 141/1000 Train loss: 0.370645 Valid loss: 0.451096 Train acc: 0.833661 Valid acc: 0.789483\n",
      "Epoch: 142/1000 Train loss: 0.369643 Valid loss: 0.450978 Train acc: 0.834239 Valid acc: 0.789626\n",
      "Epoch: 143/1000 Train loss: 0.368616 Valid loss: 0.450890 Train acc: 0.834835 Valid acc: 0.789764\n",
      "Epoch: 144/1000 Train loss: 0.367601 Valid loss: 0.450790 Train acc: 0.835411 Valid acc: 0.789909\n",
      "Epoch: 145/1000 Train loss: 0.366597 Valid loss: 0.450689 Train acc: 0.835982 Valid acc: 0.790051\n",
      "Epoch: 146/1000 Train loss: 0.365593 Valid loss: 0.450621 Train acc: 0.836553 Valid acc: 0.790193\n",
      "Epoch: 147/1000 Train loss: 0.364582 Valid loss: 0.450547 Train acc: 0.837114 Valid acc: 0.790334\n",
      "Epoch: 148/1000 Train loss: 0.363591 Valid loss: 0.450481 Train acc: 0.837676 Valid acc: 0.790472\n",
      "Epoch: 149/1000 Train loss: 0.362604 Valid loss: 0.450421 Train acc: 0.838244 Valid acc: 0.790611\n",
      "Epoch: 150/1000 Train loss: 0.361644 Valid loss: 0.450367 Train acc: 0.838790 Valid acc: 0.790748\n",
      "Epoch: 151/1000 Train loss: 0.360659 Valid loss: 0.450334 Train acc: 0.839348 Valid acc: 0.790886\n",
      "Epoch: 152/1000 Train loss: 0.359678 Valid loss: 0.450288 Train acc: 0.839884 Valid acc: 0.791021\n",
      "Epoch: 153/1000 Train loss: 0.358729 Valid loss: 0.450260 Train acc: 0.840424 Valid acc: 0.791149\n",
      "Epoch: 154/1000 Train loss: 0.357766 Valid loss: 0.450234 Train acc: 0.840958 Valid acc: 0.791272\n",
      "Epoch: 155/1000 Train loss: 0.356817 Valid loss: 0.450210 Train acc: 0.841498 Valid acc: 0.791395\n",
      "Epoch: 156/1000 Train loss: 0.355861 Valid loss: 0.450187 Train acc: 0.842019 Valid acc: 0.791518\n",
      "Epoch: 157/1000 Train loss: 0.354909 Valid loss: 0.450169 Train acc: 0.842546 Valid acc: 0.791639\n",
      "Epoch: 158/1000 Train loss: 0.353971 Valid loss: 0.450179 Train acc: 0.843067 Valid acc: 0.791749\n",
      "Epoch: 159/1000 Train loss: 0.353021 Valid loss: 0.450181 Train acc: 0.843601 Valid acc: 0.791853\n",
      "Epoch: 160/1000 Train loss: 0.352072 Valid loss: 0.450187 Train acc: 0.844126 Valid acc: 0.791962\n",
      "Epoch: 161/1000 Train loss: 0.351144 Valid loss: 0.450206 Train acc: 0.844636 Valid acc: 0.792059\n",
      "Epoch: 162/1000 Train loss: 0.350195 Valid loss: 0.450236 Train acc: 0.845175 Valid acc: 0.792152\n",
      "Epoch: 163/1000 Train loss: 0.349260 Valid loss: 0.450248 Train acc: 0.845702 Valid acc: 0.792251\n",
      "Epoch: 164/1000 Train loss: 0.348334 Valid loss: 0.450288 Train acc: 0.846208 Valid acc: 0.792350\n",
      "Epoch: 165/1000 Train loss: 0.347415 Valid loss: 0.450323 Train acc: 0.846703 Valid acc: 0.792451\n",
      "Epoch: 166/1000 Train loss: 0.346477 Valid loss: 0.450374 Train acc: 0.847221 Valid acc: 0.792544\n",
      "Epoch: 167/1000 Train loss: 0.345561 Valid loss: 0.450429 Train acc: 0.847738 Valid acc: 0.792633\n",
      "Epoch: 168/1000 Train loss: 0.344656 Valid loss: 0.450474 Train acc: 0.848231 Valid acc: 0.792723\n",
      "Epoch: 169/1000 Train loss: 0.343742 Valid loss: 0.450527 Train acc: 0.848738 Valid acc: 0.792818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170/1000 Train loss: 0.342851 Valid loss: 0.450582 Train acc: 0.849231 Valid acc: 0.792900\n",
      "Epoch: 171/1000 Train loss: 0.341946 Valid loss: 0.450643 Train acc: 0.849724 Valid acc: 0.792985\n",
      "Epoch: 172/1000 Train loss: 0.341057 Valid loss: 0.450725 Train acc: 0.850223 Valid acc: 0.793062\n",
      "Epoch: 173/1000 Train loss: 0.340158 Valid loss: 0.450803 Train acc: 0.850715 Valid acc: 0.793144\n",
      "Epoch: 174/1000 Train loss: 0.339271 Valid loss: 0.450872 Train acc: 0.851198 Valid acc: 0.793224\n",
      "Epoch: 175/1000 Train loss: 0.338378 Valid loss: 0.450985 Train acc: 0.851684 Valid acc: 0.793299\n",
      "Epoch: 176/1000 Train loss: 0.337472 Valid loss: 0.451086 Train acc: 0.852189 Valid acc: 0.793377\n",
      "Epoch: 177/1000 Train loss: 0.336565 Valid loss: 0.451169 Train acc: 0.852681 Valid acc: 0.793459\n",
      "Epoch: 178/1000 Train loss: 0.335689 Valid loss: 0.451303 Train acc: 0.853150 Valid acc: 0.793532\n",
      "Epoch: 179/1000 Train loss: 0.334826 Valid loss: 0.451410 Train acc: 0.853613 Valid acc: 0.793604\n",
      "Epoch: 180/1000 Train loss: 0.333960 Valid loss: 0.451528 Train acc: 0.854076 Valid acc: 0.793677\n",
      "Epoch: 181/1000 Train loss: 0.333096 Valid loss: 0.451682 Train acc: 0.854541 Valid acc: 0.793743\n",
      "Epoch: 182/1000 Train loss: 0.332230 Valid loss: 0.451782 Train acc: 0.855014 Valid acc: 0.793811\n",
      "Epoch: 183/1000 Train loss: 0.331376 Valid loss: 0.451905 Train acc: 0.855482 Valid acc: 0.793875\n",
      "Epoch: 184/1000 Train loss: 0.330518 Valid loss: 0.452050 Train acc: 0.855952 Valid acc: 0.793942\n",
      "Epoch: 185/1000 Train loss: 0.329675 Valid loss: 0.452188 Train acc: 0.856407 Valid acc: 0.794004\n",
      "Epoch: 186/1000 Train loss: 0.328828 Valid loss: 0.452332 Train acc: 0.856857 Valid acc: 0.794063\n",
      "Epoch: 187/1000 Train loss: 0.327974 Valid loss: 0.452496 Train acc: 0.857321 Valid acc: 0.794125\n",
      "Epoch: 188/1000 Train loss: 0.327130 Valid loss: 0.452662 Train acc: 0.857771 Valid acc: 0.794189\n",
      "Epoch: 189/1000 Train loss: 0.326293 Valid loss: 0.452829 Train acc: 0.858217 Valid acc: 0.794243\n",
      "Epoch: 190/1000 Train loss: 0.325440 Valid loss: 0.452984 Train acc: 0.858686 Valid acc: 0.794299\n",
      "Epoch: 191/1000 Train loss: 0.324591 Valid loss: 0.453160 Train acc: 0.859136 Valid acc: 0.794361\n",
      "Epoch: 192/1000 Train loss: 0.323750 Valid loss: 0.453340 Train acc: 0.859585 Valid acc: 0.794424\n",
      "Epoch: 193/1000 Train loss: 0.322919 Valid loss: 0.453525 Train acc: 0.860037 Valid acc: 0.794483\n",
      "Epoch: 194/1000 Train loss: 0.322089 Valid loss: 0.453720 Train acc: 0.860478 Valid acc: 0.794535\n",
      "Epoch: 195/1000 Train loss: 0.321263 Valid loss: 0.453931 Train acc: 0.860913 Valid acc: 0.794582\n",
      "Epoch: 196/1000 Train loss: 0.320442 Valid loss: 0.454110 Train acc: 0.861343 Valid acc: 0.794638\n",
      "Epoch: 197/1000 Train loss: 0.319636 Valid loss: 0.454329 Train acc: 0.861776 Valid acc: 0.794688\n",
      "Epoch: 198/1000 Train loss: 0.318815 Valid loss: 0.454540 Train acc: 0.862227 Valid acc: 0.794740\n",
      "Epoch: 199/1000 Train loss: 0.317994 Valid loss: 0.454723 Train acc: 0.862661 Valid acc: 0.794795\n",
      "Epoch: 200/1000 Train loss: 0.317183 Valid loss: 0.454926 Train acc: 0.863087 Valid acc: 0.794846\n",
      "Epoch: 201/1000 Train loss: 0.316384 Valid loss: 0.455137 Train acc: 0.863513 Valid acc: 0.794896\n",
      "Epoch: 202/1000 Train loss: 0.315592 Valid loss: 0.455365 Train acc: 0.863932 Valid acc: 0.794942\n",
      "Epoch: 203/1000 Train loss: 0.314797 Valid loss: 0.455568 Train acc: 0.864356 Valid acc: 0.794996\n",
      "Epoch: 204/1000 Train loss: 0.313992 Valid loss: 0.455797 Train acc: 0.864787 Valid acc: 0.795054\n",
      "Epoch: 205/1000 Train loss: 0.313183 Valid loss: 0.456044 Train acc: 0.865218 Valid acc: 0.795103\n",
      "Epoch: 206/1000 Train loss: 0.312388 Valid loss: 0.456268 Train acc: 0.865633 Valid acc: 0.795149\n",
      "Epoch: 207/1000 Train loss: 0.311586 Valid loss: 0.456515 Train acc: 0.866058 Valid acc: 0.795199\n",
      "Epoch: 208/1000 Train loss: 0.310801 Valid loss: 0.456764 Train acc: 0.866469 Valid acc: 0.795241\n",
      "Epoch: 209/1000 Train loss: 0.310013 Valid loss: 0.456990 Train acc: 0.866885 Valid acc: 0.795286\n",
      "Epoch: 210/1000 Train loss: 0.309231 Valid loss: 0.457232 Train acc: 0.867296 Valid acc: 0.795329\n",
      "Epoch: 211/1000 Train loss: 0.308453 Valid loss: 0.457497 Train acc: 0.867699 Valid acc: 0.795374\n",
      "Epoch: 212/1000 Train loss: 0.307668 Valid loss: 0.457750 Train acc: 0.868114 Valid acc: 0.795419\n",
      "Epoch: 213/1000 Train loss: 0.306888 Valid loss: 0.458005 Train acc: 0.868523 Valid acc: 0.795456\n",
      "Epoch: 214/1000 Train loss: 0.306118 Valid loss: 0.458273 Train acc: 0.868923 Valid acc: 0.795496\n",
      "Epoch: 215/1000 Train loss: 0.305344 Valid loss: 0.458550 Train acc: 0.869322 Valid acc: 0.795535\n",
      "Epoch: 216/1000 Train loss: 0.304576 Valid loss: 0.458822 Train acc: 0.869720 Valid acc: 0.795573\n",
      "Epoch: 217/1000 Train loss: 0.303804 Valid loss: 0.459106 Train acc: 0.870118 Valid acc: 0.795609\n",
      "Epoch: 218/1000 Train loss: 0.303048 Valid loss: 0.459395 Train acc: 0.870512 Valid acc: 0.795640\n",
      "Epoch: 219/1000 Train loss: 0.302287 Valid loss: 0.459652 Train acc: 0.870902 Valid acc: 0.795676\n",
      "Epoch: 220/1000 Train loss: 0.301522 Valid loss: 0.459965 Train acc: 0.871289 Valid acc: 0.795701\n",
      "Epoch: 221/1000 Train loss: 0.300755 Valid loss: 0.460244 Train acc: 0.871686 Valid acc: 0.795740\n",
      "Epoch: 222/1000 Train loss: 0.300006 Valid loss: 0.460543 Train acc: 0.872075 Valid acc: 0.795772\n",
      "Epoch: 223/1000 Train loss: 0.299259 Valid loss: 0.460838 Train acc: 0.872462 Valid acc: 0.795803\n",
      "Epoch: 224/1000 Train loss: 0.298524 Valid loss: 0.461131 Train acc: 0.872834 Valid acc: 0.795840\n",
      "Epoch: 225/1000 Train loss: 0.297772 Valid loss: 0.461456 Train acc: 0.873218 Valid acc: 0.795874\n",
      "Epoch: 226/1000 Train loss: 0.297027 Valid loss: 0.461756 Train acc: 0.873600 Valid acc: 0.795909\n",
      "Epoch: 227/1000 Train loss: 0.296296 Valid loss: 0.462064 Train acc: 0.873977 Valid acc: 0.795944\n",
      "Epoch: 228/1000 Train loss: 0.295569 Valid loss: 0.462386 Train acc: 0.874347 Valid acc: 0.795980\n",
      "Epoch: 229/1000 Train loss: 0.294834 Valid loss: 0.462720 Train acc: 0.874716 Valid acc: 0.796012\n",
      "Epoch: 230/1000 Train loss: 0.294105 Valid loss: 0.463028 Train acc: 0.875088 Valid acc: 0.796052\n",
      "Epoch: 231/1000 Train loss: 0.293383 Valid loss: 0.463360 Train acc: 0.875460 Valid acc: 0.796087\n",
      "Epoch: 232/1000 Train loss: 0.292656 Valid loss: 0.463686 Train acc: 0.875836 Valid acc: 0.796124\n",
      "Epoch: 233/1000 Train loss: 0.291940 Valid loss: 0.464011 Train acc: 0.876196 Valid acc: 0.796160\n",
      "Epoch: 234/1000 Train loss: 0.291220 Valid loss: 0.464355 Train acc: 0.876559 Valid acc: 0.796191\n",
      "Epoch: 235/1000 Train loss: 0.290502 Valid loss: 0.464700 Train acc: 0.876921 Valid acc: 0.796225\n",
      "Epoch: 236/1000 Train loss: 0.289790 Valid loss: 0.465049 Train acc: 0.877273 Valid acc: 0.796257\n",
      "Epoch: 237/1000 Train loss: 0.289076 Valid loss: 0.465418 Train acc: 0.877641 Valid acc: 0.796284\n",
      "Epoch: 238/1000 Train loss: 0.288374 Valid loss: 0.465749 Train acc: 0.877991 Valid acc: 0.796317\n",
      "Epoch: 239/1000 Train loss: 0.287673 Valid loss: 0.466093 Train acc: 0.878344 Valid acc: 0.796350\n",
      "Epoch: 240/1000 Train loss: 0.286979 Valid loss: 0.466436 Train acc: 0.878694 Valid acc: 0.796383\n",
      "Epoch: 241/1000 Train loss: 0.286286 Valid loss: 0.466803 Train acc: 0.879042 Valid acc: 0.796407\n",
      "Epoch: 242/1000 Train loss: 0.285583 Valid loss: 0.467138 Train acc: 0.879396 Valid acc: 0.796443\n",
      "Epoch: 243/1000 Train loss: 0.284887 Valid loss: 0.467510 Train acc: 0.879749 Valid acc: 0.796466\n",
      "Epoch: 244/1000 Train loss: 0.284199 Valid loss: 0.467868 Train acc: 0.880092 Valid acc: 0.796496\n",
      "Epoch: 245/1000 Train loss: 0.283523 Valid loss: 0.468236 Train acc: 0.880431 Valid acc: 0.796522\n",
      "Epoch: 246/1000 Train loss: 0.282838 Valid loss: 0.468631 Train acc: 0.880782 Valid acc: 0.796544\n",
      "Epoch: 247/1000 Train loss: 0.282169 Valid loss: 0.468955 Train acc: 0.881115 Valid acc: 0.796582\n",
      "Epoch: 248/1000 Train loss: 0.281483 Valid loss: 0.469365 Train acc: 0.881455 Valid acc: 0.796600\n",
      "Epoch: 249/1000 Train loss: 0.280809 Valid loss: 0.469729 Train acc: 0.881793 Valid acc: 0.796630\n",
      "Epoch: 250/1000 Train loss: 0.280135 Valid loss: 0.470122 Train acc: 0.882129 Valid acc: 0.796653\n",
      "Epoch: 251/1000 Train loss: 0.279466 Valid loss: 0.470507 Train acc: 0.882459 Valid acc: 0.796676\n",
      "Epoch: 252/1000 Train loss: 0.278797 Valid loss: 0.470919 Train acc: 0.882788 Valid acc: 0.796695\n",
      "Epoch: 253/1000 Train loss: 0.278136 Valid loss: 0.471287 Train acc: 0.883115 Valid acc: 0.796721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254/1000 Train loss: 0.277466 Valid loss: 0.471675 Train acc: 0.883452 Valid acc: 0.796746\n",
      "Epoch: 255/1000 Train loss: 0.276797 Valid loss: 0.472071 Train acc: 0.883789 Valid acc: 0.796768\n",
      "Epoch: 256/1000 Train loss: 0.276142 Valid loss: 0.472468 Train acc: 0.884121 Valid acc: 0.796794\n",
      "Epoch: 257/1000 Train loss: 0.275483 Valid loss: 0.472876 Train acc: 0.884453 Valid acc: 0.796821\n",
      "Epoch: 258/1000 Train loss: 0.274825 Valid loss: 0.473300 Train acc: 0.884786 Valid acc: 0.796841\n",
      "Epoch: 259/1000 Train loss: 0.274176 Valid loss: 0.473695 Train acc: 0.885111 Valid acc: 0.796866\n",
      "Epoch: 260/1000 Train loss: 0.273530 Valid loss: 0.474170 Train acc: 0.885429 Valid acc: 0.796881\n",
      "Epoch: 261/1000 Train loss: 0.272880 Valid loss: 0.474560 Train acc: 0.885745 Valid acc: 0.796908\n",
      "Epoch: 262/1000 Train loss: 0.272232 Valid loss: 0.474963 Train acc: 0.886071 Valid acc: 0.796931\n",
      "Epoch: 263/1000 Train loss: 0.271595 Valid loss: 0.475396 Train acc: 0.886384 Valid acc: 0.796956\n",
      "Epoch: 264/1000 Train loss: 0.270957 Valid loss: 0.475829 Train acc: 0.886695 Valid acc: 0.796983\n",
      "Epoch: 265/1000 Train loss: 0.270313 Valid loss: 0.476248 Train acc: 0.887013 Valid acc: 0.797004\n",
      "Epoch: 266/1000 Train loss: 0.269666 Valid loss: 0.476702 Train acc: 0.887331 Valid acc: 0.797016\n",
      "Epoch: 267/1000 Train loss: 0.269028 Valid loss: 0.477133 Train acc: 0.887648 Valid acc: 0.797031\n",
      "Epoch: 268/1000 Train loss: 0.268394 Valid loss: 0.477576 Train acc: 0.887959 Valid acc: 0.797046\n",
      "Epoch: 269/1000 Train loss: 0.267756 Valid loss: 0.478018 Train acc: 0.888270 Valid acc: 0.797059\n",
      "Epoch: 270/1000 Train loss: 0.267126 Valid loss: 0.478430 Train acc: 0.888579 Valid acc: 0.797077\n",
      "Epoch: 271/1000 Train loss: 0.266496 Valid loss: 0.478875 Train acc: 0.888892 Valid acc: 0.797087\n",
      "Epoch: 272/1000 Train loss: 0.265882 Valid loss: 0.479314 Train acc: 0.889184 Valid acc: 0.797099\n",
      "Epoch: 273/1000 Train loss: 0.265255 Valid loss: 0.479748 Train acc: 0.889488 Valid acc: 0.797114\n",
      "Epoch: 274/1000 Train loss: 0.264632 Valid loss: 0.480247 Train acc: 0.889796 Valid acc: 0.797117\n",
      "Epoch: 275/1000 Train loss: 0.264023 Valid loss: 0.480686 Train acc: 0.890091 Valid acc: 0.797128\n",
      "Epoch: 276/1000 Train loss: 0.263419 Valid loss: 0.481135 Train acc: 0.890387 Valid acc: 0.797138\n",
      "Epoch: 277/1000 Train loss: 0.262804 Valid loss: 0.481600 Train acc: 0.890690 Valid acc: 0.797136\n",
      "Epoch: 278/1000 Train loss: 0.262196 Valid loss: 0.482034 Train acc: 0.890985 Valid acc: 0.797153\n",
      "Epoch: 279/1000 Train loss: 0.261593 Valid loss: 0.482520 Train acc: 0.891271 Valid acc: 0.797157\n",
      "Epoch: 280/1000 Train loss: 0.260997 Valid loss: 0.482971 Train acc: 0.891566 Valid acc: 0.797169\n",
      "Epoch: 281/1000 Train loss: 0.260389 Valid loss: 0.483408 Train acc: 0.891860 Valid acc: 0.797184\n",
      "Epoch: 282/1000 Train loss: 0.259786 Valid loss: 0.483870 Train acc: 0.892156 Valid acc: 0.797191\n",
      "Epoch: 283/1000 Train loss: 0.259199 Valid loss: 0.484334 Train acc: 0.892438 Valid acc: 0.797193\n",
      "Epoch: 284/1000 Train loss: 0.258607 Valid loss: 0.484782 Train acc: 0.892726 Valid acc: 0.797201\n",
      "Epoch: 285/1000 Train loss: 0.258010 Valid loss: 0.485294 Train acc: 0.893009 Valid acc: 0.797192\n",
      "Epoch: 286/1000 Train loss: 0.257420 Valid loss: 0.485741 Train acc: 0.893295 Valid acc: 0.797197\n",
      "Epoch: 287/1000 Train loss: 0.256829 Valid loss: 0.486242 Train acc: 0.893581 Valid acc: 0.797185\n",
      "Epoch: 288/1000 Train loss: 0.256239 Valid loss: 0.486703 Train acc: 0.893867 Valid acc: 0.797188\n",
      "Epoch: 289/1000 Train loss: 0.255656 Valid loss: 0.487192 Train acc: 0.894148 Valid acc: 0.797192\n",
      "Epoch: 290/1000 Train loss: 0.255074 Valid loss: 0.487681 Train acc: 0.894423 Valid acc: 0.797195\n",
      "Epoch: 291/1000 Train loss: 0.254488 Valid loss: 0.488158 Train acc: 0.894706 Valid acc: 0.797194\n",
      "Epoch: 292/1000 Train loss: 0.253909 Valid loss: 0.488657 Train acc: 0.894980 Valid acc: 0.797191\n",
      "Epoch: 293/1000 Train loss: 0.253334 Valid loss: 0.489146 Train acc: 0.895254 Valid acc: 0.797188\n",
      "Epoch: 294/1000 Train loss: 0.252762 Valid loss: 0.489615 Train acc: 0.895528 Valid acc: 0.797183\n",
      "Epoch: 295/1000 Train loss: 0.252191 Valid loss: 0.490111 Train acc: 0.895803 Valid acc: 0.797168\n",
      "Epoch: 296/1000 Train loss: 0.251619 Valid loss: 0.490572 Train acc: 0.896081 Valid acc: 0.797170\n",
      "Epoch: 297/1000 Train loss: 0.251053 Valid loss: 0.491088 Train acc: 0.896351 Valid acc: 0.797162\n",
      "Epoch: 298/1000 Train loss: 0.250493 Valid loss: 0.491589 Train acc: 0.896615 Valid acc: 0.797155\n",
      "Epoch: 299/1000 Train loss: 0.249932 Valid loss: 0.492047 Train acc: 0.896884 Valid acc: 0.797161\n",
      "Epoch: 300/1000 Train loss: 0.249380 Valid loss: 0.492579 Train acc: 0.897148 Valid acc: 0.797144\n",
      "Epoch: 301/1000 Train loss: 0.248822 Valid loss: 0.493042 Train acc: 0.897417 Valid acc: 0.797149\n",
      "Epoch: 302/1000 Train loss: 0.248266 Valid loss: 0.493556 Train acc: 0.897684 Valid acc: 0.797145\n",
      "Epoch: 303/1000 Train loss: 0.247716 Valid loss: 0.494083 Train acc: 0.897943 Valid acc: 0.797139\n",
      "Epoch: 304/1000 Train loss: 0.247168 Valid loss: 0.494542 Train acc: 0.898206 Valid acc: 0.797148\n",
      "Epoch: 305/1000 Train loss: 0.246611 Valid loss: 0.495106 Train acc: 0.898473 Valid acc: 0.797128\n",
      "Epoch: 306/1000 Train loss: 0.246063 Valid loss: 0.495579 Train acc: 0.898736 Valid acc: 0.797131\n",
      "Epoch: 307/1000 Train loss: 0.245518 Valid loss: 0.496091 Train acc: 0.898991 Valid acc: 0.797124\n",
      "Epoch: 308/1000 Train loss: 0.244967 Valid loss: 0.496626 Train acc: 0.899253 Valid acc: 0.797110\n",
      "Epoch: 309/1000 Train loss: 0.244431 Valid loss: 0.497128 Train acc: 0.899506 Valid acc: 0.797110\n",
      "Epoch: 310/1000 Train loss: 0.243897 Valid loss: 0.497660 Train acc: 0.899762 Valid acc: 0.797094\n",
      "Epoch: 311/1000 Train loss: 0.243358 Valid loss: 0.498173 Train acc: 0.900016 Valid acc: 0.797080\n",
      "Epoch: 312/1000 Train loss: 0.242827 Valid loss: 0.498674 Train acc: 0.900266 Valid acc: 0.797070\n",
      "Epoch: 313/1000 Train loss: 0.242302 Valid loss: 0.499197 Train acc: 0.900517 Valid acc: 0.797052\n",
      "Epoch: 314/1000 Train loss: 0.241774 Valid loss: 0.499687 Train acc: 0.900769 Valid acc: 0.797046\n",
      "Epoch: 315/1000 Train loss: 0.241241 Valid loss: 0.500188 Train acc: 0.901022 Valid acc: 0.797041\n",
      "Epoch: 316/1000 Train loss: 0.240720 Valid loss: 0.500734 Train acc: 0.901270 Valid acc: 0.797028\n",
      "Epoch: 317/1000 Train loss: 0.240198 Valid loss: 0.501258 Train acc: 0.901516 Valid acc: 0.797019\n",
      "Epoch: 318/1000 Train loss: 0.239676 Valid loss: 0.501763 Train acc: 0.901762 Valid acc: 0.797015\n",
      "Epoch: 319/1000 Train loss: 0.239145 Valid loss: 0.502297 Train acc: 0.902011 Valid acc: 0.797005\n",
      "Epoch: 320/1000 Train loss: 0.238630 Valid loss: 0.502812 Train acc: 0.902252 Valid acc: 0.796994\n",
      "Epoch: 321/1000 Train loss: 0.238117 Valid loss: 0.503350 Train acc: 0.902492 Valid acc: 0.796982\n",
      "Epoch: 322/1000 Train loss: 0.237596 Valid loss: 0.503863 Train acc: 0.902736 Valid acc: 0.796971\n",
      "Epoch: 323/1000 Train loss: 0.237087 Valid loss: 0.504420 Train acc: 0.902975 Valid acc: 0.796950\n",
      "Epoch: 324/1000 Train loss: 0.236574 Valid loss: 0.504945 Train acc: 0.903216 Valid acc: 0.796936\n",
      "Epoch: 325/1000 Train loss: 0.236059 Valid loss: 0.505469 Train acc: 0.903455 Valid acc: 0.796919\n",
      "Epoch: 326/1000 Train loss: 0.235551 Valid loss: 0.506035 Train acc: 0.903693 Valid acc: 0.796902\n",
      "Epoch: 327/1000 Train loss: 0.235049 Valid loss: 0.506552 Train acc: 0.903924 Valid acc: 0.796894\n",
      "Epoch: 328/1000 Train loss: 0.234542 Valid loss: 0.507113 Train acc: 0.904159 Valid acc: 0.796880\n",
      "Epoch: 329/1000 Train loss: 0.234043 Valid loss: 0.507638 Train acc: 0.904393 Valid acc: 0.796871\n",
      "Epoch: 330/1000 Train loss: 0.233540 Valid loss: 0.508197 Train acc: 0.904628 Valid acc: 0.796852\n",
      "Epoch: 331/1000 Train loss: 0.233039 Valid loss: 0.508705 Train acc: 0.904859 Valid acc: 0.796843\n",
      "Epoch: 332/1000 Train loss: 0.232540 Valid loss: 0.509240 Train acc: 0.905091 Valid acc: 0.796830\n",
      "Epoch: 333/1000 Train loss: 0.232040 Valid loss: 0.509791 Train acc: 0.905323 Valid acc: 0.796822\n",
      "Epoch: 334/1000 Train loss: 0.231546 Valid loss: 0.510322 Train acc: 0.905551 Valid acc: 0.796810\n",
      "Epoch: 335/1000 Train loss: 0.231055 Valid loss: 0.510855 Train acc: 0.905780 Valid acc: 0.796797\n",
      "Epoch: 336/1000 Train loss: 0.230569 Valid loss: 0.511408 Train acc: 0.906002 Valid acc: 0.796779\n",
      "Epoch: 337/1000 Train loss: 0.230086 Valid loss: 0.511971 Train acc: 0.906227 Valid acc: 0.796764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 338/1000 Train loss: 0.229602 Valid loss: 0.512531 Train acc: 0.906450 Valid acc: 0.796748\n",
      "Epoch: 339/1000 Train loss: 0.229121 Valid loss: 0.513067 Train acc: 0.906675 Valid acc: 0.796735\n",
      "Epoch: 340/1000 Train loss: 0.228636 Valid loss: 0.513639 Train acc: 0.906901 Valid acc: 0.796720\n",
      "Epoch: 341/1000 Train loss: 0.228155 Valid loss: 0.514184 Train acc: 0.907123 Valid acc: 0.796708\n",
      "Epoch: 342/1000 Train loss: 0.227673 Valid loss: 0.514738 Train acc: 0.907346 Valid acc: 0.796696\n",
      "Epoch: 343/1000 Train loss: 0.227197 Valid loss: 0.515355 Train acc: 0.907567 Valid acc: 0.796666\n",
      "Epoch: 344/1000 Train loss: 0.226723 Valid loss: 0.515880 Train acc: 0.907783 Valid acc: 0.796658\n",
      "Epoch: 345/1000 Train loss: 0.226254 Valid loss: 0.516454 Train acc: 0.908000 Valid acc: 0.796642\n",
      "Epoch: 346/1000 Train loss: 0.225781 Valid loss: 0.517026 Train acc: 0.908217 Valid acc: 0.796623\n",
      "Epoch: 347/1000 Train loss: 0.225311 Valid loss: 0.517538 Train acc: 0.908433 Valid acc: 0.796615\n",
      "Epoch: 348/1000 Train loss: 0.224840 Valid loss: 0.518104 Train acc: 0.908650 Valid acc: 0.796596\n",
      "Epoch: 349/1000 Train loss: 0.224371 Valid loss: 0.518678 Train acc: 0.908864 Valid acc: 0.796579\n",
      "Epoch: 350/1000 Train loss: 0.223908 Valid loss: 0.519209 Train acc: 0.909077 Valid acc: 0.796568\n",
      "Epoch: 351/1000 Train loss: 0.223445 Valid loss: 0.519774 Train acc: 0.909289 Valid acc: 0.796546\n",
      "Epoch: 352/1000 Train loss: 0.222974 Valid loss: 0.520317 Train acc: 0.909504 Valid acc: 0.796528\n",
      "Epoch: 353/1000 Train loss: 0.222511 Valid loss: 0.520879 Train acc: 0.909716 Valid acc: 0.796512\n",
      "Epoch: 354/1000 Train loss: 0.222056 Valid loss: 0.521430 Train acc: 0.909924 Valid acc: 0.796503\n",
      "Epoch: 355/1000 Train loss: 0.221598 Valid loss: 0.521982 Train acc: 0.910134 Valid acc: 0.796494\n",
      "Epoch: 356/1000 Train loss: 0.221139 Valid loss: 0.522578 Train acc: 0.910342 Valid acc: 0.796475\n",
      "Epoch: 357/1000 Train loss: 0.220679 Valid loss: 0.523129 Train acc: 0.910552 Valid acc: 0.796462\n",
      "Epoch: 358/1000 Train loss: 0.220230 Valid loss: 0.523724 Train acc: 0.910760 Valid acc: 0.796441\n",
      "Epoch: 359/1000 Train loss: 0.219783 Valid loss: 0.524287 Train acc: 0.910963 Valid acc: 0.796427\n",
      "Epoch: 360/1000 Train loss: 0.219331 Valid loss: 0.524889 Train acc: 0.911168 Valid acc: 0.796403\n",
      "Epoch: 361/1000 Train loss: 0.218875 Valid loss: 0.525451 Train acc: 0.911375 Valid acc: 0.796387\n",
      "Epoch: 362/1000 Train loss: 0.218423 Valid loss: 0.525998 Train acc: 0.911578 Valid acc: 0.796378\n",
      "Epoch: 363/1000 Train loss: 0.217972 Valid loss: 0.526603 Train acc: 0.911784 Valid acc: 0.796362\n",
      "Epoch: 364/1000 Train loss: 0.217535 Valid loss: 0.527218 Train acc: 0.911983 Valid acc: 0.796343\n",
      "Epoch: 365/1000 Train loss: 0.217093 Valid loss: 0.527767 Train acc: 0.912183 Valid acc: 0.796328\n",
      "Epoch: 366/1000 Train loss: 0.216652 Valid loss: 0.528384 Train acc: 0.912385 Valid acc: 0.796301\n",
      "Epoch: 367/1000 Train loss: 0.216215 Valid loss: 0.528942 Train acc: 0.912585 Valid acc: 0.796289\n",
      "Epoch: 368/1000 Train loss: 0.215780 Valid loss: 0.529522 Train acc: 0.912781 Valid acc: 0.796274\n",
      "Epoch: 369/1000 Train loss: 0.215339 Valid loss: 0.530115 Train acc: 0.912982 Valid acc: 0.796252\n",
      "Epoch: 370/1000 Train loss: 0.214904 Valid loss: 0.530745 Train acc: 0.913179 Valid acc: 0.796223\n",
      "Epoch: 371/1000 Train loss: 0.214471 Valid loss: 0.531323 Train acc: 0.913373 Valid acc: 0.796208\n",
      "Epoch: 372/1000 Train loss: 0.214044 Valid loss: 0.531928 Train acc: 0.913569 Valid acc: 0.796187\n",
      "Epoch: 373/1000 Train loss: 0.213616 Valid loss: 0.532506 Train acc: 0.913762 Valid acc: 0.796176\n",
      "Epoch: 374/1000 Train loss: 0.213190 Valid loss: 0.533131 Train acc: 0.913954 Valid acc: 0.796157\n",
      "Epoch: 375/1000 Train loss: 0.212766 Valid loss: 0.533740 Train acc: 0.914148 Valid acc: 0.796138\n",
      "Epoch: 376/1000 Train loss: 0.212348 Valid loss: 0.534341 Train acc: 0.914332 Valid acc: 0.796121\n",
      "Epoch: 377/1000 Train loss: 0.211931 Valid loss: 0.534921 Train acc: 0.914522 Valid acc: 0.796103\n",
      "Epoch: 378/1000 Train loss: 0.211510 Valid loss: 0.535505 Train acc: 0.914713 Valid acc: 0.796081\n",
      "Epoch: 379/1000 Train loss: 0.211092 Valid loss: 0.536092 Train acc: 0.914902 Valid acc: 0.796058\n",
      "Epoch: 380/1000 Train loss: 0.210671 Valid loss: 0.536685 Train acc: 0.915093 Valid acc: 0.796042\n",
      "Epoch: 381/1000 Train loss: 0.210251 Valid loss: 0.537299 Train acc: 0.915283 Valid acc: 0.796021\n",
      "Epoch: 382/1000 Train loss: 0.209833 Valid loss: 0.537907 Train acc: 0.915471 Valid acc: 0.796004\n",
      "Epoch: 383/1000 Train loss: 0.209419 Valid loss: 0.538522 Train acc: 0.915658 Valid acc: 0.795979\n",
      "Epoch: 384/1000 Train loss: 0.209004 Valid loss: 0.539141 Train acc: 0.915847 Valid acc: 0.795952\n",
      "Epoch: 385/1000 Train loss: 0.208593 Valid loss: 0.539755 Train acc: 0.916033 Valid acc: 0.795933\n",
      "Epoch: 386/1000 Train loss: 0.208182 Valid loss: 0.540353 Train acc: 0.916218 Valid acc: 0.795915\n",
      "Epoch: 387/1000 Train loss: 0.207773 Valid loss: 0.540944 Train acc: 0.916403 Valid acc: 0.795898\n",
      "Epoch: 388/1000 Train loss: 0.207371 Valid loss: 0.541520 Train acc: 0.916584 Valid acc: 0.795880\n",
      "Epoch: 389/1000 Train loss: 0.206967 Valid loss: 0.542108 Train acc: 0.916766 Valid acc: 0.795860\n",
      "Epoch: 390/1000 Train loss: 0.206565 Valid loss: 0.542709 Train acc: 0.916944 Valid acc: 0.795840\n",
      "Epoch: 391/1000 Train loss: 0.206163 Valid loss: 0.543301 Train acc: 0.917123 Valid acc: 0.795827\n",
      "Epoch: 392/1000 Train loss: 0.205764 Valid loss: 0.543894 Train acc: 0.917302 Valid acc: 0.795815\n",
      "Epoch: 393/1000 Train loss: 0.205365 Valid loss: 0.544468 Train acc: 0.917480 Valid acc: 0.795804\n",
      "Epoch: 394/1000 Train loss: 0.204970 Valid loss: 0.545101 Train acc: 0.917661 Valid acc: 0.795780\n",
      "Epoch: 395/1000 Train loss: 0.204572 Valid loss: 0.545673 Train acc: 0.917839 Valid acc: 0.795766\n",
      "Epoch: 396/1000 Train loss: 0.204175 Valid loss: 0.546296 Train acc: 0.918018 Valid acc: 0.795740\n",
      "Epoch: 397/1000 Train loss: 0.203784 Valid loss: 0.546909 Train acc: 0.918193 Valid acc: 0.795722\n",
      "Epoch: 398/1000 Train loss: 0.203391 Valid loss: 0.547536 Train acc: 0.918366 Valid acc: 0.795696\n",
      "Epoch: 399/1000 Train loss: 0.202997 Valid loss: 0.548154 Train acc: 0.918542 Valid acc: 0.795671\n",
      "Epoch: 400/1000 Train loss: 0.202604 Valid loss: 0.548768 Train acc: 0.918720 Valid acc: 0.795652\n",
      "Epoch: 401/1000 Train loss: 0.202216 Valid loss: 0.549383 Train acc: 0.918891 Valid acc: 0.795633\n",
      "Epoch: 402/1000 Train loss: 0.201826 Valid loss: 0.549980 Train acc: 0.919068 Valid acc: 0.795619\n",
      "Epoch: 403/1000 Train loss: 0.201435 Valid loss: 0.550594 Train acc: 0.919241 Valid acc: 0.795603\n",
      "Epoch: 404/1000 Train loss: 0.201049 Valid loss: 0.551190 Train acc: 0.919415 Valid acc: 0.795589\n",
      "Epoch: 405/1000 Train loss: 0.200665 Valid loss: 0.551811 Train acc: 0.919585 Valid acc: 0.795565\n",
      "Epoch: 406/1000 Train loss: 0.200282 Valid loss: 0.552406 Train acc: 0.919757 Valid acc: 0.795550\n",
      "Epoch: 407/1000 Train loss: 0.199905 Valid loss: 0.553046 Train acc: 0.919924 Valid acc: 0.795523\n",
      "Epoch: 408/1000 Train loss: 0.199524 Valid loss: 0.553623 Train acc: 0.920095 Valid acc: 0.795507\n",
      "Epoch: 409/1000 Train loss: 0.199144 Valid loss: 0.554288 Train acc: 0.920267 Valid acc: 0.795476\n",
      "Epoch: 410/1000 Train loss: 0.198768 Valid loss: 0.554900 Train acc: 0.920432 Valid acc: 0.795465\n",
      "Epoch: 411/1000 Train loss: 0.198390 Valid loss: 0.555537 Train acc: 0.920601 Valid acc: 0.795445\n",
      "Epoch: 412/1000 Train loss: 0.198015 Valid loss: 0.556157 Train acc: 0.920768 Valid acc: 0.795424\n",
      "Epoch: 413/1000 Train loss: 0.197638 Valid loss: 0.556741 Train acc: 0.920934 Valid acc: 0.795410\n",
      "Epoch: 414/1000 Train loss: 0.197267 Valid loss: 0.557374 Train acc: 0.921098 Valid acc: 0.795391\n",
      "Epoch: 415/1000 Train loss: 0.196894 Valid loss: 0.558009 Train acc: 0.921264 Valid acc: 0.795376\n",
      "Epoch: 416/1000 Train loss: 0.196522 Valid loss: 0.558629 Train acc: 0.921428 Valid acc: 0.795363\n",
      "Epoch: 417/1000 Train loss: 0.196156 Valid loss: 0.559241 Train acc: 0.921592 Valid acc: 0.795345\n",
      "Epoch: 418/1000 Train loss: 0.195789 Valid loss: 0.559863 Train acc: 0.921753 Valid acc: 0.795323\n",
      "Epoch: 419/1000 Train loss: 0.195430 Valid loss: 0.560458 Train acc: 0.921913 Valid acc: 0.795309\n",
      "Epoch: 420/1000 Train loss: 0.195064 Valid loss: 0.561077 Train acc: 0.922077 Valid acc: 0.795296\n",
      "Epoch: 421/1000 Train loss: 0.194702 Valid loss: 0.561693 Train acc: 0.922237 Valid acc: 0.795278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 422/1000 Train loss: 0.194340 Valid loss: 0.562306 Train acc: 0.922396 Valid acc: 0.795250\n",
      "Epoch: 423/1000 Train loss: 0.193980 Valid loss: 0.562942 Train acc: 0.922556 Valid acc: 0.795220\n",
      "Epoch: 424/1000 Train loss: 0.193621 Valid loss: 0.563571 Train acc: 0.922714 Valid acc: 0.795197\n",
      "Epoch: 425/1000 Train loss: 0.193257 Valid loss: 0.564189 Train acc: 0.922875 Valid acc: 0.795179\n",
      "Epoch: 426/1000 Train loss: 0.192901 Valid loss: 0.564814 Train acc: 0.923031 Valid acc: 0.795157\n",
      "Epoch: 427/1000 Train loss: 0.192545 Valid loss: 0.565431 Train acc: 0.923188 Valid acc: 0.795136\n",
      "Epoch: 428/1000 Train loss: 0.192189 Valid loss: 0.566074 Train acc: 0.923342 Valid acc: 0.795105\n",
      "Epoch: 429/1000 Train loss: 0.191836 Valid loss: 0.566658 Train acc: 0.923497 Valid acc: 0.795092\n",
      "Epoch: 430/1000 Train loss: 0.191484 Valid loss: 0.567319 Train acc: 0.923650 Valid acc: 0.795065\n",
      "Epoch: 431/1000 Train loss: 0.191131 Valid loss: 0.567916 Train acc: 0.923807 Valid acc: 0.795056\n",
      "Epoch: 432/1000 Train loss: 0.190781 Valid loss: 0.568550 Train acc: 0.923961 Valid acc: 0.795041\n",
      "Epoch: 433/1000 Train loss: 0.190432 Valid loss: 0.569156 Train acc: 0.924115 Valid acc: 0.795029\n",
      "Epoch: 434/1000 Train loss: 0.190089 Valid loss: 0.569761 Train acc: 0.924266 Valid acc: 0.795013\n",
      "Epoch: 435/1000 Train loss: 0.189742 Valid loss: 0.570389 Train acc: 0.924419 Valid acc: 0.794991\n",
      "Epoch: 436/1000 Train loss: 0.189397 Valid loss: 0.570991 Train acc: 0.924571 Valid acc: 0.794978\n",
      "Epoch: 437/1000 Train loss: 0.189052 Valid loss: 0.571650 Train acc: 0.924722 Valid acc: 0.794951\n",
      "Epoch: 438/1000 Train loss: 0.188710 Valid loss: 0.572245 Train acc: 0.924871 Valid acc: 0.794937\n",
      "Epoch: 439/1000 Train loss: 0.188368 Valid loss: 0.572883 Train acc: 0.925021 Valid acc: 0.794913\n",
      "Epoch: 440/1000 Train loss: 0.188030 Valid loss: 0.573497 Train acc: 0.925167 Valid acc: 0.794894\n",
      "Epoch: 441/1000 Train loss: 0.187692 Valid loss: 0.574088 Train acc: 0.925314 Valid acc: 0.794885\n",
      "Epoch: 442/1000 Train loss: 0.187349 Valid loss: 0.574734 Train acc: 0.925463 Valid acc: 0.794862\n",
      "Epoch: 443/1000 Train loss: 0.187012 Valid loss: 0.575350 Train acc: 0.925611 Valid acc: 0.794852\n",
      "Epoch: 444/1000 Train loss: 0.186673 Valid loss: 0.575985 Train acc: 0.925759 Valid acc: 0.794837\n",
      "Epoch: 445/1000 Train loss: 0.186340 Valid loss: 0.576623 Train acc: 0.925904 Valid acc: 0.794818\n",
      "Epoch: 446/1000 Train loss: 0.186005 Valid loss: 0.577259 Train acc: 0.926052 Valid acc: 0.794799\n",
      "Epoch: 447/1000 Train loss: 0.185672 Valid loss: 0.577898 Train acc: 0.926199 Valid acc: 0.794776\n",
      "Epoch: 448/1000 Train loss: 0.185337 Valid loss: 0.578528 Train acc: 0.926346 Valid acc: 0.794753\n",
      "Epoch: 449/1000 Train loss: 0.185006 Valid loss: 0.579184 Train acc: 0.926489 Valid acc: 0.794723\n",
      "Epoch: 450/1000 Train loss: 0.184676 Valid loss: 0.579829 Train acc: 0.926633 Valid acc: 0.794701\n",
      "Epoch: 451/1000 Train loss: 0.184344 Valid loss: 0.580473 Train acc: 0.926778 Valid acc: 0.794680\n",
      "Epoch: 452/1000 Train loss: 0.184017 Valid loss: 0.581116 Train acc: 0.926920 Valid acc: 0.794658\n",
      "Epoch: 453/1000 Train loss: 0.183689 Valid loss: 0.581737 Train acc: 0.927062 Valid acc: 0.794643\n",
      "Epoch: 454/1000 Train loss: 0.183362 Valid loss: 0.582371 Train acc: 0.927205 Valid acc: 0.794629\n",
      "Epoch: 455/1000 Train loss: 0.183039 Valid loss: 0.583031 Train acc: 0.927346 Valid acc: 0.794610\n",
      "Epoch: 456/1000 Train loss: 0.182716 Valid loss: 0.583677 Train acc: 0.927486 Valid acc: 0.794591\n",
      "Epoch: 457/1000 Train loss: 0.182394 Valid loss: 0.584291 Train acc: 0.927625 Valid acc: 0.794578\n",
      "Epoch: 458/1000 Train loss: 0.182073 Valid loss: 0.584940 Train acc: 0.927764 Valid acc: 0.794556\n",
      "Epoch: 459/1000 Train loss: 0.181751 Valid loss: 0.585591 Train acc: 0.927903 Valid acc: 0.794536\n",
      "Epoch: 460/1000 Train loss: 0.181431 Valid loss: 0.586239 Train acc: 0.928041 Valid acc: 0.794517\n",
      "Epoch: 461/1000 Train loss: 0.181111 Valid loss: 0.586867 Train acc: 0.928181 Valid acc: 0.794493\n",
      "Epoch: 462/1000 Train loss: 0.180792 Valid loss: 0.587515 Train acc: 0.928318 Valid acc: 0.794469\n",
      "Epoch: 463/1000 Train loss: 0.180475 Valid loss: 0.588149 Train acc: 0.928456 Valid acc: 0.794453\n",
      "Epoch: 464/1000 Train loss: 0.180160 Valid loss: 0.588814 Train acc: 0.928593 Valid acc: 0.794433\n",
      "Epoch: 465/1000 Train loss: 0.179848 Valid loss: 0.589457 Train acc: 0.928728 Valid acc: 0.794414\n",
      "Epoch: 466/1000 Train loss: 0.179533 Valid loss: 0.590117 Train acc: 0.928863 Valid acc: 0.794390\n",
      "Epoch: 467/1000 Train loss: 0.179222 Valid loss: 0.590731 Train acc: 0.928999 Valid acc: 0.794375\n",
      "Epoch: 468/1000 Train loss: 0.178907 Valid loss: 0.591395 Train acc: 0.929137 Valid acc: 0.794353\n",
      "Epoch: 469/1000 Train loss: 0.178599 Valid loss: 0.592031 Train acc: 0.929271 Valid acc: 0.794338\n",
      "Epoch: 470/1000 Train loss: 0.178286 Valid loss: 0.592653 Train acc: 0.929406 Valid acc: 0.794325\n",
      "Epoch: 471/1000 Train loss: 0.177974 Valid loss: 0.593297 Train acc: 0.929540 Valid acc: 0.794300\n",
      "Epoch: 472/1000 Train loss: 0.177670 Valid loss: 0.593909 Train acc: 0.929669 Valid acc: 0.794290\n",
      "Epoch: 473/1000 Train loss: 0.177363 Valid loss: 0.594589 Train acc: 0.929800 Valid acc: 0.794265\n",
      "Epoch: 474/1000 Train loss: 0.177056 Valid loss: 0.595182 Train acc: 0.929932 Valid acc: 0.794260\n",
      "Epoch: 475/1000 Train loss: 0.176748 Valid loss: 0.595819 Train acc: 0.930064 Valid acc: 0.794243\n",
      "Epoch: 476/1000 Train loss: 0.176448 Valid loss: 0.596479 Train acc: 0.930193 Valid acc: 0.794223\n",
      "Epoch: 477/1000 Train loss: 0.176147 Valid loss: 0.597112 Train acc: 0.930323 Valid acc: 0.794205\n",
      "Epoch: 478/1000 Train loss: 0.175844 Valid loss: 0.597778 Train acc: 0.930453 Valid acc: 0.794179\n",
      "Epoch: 479/1000 Train loss: 0.175546 Valid loss: 0.598371 Train acc: 0.930582 Valid acc: 0.794170\n",
      "Epoch: 480/1000 Train loss: 0.175247 Valid loss: 0.598993 Train acc: 0.930711 Valid acc: 0.794151\n",
      "Epoch: 481/1000 Train loss: 0.174946 Valid loss: 0.599647 Train acc: 0.930840 Valid acc: 0.794131\n",
      "Epoch: 482/1000 Train loss: 0.174648 Valid loss: 0.600262 Train acc: 0.930968 Valid acc: 0.794117\n",
      "Epoch: 483/1000 Train loss: 0.174343 Valid loss: 0.600890 Train acc: 0.931100 Valid acc: 0.794097\n",
      "Epoch: 484/1000 Train loss: 0.174046 Valid loss: 0.601526 Train acc: 0.931227 Valid acc: 0.794079\n",
      "Epoch: 485/1000 Train loss: 0.173751 Valid loss: 0.602177 Train acc: 0.931355 Valid acc: 0.794059\n",
      "Epoch: 486/1000 Train loss: 0.173456 Valid loss: 0.602791 Train acc: 0.931481 Valid acc: 0.794043\n",
      "Epoch: 487/1000 Train loss: 0.173162 Valid loss: 0.603462 Train acc: 0.931607 Valid acc: 0.794015\n",
      "Epoch: 488/1000 Train loss: 0.172871 Valid loss: 0.604062 Train acc: 0.931732 Valid acc: 0.794003\n",
      "Epoch: 489/1000 Train loss: 0.172579 Valid loss: 0.604716 Train acc: 0.931856 Valid acc: 0.793973\n",
      "Epoch: 490/1000 Train loss: 0.172289 Valid loss: 0.605333 Train acc: 0.931981 Valid acc: 0.793950\n",
      "Epoch: 491/1000 Train loss: 0.172001 Valid loss: 0.605981 Train acc: 0.932106 Valid acc: 0.793926\n",
      "Epoch: 492/1000 Train loss: 0.171708 Valid loss: 0.606609 Train acc: 0.932229 Valid acc: 0.793913\n",
      "Epoch: 493/1000 Train loss: 0.171420 Valid loss: 0.607260 Train acc: 0.932352 Valid acc: 0.793893\n",
      "Epoch: 494/1000 Train loss: 0.171132 Valid loss: 0.607894 Train acc: 0.932474 Valid acc: 0.793873\n",
      "Epoch: 495/1000 Train loss: 0.170843 Valid loss: 0.608552 Train acc: 0.932599 Valid acc: 0.793850\n",
      "Epoch: 496/1000 Train loss: 0.170561 Valid loss: 0.609207 Train acc: 0.932720 Valid acc: 0.793833\n",
      "Epoch: 497/1000 Train loss: 0.170276 Valid loss: 0.609848 Train acc: 0.932843 Valid acc: 0.793816\n",
      "Epoch: 498/1000 Train loss: 0.169994 Valid loss: 0.610520 Train acc: 0.932964 Valid acc: 0.793791\n",
      "Epoch: 499/1000 Train loss: 0.169710 Valid loss: 0.611128 Train acc: 0.933087 Valid acc: 0.793780\n",
      "Epoch: 500/1000 Train loss: 0.169427 Valid loss: 0.611770 Train acc: 0.933208 Valid acc: 0.793764\n",
      "Epoch: 501/1000 Train loss: 0.169145 Valid loss: 0.612419 Train acc: 0.933327 Valid acc: 0.793745\n",
      "Epoch: 502/1000 Train loss: 0.168864 Valid loss: 0.613085 Train acc: 0.933448 Valid acc: 0.793724\n",
      "Epoch: 503/1000 Train loss: 0.168583 Valid loss: 0.613721 Train acc: 0.933569 Valid acc: 0.793707\n",
      "Epoch: 504/1000 Train loss: 0.168303 Valid loss: 0.614371 Train acc: 0.933690 Valid acc: 0.793687\n",
      "Epoch: 505/1000 Train loss: 0.168027 Valid loss: 0.614988 Train acc: 0.933807 Valid acc: 0.793667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 506/1000 Train loss: 0.167749 Valid loss: 0.615668 Train acc: 0.933926 Valid acc: 0.793639\n",
      "Epoch: 507/1000 Train loss: 0.167472 Valid loss: 0.616273 Train acc: 0.934044 Valid acc: 0.793628\n",
      "Epoch: 508/1000 Train loss: 0.167196 Valid loss: 0.616928 Train acc: 0.934162 Valid acc: 0.793605\n",
      "Epoch: 509/1000 Train loss: 0.166925 Valid loss: 0.617546 Train acc: 0.934278 Valid acc: 0.793593\n",
      "Epoch: 510/1000 Train loss: 0.166652 Valid loss: 0.618220 Train acc: 0.934393 Valid acc: 0.793566\n",
      "Epoch: 511/1000 Train loss: 0.166379 Valid loss: 0.618850 Train acc: 0.934510 Valid acc: 0.793553\n",
      "Epoch: 512/1000 Train loss: 0.166107 Valid loss: 0.619538 Train acc: 0.934628 Valid acc: 0.793526\n",
      "Epoch: 513/1000 Train loss: 0.165835 Valid loss: 0.620144 Train acc: 0.934744 Valid acc: 0.793514\n",
      "Epoch: 514/1000 Train loss: 0.165564 Valid loss: 0.620795 Train acc: 0.934861 Valid acc: 0.793495\n",
      "Epoch: 515/1000 Train loss: 0.165296 Valid loss: 0.621438 Train acc: 0.934974 Valid acc: 0.793474\n",
      "Epoch: 516/1000 Train loss: 0.165029 Valid loss: 0.622081 Train acc: 0.935087 Valid acc: 0.793450\n",
      "Epoch: 517/1000 Train loss: 0.164761 Valid loss: 0.622713 Train acc: 0.935200 Valid acc: 0.793437\n",
      "Epoch: 518/1000 Train loss: 0.164494 Valid loss: 0.623392 Train acc: 0.935313 Valid acc: 0.793418\n",
      "Epoch: 519/1000 Train loss: 0.164229 Valid loss: 0.624028 Train acc: 0.935424 Valid acc: 0.793403\n",
      "Epoch: 520/1000 Train loss: 0.163963 Valid loss: 0.624647 Train acc: 0.935537 Valid acc: 0.793386\n",
      "Epoch: 521/1000 Train loss: 0.163697 Valid loss: 0.625258 Train acc: 0.935650 Valid acc: 0.793376\n",
      "Epoch: 522/1000 Train loss: 0.163434 Valid loss: 0.625914 Train acc: 0.935761 Valid acc: 0.793359\n",
      "Epoch: 523/1000 Train loss: 0.163172 Valid loss: 0.626563 Train acc: 0.935872 Valid acc: 0.793337\n",
      "Epoch: 524/1000 Train loss: 0.162909 Valid loss: 0.627174 Train acc: 0.935983 Valid acc: 0.793319\n",
      "Epoch: 525/1000 Train loss: 0.162648 Valid loss: 0.627838 Train acc: 0.936094 Valid acc: 0.793295\n",
      "Epoch: 526/1000 Train loss: 0.162388 Valid loss: 0.628510 Train acc: 0.936205 Valid acc: 0.793277\n",
      "Epoch: 527/1000 Train loss: 0.162129 Valid loss: 0.629157 Train acc: 0.936314 Valid acc: 0.793263\n",
      "Epoch: 528/1000 Train loss: 0.161873 Valid loss: 0.629776 Train acc: 0.936424 Valid acc: 0.793251\n",
      "Epoch: 529/1000 Train loss: 0.161612 Valid loss: 0.630407 Train acc: 0.936535 Valid acc: 0.793230\n",
      "Epoch: 530/1000 Train loss: 0.161351 Valid loss: 0.631025 Train acc: 0.936645 Valid acc: 0.793210\n",
      "Epoch: 531/1000 Train loss: 0.161095 Valid loss: 0.631654 Train acc: 0.936754 Valid acc: 0.793186\n",
      "Epoch: 532/1000 Train loss: 0.160839 Valid loss: 0.632291 Train acc: 0.936862 Valid acc: 0.793162\n",
      "Epoch: 533/1000 Train loss: 0.160586 Valid loss: 0.632925 Train acc: 0.936968 Valid acc: 0.793142\n",
      "Epoch: 534/1000 Train loss: 0.160330 Valid loss: 0.633593 Train acc: 0.937077 Valid acc: 0.793118\n",
      "Epoch: 535/1000 Train loss: 0.160076 Valid loss: 0.634189 Train acc: 0.937184 Valid acc: 0.793107\n",
      "Epoch: 536/1000 Train loss: 0.159822 Valid loss: 0.634866 Train acc: 0.937292 Valid acc: 0.793078\n",
      "Epoch: 537/1000 Train loss: 0.159570 Valid loss: 0.635467 Train acc: 0.937397 Valid acc: 0.793067\n",
      "Epoch: 538/1000 Train loss: 0.159320 Valid loss: 0.636132 Train acc: 0.937502 Valid acc: 0.793044\n",
      "Epoch: 539/1000 Train loss: 0.159069 Valid loss: 0.636728 Train acc: 0.937610 Valid acc: 0.793037\n",
      "Epoch: 540/1000 Train loss: 0.158821 Valid loss: 0.637389 Train acc: 0.937715 Valid acc: 0.793014\n",
      "Epoch: 541/1000 Train loss: 0.158571 Valid loss: 0.637994 Train acc: 0.937820 Valid acc: 0.793000\n",
      "Epoch: 542/1000 Train loss: 0.158322 Valid loss: 0.638638 Train acc: 0.937925 Valid acc: 0.792980\n",
      "Epoch: 543/1000 Train loss: 0.158074 Valid loss: 0.639240 Train acc: 0.938030 Valid acc: 0.792970\n",
      "Epoch: 544/1000 Train loss: 0.157827 Valid loss: 0.639886 Train acc: 0.938134 Valid acc: 0.792953\n",
      "Epoch: 545/1000 Train loss: 0.157580 Valid loss: 0.640516 Train acc: 0.938239 Valid acc: 0.792933\n",
      "Epoch: 546/1000 Train loss: 0.157338 Valid loss: 0.641120 Train acc: 0.938344 Valid acc: 0.792914\n",
      "Epoch: 547/1000 Train loss: 0.157093 Valid loss: 0.641774 Train acc: 0.938447 Valid acc: 0.792891\n",
      "Epoch: 548/1000 Train loss: 0.156847 Valid loss: 0.642386 Train acc: 0.938549 Valid acc: 0.792875\n",
      "Epoch: 549/1000 Train loss: 0.156603 Valid loss: 0.643034 Train acc: 0.938652 Valid acc: 0.792848\n",
      "Epoch: 550/1000 Train loss: 0.156362 Valid loss: 0.643693 Train acc: 0.938753 Valid acc: 0.792818\n",
      "Epoch: 551/1000 Train loss: 0.156120 Valid loss: 0.644345 Train acc: 0.938856 Valid acc: 0.792791\n",
      "Epoch: 552/1000 Train loss: 0.155881 Valid loss: 0.644969 Train acc: 0.938957 Valid acc: 0.792772\n",
      "Epoch: 553/1000 Train loss: 0.155642 Valid loss: 0.645623 Train acc: 0.939057 Valid acc: 0.792750\n",
      "Epoch: 554/1000 Train loss: 0.155401 Valid loss: 0.646249 Train acc: 0.939159 Valid acc: 0.792734\n",
      "Epoch: 555/1000 Train loss: 0.155160 Valid loss: 0.646872 Train acc: 0.939260 Valid acc: 0.792717\n",
      "Epoch: 556/1000 Train loss: 0.154923 Valid loss: 0.647485 Train acc: 0.939359 Valid acc: 0.792696\n",
      "Epoch: 557/1000 Train loss: 0.154686 Valid loss: 0.648114 Train acc: 0.939459 Valid acc: 0.792677\n",
      "Epoch: 558/1000 Train loss: 0.154451 Valid loss: 0.648763 Train acc: 0.939559 Valid acc: 0.792657\n",
      "Epoch: 559/1000 Train loss: 0.154214 Valid loss: 0.649404 Train acc: 0.939660 Valid acc: 0.792637\n",
      "Epoch: 560/1000 Train loss: 0.153977 Valid loss: 0.650040 Train acc: 0.939759 Valid acc: 0.792621\n",
      "Epoch: 561/1000 Train loss: 0.153741 Valid loss: 0.650691 Train acc: 0.939858 Valid acc: 0.792602\n",
      "Epoch: 562/1000 Train loss: 0.153507 Valid loss: 0.651328 Train acc: 0.939956 Valid acc: 0.792584\n",
      "Epoch: 563/1000 Train loss: 0.153269 Valid loss: 0.651958 Train acc: 0.940057 Valid acc: 0.792567\n",
      "Epoch: 564/1000 Train loss: 0.153036 Valid loss: 0.652628 Train acc: 0.940155 Valid acc: 0.792540\n",
      "Epoch: 565/1000 Train loss: 0.152803 Valid loss: 0.653241 Train acc: 0.940253 Valid acc: 0.792522\n",
      "Epoch: 566/1000 Train loss: 0.152572 Valid loss: 0.653885 Train acc: 0.940350 Valid acc: 0.792502\n",
      "Epoch: 567/1000 Train loss: 0.152340 Valid loss: 0.654520 Train acc: 0.940448 Valid acc: 0.792483\n",
      "Epoch: 568/1000 Train loss: 0.152107 Valid loss: 0.655175 Train acc: 0.940547 Valid acc: 0.792459\n",
      "Epoch: 569/1000 Train loss: 0.151877 Valid loss: 0.655835 Train acc: 0.940642 Valid acc: 0.792434\n",
      "Epoch: 570/1000 Train loss: 0.151650 Valid loss: 0.656503 Train acc: 0.940736 Valid acc: 0.792410\n",
      "Epoch: 571/1000 Train loss: 0.151424 Valid loss: 0.657104 Train acc: 0.940830 Valid acc: 0.792397\n",
      "Epoch: 572/1000 Train loss: 0.151196 Valid loss: 0.657766 Train acc: 0.940926 Valid acc: 0.792366\n",
      "Epoch: 573/1000 Train loss: 0.150968 Valid loss: 0.658383 Train acc: 0.941022 Valid acc: 0.792345\n",
      "Epoch: 574/1000 Train loss: 0.150739 Valid loss: 0.659024 Train acc: 0.941119 Valid acc: 0.792328\n",
      "Epoch: 575/1000 Train loss: 0.150513 Valid loss: 0.659641 Train acc: 0.941214 Valid acc: 0.792311\n",
      "Epoch: 576/1000 Train loss: 0.150290 Valid loss: 0.660297 Train acc: 0.941308 Valid acc: 0.792287\n",
      "Epoch: 577/1000 Train loss: 0.150068 Valid loss: 0.660918 Train acc: 0.941402 Valid acc: 0.792270\n",
      "Epoch: 578/1000 Train loss: 0.149846 Valid loss: 0.661584 Train acc: 0.941495 Valid acc: 0.792244\n",
      "Epoch: 579/1000 Train loss: 0.149623 Valid loss: 0.662205 Train acc: 0.941588 Valid acc: 0.792223\n",
      "Epoch: 580/1000 Train loss: 0.149400 Valid loss: 0.662827 Train acc: 0.941682 Valid acc: 0.792204\n",
      "Epoch: 581/1000 Train loss: 0.149179 Valid loss: 0.663479 Train acc: 0.941774 Valid acc: 0.792182\n",
      "Epoch: 582/1000 Train loss: 0.148958 Valid loss: 0.664083 Train acc: 0.941866 Valid acc: 0.792173\n",
      "Epoch: 583/1000 Train loss: 0.148737 Valid loss: 0.664746 Train acc: 0.941958 Valid acc: 0.792146\n",
      "Epoch: 584/1000 Train loss: 0.148518 Valid loss: 0.665351 Train acc: 0.942049 Valid acc: 0.792128\n",
      "Epoch: 585/1000 Train loss: 0.148297 Valid loss: 0.665979 Train acc: 0.942142 Valid acc: 0.792105\n",
      "Epoch: 586/1000 Train loss: 0.148076 Valid loss: 0.666616 Train acc: 0.942234 Valid acc: 0.792088\n",
      "Epoch: 587/1000 Train loss: 0.147856 Valid loss: 0.667264 Train acc: 0.942326 Valid acc: 0.792070\n",
      "Epoch: 588/1000 Train loss: 0.147639 Valid loss: 0.667890 Train acc: 0.942417 Valid acc: 0.792048\n",
      "Epoch: 589/1000 Train loss: 0.147421 Valid loss: 0.668511 Train acc: 0.942508 Valid acc: 0.792025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 590/1000 Train loss: 0.147203 Valid loss: 0.669139 Train acc: 0.942598 Valid acc: 0.792006\n",
      "Epoch: 591/1000 Train loss: 0.146987 Valid loss: 0.669804 Train acc: 0.942688 Valid acc: 0.791978\n",
      "Epoch: 592/1000 Train loss: 0.146770 Valid loss: 0.670414 Train acc: 0.942779 Valid acc: 0.791955\n",
      "Epoch: 593/1000 Train loss: 0.146555 Valid loss: 0.671046 Train acc: 0.942868 Valid acc: 0.791931\n",
      "Epoch: 594/1000 Train loss: 0.146341 Valid loss: 0.671664 Train acc: 0.942957 Valid acc: 0.791909\n",
      "Epoch: 595/1000 Train loss: 0.146129 Valid loss: 0.672337 Train acc: 0.943044 Valid acc: 0.791880\n",
      "Epoch: 596/1000 Train loss: 0.145917 Valid loss: 0.672905 Train acc: 0.943132 Valid acc: 0.791870\n",
      "Epoch: 597/1000 Train loss: 0.145706 Valid loss: 0.673545 Train acc: 0.943221 Valid acc: 0.791846\n",
      "Epoch: 598/1000 Train loss: 0.145494 Valid loss: 0.674161 Train acc: 0.943309 Valid acc: 0.791823\n",
      "Epoch: 599/1000 Train loss: 0.145282 Valid loss: 0.674794 Train acc: 0.943398 Valid acc: 0.791799\n",
      "Epoch: 600/1000 Train loss: 0.145070 Valid loss: 0.675413 Train acc: 0.943486 Valid acc: 0.791780\n",
      "Epoch: 601/1000 Train loss: 0.144860 Valid loss: 0.676037 Train acc: 0.943573 Valid acc: 0.791761\n",
      "Epoch: 602/1000 Train loss: 0.144649 Valid loss: 0.676679 Train acc: 0.943661 Valid acc: 0.791739\n",
      "Epoch: 603/1000 Train loss: 0.144438 Valid loss: 0.677297 Train acc: 0.943748 Valid acc: 0.791720\n",
      "Epoch: 604/1000 Train loss: 0.144230 Valid loss: 0.677959 Train acc: 0.943835 Valid acc: 0.791695\n",
      "Epoch: 605/1000 Train loss: 0.144021 Valid loss: 0.678617 Train acc: 0.943922 Valid acc: 0.791669\n",
      "Epoch: 606/1000 Train loss: 0.143812 Valid loss: 0.679235 Train acc: 0.944009 Valid acc: 0.791651\n",
      "Epoch: 607/1000 Train loss: 0.143606 Valid loss: 0.679887 Train acc: 0.944094 Valid acc: 0.791630\n",
      "Epoch: 608/1000 Train loss: 0.143402 Valid loss: 0.680505 Train acc: 0.944178 Valid acc: 0.791612\n",
      "Epoch: 609/1000 Train loss: 0.143195 Valid loss: 0.681155 Train acc: 0.944264 Valid acc: 0.791589\n",
      "Epoch: 610/1000 Train loss: 0.142989 Valid loss: 0.681769 Train acc: 0.944349 Valid acc: 0.791570\n",
      "Epoch: 611/1000 Train loss: 0.142785 Valid loss: 0.682384 Train acc: 0.944434 Valid acc: 0.791550\n",
      "Epoch: 612/1000 Train loss: 0.142581 Valid loss: 0.683005 Train acc: 0.944519 Valid acc: 0.791530\n",
      "Epoch: 613/1000 Train loss: 0.142379 Valid loss: 0.683646 Train acc: 0.944601 Valid acc: 0.791510\n",
      "Epoch: 614/1000 Train loss: 0.142175 Valid loss: 0.684249 Train acc: 0.944686 Valid acc: 0.791496\n",
      "Epoch: 615/1000 Train loss: 0.141973 Valid loss: 0.684882 Train acc: 0.944769 Valid acc: 0.791476\n",
      "Epoch: 616/1000 Train loss: 0.141770 Valid loss: 0.685503 Train acc: 0.944854 Valid acc: 0.791455\n",
      "Epoch: 617/1000 Train loss: 0.141571 Valid loss: 0.686150 Train acc: 0.944936 Valid acc: 0.791432\n",
      "Epoch: 618/1000 Train loss: 0.141369 Valid loss: 0.686767 Train acc: 0.945019 Valid acc: 0.791416\n",
      "Epoch: 619/1000 Train loss: 0.141168 Valid loss: 0.687430 Train acc: 0.945103 Valid acc: 0.791395\n",
      "Epoch: 620/1000 Train loss: 0.140971 Valid loss: 0.688022 Train acc: 0.945184 Valid acc: 0.791378\n",
      "Epoch: 621/1000 Train loss: 0.140771 Valid loss: 0.688696 Train acc: 0.945268 Valid acc: 0.791352\n",
      "Epoch: 622/1000 Train loss: 0.140575 Valid loss: 0.689288 Train acc: 0.945350 Valid acc: 0.791341\n",
      "Epoch: 623/1000 Train loss: 0.140376 Valid loss: 0.689945 Train acc: 0.945432 Valid acc: 0.791320\n",
      "Epoch: 624/1000 Train loss: 0.140179 Valid loss: 0.690558 Train acc: 0.945513 Valid acc: 0.791302\n",
      "Epoch: 625/1000 Train loss: 0.139982 Valid loss: 0.691192 Train acc: 0.945595 Valid acc: 0.791280\n",
      "Epoch: 626/1000 Train loss: 0.139786 Valid loss: 0.691831 Train acc: 0.945675 Valid acc: 0.791258\n",
      "Epoch: 627/1000 Train loss: 0.139588 Valid loss: 0.692452 Train acc: 0.945757 Valid acc: 0.791242\n",
      "Epoch: 628/1000 Train loss: 0.139394 Valid loss: 0.693097 Train acc: 0.945838 Valid acc: 0.791224\n",
      "Epoch: 629/1000 Train loss: 0.139200 Valid loss: 0.693728 Train acc: 0.945919 Valid acc: 0.791208\n",
      "Epoch: 630/1000 Train loss: 0.139007 Valid loss: 0.694356 Train acc: 0.945999 Valid acc: 0.791186\n",
      "Epoch: 631/1000 Train loss: 0.138816 Valid loss: 0.694982 Train acc: 0.946078 Valid acc: 0.791164\n",
      "Epoch: 632/1000 Train loss: 0.138621 Valid loss: 0.695619 Train acc: 0.946158 Valid acc: 0.791141\n",
      "Epoch: 633/1000 Train loss: 0.138428 Valid loss: 0.696226 Train acc: 0.946237 Valid acc: 0.791122\n",
      "Epoch: 634/1000 Train loss: 0.138237 Valid loss: 0.696857 Train acc: 0.946315 Valid acc: 0.791100\n",
      "Epoch: 635/1000 Train loss: 0.138045 Valid loss: 0.697454 Train acc: 0.946394 Valid acc: 0.791082\n",
      "Epoch: 636/1000 Train loss: 0.137853 Valid loss: 0.698091 Train acc: 0.946473 Valid acc: 0.791061\n",
      "Epoch: 637/1000 Train loss: 0.137662 Valid loss: 0.698741 Train acc: 0.946552 Valid acc: 0.791039\n",
      "Epoch: 638/1000 Train loss: 0.137471 Valid loss: 0.699365 Train acc: 0.946631 Valid acc: 0.791019\n",
      "Epoch: 639/1000 Train loss: 0.137279 Valid loss: 0.700006 Train acc: 0.946710 Valid acc: 0.790999\n",
      "Epoch: 640/1000 Train loss: 0.137091 Valid loss: 0.700620 Train acc: 0.946787 Valid acc: 0.790980\n",
      "Epoch: 641/1000 Train loss: 0.136903 Valid loss: 0.701238 Train acc: 0.946865 Valid acc: 0.790961\n",
      "Epoch: 642/1000 Train loss: 0.136715 Valid loss: 0.701861 Train acc: 0.946943 Valid acc: 0.790941\n",
      "Epoch: 643/1000 Train loss: 0.136528 Valid loss: 0.702482 Train acc: 0.947021 Valid acc: 0.790922\n",
      "Epoch: 644/1000 Train loss: 0.136342 Valid loss: 0.703133 Train acc: 0.947098 Valid acc: 0.790900\n",
      "Epoch: 645/1000 Train loss: 0.136156 Valid loss: 0.703756 Train acc: 0.947174 Valid acc: 0.790880\n",
      "Epoch: 646/1000 Train loss: 0.135970 Valid loss: 0.704376 Train acc: 0.947250 Valid acc: 0.790861\n",
      "Epoch: 647/1000 Train loss: 0.135783 Valid loss: 0.705000 Train acc: 0.947327 Valid acc: 0.790839\n",
      "Epoch: 648/1000 Train loss: 0.135597 Valid loss: 0.705635 Train acc: 0.947403 Valid acc: 0.790816\n",
      "Epoch: 649/1000 Train loss: 0.135412 Valid loss: 0.706241 Train acc: 0.947480 Valid acc: 0.790795\n",
      "Epoch: 650/1000 Train loss: 0.135228 Valid loss: 0.706873 Train acc: 0.947555 Valid acc: 0.790771\n",
      "Epoch: 651/1000 Train loss: 0.135045 Valid loss: 0.707516 Train acc: 0.947630 Valid acc: 0.790748\n",
      "Epoch: 652/1000 Train loss: 0.134863 Valid loss: 0.708115 Train acc: 0.947703 Valid acc: 0.790734\n",
      "Epoch: 653/1000 Train loss: 0.134679 Valid loss: 0.708733 Train acc: 0.947780 Valid acc: 0.790717\n",
      "Epoch: 654/1000 Train loss: 0.134498 Valid loss: 0.709361 Train acc: 0.947854 Valid acc: 0.790696\n",
      "Epoch: 655/1000 Train loss: 0.134317 Valid loss: 0.709989 Train acc: 0.947928 Valid acc: 0.790676\n",
      "Epoch: 656/1000 Train loss: 0.134135 Valid loss: 0.710641 Train acc: 0.948004 Valid acc: 0.790653\n",
      "Epoch: 657/1000 Train loss: 0.133953 Valid loss: 0.711292 Train acc: 0.948078 Valid acc: 0.790632\n",
      "Epoch: 658/1000 Train loss: 0.133773 Valid loss: 0.711938 Train acc: 0.948152 Valid acc: 0.790614\n",
      "Epoch: 659/1000 Train loss: 0.133592 Valid loss: 0.712537 Train acc: 0.948226 Valid acc: 0.790604\n",
      "Epoch: 660/1000 Train loss: 0.133411 Valid loss: 0.713192 Train acc: 0.948300 Valid acc: 0.790589\n",
      "Epoch: 661/1000 Train loss: 0.133232 Valid loss: 0.713800 Train acc: 0.948374 Valid acc: 0.790574\n",
      "Epoch: 662/1000 Train loss: 0.133055 Valid loss: 0.714418 Train acc: 0.948447 Valid acc: 0.790554\n",
      "Epoch: 663/1000 Train loss: 0.132877 Valid loss: 0.715017 Train acc: 0.948520 Valid acc: 0.790535\n",
      "Epoch: 664/1000 Train loss: 0.132700 Valid loss: 0.715601 Train acc: 0.948593 Valid acc: 0.790519\n",
      "Epoch: 665/1000 Train loss: 0.132521 Valid loss: 0.716233 Train acc: 0.948667 Valid acc: 0.790499\n",
      "Epoch: 666/1000 Train loss: 0.132346 Valid loss: 0.716847 Train acc: 0.948739 Valid acc: 0.790481\n",
      "Epoch: 667/1000 Train loss: 0.132169 Valid loss: 0.717460 Train acc: 0.948812 Valid acc: 0.790463\n",
      "Epoch: 668/1000 Train loss: 0.131993 Valid loss: 0.718068 Train acc: 0.948884 Valid acc: 0.790447\n",
      "Epoch: 669/1000 Train loss: 0.131819 Valid loss: 0.718692 Train acc: 0.948955 Valid acc: 0.790426\n",
      "Epoch: 670/1000 Train loss: 0.131644 Valid loss: 0.719291 Train acc: 0.949027 Valid acc: 0.790409\n",
      "Epoch: 671/1000 Train loss: 0.131469 Valid loss: 0.719945 Train acc: 0.949098 Valid acc: 0.790385\n",
      "Epoch: 672/1000 Train loss: 0.131294 Valid loss: 0.720531 Train acc: 0.949170 Valid acc: 0.790371\n",
      "Epoch: 673/1000 Train loss: 0.131120 Valid loss: 0.721171 Train acc: 0.949241 Valid acc: 0.790347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 674/1000 Train loss: 0.130946 Valid loss: 0.721768 Train acc: 0.949312 Valid acc: 0.790332\n",
      "Epoch: 675/1000 Train loss: 0.130773 Valid loss: 0.722405 Train acc: 0.949383 Valid acc: 0.790310\n",
      "Epoch: 676/1000 Train loss: 0.130601 Valid loss: 0.722995 Train acc: 0.949454 Valid acc: 0.790293\n",
      "Epoch: 677/1000 Train loss: 0.130430 Valid loss: 0.723613 Train acc: 0.949524 Valid acc: 0.790273\n",
      "Epoch: 678/1000 Train loss: 0.130259 Valid loss: 0.724209 Train acc: 0.949595 Valid acc: 0.790256\n",
      "Epoch: 679/1000 Train loss: 0.130087 Valid loss: 0.724829 Train acc: 0.949664 Valid acc: 0.790231\n",
      "Epoch: 680/1000 Train loss: 0.129917 Valid loss: 0.725421 Train acc: 0.949734 Valid acc: 0.790207\n",
      "Epoch: 681/1000 Train loss: 0.129747 Valid loss: 0.726047 Train acc: 0.949804 Valid acc: 0.790182\n",
      "Epoch: 682/1000 Train loss: 0.129579 Valid loss: 0.726680 Train acc: 0.949872 Valid acc: 0.790158\n",
      "Epoch: 683/1000 Train loss: 0.129408 Valid loss: 0.727286 Train acc: 0.949942 Valid acc: 0.790139\n",
      "Epoch: 684/1000 Train loss: 0.129238 Valid loss: 0.727876 Train acc: 0.950011 Valid acc: 0.790123\n",
      "Epoch: 685/1000 Train loss: 0.129070 Valid loss: 0.728495 Train acc: 0.950080 Valid acc: 0.790100\n",
      "Epoch: 686/1000 Train loss: 0.128901 Valid loss: 0.729121 Train acc: 0.950149 Valid acc: 0.790075\n",
      "Epoch: 687/1000 Train loss: 0.128733 Valid loss: 0.729721 Train acc: 0.950217 Valid acc: 0.790058\n",
      "Epoch: 688/1000 Train loss: 0.128565 Valid loss: 0.730307 Train acc: 0.950286 Valid acc: 0.790042\n",
      "Epoch: 689/1000 Train loss: 0.128398 Valid loss: 0.730920 Train acc: 0.950355 Valid acc: 0.790021\n",
      "Epoch: 690/1000 Train loss: 0.128233 Valid loss: 0.731524 Train acc: 0.950422 Valid acc: 0.789999\n",
      "Epoch: 691/1000 Train loss: 0.128066 Valid loss: 0.732144 Train acc: 0.950491 Valid acc: 0.789978\n",
      "Epoch: 692/1000 Train loss: 0.127900 Valid loss: 0.732739 Train acc: 0.950559 Valid acc: 0.789960\n",
      "Epoch: 693/1000 Train loss: 0.127735 Valid loss: 0.733401 Train acc: 0.950626 Valid acc: 0.789936\n",
      "Epoch: 694/1000 Train loss: 0.127571 Valid loss: 0.733991 Train acc: 0.950692 Valid acc: 0.789916\n",
      "Epoch: 695/1000 Train loss: 0.127406 Valid loss: 0.734626 Train acc: 0.950759 Valid acc: 0.789893\n",
      "Epoch: 696/1000 Train loss: 0.127242 Valid loss: 0.735250 Train acc: 0.950827 Valid acc: 0.789875\n",
      "Epoch: 697/1000 Train loss: 0.127079 Valid loss: 0.735861 Train acc: 0.950893 Valid acc: 0.789856\n",
      "Epoch: 698/1000 Train loss: 0.126916 Valid loss: 0.736447 Train acc: 0.950959 Valid acc: 0.789838\n",
      "Epoch: 699/1000 Train loss: 0.126754 Valid loss: 0.737079 Train acc: 0.951026 Valid acc: 0.789812\n",
      "Epoch: 700/1000 Train loss: 0.126592 Valid loss: 0.737684 Train acc: 0.951093 Valid acc: 0.789791\n",
      "Epoch: 701/1000 Train loss: 0.126430 Valid loss: 0.738310 Train acc: 0.951158 Valid acc: 0.789769\n",
      "Epoch: 702/1000 Train loss: 0.126268 Valid loss: 0.738872 Train acc: 0.951224 Valid acc: 0.789756\n",
      "Epoch: 703/1000 Train loss: 0.126106 Valid loss: 0.739533 Train acc: 0.951290 Valid acc: 0.789727\n",
      "Epoch: 704/1000 Train loss: 0.125946 Valid loss: 0.740114 Train acc: 0.951356 Valid acc: 0.789708\n",
      "Epoch: 705/1000 Train loss: 0.125786 Valid loss: 0.740724 Train acc: 0.951420 Valid acc: 0.789685\n",
      "Epoch: 706/1000 Train loss: 0.125627 Valid loss: 0.741287 Train acc: 0.951485 Valid acc: 0.789670\n",
      "Epoch: 707/1000 Train loss: 0.125467 Valid loss: 0.741877 Train acc: 0.951550 Valid acc: 0.789656\n",
      "Epoch: 708/1000 Train loss: 0.125307 Valid loss: 0.742463 Train acc: 0.951615 Valid acc: 0.789643\n",
      "Epoch: 709/1000 Train loss: 0.125148 Valid loss: 0.743048 Train acc: 0.951680 Valid acc: 0.789628\n",
      "Epoch: 710/1000 Train loss: 0.124990 Valid loss: 0.743640 Train acc: 0.951745 Valid acc: 0.789610\n",
      "Epoch: 711/1000 Train loss: 0.124835 Valid loss: 0.744261 Train acc: 0.951808 Valid acc: 0.789588\n",
      "Epoch: 712/1000 Train loss: 0.124679 Valid loss: 0.744874 Train acc: 0.951871 Valid acc: 0.789568\n",
      "Epoch: 713/1000 Train loss: 0.124522 Valid loss: 0.745500 Train acc: 0.951935 Valid acc: 0.789543\n",
      "Epoch: 714/1000 Train loss: 0.124364 Valid loss: 0.746070 Train acc: 0.952000 Valid acc: 0.789526\n",
      "Epoch: 715/1000 Train loss: 0.124209 Valid loss: 0.746720 Train acc: 0.952062 Valid acc: 0.789502\n",
      "Epoch: 716/1000 Train loss: 0.124053 Valid loss: 0.747284 Train acc: 0.952126 Valid acc: 0.789488\n",
      "Epoch: 717/1000 Train loss: 0.123896 Valid loss: 0.747912 Train acc: 0.952190 Valid acc: 0.789468\n",
      "Epoch: 718/1000 Train loss: 0.123741 Valid loss: 0.748533 Train acc: 0.952253 Valid acc: 0.789444\n",
      "Epoch: 719/1000 Train loss: 0.123588 Valid loss: 0.749129 Train acc: 0.952316 Valid acc: 0.789427\n",
      "Epoch: 720/1000 Train loss: 0.123435 Valid loss: 0.749725 Train acc: 0.952378 Valid acc: 0.789411\n",
      "Epoch: 721/1000 Train loss: 0.123280 Valid loss: 0.750353 Train acc: 0.952441 Valid acc: 0.789395\n",
      "Epoch: 722/1000 Train loss: 0.123127 Valid loss: 0.750966 Train acc: 0.952504 Valid acc: 0.789380\n",
      "Epoch: 723/1000 Train loss: 0.122974 Valid loss: 0.751564 Train acc: 0.952565 Valid acc: 0.789365\n",
      "Epoch: 724/1000 Train loss: 0.122820 Valid loss: 0.752221 Train acc: 0.952628 Valid acc: 0.789342\n",
      "Epoch: 725/1000 Train loss: 0.122668 Valid loss: 0.752802 Train acc: 0.952690 Valid acc: 0.789325\n",
      "Epoch: 726/1000 Train loss: 0.122516 Valid loss: 0.753417 Train acc: 0.952751 Valid acc: 0.789304\n",
      "Epoch: 727/1000 Train loss: 0.122364 Valid loss: 0.754023 Train acc: 0.952812 Valid acc: 0.789284\n",
      "Epoch: 728/1000 Train loss: 0.122211 Valid loss: 0.754625 Train acc: 0.952874 Valid acc: 0.789266\n",
      "Epoch: 729/1000 Train loss: 0.122060 Valid loss: 0.755225 Train acc: 0.952935 Valid acc: 0.789246\n",
      "Epoch: 730/1000 Train loss: 0.121908 Valid loss: 0.755813 Train acc: 0.952997 Valid acc: 0.789229\n",
      "Epoch: 731/1000 Train loss: 0.121756 Valid loss: 0.756438 Train acc: 0.953059 Valid acc: 0.789210\n",
      "Epoch: 732/1000 Train loss: 0.121606 Valid loss: 0.757028 Train acc: 0.953119 Valid acc: 0.789193\n",
      "Epoch: 733/1000 Train loss: 0.121457 Valid loss: 0.757632 Train acc: 0.953180 Valid acc: 0.789170\n",
      "Epoch: 734/1000 Train loss: 0.121308 Valid loss: 0.758187 Train acc: 0.953241 Valid acc: 0.789153\n",
      "Epoch: 735/1000 Train loss: 0.121159 Valid loss: 0.758777 Train acc: 0.953301 Valid acc: 0.789136\n",
      "Epoch: 736/1000 Train loss: 0.121009 Valid loss: 0.759319 Train acc: 0.953362 Valid acc: 0.789124\n",
      "Epoch: 737/1000 Train loss: 0.120858 Valid loss: 0.759905 Train acc: 0.953423 Valid acc: 0.789104\n",
      "Epoch: 738/1000 Train loss: 0.120712 Valid loss: 0.760507 Train acc: 0.953483 Valid acc: 0.789086\n",
      "Epoch: 739/1000 Train loss: 0.120563 Valid loss: 0.761101 Train acc: 0.953543 Valid acc: 0.789069\n",
      "Epoch: 740/1000 Train loss: 0.120415 Valid loss: 0.761753 Train acc: 0.953603 Valid acc: 0.789046\n",
      "Epoch: 741/1000 Train loss: 0.120270 Valid loss: 0.762323 Train acc: 0.953662 Valid acc: 0.789029\n",
      "Epoch: 742/1000 Train loss: 0.120123 Valid loss: 0.762946 Train acc: 0.953722 Valid acc: 0.789007\n",
      "Epoch: 743/1000 Train loss: 0.119977 Valid loss: 0.763514 Train acc: 0.953781 Valid acc: 0.788991\n",
      "Epoch: 744/1000 Train loss: 0.119830 Valid loss: 0.764106 Train acc: 0.953840 Valid acc: 0.788972\n",
      "Epoch: 745/1000 Train loss: 0.119686 Valid loss: 0.764674 Train acc: 0.953899 Valid acc: 0.788953\n",
      "Epoch: 746/1000 Train loss: 0.119540 Valid loss: 0.765301 Train acc: 0.953958 Valid acc: 0.788931\n",
      "Epoch: 747/1000 Train loss: 0.119395 Valid loss: 0.765859 Train acc: 0.954016 Valid acc: 0.788920\n",
      "Epoch: 748/1000 Train loss: 0.119251 Valid loss: 0.766474 Train acc: 0.954075 Valid acc: 0.788897\n",
      "Epoch: 749/1000 Train loss: 0.119104 Valid loss: 0.766975 Train acc: 0.954134 Valid acc: 0.788888\n",
      "Epoch: 750/1000 Train loss: 0.118961 Valid loss: 0.767648 Train acc: 0.954191 Valid acc: 0.788862\n",
      "Epoch: 751/1000 Train loss: 0.118818 Valid loss: 0.768219 Train acc: 0.954249 Valid acc: 0.788847\n",
      "Epoch: 752/1000 Train loss: 0.118674 Valid loss: 0.768822 Train acc: 0.954307 Valid acc: 0.788831\n",
      "Epoch: 753/1000 Train loss: 0.118531 Valid loss: 0.769422 Train acc: 0.954365 Valid acc: 0.788814\n",
      "Epoch: 754/1000 Train loss: 0.118386 Valid loss: 0.770014 Train acc: 0.954423 Valid acc: 0.788795\n",
      "Epoch: 755/1000 Train loss: 0.118244 Valid loss: 0.770628 Train acc: 0.954481 Valid acc: 0.788773\n",
      "Epoch: 756/1000 Train loss: 0.118102 Valid loss: 0.771204 Train acc: 0.954538 Valid acc: 0.788756\n",
      "Epoch: 757/1000 Train loss: 0.117961 Valid loss: 0.771833 Train acc: 0.954596 Valid acc: 0.788732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 758/1000 Train loss: 0.117820 Valid loss: 0.772396 Train acc: 0.954652 Valid acc: 0.788713\n",
      "Epoch: 759/1000 Train loss: 0.117679 Valid loss: 0.772996 Train acc: 0.954709 Valid acc: 0.788692\n",
      "Epoch: 760/1000 Train loss: 0.117538 Valid loss: 0.773537 Train acc: 0.954766 Valid acc: 0.788679\n",
      "Epoch: 761/1000 Train loss: 0.117398 Valid loss: 0.774173 Train acc: 0.954822 Valid acc: 0.788656\n",
      "Epoch: 762/1000 Train loss: 0.117259 Valid loss: 0.774762 Train acc: 0.954877 Valid acc: 0.788637\n",
      "Epoch: 763/1000 Train loss: 0.117120 Valid loss: 0.775372 Train acc: 0.954934 Valid acc: 0.788616\n",
      "Epoch: 764/1000 Train loss: 0.116980 Valid loss: 0.775963 Train acc: 0.954991 Valid acc: 0.788597\n",
      "Epoch: 765/1000 Train loss: 0.116841 Valid loss: 0.776527 Train acc: 0.955047 Valid acc: 0.788580\n",
      "Epoch: 766/1000 Train loss: 0.116702 Valid loss: 0.777102 Train acc: 0.955104 Valid acc: 0.788561\n",
      "Epoch: 767/1000 Train loss: 0.116563 Valid loss: 0.777666 Train acc: 0.955160 Valid acc: 0.788543\n",
      "Epoch: 768/1000 Train loss: 0.116425 Valid loss: 0.778243 Train acc: 0.955216 Valid acc: 0.788524\n",
      "Epoch: 769/1000 Train loss: 0.116288 Valid loss: 0.778807 Train acc: 0.955271 Valid acc: 0.788508\n",
      "Epoch: 770/1000 Train loss: 0.116150 Valid loss: 0.779399 Train acc: 0.955327 Valid acc: 0.788491\n",
      "Epoch: 771/1000 Train loss: 0.116013 Valid loss: 0.779978 Train acc: 0.955382 Valid acc: 0.788477\n",
      "Epoch: 772/1000 Train loss: 0.115877 Valid loss: 0.780584 Train acc: 0.955437 Valid acc: 0.788456\n",
      "Epoch: 773/1000 Train loss: 0.115741 Valid loss: 0.781165 Train acc: 0.955492 Valid acc: 0.788438\n",
      "Epoch: 774/1000 Train loss: 0.115605 Valid loss: 0.781737 Train acc: 0.955547 Valid acc: 0.788420\n",
      "Epoch: 775/1000 Train loss: 0.115467 Valid loss: 0.782340 Train acc: 0.955602 Valid acc: 0.788400\n",
      "Epoch: 776/1000 Train loss: 0.115332 Valid loss: 0.782901 Train acc: 0.955657 Valid acc: 0.788388\n",
      "Epoch: 777/1000 Train loss: 0.115197 Valid loss: 0.783508 Train acc: 0.955711 Valid acc: 0.788370\n",
      "Epoch: 778/1000 Train loss: 0.115061 Valid loss: 0.784076 Train acc: 0.955766 Valid acc: 0.788359\n",
      "Epoch: 779/1000 Train loss: 0.114926 Valid loss: 0.784657 Train acc: 0.955821 Valid acc: 0.788343\n",
      "Epoch: 780/1000 Train loss: 0.114791 Valid loss: 0.785225 Train acc: 0.955875 Valid acc: 0.788328\n",
      "Epoch: 781/1000 Train loss: 0.114657 Valid loss: 0.785816 Train acc: 0.955929 Valid acc: 0.788308\n",
      "Epoch: 782/1000 Train loss: 0.114521 Valid loss: 0.786389 Train acc: 0.955985 Valid acc: 0.788288\n",
      "Epoch: 783/1000 Train loss: 0.114388 Valid loss: 0.786944 Train acc: 0.956038 Valid acc: 0.788270\n",
      "Epoch: 784/1000 Train loss: 0.114255 Valid loss: 0.787530 Train acc: 0.956092 Valid acc: 0.788250\n",
      "Epoch: 785/1000 Train loss: 0.114123 Valid loss: 0.788065 Train acc: 0.956145 Valid acc: 0.788239\n",
      "Epoch: 786/1000 Train loss: 0.113990 Valid loss: 0.788646 Train acc: 0.956199 Valid acc: 0.788223\n",
      "Epoch: 787/1000 Train loss: 0.113859 Valid loss: 0.789197 Train acc: 0.956252 Valid acc: 0.788208\n",
      "Epoch: 788/1000 Train loss: 0.113727 Valid loss: 0.789784 Train acc: 0.956304 Valid acc: 0.788187\n",
      "Epoch: 789/1000 Train loss: 0.113596 Valid loss: 0.790325 Train acc: 0.956357 Valid acc: 0.788169\n",
      "Epoch: 790/1000 Train loss: 0.113464 Valid loss: 0.790903 Train acc: 0.956411 Valid acc: 0.788155\n",
      "Epoch: 791/1000 Train loss: 0.113333 Valid loss: 0.791487 Train acc: 0.956463 Valid acc: 0.788142\n",
      "Epoch: 792/1000 Train loss: 0.113202 Valid loss: 0.792069 Train acc: 0.956515 Valid acc: 0.788127\n",
      "Epoch: 793/1000 Train loss: 0.113072 Valid loss: 0.792654 Train acc: 0.956568 Valid acc: 0.788107\n",
      "Epoch: 794/1000 Train loss: 0.112941 Valid loss: 0.793172 Train acc: 0.956621 Valid acc: 0.788096\n",
      "Epoch: 795/1000 Train loss: 0.112811 Valid loss: 0.793780 Train acc: 0.956673 Valid acc: 0.788077\n",
      "Epoch: 796/1000 Train loss: 0.112680 Valid loss: 0.794342 Train acc: 0.956725 Valid acc: 0.788061\n",
      "Epoch: 797/1000 Train loss: 0.112551 Valid loss: 0.794949 Train acc: 0.956777 Valid acc: 0.788044\n",
      "Epoch: 798/1000 Train loss: 0.112421 Valid loss: 0.795542 Train acc: 0.956829 Valid acc: 0.788027\n",
      "Epoch: 799/1000 Train loss: 0.112293 Valid loss: 0.796118 Train acc: 0.956880 Valid acc: 0.788011\n",
      "Epoch: 800/1000 Train loss: 0.112164 Valid loss: 0.796678 Train acc: 0.956932 Valid acc: 0.787996\n",
      "Epoch: 801/1000 Train loss: 0.112037 Valid loss: 0.797265 Train acc: 0.956983 Valid acc: 0.787978\n",
      "Epoch: 802/1000 Train loss: 0.111908 Valid loss: 0.797844 Train acc: 0.957035 Valid acc: 0.787959\n",
      "Epoch: 803/1000 Train loss: 0.111781 Valid loss: 0.798405 Train acc: 0.957086 Valid acc: 0.787940\n",
      "Epoch: 804/1000 Train loss: 0.111653 Valid loss: 0.798974 Train acc: 0.957138 Valid acc: 0.787924\n",
      "Epoch: 805/1000 Train loss: 0.111527 Valid loss: 0.799549 Train acc: 0.957189 Valid acc: 0.787913\n",
      "Epoch: 806/1000 Train loss: 0.111399 Valid loss: 0.800124 Train acc: 0.957240 Valid acc: 0.787901\n",
      "Epoch: 807/1000 Train loss: 0.111273 Valid loss: 0.800687 Train acc: 0.957291 Valid acc: 0.787886\n",
      "Epoch: 808/1000 Train loss: 0.111146 Valid loss: 0.801225 Train acc: 0.957342 Valid acc: 0.787873\n",
      "Epoch: 809/1000 Train loss: 0.111020 Valid loss: 0.801809 Train acc: 0.957392 Valid acc: 0.787857\n",
      "Epoch: 810/1000 Train loss: 0.110895 Valid loss: 0.802367 Train acc: 0.957443 Valid acc: 0.787841\n",
      "Epoch: 811/1000 Train loss: 0.110769 Valid loss: 0.802929 Train acc: 0.957493 Valid acc: 0.787825\n",
      "Epoch: 812/1000 Train loss: 0.110643 Valid loss: 0.803476 Train acc: 0.957543 Valid acc: 0.787811\n",
      "Epoch: 813/1000 Train loss: 0.110517 Valid loss: 0.804070 Train acc: 0.957594 Valid acc: 0.787791\n",
      "Epoch: 814/1000 Train loss: 0.110393 Valid loss: 0.804668 Train acc: 0.957643 Valid acc: 0.787773\n",
      "Epoch: 815/1000 Train loss: 0.110269 Valid loss: 0.805229 Train acc: 0.957692 Valid acc: 0.787760\n",
      "Epoch: 816/1000 Train loss: 0.110145 Valid loss: 0.805821 Train acc: 0.957742 Valid acc: 0.787741\n",
      "Epoch: 817/1000 Train loss: 0.110022 Valid loss: 0.806356 Train acc: 0.957791 Valid acc: 0.787727\n",
      "Epoch: 818/1000 Train loss: 0.109899 Valid loss: 0.806958 Train acc: 0.957841 Valid acc: 0.787707\n",
      "Epoch: 819/1000 Train loss: 0.109776 Valid loss: 0.807498 Train acc: 0.957890 Valid acc: 0.787693\n",
      "Epoch: 820/1000 Train loss: 0.109653 Valid loss: 0.808105 Train acc: 0.957940 Valid acc: 0.787674\n",
      "Epoch: 821/1000 Train loss: 0.109530 Valid loss: 0.808677 Train acc: 0.957990 Valid acc: 0.787660\n",
      "Epoch: 822/1000 Train loss: 0.109407 Valid loss: 0.809312 Train acc: 0.958039 Valid acc: 0.787641\n",
      "Epoch: 823/1000 Train loss: 0.109286 Valid loss: 0.809858 Train acc: 0.958088 Valid acc: 0.787628\n",
      "Epoch: 824/1000 Train loss: 0.109164 Valid loss: 0.810465 Train acc: 0.958137 Valid acc: 0.787609\n",
      "Epoch: 825/1000 Train loss: 0.109043 Valid loss: 0.811038 Train acc: 0.958185 Valid acc: 0.787596\n",
      "Epoch: 826/1000 Train loss: 0.108923 Valid loss: 0.811615 Train acc: 0.958234 Valid acc: 0.787583\n",
      "Epoch: 827/1000 Train loss: 0.108802 Valid loss: 0.812189 Train acc: 0.958283 Valid acc: 0.787570\n",
      "Epoch: 828/1000 Train loss: 0.108681 Valid loss: 0.812733 Train acc: 0.958331 Valid acc: 0.787556\n",
      "Epoch: 829/1000 Train loss: 0.108560 Valid loss: 0.813328 Train acc: 0.958380 Valid acc: 0.787538\n",
      "Epoch: 830/1000 Train loss: 0.108442 Valid loss: 0.813877 Train acc: 0.958428 Valid acc: 0.787524\n",
      "Epoch: 831/1000 Train loss: 0.108321 Valid loss: 0.814510 Train acc: 0.958477 Valid acc: 0.787504\n",
      "Epoch: 832/1000 Train loss: 0.108200 Valid loss: 0.815047 Train acc: 0.958525 Valid acc: 0.787491\n",
      "Epoch: 833/1000 Train loss: 0.108080 Valid loss: 0.815635 Train acc: 0.958573 Valid acc: 0.787474\n",
      "Epoch: 834/1000 Train loss: 0.107960 Valid loss: 0.816158 Train acc: 0.958621 Valid acc: 0.787468\n",
      "Epoch: 835/1000 Train loss: 0.107841 Valid loss: 0.816716 Train acc: 0.958670 Valid acc: 0.787460\n",
      "Epoch: 836/1000 Train loss: 0.107722 Valid loss: 0.817274 Train acc: 0.958717 Valid acc: 0.787447\n",
      "Epoch: 837/1000 Train loss: 0.107603 Valid loss: 0.817833 Train acc: 0.958765 Valid acc: 0.787434\n",
      "Epoch: 838/1000 Train loss: 0.107485 Valid loss: 0.818378 Train acc: 0.958811 Valid acc: 0.787422\n",
      "Epoch: 839/1000 Train loss: 0.107367 Valid loss: 0.818927 Train acc: 0.958858 Valid acc: 0.787408\n",
      "Epoch: 840/1000 Train loss: 0.107249 Valid loss: 0.819458 Train acc: 0.958905 Valid acc: 0.787392\n",
      "Epoch: 841/1000 Train loss: 0.107131 Valid loss: 0.820006 Train acc: 0.958953 Valid acc: 0.787376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 842/1000 Train loss: 0.107013 Valid loss: 0.820547 Train acc: 0.959000 Valid acc: 0.787362\n",
      "Epoch: 843/1000 Train loss: 0.106896 Valid loss: 0.821100 Train acc: 0.959047 Valid acc: 0.787349\n",
      "Epoch: 844/1000 Train loss: 0.106779 Valid loss: 0.821668 Train acc: 0.959094 Valid acc: 0.787335\n",
      "Epoch: 845/1000 Train loss: 0.106662 Valid loss: 0.822232 Train acc: 0.959140 Valid acc: 0.787320\n",
      "Epoch: 846/1000 Train loss: 0.106546 Valid loss: 0.822818 Train acc: 0.959187 Valid acc: 0.787303\n",
      "Epoch: 847/1000 Train loss: 0.106429 Valid loss: 0.823340 Train acc: 0.959233 Valid acc: 0.787293\n",
      "Epoch: 848/1000 Train loss: 0.106313 Valid loss: 0.823945 Train acc: 0.959280 Valid acc: 0.787277\n",
      "Epoch: 849/1000 Train loss: 0.106197 Valid loss: 0.824472 Train acc: 0.959326 Valid acc: 0.787266\n",
      "Epoch: 850/1000 Train loss: 0.106081 Valid loss: 0.825059 Train acc: 0.959373 Valid acc: 0.787248\n",
      "Epoch: 851/1000 Train loss: 0.105966 Valid loss: 0.825632 Train acc: 0.959419 Valid acc: 0.787233\n",
      "Epoch: 852/1000 Train loss: 0.105851 Valid loss: 0.826212 Train acc: 0.959465 Valid acc: 0.787220\n",
      "Epoch: 853/1000 Train loss: 0.105736 Valid loss: 0.826756 Train acc: 0.959511 Valid acc: 0.787208\n",
      "Epoch: 854/1000 Train loss: 0.105621 Valid loss: 0.827307 Train acc: 0.959557 Valid acc: 0.787191\n",
      "Epoch: 855/1000 Train loss: 0.105508 Valid loss: 0.827854 Train acc: 0.959602 Valid acc: 0.787174\n",
      "Epoch: 856/1000 Train loss: 0.105393 Valid loss: 0.828424 Train acc: 0.959648 Valid acc: 0.787155\n",
      "Epoch: 857/1000 Train loss: 0.105280 Valid loss: 0.828943 Train acc: 0.959694 Valid acc: 0.787144\n",
      "Epoch: 858/1000 Train loss: 0.105167 Valid loss: 0.829538 Train acc: 0.959739 Valid acc: 0.787127\n",
      "Epoch: 859/1000 Train loss: 0.105054 Valid loss: 0.830049 Train acc: 0.959784 Valid acc: 0.787117\n",
      "Epoch: 860/1000 Train loss: 0.104942 Valid loss: 0.830654 Train acc: 0.959828 Valid acc: 0.787098\n",
      "Epoch: 861/1000 Train loss: 0.104830 Valid loss: 0.831147 Train acc: 0.959873 Valid acc: 0.787088\n",
      "Epoch: 862/1000 Train loss: 0.104718 Valid loss: 0.831764 Train acc: 0.959918 Valid acc: 0.787071\n",
      "Epoch: 863/1000 Train loss: 0.104606 Valid loss: 0.832318 Train acc: 0.959963 Valid acc: 0.787059\n",
      "Epoch: 864/1000 Train loss: 0.104494 Valid loss: 0.832888 Train acc: 0.960007 Valid acc: 0.787043\n",
      "Epoch: 865/1000 Train loss: 0.104381 Valid loss: 0.833451 Train acc: 0.960052 Valid acc: 0.787027\n",
      "Epoch: 866/1000 Train loss: 0.104271 Valid loss: 0.833968 Train acc: 0.960096 Valid acc: 0.787014\n",
      "Epoch: 867/1000 Train loss: 0.104159 Valid loss: 0.834523 Train acc: 0.960141 Valid acc: 0.786997\n",
      "Epoch: 868/1000 Train loss: 0.104049 Valid loss: 0.835063 Train acc: 0.960185 Valid acc: 0.786982\n",
      "Epoch: 869/1000 Train loss: 0.103937 Valid loss: 0.835614 Train acc: 0.960230 Valid acc: 0.786968\n",
      "Epoch: 870/1000 Train loss: 0.103826 Valid loss: 0.836147 Train acc: 0.960274 Valid acc: 0.786954\n",
      "Epoch: 871/1000 Train loss: 0.103717 Valid loss: 0.836698 Train acc: 0.960318 Valid acc: 0.786938\n",
      "Epoch: 872/1000 Train loss: 0.103606 Valid loss: 0.837241 Train acc: 0.960362 Valid acc: 0.786922\n",
      "Epoch: 873/1000 Train loss: 0.103495 Valid loss: 0.837804 Train acc: 0.960406 Valid acc: 0.786907\n",
      "Epoch: 874/1000 Train loss: 0.103386 Valid loss: 0.838376 Train acc: 0.960449 Valid acc: 0.786892\n",
      "Epoch: 875/1000 Train loss: 0.103277 Valid loss: 0.838858 Train acc: 0.960492 Valid acc: 0.786885\n",
      "Epoch: 876/1000 Train loss: 0.103168 Valid loss: 0.839421 Train acc: 0.960536 Valid acc: 0.786868\n",
      "Epoch: 877/1000 Train loss: 0.103059 Valid loss: 0.839933 Train acc: 0.960579 Valid acc: 0.786856\n",
      "Epoch: 878/1000 Train loss: 0.102950 Valid loss: 0.840467 Train acc: 0.960623 Valid acc: 0.786843\n",
      "Epoch: 879/1000 Train loss: 0.102841 Valid loss: 0.841028 Train acc: 0.960666 Valid acc: 0.786830\n",
      "Epoch: 880/1000 Train loss: 0.102732 Valid loss: 0.841562 Train acc: 0.960710 Valid acc: 0.786820\n",
      "Epoch: 881/1000 Train loss: 0.102624 Valid loss: 0.842163 Train acc: 0.960753 Valid acc: 0.786802\n",
      "Epoch: 882/1000 Train loss: 0.102516 Valid loss: 0.842709 Train acc: 0.960795 Valid acc: 0.786789\n",
      "Epoch: 883/1000 Train loss: 0.102408 Valid loss: 0.843261 Train acc: 0.960838 Valid acc: 0.786774\n",
      "Epoch: 884/1000 Train loss: 0.102301 Valid loss: 0.843826 Train acc: 0.960881 Valid acc: 0.786757\n",
      "Epoch: 885/1000 Train loss: 0.102193 Valid loss: 0.844352 Train acc: 0.960924 Valid acc: 0.786742\n",
      "Epoch: 886/1000 Train loss: 0.102086 Valid loss: 0.844921 Train acc: 0.960966 Valid acc: 0.786727\n",
      "Epoch: 887/1000 Train loss: 0.101979 Valid loss: 0.845464 Train acc: 0.961008 Valid acc: 0.786713\n",
      "Epoch: 888/1000 Train loss: 0.101873 Valid loss: 0.846017 Train acc: 0.961051 Valid acc: 0.786699\n",
      "Epoch: 889/1000 Train loss: 0.101767 Valid loss: 0.846571 Train acc: 0.961093 Valid acc: 0.786684\n",
      "Epoch: 890/1000 Train loss: 0.101661 Valid loss: 0.847104 Train acc: 0.961135 Valid acc: 0.786670\n",
      "Epoch: 891/1000 Train loss: 0.101554 Valid loss: 0.847648 Train acc: 0.961178 Valid acc: 0.786656\n",
      "Epoch: 892/1000 Train loss: 0.101447 Valid loss: 0.848196 Train acc: 0.961220 Valid acc: 0.786643\n",
      "Epoch: 893/1000 Train loss: 0.101342 Valid loss: 0.848752 Train acc: 0.961262 Valid acc: 0.786628\n",
      "Epoch: 894/1000 Train loss: 0.101237 Valid loss: 0.849274 Train acc: 0.961304 Valid acc: 0.786614\n",
      "Epoch: 895/1000 Train loss: 0.101132 Valid loss: 0.849842 Train acc: 0.961345 Valid acc: 0.786598\n",
      "Epoch: 896/1000 Train loss: 0.101027 Valid loss: 0.850386 Train acc: 0.961387 Valid acc: 0.786586\n",
      "Epoch: 897/1000 Train loss: 0.100922 Valid loss: 0.850944 Train acc: 0.961429 Valid acc: 0.786571\n",
      "Epoch: 898/1000 Train loss: 0.100818 Valid loss: 0.851476 Train acc: 0.961470 Valid acc: 0.786558\n",
      "Epoch: 899/1000 Train loss: 0.100714 Valid loss: 0.852020 Train acc: 0.961512 Valid acc: 0.786542\n",
      "Epoch: 900/1000 Train loss: 0.100610 Valid loss: 0.852529 Train acc: 0.961553 Valid acc: 0.786528\n",
      "Epoch: 901/1000 Train loss: 0.100505 Valid loss: 0.853072 Train acc: 0.961595 Valid acc: 0.786511\n",
      "Epoch: 902/1000 Train loss: 0.100401 Valid loss: 0.853611 Train acc: 0.961636 Valid acc: 0.786496\n",
      "Epoch: 903/1000 Train loss: 0.100298 Valid loss: 0.854141 Train acc: 0.961678 Valid acc: 0.786481\n",
      "Epoch: 904/1000 Train loss: 0.100195 Valid loss: 0.854705 Train acc: 0.961719 Valid acc: 0.786464\n",
      "Epoch: 905/1000 Train loss: 0.100091 Valid loss: 0.855235 Train acc: 0.961760 Valid acc: 0.786449\n",
      "Epoch: 906/1000 Train loss: 0.099988 Valid loss: 0.855767 Train acc: 0.961801 Valid acc: 0.786435\n",
      "Epoch: 907/1000 Train loss: 0.099886 Valid loss: 0.856318 Train acc: 0.961842 Valid acc: 0.786419\n",
      "Epoch: 908/1000 Train loss: 0.099784 Valid loss: 0.856866 Train acc: 0.961882 Valid acc: 0.786402\n",
      "Epoch: 909/1000 Train loss: 0.099682 Valid loss: 0.857378 Train acc: 0.961922 Valid acc: 0.786389\n",
      "Epoch: 910/1000 Train loss: 0.099581 Valid loss: 0.857971 Train acc: 0.961962 Valid acc: 0.786373\n",
      "Epoch: 911/1000 Train loss: 0.099481 Valid loss: 0.858467 Train acc: 0.962002 Valid acc: 0.786365\n",
      "Epoch: 912/1000 Train loss: 0.099382 Valid loss: 0.859087 Train acc: 0.962041 Valid acc: 0.786347\n",
      "Epoch: 913/1000 Train loss: 0.099282 Valid loss: 0.859570 Train acc: 0.962081 Valid acc: 0.786338\n",
      "Epoch: 914/1000 Train loss: 0.099180 Valid loss: 0.860186 Train acc: 0.962122 Valid acc: 0.786316\n",
      "Epoch: 915/1000 Train loss: 0.099080 Valid loss: 0.860665 Train acc: 0.962161 Valid acc: 0.786308\n",
      "Epoch: 916/1000 Train loss: 0.098979 Valid loss: 0.861249 Train acc: 0.962201 Valid acc: 0.786292\n",
      "Epoch: 917/1000 Train loss: 0.098879 Valid loss: 0.861816 Train acc: 0.962241 Valid acc: 0.786278\n",
      "Epoch: 918/1000 Train loss: 0.098778 Valid loss: 0.862315 Train acc: 0.962281 Valid acc: 0.786269\n",
      "Epoch: 919/1000 Train loss: 0.098678 Valid loss: 0.862885 Train acc: 0.962321 Valid acc: 0.786255\n",
      "Epoch: 920/1000 Train loss: 0.098578 Valid loss: 0.863423 Train acc: 0.962361 Valid acc: 0.786243\n",
      "Epoch: 921/1000 Train loss: 0.098477 Valid loss: 0.863950 Train acc: 0.962401 Valid acc: 0.786233\n",
      "Epoch: 922/1000 Train loss: 0.098378 Valid loss: 0.864527 Train acc: 0.962440 Valid acc: 0.786219\n",
      "Epoch: 923/1000 Train loss: 0.098278 Valid loss: 0.865044 Train acc: 0.962479 Valid acc: 0.786209\n",
      "Epoch: 924/1000 Train loss: 0.098179 Valid loss: 0.865613 Train acc: 0.962518 Valid acc: 0.786194\n",
      "Epoch: 925/1000 Train loss: 0.098081 Valid loss: 0.866108 Train acc: 0.962557 Valid acc: 0.786185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 926/1000 Train loss: 0.097982 Valid loss: 0.866651 Train acc: 0.962597 Valid acc: 0.786171\n",
      "Epoch: 927/1000 Train loss: 0.097884 Valid loss: 0.867201 Train acc: 0.962636 Valid acc: 0.786158\n",
      "Epoch: 928/1000 Train loss: 0.097785 Valid loss: 0.867712 Train acc: 0.962675 Valid acc: 0.786150\n",
      "Epoch: 929/1000 Train loss: 0.097687 Valid loss: 0.868232 Train acc: 0.962714 Valid acc: 0.786138\n",
      "Epoch: 930/1000 Train loss: 0.097589 Valid loss: 0.868777 Train acc: 0.962753 Valid acc: 0.786123\n",
      "Epoch: 931/1000 Train loss: 0.097491 Valid loss: 0.869256 Train acc: 0.962792 Valid acc: 0.786111\n",
      "Epoch: 932/1000 Train loss: 0.097394 Valid loss: 0.869802 Train acc: 0.962830 Valid acc: 0.786095\n",
      "Epoch: 933/1000 Train loss: 0.097298 Valid loss: 0.870288 Train acc: 0.962868 Valid acc: 0.786081\n",
      "Epoch: 934/1000 Train loss: 0.097201 Valid loss: 0.870830 Train acc: 0.962906 Valid acc: 0.786067\n",
      "Epoch: 935/1000 Train loss: 0.097104 Valid loss: 0.871329 Train acc: 0.962944 Valid acc: 0.786058\n",
      "Epoch: 936/1000 Train loss: 0.097007 Valid loss: 0.871908 Train acc: 0.962982 Valid acc: 0.786042\n",
      "Epoch: 937/1000 Train loss: 0.096911 Valid loss: 0.872405 Train acc: 0.963020 Valid acc: 0.786029\n",
      "Epoch: 938/1000 Train loss: 0.096815 Valid loss: 0.872923 Train acc: 0.963058 Valid acc: 0.786014\n",
      "Epoch: 939/1000 Train loss: 0.096719 Valid loss: 0.873461 Train acc: 0.963096 Valid acc: 0.785999\n",
      "Epoch: 940/1000 Train loss: 0.096623 Valid loss: 0.873962 Train acc: 0.963135 Valid acc: 0.785991\n",
      "Epoch: 941/1000 Train loss: 0.096527 Valid loss: 0.874487 Train acc: 0.963172 Valid acc: 0.785978\n",
      "Epoch: 942/1000 Train loss: 0.096431 Valid loss: 0.875010 Train acc: 0.963211 Valid acc: 0.785965\n",
      "Epoch: 943/1000 Train loss: 0.096337 Valid loss: 0.875539 Train acc: 0.963248 Valid acc: 0.785949\n",
      "Epoch: 944/1000 Train loss: 0.096242 Valid loss: 0.876054 Train acc: 0.963285 Valid acc: 0.785934\n",
      "Epoch: 945/1000 Train loss: 0.096147 Valid loss: 0.876583 Train acc: 0.963323 Valid acc: 0.785922\n",
      "Epoch: 946/1000 Train loss: 0.096053 Valid loss: 0.877116 Train acc: 0.963361 Valid acc: 0.785912\n",
      "Epoch: 947/1000 Train loss: 0.095959 Valid loss: 0.877671 Train acc: 0.963398 Valid acc: 0.785900\n",
      "Epoch: 948/1000 Train loss: 0.095864 Valid loss: 0.878197 Train acc: 0.963436 Valid acc: 0.785888\n",
      "Epoch: 949/1000 Train loss: 0.095770 Valid loss: 0.878742 Train acc: 0.963473 Valid acc: 0.785875\n",
      "Epoch: 950/1000 Train loss: 0.095677 Valid loss: 0.879270 Train acc: 0.963509 Valid acc: 0.785861\n",
      "Epoch: 951/1000 Train loss: 0.095584 Valid loss: 0.879777 Train acc: 0.963546 Valid acc: 0.785848\n",
      "Epoch: 952/1000 Train loss: 0.095491 Valid loss: 0.880272 Train acc: 0.963583 Valid acc: 0.785838\n",
      "Epoch: 953/1000 Train loss: 0.095396 Valid loss: 0.880775 Train acc: 0.963620 Valid acc: 0.785827\n",
      "Epoch: 954/1000 Train loss: 0.095303 Valid loss: 0.881302 Train acc: 0.963657 Valid acc: 0.785814\n",
      "Epoch: 955/1000 Train loss: 0.095211 Valid loss: 0.881791 Train acc: 0.963694 Valid acc: 0.785804\n",
      "Epoch: 956/1000 Train loss: 0.095117 Valid loss: 0.882329 Train acc: 0.963731 Valid acc: 0.785790\n",
      "Epoch: 957/1000 Train loss: 0.095024 Valid loss: 0.882809 Train acc: 0.963767 Valid acc: 0.785786\n",
      "Epoch: 958/1000 Train loss: 0.094932 Valid loss: 0.883366 Train acc: 0.963804 Valid acc: 0.785773\n",
      "Epoch: 959/1000 Train loss: 0.094840 Valid loss: 0.883911 Train acc: 0.963840 Valid acc: 0.785762\n",
      "Epoch: 960/1000 Train loss: 0.094747 Valid loss: 0.884395 Train acc: 0.963877 Valid acc: 0.785757\n",
      "Epoch: 961/1000 Train loss: 0.094655 Valid loss: 0.884930 Train acc: 0.963913 Valid acc: 0.785745\n",
      "Epoch: 962/1000 Train loss: 0.094562 Valid loss: 0.885481 Train acc: 0.963949 Valid acc: 0.785732\n",
      "Epoch: 963/1000 Train loss: 0.094470 Valid loss: 0.885971 Train acc: 0.963986 Valid acc: 0.785723\n",
      "Epoch: 964/1000 Train loss: 0.094378 Valid loss: 0.886498 Train acc: 0.964022 Valid acc: 0.785710\n",
      "Epoch: 965/1000 Train loss: 0.094287 Valid loss: 0.887013 Train acc: 0.964058 Valid acc: 0.785698\n",
      "Epoch: 966/1000 Train loss: 0.094196 Valid loss: 0.887478 Train acc: 0.964094 Valid acc: 0.785691\n",
      "Epoch: 967/1000 Train loss: 0.094105 Valid loss: 0.888035 Train acc: 0.964130 Valid acc: 0.785676\n",
      "Epoch: 968/1000 Train loss: 0.094014 Valid loss: 0.888565 Train acc: 0.964166 Valid acc: 0.785665\n",
      "Epoch: 969/1000 Train loss: 0.093923 Valid loss: 0.889081 Train acc: 0.964202 Valid acc: 0.785655\n",
      "Epoch: 970/1000 Train loss: 0.093833 Valid loss: 0.889647 Train acc: 0.964238 Valid acc: 0.785641\n",
      "Epoch: 971/1000 Train loss: 0.093743 Valid loss: 0.890156 Train acc: 0.964273 Valid acc: 0.785630\n",
      "Epoch: 972/1000 Train loss: 0.093654 Valid loss: 0.890698 Train acc: 0.964309 Valid acc: 0.785617\n",
      "Epoch: 973/1000 Train loss: 0.093563 Valid loss: 0.891210 Train acc: 0.964344 Valid acc: 0.785606\n",
      "Epoch: 974/1000 Train loss: 0.093474 Valid loss: 0.891782 Train acc: 0.964380 Valid acc: 0.785592\n",
      "Epoch: 975/1000 Train loss: 0.093383 Valid loss: 0.892291 Train acc: 0.964416 Valid acc: 0.785582\n",
      "Epoch: 976/1000 Train loss: 0.093294 Valid loss: 0.892813 Train acc: 0.964451 Valid acc: 0.785572\n",
      "Epoch: 977/1000 Train loss: 0.093205 Valid loss: 0.893349 Train acc: 0.964486 Valid acc: 0.785559\n",
      "Epoch: 978/1000 Train loss: 0.093116 Valid loss: 0.893873 Train acc: 0.964521 Valid acc: 0.785547\n",
      "Epoch: 979/1000 Train loss: 0.093026 Valid loss: 0.894402 Train acc: 0.964556 Valid acc: 0.785534\n",
      "Epoch: 980/1000 Train loss: 0.092937 Valid loss: 0.894942 Train acc: 0.964591 Valid acc: 0.785521\n",
      "Epoch: 981/1000 Train loss: 0.092848 Valid loss: 0.895450 Train acc: 0.964626 Valid acc: 0.785513\n",
      "Epoch: 982/1000 Train loss: 0.092759 Valid loss: 0.895994 Train acc: 0.964661 Valid acc: 0.785501\n",
      "Epoch: 983/1000 Train loss: 0.092671 Valid loss: 0.896522 Train acc: 0.964696 Valid acc: 0.785489\n",
      "Epoch: 984/1000 Train loss: 0.092583 Valid loss: 0.897047 Train acc: 0.964730 Valid acc: 0.785477\n",
      "Epoch: 985/1000 Train loss: 0.092495 Valid loss: 0.897552 Train acc: 0.964765 Valid acc: 0.785465\n",
      "Epoch: 986/1000 Train loss: 0.092406 Valid loss: 0.898069 Train acc: 0.964800 Valid acc: 0.785454\n",
      "Epoch: 987/1000 Train loss: 0.092318 Valid loss: 0.898625 Train acc: 0.964835 Valid acc: 0.785442\n",
      "Epoch: 988/1000 Train loss: 0.092230 Valid loss: 0.899122 Train acc: 0.964869 Valid acc: 0.785435\n",
      "Epoch: 989/1000 Train loss: 0.092142 Valid loss: 0.899610 Train acc: 0.964904 Valid acc: 0.785428\n",
      "Epoch: 990/1000 Train loss: 0.092055 Valid loss: 0.900147 Train acc: 0.964938 Valid acc: 0.785416\n",
      "Epoch: 991/1000 Train loss: 0.091968 Valid loss: 0.900640 Train acc: 0.964972 Valid acc: 0.785405\n",
      "Epoch: 992/1000 Train loss: 0.091882 Valid loss: 0.901215 Train acc: 0.965006 Valid acc: 0.785391\n",
      "Epoch: 993/1000 Train loss: 0.091795 Valid loss: 0.901683 Train acc: 0.965041 Valid acc: 0.785385\n",
      "Epoch: 994/1000 Train loss: 0.091708 Valid loss: 0.902217 Train acc: 0.965075 Valid acc: 0.785374\n",
      "Epoch: 995/1000 Train loss: 0.091621 Valid loss: 0.902739 Train acc: 0.965110 Valid acc: 0.785363\n",
      "Epoch: 996/1000 Train loss: 0.091534 Valid loss: 0.903228 Train acc: 0.965144 Valid acc: 0.785351\n",
      "Epoch: 997/1000 Train loss: 0.091449 Valid loss: 0.903799 Train acc: 0.965178 Valid acc: 0.785336\n",
      "Epoch: 998/1000 Train loss: 0.091363 Valid loss: 0.904339 Train acc: 0.965212 Valid acc: 0.785324\n",
      "Epoch: 999/1000 Train loss: 0.091277 Valid loss: 0.904837 Train acc: 0.965245 Valid acc: 0.785316\n",
      "Epoch: 1000/1000 Train loss: 0.091192 Valid loss: 0.905402 Train acc: 0.965279 Valid acc: 0.785303\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "\n",
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Loop over batches\n",
    "#         for x, y in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "        for x, y in get_batches2(X_norm=X_train_norm, Y_labels=Y_train):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : keep_prob, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            ################## Validation\n",
    "            acc_batch = []\n",
    "            loss_batch = []    \n",
    "            # Loop over batches\n",
    "#             for x, y in get_batches(X_valid_norm, Y_valid_onehot, batch_size):\n",
    "            for x, y in get_batches2(X_norm=X_valid_norm, Y_labels=Y_valid):\n",
    "\n",
    "                # Feed dictionary\n",
    "                feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "                # Loss\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "                acc_batch.append(acc)\n",
    "                loss_batch.append(loss)\n",
    "\n",
    "            # Store\n",
    "            valid_acc.append(np.mean(acc_batch))\n",
    "            valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "        # Print info for every iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(valid_loss)),\n",
    "              \"Train acc: {:6f}\".format(np.mean(train_acc)),\n",
    "              \"Valid acc: {:.6f}\".format(np.mean(valid_acc)))\n",
    "                \n",
    "    saver.save(sess,\"checkpoints_/dcnn-face-yalda.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a112ab828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_loss, label='Face train_loss')\n",
    "mplot.plot(valid_loss, label='Face valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noau68koAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f699c5c5cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as mplot\n",
    "mplot.plot(train_acc, label='Face train_acc')\n",
    "mplot.plot(valid_acc, label='Face valid_acc')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_/dcnn-face-yalda.ckpt\n",
      "Test loss: 0.680941 Test acc: 0.887857\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints_/'))\n",
    "    \n",
    "    ################## Test\n",
    "    acc_batch = []\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for x, y in get_batches(batch_size=100, X=X_test_norm, y=Y_test_onehot):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "        # Loss\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Store\n",
    "    test_acc.append(np.mean(acc_batch))\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)),\n",
    "          \"Test acc: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

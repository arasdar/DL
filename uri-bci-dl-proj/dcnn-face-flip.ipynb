{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62700, 205, 16) float64 (62700, 1) uint8\n",
      "0.5 0.5 0.0 0.0\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "\n",
    "BahramFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/BahramFace.mat')\n",
    "DJFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/DJFace.mat')\n",
    "NickFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/NickFace.mat')\n",
    "RoohiFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/RoohiFace.mat')\n",
    "SarahFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/SarahFace.mat')\n",
    "\n",
    "BahramFlip = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/BahramFlipp.mat')\n",
    "DJFlip = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/DJFlipp.mat')\n",
    "NickFlip = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/NickFlipp.mat')\n",
    "RoohiFlip = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/RoohiFlipp.mat')\n",
    "SarahFlip = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/SarahFlipp.mat')\n",
    "\n",
    "\n",
    "AllData = np.concatenate((BahramFace['Intensification_Data'],\n",
    "                            DJFace['Intensification_Data'],\n",
    "                            NickFace['Intensification_Data'],\n",
    "                            RoohiFace['Intensification_Data'],\n",
    "                            SarahFace['Intensification_Data'],\n",
    "                            BahramFlip['Intensification_Data'],\n",
    "                            DJFlip['Intensification_Data'],\n",
    "                            NickFlip['Intensification_Data'],\n",
    "                            RoohiFlip['Intensification_Data'],\n",
    "                            SarahFlip['Intensification_Data']), axis=0)\n",
    "\n",
    "AllLabels = np.concatenate((BahramFace['Intensification_Label'],\n",
    "                            DJFace['Intensification_Label'],\n",
    "                            NickFace['Intensification_Label'],\n",
    "                            RoohiFace['Intensification_Label'],\n",
    "                            SarahFace['Intensification_Label'],\n",
    "                            BahramFlip['Intensification_Label'],\n",
    "                            DJFlip['Intensification_Label'],\n",
    "                            NickFlip['Intensification_Label'],\n",
    "                            RoohiFlip['Intensification_Label'],\n",
    "                            SarahFlip['Intensification_Label']), axis=0)\n",
    "\n",
    "AllLabelOne = (AllLabels==1).reshape(-1)\n",
    "AllData_LabelOne = AllData[AllLabelOne]\n",
    "\n",
    "AllLabelZero = (AllLabels==0).reshape(-1)\n",
    "AllData_LabelZero = AllData[AllLabelZero]\n",
    "\n",
    "# Linear synthetic data augmentatiion or creation or generation\n",
    "AllData_LabelOneNew_list = []\n",
    "\n",
    "# w*data+b: translation, rotation, and scaling\n",
    "# linear transformation of the target data\n",
    "w, b = 1.0, 0.0\n",
    "for idx in range(5):\n",
    "    AllData_LabelOneNew = (w * AllData_LabelOne) + b\n",
    "    AllData_LabelOneNew_list.append(AllData_LabelOneNew)\n",
    "    w *= 0.9 # 1.0, 0.9, 0.81, 0.729\n",
    "    b += 0.1 # 0.1, 0.2, 0.3\n",
    "\n",
    "AllData_LabelOneNew_total = np.array(AllData_LabelOneNew_list, \n",
    "                                      dtype=AllData_LabelOne.dtype).reshape(-1, 205, 16)\n",
    "\n",
    "AllDataNew = np.concatenate((AllData_LabelOneNew_total, AllData_LabelZero), axis=0)\n",
    "AllLabelsNew = np.concatenate((AllLabels[AllLabelZero], # Tgt = 6*Non-Tgt\n",
    "                                   AllLabels[AllLabelOne],\n",
    "                                   AllLabels[AllLabelOne],\n",
    "                                   AllLabels[AllLabelOne], \n",
    "                                   AllLabels[AllLabelOne], \n",
    "                                   AllLabels[AllLabelOne]), axis=0)\n",
    "\n",
    "print(AllDataNew.shape, AllDataNew.dtype, AllLabelsNew.shape, AllLabelsNew.dtype)\n",
    "print(np.mean(AllLabelsNew==0), np.mean(AllLabelsNew==1), np.mean(AllLabelsNew==2), np.mean(AllLabelsNew==3))\n",
    "print((AllLabelsNew +  1).max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43890, 205, 16) (18810, 205, 16) (43890, 1) (18810, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_valid, X_test, Y_train_valid, Y_test = train_test_split(AllDataNew, AllLabelsNew, test_size=0.30)\n",
    "\n",
    "print(X_train_valid.shape, X_test.shape, Y_train_valid.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.4999316473 0.5000683527 0.0\n",
      "(43890, 2) float64 (18810, 2) float64\n"
     ]
    }
   ],
   "source": [
    "# Preparing input and output data\n",
    "from utilities import *\n",
    "\n",
    "# Normalizing/standardizing the input data features\n",
    "X_train_valid_norm, X_test_norm = standardize(test=X_test, train=X_train_valid)\n",
    "\n",
    "# Onehot encoding/vectorizing the output data labels\n",
    "print(np.mean((Y_train_valid+1).reshape(-1)==0), np.mean((Y_train_valid+1).reshape(-1)==1),\n",
    "     np.mean((Y_train_valid+1).reshape(-1)==2), np.mean((Y_train_valid+1).reshape(-1)==3))\n",
    "\n",
    "Y_train_valid_onehot = one_hot(labels=(Y_train_valid+1).reshape(-1), n_class=2) \n",
    "Y_test_onehot = one_hot(labels=(Y_test+1).reshape(-1), n_class=2) \n",
    "\n",
    "print(Y_train_valid_onehot.shape, Y_train_valid_onehot.dtype, \n",
    "      Y_test_onehot.shape, Y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30723, 205, 16) (13167, 205, 16) (30723, 2) (13167, 2)\n"
     ]
    }
   ],
   "source": [
    "# Train and valid split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_norm, X_valid_norm, Y_train_onehot, Y_valid_onehot = train_test_split(X_train_valid_norm, \n",
    "                                                                              Y_train_valid_onehot,\n",
    "                                                                              test_size=0.30)\n",
    "\n",
    "print(X_train_norm.shape, X_valid_norm.shape, Y_train_onehot.shape, Y_valid_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len, n_channels 307 205 16\n",
      "n_classes [2]\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = Y_train_valid.max(axis=0)+1\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.0001 #1e-4\n",
    "epochs = 100 # num iterations for updating model\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the data from python/numpy to tensorflow framework\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs_')\n",
    "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_')\n",
    "keep_prob_ = tf.placeholder(tf.float32, name = 'keep_prob_')\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate_')# Construct the LSTM inputs and LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_.shape, conv1.shape, max_pool_1.shape (?, 205, 16) (?, 204, 32) (?, 102, 32)\n",
      "max_pool_1.shape, conv2.shape, max_pool_2.shape (?, 102, 32) (?, 102, 64) (?, 51, 64)\n",
      "max_pool_2.shape, conv3.shape, max_pool_3.shape (?, 51, 64) (?, 50, 128) (?, 25, 128)\n",
      "max_pool_3.shape, conv4.shape, max_pool_4.shape (?, 25, 128) (?, 24, 256) (?, 12, 256)\n",
      "max_pool_4.shape, flat.shape, logits.shape (?, 12, 256) (?, 3072) (?, 2)\n"
     ]
    }
   ],
   "source": [
    "# batch_size, seq_len, n_channels: 307 205 16; n_classes: [2]\n",
    "# (batch, 205, 16) --> (batch, 102, 32)\n",
    "# conv valid: (205-2+0)/1 + 1 = (203/1)+1 = 203 + 1=204\n",
    "# pool same: (204-2+0)/2 + 1 = (202/2)+1 = 101 + 1=102\n",
    "conv1 = tf.layers.conv1d(inputs=inputs_, filters=32, kernel_size=2, strides=1, padding='valid', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_1 = tf.nn.dropout(max_pool_1, keep_prob=keep_prob_)\n",
    "print('inputs_.shape, conv1.shape, max_pool_1.shape', inputs_.shape, conv1.shape, max_pool_1.shape)\n",
    "\n",
    "# (batch, 102, 32) --> (batch, 51, 64)\n",
    "# conv same\n",
    "# pool same: (102-2+0)/2 + 1 = (100/2)+1 = 50 + 1=51\n",
    "conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=2, strides=1, padding='same', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_2 = tf.nn.dropout(max_pool_2, keep_prob=keep_prob_)\n",
    "print('max_pool_1.shape, conv2.shape, max_pool_2.shape', max_pool_1.shape, conv2.shape, max_pool_2.shape)\n",
    "\n",
    "# (batch, 51, 64) --> (batch, 25, 128)\n",
    "# conv valid: (51-2+0)/1 + 1 = (49/1)+1 = 49 + 1=50\n",
    "# pool same: (50-2+0)/2 + 1 = (48/2)+1 = 24 + 1=25\n",
    "conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=128, kernel_size=2, strides=1, padding='valid', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_3 = tf.nn.dropout(max_pool_3, keep_prob=keep_prob_)\n",
    "print('max_pool_2.shape, conv3.shape, max_pool_3.shape', max_pool_2.shape, conv3.shape, max_pool_3.shape)\n",
    "\n",
    "# (batch, 25, 128) --> (batch, 12, 256)\n",
    "# conv valid: (25-2+0)/1 + 1 = (23/1)+1 = 23 + 1=24\n",
    "# pool same: (24-2+0)/2 + 1 = (22/2)+1 = 11 + 1=12\n",
    "conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=256, kernel_size=2, strides=1, padding='valid', \n",
    "                         activation = tf.nn.relu)\n",
    "max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "# max_pool_4 = tf.nn.dropout(max_pool_4, keep_prob=keep_prob_)\n",
    "print('max_pool_3.shape, conv4.shape, max_pool_4.shape', max_pool_3.shape, conv4.shape, max_pool_4.shape)\n",
    "\n",
    "# Flatten and add dropout + predicted output\n",
    "flat = tf.reshape(max_pool_4, (-1, 12*256))\n",
    "flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "logits = tf.layers.dense(flat, n_classes)\n",
    "print('max_pool_4.shape, flat.shape, logits.shape', max_pool_4.shape, flat.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_tensor, cost Tensor(\"Reshape_3:0\", shape=(?,), dtype=float32) Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "optimizer name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_conv1d/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_1/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_1/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_2/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_2/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_3/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_conv1d_3/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_dense/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_dense/bias/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n",
      "correct_pred, accuracy Tensor(\"Equal:0\", shape=(?,), dtype=bool) Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "confusion_matrix Tensor(\"confusion_matrix/SparseTensorDenseAdd:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "print('cost_tensor, cost', cost_tensor, cost)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "print('optimizer', optimizer)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "print('correct_pred, accuracy', correct_pred, accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = tf.confusion_matrix(predictions=tf.argmax(logits, 1),\n",
    "                                       labels=tf.argmax(labels_, 1))\n",
    "print('confusion_matrix', confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train loss: 0.681669 Valid loss: 0.669874 Train acc: 0.563062 Valid acc: 0.587567\n",
      "Epoch: 2/100 Train loss: 0.652739 Valid loss: 0.643571 Train acc: 0.620619 Valid acc: 0.647710\n",
      "Epoch: 3/100 Train loss: 0.620144 Valid loss: 0.612750 Train acc: 0.660358 Valid acc: 0.683160\n",
      "Epoch: 4/100 Train loss: 0.591321 Valid loss: 0.584695 Train acc: 0.687907 Valid acc: 0.708243\n",
      "Epoch: 5/100 Train loss: 0.567839 Valid loss: 0.561482 Train acc: 0.709042 Valid acc: 0.726878\n",
      "Epoch: 6/100 Train loss: 0.548172 Valid loss: 0.542954 Train acc: 0.724957 Valid acc: 0.740621\n",
      "Epoch: 7/100 Train loss: 0.531949 Valid loss: 0.527610 Train acc: 0.737143 Valid acc: 0.751452\n",
      "Epoch: 8/100 Train loss: 0.518265 Valid loss: 0.514876 Train acc: 0.747068 Valid acc: 0.760104\n",
      "Epoch: 9/100 Train loss: 0.506482 Valid loss: 0.504164 Train acc: 0.755320 Valid acc: 0.767109\n",
      "Epoch: 10/100 Train loss: 0.496166 Valid loss: 0.495012 Train acc: 0.762560 Valid acc: 0.772973\n",
      "Epoch: 11/100 Train loss: 0.487143 Valid loss: 0.487000 Train acc: 0.768528 Valid acc: 0.777954\n",
      "Epoch: 12/100 Train loss: 0.479113 Valid loss: 0.479848 Train acc: 0.773808 Valid acc: 0.782311\n",
      "Epoch: 13/100 Train loss: 0.471904 Valid loss: 0.473475 Train acc: 0.778509 Valid acc: 0.786118\n",
      "Epoch: 14/100 Train loss: 0.465436 Valid loss: 0.467827 Train acc: 0.782755 Valid acc: 0.789462\n",
      "Epoch: 15/100 Train loss: 0.459571 Valid loss: 0.462611 Train acc: 0.786465 Valid acc: 0.792502\n",
      "Epoch: 16/100 Train loss: 0.454135 Valid loss: 0.457786 Train acc: 0.789819 Valid acc: 0.795284\n",
      "Epoch: 17/100 Train loss: 0.449071 Valid loss: 0.453345 Train acc: 0.792953 Valid acc: 0.797797\n",
      "Epoch: 18/100 Train loss: 0.444420 Valid loss: 0.449199 Train acc: 0.795776 Valid acc: 0.800144\n",
      "Epoch: 19/100 Train loss: 0.439860 Valid loss: 0.445347 Train acc: 0.798438 Valid acc: 0.802304\n",
      "Epoch: 20/100 Train loss: 0.435768 Valid loss: 0.441846 Train acc: 0.800901 Valid acc: 0.804285\n",
      "Epoch: 21/100 Train loss: 0.431660 Valid loss: 0.438408 Train acc: 0.803445 Valid acc: 0.806169\n",
      "Epoch: 22/100 Train loss: 0.427943 Valid loss: 0.435248 Train acc: 0.805634 Valid acc: 0.807900\n",
      "Epoch: 23/100 Train loss: 0.424334 Valid loss: 0.432183 Train acc: 0.807771 Valid acc: 0.809551\n",
      "Epoch: 24/100 Train loss: 0.420949 Valid loss: 0.429303 Train acc: 0.809667 Valid acc: 0.811104\n",
      "Epoch: 25/100 Train loss: 0.417712 Valid loss: 0.426545 Train acc: 0.811584 Valid acc: 0.812593\n",
      "Epoch: 26/100 Train loss: 0.414650 Valid loss: 0.423945 Train acc: 0.813336 Valid acc: 0.813974\n",
      "Epoch: 27/100 Train loss: 0.411741 Valid loss: 0.421438 Train acc: 0.815025 Valid acc: 0.815296\n",
      "Epoch: 28/100 Train loss: 0.408850 Valid loss: 0.419011 Train acc: 0.816622 Valid acc: 0.816564\n",
      "Epoch: 29/100 Train loss: 0.406103 Valid loss: 0.416736 Train acc: 0.818178 Valid acc: 0.817747\n",
      "Epoch: 30/100 Train loss: 0.403441 Valid loss: 0.414560 Train acc: 0.819711 Valid acc: 0.818882\n",
      "Epoch: 31/100 Train loss: 0.400917 Valid loss: 0.412434 Train acc: 0.821089 Valid acc: 0.819978\n",
      "Epoch: 32/100 Train loss: 0.398434 Valid loss: 0.410355 Train acc: 0.822447 Valid acc: 0.821041\n",
      "Epoch: 33/100 Train loss: 0.395988 Valid loss: 0.408354 Train acc: 0.823782 Valid acc: 0.822063\n",
      "Epoch: 34/100 Train loss: 0.393591 Valid loss: 0.406421 Train acc: 0.825125 Valid acc: 0.823042\n",
      "Epoch: 35/100 Train loss: 0.391309 Valid loss: 0.404543 Train acc: 0.826331 Valid acc: 0.823986\n",
      "Epoch: 36/100 Train loss: 0.389069 Valid loss: 0.402766 Train acc: 0.827536 Valid acc: 0.824880\n",
      "Epoch: 37/100 Train loss: 0.386900 Valid loss: 0.401021 Train acc: 0.828682 Valid acc: 0.825748\n",
      "Epoch: 38/100 Train loss: 0.384784 Valid loss: 0.399334 Train acc: 0.829817 Valid acc: 0.826601\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "\n",
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x, y in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : keep_prob, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            ################## Validation\n",
    "            acc_batch = []\n",
    "            loss_batch = []    \n",
    "            # Loop over batches\n",
    "            for x, y in get_batches(X_valid_norm, Y_valid_onehot, batch_size):\n",
    "\n",
    "                # Feed dictionary\n",
    "                feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "                # Loss\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "                acc_batch.append(acc)\n",
    "                loss_batch.append(loss)\n",
    "\n",
    "            # Store\n",
    "            valid_acc.append(np.mean(acc_batch))\n",
    "            valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "        # Print info for every iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(valid_loss)),\n",
    "              \"Train acc: {:6f}\".format(np.mean(train_acc)),\n",
    "              \"Valid acc: {:.6f}\".format(np.mean(valid_acc)))\n",
    "                \n",
    "    saver.save(sess,\"checkpoints-dcnn/face-flip.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_loss, label='face-flip train_loss')\n",
    "mplot.plot(valid_loss, label='face-flip valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as mplot\n",
    "mplot.plot(train_acc, label='face-flip train_acc')\n",
    "mplot.plot(valid_acc, label='face-flip valid_acc')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-dcnn'))\n",
    "    \n",
    "    ################## Test\n",
    "    acc_batch = []\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for x, y in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "        # Loss\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict = feed)\n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Store\n",
    "    test_acc.append(np.mean(acc_batch))\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)),\n",
    "          \"Test acc: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"A binary to train CIFAR-10 using a single GPU.\n",
    "\n",
    "Accuracy:\n",
    "cifar10_train.py achieves ~86% accuracy after 100K (1000000) steps (256 epochs of\n",
    "data) as judged by cifar10_eval.py.\n",
    "\n",
    "Speed: With batch_size 128.\n",
    "\n",
    "System        | Step Time (sec/batch)  |     Accuracy\n",
    "------------------------------------------------------------------\n",
    "1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n",
    "1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n",
    "\n",
    "Usage:\n",
    "Please see the tutorial and website for how to download the CIFAR-10\n",
    "data set, compile the program and train the model.\n",
    "\n",
    "http://tensorflow.org/tutorials/deep_cnn/\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('log_frequency', 10,\n",
    "                            \"\"\"How often to log results to the console.\"\"\")\n",
    "\n",
    "\n",
    "def train():\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on\n",
    "    # GPU and resulting in a slow down.\n",
    "    with tf.device('/cpu:0'):\n",
    "      images, labels = cifar10.distorted_inputs()\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = cifar10.inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = cifar10.loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = cifar10.train(loss, global_step)\n",
    "\n",
    "    class _LoggerHook(tf.train.SessionRunHook):\n",
    "      \"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "      def begin(self):\n",
    "        self._step = -1\n",
    "        self._start_time = time.time()\n",
    "\n",
    "      def before_run(self, run_context):\n",
    "        self._step += 1\n",
    "        return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "      def after_run(self, run_context, run_values):\n",
    "        if self._step % FLAGS.log_frequency == 0:\n",
    "          current_time = time.time()\n",
    "          duration = current_time - self._start_time\n",
    "          self._start_time = current_time\n",
    "\n",
    "          loss_value = run_values.results\n",
    "          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n",
    "          sec_per_batch = float(duration / FLAGS.log_frequency)\n",
    "\n",
    "          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                        'sec/batch)')\n",
    "          print (format_str % (datetime.now(), self._step, loss_value,\n",
    "                               examples_per_sec, sec_per_batch))\n",
    "\n",
    "    with tf.train.MonitoredTrainingSession(\n",
    "        checkpoint_dir=FLAGS.train_dir,\n",
    "        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n",
    "               tf.train.NanTensorHook(loss),\n",
    "               _LoggerHook()],\n",
    "        config=tf.ConfigProto(\n",
    "            log_device_placement=FLAGS.log_device_placement)) as mon_sess:\n",
    "      while not mon_sess.should_stop():\n",
    "        mon_sess.run(train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "INFO:tensorflow:Summary name conv1/weight_loss (raw) is illegal; using conv1/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv2/weight_loss (raw) is illegal; using conv2/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name softmax_linear/weight_loss (raw) is illegal; using softmax_linear/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n",
      "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/cifar10_train/model.ckpt.\n",
      "2018-01-30 20:02:12.083405: step 0, loss = 4.68 (210.2 examples/sec; 0.609 sec/batch)\n",
      "2018-01-30 20:02:12.421081: step 10, loss = 4.63 (3790.7 examples/sec; 0.034 sec/batch)\n",
      "2018-01-30 20:02:12.607579: step 20, loss = 4.55 (6863.2 examples/sec; 0.019 sec/batch)\n",
      "2018-01-30 20:02:12.788252: step 30, loss = 4.75 (7084.6 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:12.961932: step 40, loss = 4.39 (7369.9 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:13.138180: step 50, loss = 4.40 (7262.5 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:13.302090: step 60, loss = 4.33 (7809.1 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:13.467079: step 70, loss = 4.16 (7758.0 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:13.634238: step 80, loss = 4.36 (7657.4 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:13.806181: step 90, loss = 4.45 (7444.9 examples/sec; 0.017 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 44.4904\n",
      "2018-01-30 20:02:14.170256: step 100, loss = 4.06 (3515.7 examples/sec; 0.036 sec/batch)\n",
      "2018-01-30 20:02:14.341745: step 110, loss = 4.03 (7463.9 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:14.510695: step 120, loss = 3.95 (7576.2 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:14.674233: step 130, loss = 3.99 (7826.9 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:14.841389: step 140, loss = 4.06 (7657.5 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:15.003771: step 150, loss = 3.89 (7882.7 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:15.166014: step 160, loss = 3.82 (7889.3 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:15.328134: step 170, loss = 3.76 (7895.4 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:15.502943: step 180, loss = 3.77 (7322.3 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:15.679943: step 190, loss = 3.90 (7232.1 examples/sec; 0.018 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 54.0238\n",
      "2018-01-30 20:02:16.019728: step 200, loss = 3.89 (3766.9 examples/sec; 0.034 sec/batch)\n",
      "2018-01-30 20:02:16.184502: step 210, loss = 3.72 (7768.5 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:16.347749: step 220, loss = 3.61 (7841.0 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:16.505574: step 230, loss = 3.83 (8110.0 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:16.664238: step 240, loss = 3.90 (8067.5 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:16.836363: step 250, loss = 3.79 (7436.4 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:17.002996: step 260, loss = 3.71 (7681.6 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:17.180873: step 270, loss = 3.53 (7196.1 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:17.338443: step 280, loss = 3.63 (8123.2 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:17.502734: step 290, loss = 3.73 (7791.1 examples/sec; 0.016 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 54.3692\n",
      "2018-01-30 20:02:17.859923: step 300, loss = 3.57 (3583.6 examples/sec; 0.036 sec/batch)\n",
      "2018-01-30 20:02:18.027743: step 310, loss = 3.56 (7627.1 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:18.190038: step 320, loss = 3.49 (7886.8 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:18.348979: step 330, loss = 3.46 (8053.3 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:18.513205: step 340, loss = 3.66 (7794.3 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:18.687214: step 350, loss = 3.34 (7355.8 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:18.860128: step 360, loss = 3.40 (7402.9 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:19.026935: step 370, loss = 3.37 (7673.6 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:19.190918: step 380, loss = 3.47 (7805.2 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:19.353908: step 390, loss = 3.42 (7853.2 examples/sec; 0.016 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 54.6707\n",
      "2018-01-30 20:02:19.689331: step 400, loss = 3.27 (3816.1 examples/sec; 0.034 sec/batch)\n",
      "2018-01-30 20:02:19.868973: step 410, loss = 3.38 (7125.9 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:20.027016: step 420, loss = 3.48 (8098.4 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:20.190525: step 430, loss = 3.24 (7828.2 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:20.370242: step 440, loss = 3.26 (7122.3 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:20.529146: step 450, loss = 3.39 (8055.1 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:20.693129: step 460, loss = 3.50 (7805.7 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:20.848486: step 470, loss = 3.22 (8239.0 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:21.008364: step 480, loss = 3.15 (8006.1 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:21.165267: step 490, loss = 3.26 (8157.9 examples/sec; 0.016 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 54.9436\n",
      "2018-01-30 20:02:21.508672: step 500, loss = 3.28 (3727.4 examples/sec; 0.034 sec/batch)\n",
      "2018-01-30 20:02:21.673572: step 510, loss = 3.21 (7762.2 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:21.833905: step 520, loss = 3.10 (7983.3 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:21.992430: step 530, loss = 3.11 (8074.5 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:22.150306: step 540, loss = 3.21 (8108.3 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:22.308673: step 550, loss = 2.95 (8081.8 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:22.469234: step 560, loss = 3.17 (7972.1 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:22.627676: step 570, loss = 2.96 (8078.6 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:22.802020: step 580, loss = 3.08 (7341.8 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:22.973889: step 590, loss = 2.88 (7447.5 examples/sec; 0.017 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 55.3327\n",
      "2018-01-30 20:02:23.315318: step 600, loss = 3.05 (3748.9 examples/sec; 0.034 sec/batch)\n",
      "2018-01-30 20:02:23.481589: step 610, loss = 3.05 (7698.3 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:23.647348: step 620, loss = 2.99 (7722.1 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:23.821233: step 630, loss = 3.03 (7361.2 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:23.992642: step 640, loss = 2.87 (7467.5 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:24.157984: step 650, loss = 2.93 (7742.9 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:24.321883: step 660, loss = 2.86 (7808.2 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:24.493790: step 670, loss = 2.87 (7446.0 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:24.661125: step 680, loss = 3.03 (7649.1 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:24.823151: step 690, loss = 2.94 (7900.0 examples/sec; 0.016 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 54.2825\n",
      "2018-01-30 20:02:25.158117: step 700, loss = 2.87 (3821.2 examples/sec; 0.033 sec/batch)\n",
      "2018-01-30 20:02:25.335903: step 710, loss = 2.56 (7199.8 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:25.498871: step 720, loss = 2.70 (7855.1 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:25.660559: step 730, loss = 2.78 (7915.8 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:25.821999: step 740, loss = 2.92 (7928.5 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:25.984510: step 750, loss = 2.81 (7876.4 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:26.148550: step 760, loss = 2.81 (7803.7 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:26.321110: step 770, loss = 2.69 (7417.7 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:26.485766: step 780, loss = 2.75 (7773.1 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:26.647019: step 790, loss = 2.69 (7938.5 examples/sec; 0.016 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 54.8655\n",
      "2018-01-30 20:02:26.982045: step 800, loss = 2.64 (3820.4 examples/sec; 0.034 sec/batch)\n",
      "2018-01-30 20:02:27.158487: step 810, loss = 2.65 (7254.7 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:27.338409: step 820, loss = 2.67 (7114.3 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:27.514713: step 830, loss = 2.69 (7260.1 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:27.684423: step 840, loss = 2.54 (7542.3 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:27.858877: step 850, loss = 2.61 (7337.1 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:28.033565: step 860, loss = 2.65 (7327.6 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:28.196497: step 870, loss = 2.67 (7856.5 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:28.363657: step 880, loss = 2.61 (7658.2 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:28.523450: step 890, loss = 2.70 (8008.7 examples/sec; 0.016 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 53.7345\n",
      "2018-01-30 20:02:28.842616: step 900, loss = 2.55 (4010.5 examples/sec; 0.032 sec/batch)\n",
      "2018-01-30 20:02:29.014977: step 910, loss = 2.70 (7426.2 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:29.180411: step 920, loss = 2.75 (7737.2 examples/sec; 0.017 sec/batch)\n",
      "2018-01-30 20:02:29.342217: step 930, loss = 2.57 (7911.5 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:29.502075: step 940, loss = 2.65 (8006.3 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:29.664333: step 950, loss = 2.49 (7891.9 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:29.829102: step 960, loss = 2.62 (7765.2 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 20:02:30.004934: step 970, loss = 2.34 (7279.7 examples/sec; 0.018 sec/batch)\n",
      "2018-01-30 20:02:30.192629: step 980, loss = 2.45 (6819.7 examples/sec; 0.019 sec/batch)\n",
      "2018-01-30 20:02:30.362905: step 990, loss = 2.34 (7517.1 examples/sec; 0.017 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/cifar10_train/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "# def main(argv=None):  # pylint: disable=unused-argument\n",
    "cifar10.maybe_download_and_extract()\n",
    "if tf.gfile.Exists(FLAGS.train_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "train()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

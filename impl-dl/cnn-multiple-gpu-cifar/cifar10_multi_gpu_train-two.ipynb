{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"A binary to train CIFAR-10 using multiple GPUs with synchronous updates.\n",
    "\n",
    "Accuracy:\n",
    "cifar10_multi_gpu_train.py achieves ~86% accuracy after 100K steps (256\n",
    "epochs of data) as judged by cifar10_eval.py.\n",
    "\n",
    "Speed: With batch_size 128.\n",
    "\n",
    "System        | Step Time (sec/batch)  |     Accuracy\n",
    "--------------------------------------------------------------------\n",
    "1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n",
    "1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n",
    "2 Tesla K20m  | 0.13-0.20              | ~84% at 30K steps  (2.5 hours)\n",
    "3 Tesla K20m  | 0.13-0.18              | ~84% at 30K steps\n",
    "4 Tesla K20m  | ~0.10                  | ~84% at 30K steps\n",
    "\n",
    "Usage:\n",
    "Please see the tutorial and website for how to download the CIFAR-10\n",
    "data set, compile the program and train the model.\n",
    "\n",
    "http://tensorflow.org/tutorials/deep_cnn/\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import cifar10\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_gpus', 2,\n",
    "                            \"\"\"How many GPUs to use.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tower_loss(scope, images, labels):\n",
    "  \"\"\"Calculate the total loss on a single tower running the CIFAR model.\n",
    "\n",
    "  Args:\n",
    "    scope: unique prefix string identifying the CIFAR tower, e.g. 'tower_0'\n",
    "    images: Images. 4D tensor of shape [batch_size, height, width, 3].\n",
    "    labels: Labels. 1D tensor of shape [batch_size].\n",
    "\n",
    "  Returns:\n",
    "     Tensor of shape [] containing the total loss for a batch of data\n",
    "  \"\"\"\n",
    "\n",
    "  # Build inference Graph.\n",
    "  logits = cifar10.inference(images)\n",
    "\n",
    "  # Build the portion of the Graph calculating the losses. Note that we will\n",
    "  # assemble the total_loss using a custom function below.\n",
    "  _ = cifar10.loss(logits, labels)\n",
    "\n",
    "  # Assemble all of the losses for the current tower only.\n",
    "  losses = tf.get_collection('losses', scope)\n",
    "\n",
    "  # Calculate the total loss for the current tower.\n",
    "  total_loss = tf.add_n(losses, name='total_loss')\n",
    "\n",
    "  # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "  # same for the averaged version of the losses.\n",
    "  for l in losses + [total_loss]:\n",
    "    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "    # session. This helps the clarity of presentation on tensorboard.\n",
    "    loss_name = re.sub('%s_[0-9]*/' % cifar10.TOWER_NAME, '', l.op.name)\n",
    "    tf.summary.scalar(loss_name, l)\n",
    "\n",
    "  return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "  \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "\n",
    "  Note that this function provides a synchronization point across all towers.\n",
    "\n",
    "  Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "  Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "  \"\"\"\n",
    "  average_grads = []\n",
    "  for grad_and_vars in zip(*tower_grads):\n",
    "    # Note that each grad_and_vars looks like the following:\n",
    "    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "    grads = []\n",
    "    for g, _ in grad_and_vars:\n",
    "      # Add 0 dimension to the gradients to represent the tower.\n",
    "      expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "      # Append on a 'tower' dimension which we will average over below.\n",
    "      grads.append(expanded_g)\n",
    "\n",
    "    # Average over the 'tower' dimension.\n",
    "    grad = tf.concat(axis=0, values=grads)\n",
    "    grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "    # Keep in mind that the Variables are redundant because they are shared\n",
    "    # across towers. So .. we will just return the first tower's pointer to\n",
    "    # the Variable.\n",
    "    v = grad_and_vars[0][1]\n",
    "    grad_and_var = (grad, v)\n",
    "    average_grads.append(grad_and_var)\n",
    "  return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    # Create a variable to count the number of train() calls. This equals the\n",
    "    # number of batches processed * FLAGS.num_gpus.\n",
    "    global_step = tf.get_variable(\n",
    "        'global_step', [],\n",
    "        initializer=tf.constant_initializer(0), trainable=False)\n",
    "\n",
    "    # Calculate the learning rate schedule.\n",
    "    num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /\n",
    "                             FLAGS.batch_size)\n",
    "    decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,\n",
    "                                    global_step,\n",
    "                                    decay_steps,\n",
    "                                    cifar10.LEARNING_RATE_DECAY_FACTOR,\n",
    "                                    staircase=True)\n",
    "\n",
    "    # Create an optimizer that performs gradient descent.\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar10.distorted_inputs()\n",
    "    batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(\n",
    "          [images, labels], capacity=2 * FLAGS.num_gpus)\n",
    "    # Calculate the gradients for each model tower.\n",
    "    tower_grads = []\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "      for i in xrange(FLAGS.num_gpus):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "          with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\n",
    "            # Dequeues one batch for the GPU\n",
    "            image_batch, label_batch = batch_queue.dequeue()\n",
    "            # Calculate the loss for one tower of the CIFAR model. This function\n",
    "            # constructs the entire CIFAR model but shares the variables across\n",
    "            # all towers.\n",
    "            loss = tower_loss(scope, image_batch, label_batch)\n",
    "\n",
    "            # Reuse variables for the next tower.\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            # Retain the summaries from the final tower.\n",
    "            summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n",
    "\n",
    "            # Calculate the gradients for the batch of data on this CIFAR tower.\n",
    "            grads = opt.compute_gradients(loss)\n",
    "\n",
    "            # Keep track of the gradients across all towers.\n",
    "            tower_grads.append(grads)\n",
    "\n",
    "    # We must calculate the mean of each gradient. Note that this is the\n",
    "    # synchronization point across all towers.\n",
    "    grads = average_gradients(tower_grads)\n",
    "\n",
    "    # Add a summary to track the learning rate.\n",
    "    summaries.append(tf.summary.scalar('learning_rate', lr))\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "      if grad is not None:\n",
    "        summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))\n",
    "\n",
    "    # Apply the gradients to adjust the shared variables.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "      summaries.append(tf.summary.histogram(var.op.name, var))\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        cifar10.MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # Group all updates to into a single train op.\n",
    "    train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # Build the summary operation from the last tower summaries.\n",
    "    summary_op = tf.summary.merge(summaries)\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start running operations on the Graph. allow_soft_placement must be set to\n",
    "    # True to build towers on GPU, as some of the ops do not have GPU\n",
    "    # implementations.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=FLAGS.log_device_placement))\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n",
    "\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "      _, loss_value = sess.run([train_op, loss])\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "      if step % 10 == 0:\n",
    "        num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus\n",
    "        examples_per_sec = num_examples_per_step / duration\n",
    "        sec_per_batch = duration / FLAGS.num_gpus\n",
    "\n",
    "        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                      'sec/batch)')\n",
    "        print (format_str % (datetime.now(), step, loss_value,\n",
    "                             examples_per_sec, sec_per_batch))\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "      # Save the model checkpoint periodically.\n",
    "      if step % 100 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_path, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "2018-01-30 21:44:27.329876: step 0, loss = 4.68 (34.5 examples/sec; 1.853 sec/batch)\n",
      "2018-01-30 21:44:27.873035: step 10, loss = 4.61 (7250.4 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:28.034665: step 20, loss = 4.49 (9029.9 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:28.193795: step 30, loss = 4.52 (8383.1 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:28.347103: step 40, loss = 4.23 (8030.0 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:28.497621: step 50, loss = 4.36 (7849.2 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:28.654498: step 60, loss = 4.23 (11951.7 examples/sec; 0.005 sec/batch)\n",
      "2018-01-30 21:44:28.802377: step 70, loss = 4.23 (8432.5 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:28.941541: step 80, loss = 4.21 (9354.6 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:29.075754: step 90, loss = 4.12 (8661.3 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:29.203652: step 100, loss = 4.03 (9419.3 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:29.493886: step 110, loss = 4.15 (12903.4 examples/sec; 0.005 sec/batch)\n",
      "2018-01-30 21:44:29.647915: step 120, loss = 4.03 (7404.3 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:29.806369: step 130, loss = 4.19 (10287.1 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:29.957636: step 140, loss = 3.86 (7449.2 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:30.104554: step 150, loss = 3.80 (7774.4 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:30.248233: step 160, loss = 4.12 (7883.3 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:30.399266: step 170, loss = 3.96 (9899.0 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:30.542091: step 180, loss = 3.69 (10625.8 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:30.685149: step 190, loss = 3.75 (7601.9 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:30.833922: step 200, loss = 3.82 (8761.2 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:31.151571: step 210, loss = 3.86 (4086.9 examples/sec; 0.016 sec/batch)\n",
      "2018-01-30 21:44:31.299688: step 220, loss = 3.78 (8697.5 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:31.441842: step 230, loss = 3.64 (8567.0 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:31.587804: step 240, loss = 3.68 (7359.4 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:31.747550: step 250, loss = 3.71 (4971.4 examples/sec; 0.013 sec/batch)\n",
      "2018-01-30 21:44:31.895313: step 260, loss = 3.63 (8880.5 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:32.054779: step 270, loss = 3.64 (8658.5 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:32.193419: step 280, loss = 3.48 (11380.9 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:32.336801: step 290, loss = 3.58 (8258.9 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:32.478471: step 300, loss = 3.40 (9125.3 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:32.813009: step 310, loss = 3.54 (14009.8 examples/sec; 0.005 sec/batch)\n",
      "2018-01-30 21:44:32.948059: step 320, loss = 3.25 (8112.2 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:33.082292: step 330, loss = 3.57 (11734.9 examples/sec; 0.005 sec/batch)\n",
      "2018-01-30 21:44:33.214095: step 340, loss = 3.35 (8238.6 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:33.357133: step 350, loss = 3.60 (10149.6 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:33.497928: step 360, loss = 3.29 (8750.2 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:33.650796: step 370, loss = 3.30 (8274.6 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:33.800388: step 380, loss = 3.54 (8607.7 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:33.953731: step 390, loss = 3.60 (7233.8 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:34.102032: step 400, loss = 3.43 (7147.9 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:34.401611: step 410, loss = 3.34 (12025.1 examples/sec; 0.005 sec/batch)\n",
      "2018-01-30 21:44:34.543978: step 420, loss = 3.18 (8406.7 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:34.693523: step 430, loss = 3.39 (9549.1 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:34.840251: step 440, loss = 3.30 (7717.2 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:34.985281: step 450, loss = 3.33 (9099.7 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:35.134086: step 460, loss = 3.28 (8557.8 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:35.295359: step 470, loss = 3.20 (8522.6 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:35.453546: step 480, loss = 3.08 (7229.3 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:35.598996: step 490, loss = 3.36 (10400.8 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:35.748029: step 500, loss = 3.08 (8323.2 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:36.102604: step 510, loss = 3.12 (13614.8 examples/sec; 0.005 sec/batch)\n",
      "2018-01-30 21:44:36.247823: step 520, loss = 3.03 (8460.0 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:36.397133: step 530, loss = 3.51 (8077.7 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:36.539717: step 540, loss = 2.93 (9410.4 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:36.689413: step 550, loss = 2.93 (5679.1 examples/sec; 0.011 sec/batch)\n",
      "2018-01-30 21:44:36.827177: step 560, loss = 2.98 (9125.3 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:36.963314: step 570, loss = 3.14 (9785.3 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:37.093431: step 580, loss = 2.99 (9099.4 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:37.224243: step 590, loss = 3.00 (11997.1 examples/sec; 0.005 sec/batch)\n",
      "2018-01-30 21:44:37.357368: step 600, loss = 3.03 (10281.3 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:37.771837: step 610, loss = 2.85 (6990.9 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:37.923538: step 620, loss = 3.05 (9818.8 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:38.072508: step 630, loss = 2.91 (8866.9 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:38.221067: step 640, loss = 3.06 (11353.2 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:38.368549: step 650, loss = 2.92 (8115.0 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:38.524031: step 660, loss = 3.02 (4862.6 examples/sec; 0.013 sec/batch)\n",
      "2018-01-30 21:44:38.673282: step 670, loss = 2.93 (7996.1 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:38.820801: step 680, loss = 3.16 (8692.6 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:38.965725: step 690, loss = 2.92 (9909.2 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:39.111818: step 700, loss = 2.64 (9869.9 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:39.408814: step 710, loss = 2.82 (10123.5 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:39.560588: step 720, loss = 3.10 (5217.2 examples/sec; 0.012 sec/batch)\n",
      "2018-01-30 21:44:39.707590: step 730, loss = 2.90 (8484.5 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:39.858571: step 740, loss = 2.71 (7560.3 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:40.004987: step 750, loss = 2.75 (8351.5 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:40.146536: step 760, loss = 2.83 (9798.7 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:40.291559: step 770, loss = 2.93 (7568.7 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:40.446867: step 780, loss = 2.70 (9446.8 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:40.599019: step 790, loss = 2.82 (9521.0 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:40.744299: step 800, loss = 2.82 (8787.0 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:41.057424: step 810, loss = 2.63 (15309.9 examples/sec; 0.004 sec/batch)\n",
      "2018-01-30 21:44:41.186617: step 820, loss = 2.44 (9022.6 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:41.317648: step 830, loss = 2.64 (9949.4 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:41.452936: step 840, loss = 2.66 (9450.5 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:41.595617: step 850, loss = 2.54 (10315.3 examples/sec; 0.006 sec/batch)\n",
      "2018-01-30 21:44:41.751046: step 860, loss = 2.67 (7436.5 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:41.900419: step 870, loss = 2.66 (7497.4 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:42.054744: step 880, loss = 2.51 (6893.7 examples/sec; 0.009 sec/batch)\n",
      "2018-01-30 21:44:42.204046: step 890, loss = 2.49 (9538.8 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:42.367289: step 900, loss = 2.60 (7576.6 examples/sec; 0.008 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-30 21:44:42.693250: step 910, loss = 2.66 (12605.0 examples/sec; 0.005 sec/batch)\n",
      "2018-01-30 21:44:42.846034: step 920, loss = 2.49 (7938.1 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:42.995083: step 930, loss = 2.71 (8078.1 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:43.150069: step 940, loss = 2.72 (6691.9 examples/sec; 0.010 sec/batch)\n",
      "2018-01-30 21:44:43.296706: step 950, loss = 2.47 (7904.2 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:43.444526: step 960, loss = 2.38 (9508.7 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:43.592851: step 970, loss = 2.45 (8027.7 examples/sec; 0.008 sec/batch)\n",
      "2018-01-30 21:44:43.740663: step 980, loss = 2.42 (9098.3 examples/sec; 0.007 sec/batch)\n",
      "2018-01-30 21:44:43.883697: step 990, loss = 2.54 (8522.8 examples/sec; 0.008 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "# def main(argv=None):  # pylint: disable=unused-argument\n",
    "cifar10.maybe_download_and_extract()\n",
    "if tf.gfile.Exists(FLAGS.train_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "train()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

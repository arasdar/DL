{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to build a convolutional neural network for recognizing handwritten digits from scratch. By from scratch, I mean without using tensorflow's almighty neural network functions like `tf.nn.conv2d`. This way, you'll be able to uncover the blackbox and understand how CNN works more clearly. We'll use tensorflow interactively, so you can check the intermediate results along the way. This will also help your understanding.\n",
    "\n",
    "\n",
    "### Outline\n",
    "Here are some functions we will implement from scratch in this notebook.\n",
    "\n",
    "1. Convolutional layer\n",
    "2. ReLU\n",
    "3. Max Pooling\n",
    "4. Affine layer (Fully connected layer)\n",
    "5. Softmax\n",
    "6. Cross entropy error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines of code will download and read in the handwritten digits data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/arasdar/datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/arasdar/datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/arasdar/datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/arasdar/datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/home/arasdar/datasets/MNIST_data/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at only 100 examples at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first example of data. It's a representation of a picture as a bunch of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_X, example_ys = mnist.train.next_batch(batch_size)\n",
    "example_X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the convenient `InteractiveSession`, for checking the intermediate results along the way. You can now use `Tensor.eval()` and `Operation.run()` without having to specify a session explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start building the computation graph by creating placeholders for the input images(`X`) and target output labels(`t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder('float', [batch_size, 28, 28, 1])\n",
    "t = tf.placeholder('float', [batch_size, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an overview of the model we will build. It starts with a convolutional layer, pass the result to ReLU, pool, affine layer, ReLU again, second affine layer, and then apply softmax function. Keep in mind this architecture while you're following the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ conv - relu - pool - affine - relu - affine - softmax$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_h, filter_w, filter_c, filter_n = 5, 5, 1, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([filter_h, filter_w, filter_c, filter_n], stddev=0.01))\n",
    "b1 = tf.Variable(tf.zeros([filter_n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(X, W, b, padding, stride):\n",
    "    n, h, w, c = map(lambda d: d.value, X.get_shape())\n",
    "    filter_h, filter_w, filter_c, filter_n = [d.value for d in W.get_shape()]\n",
    "    \n",
    "    out_h = (h + 2*padding - filter_h)//stride + 1\n",
    "    out_w = (w + 2*padding - filter_w)//stride + 1\n",
    "\n",
    "    X_flat = flatten(X, filter_h, filter_w, filter_c, out_h, out_w, stride, padding)\n",
    "    W_flat = tf.reshape(W, [filter_h*filter_w*filter_c, filter_n])\n",
    "    \n",
    "    z = tf.matmul(X_flat, W_flat) + b     # b: 1 X filter_n\n",
    "    \n",
    "    return tf.transpose(tf.reshape(z, [out_h, out_w, n, filter_n]), [2, 0, 1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute convolution easily, we do a simple trick called flattening. After flattening, input data will be transformed into a 2D matrix, which allows for matrix multiplication with a filter (which is also flattened into 2D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X, window_h, window_w, window_c, out_h, out_w, stride=1, padding=0):\n",
    "    \n",
    "    X_padded = tf.pad(X, [[0,0], [padding, padding], [padding, padding], [0,0]])\n",
    "\n",
    "    windows = []\n",
    "    for y in range(out_h):\n",
    "        for x in range(out_w):\n",
    "            window = tf.slice(X_padded, [0, y*stride, x*stride, 0], [-1, window_h, window_w, -1])\n",
    "            windows.append(window)\n",
    "    stacked = tf.stack(windows) # shape : [out_h, out_w, n, filter_h, filter_w, c]\n",
    "\n",
    "    return tf.reshape(stacked, [-1, window_c*window_w*window_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose:0' shape=(100, 28, 28, 30) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = convolution(X, W1, b1, padding=2, stride=1)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return tf.maximum(X, tf.zeros_like(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Maximum:0' shape=(100, 28, 28, 30) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_activation_layer = relu(conv_layer)\n",
    "conv_activation_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(X, pool_h, pool_w, padding, stride):\n",
    "    n, h, w, c = [d.value for d in X.get_shape()]\n",
    "    \n",
    "    out_h = (h + 2*padding - pool_h)//stride + 1\n",
    "    out_w = (w + 2*padding - pool_w)//stride + 1\n",
    "\n",
    "    X_flat = flatten(X, pool_h, pool_w, c, out_h, out_w, stride, padding)\n",
    "\n",
    "    pool = tf.reduce_max(tf.reshape(X_flat, [out_h, out_w, n, pool_h*pool_w, c]), axis=3)\n",
    "    return tf.transpose(pool, [2, 0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose_1:0' shape=(100, 14, 14, 30) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling_layer = max_pool(conv_activation_layer, pool_h=2, pool_w=2, padding=0, stride=2)\n",
    "pooling_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size, pool_output_h, pool_output_w, filter_n = [d.value for d in pooling_layer.get_shape()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes in the hidden layer\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([pool_output_h*pool_output_w*filter_n, hidden_size], stddev=0.01))\n",
    "b2 = tf.Variable(tf.zeros([hidden_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine(X, W, b):\n",
    "    n = X.get_shape()[0].value # number of samples\n",
    "    X_flat = tf.reshape(X, [n, -1])\n",
    "    return tf.matmul(X_flat, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(100, 100) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_layer1 = affine(pooling_layer, W2, b2)\n",
    "affine_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0007422 ,  0.00012643, -0.00155881,  0.01961509, -0.00859066,\n",
       "       -0.00033119,  0.00343027, -0.00958689,  0.00936494,  0.00826222,\n",
       "       -0.01193877,  0.00906352, -0.00564169,  0.02327353,  0.00395058,\n",
       "        0.00056864,  0.02171693, -0.00322108, -0.00098383, -0.01839806,\n",
       "       -0.002252  ,  0.00477656, -0.01111817, -0.01797424, -0.01943753,\n",
       "        0.02208962,  0.00555941, -0.00505572,  0.00183791, -0.00555582,\n",
       "        0.01254911, -0.0074481 ,  0.02289553, -0.02554271, -0.01616727,\n",
       "       -0.00184879, -0.00088189,  0.01034675,  0.00582147,  0.01361462,\n",
       "       -0.02374761,  0.00341097,  0.01175604,  0.00232195, -0.00484839,\n",
       "       -0.01782613, -0.00154479, -0.0191502 , -0.00408084,  0.00103437,\n",
       "       -0.02297377, -0.01956699,  0.00276269, -0.01176494, -0.00460986,\n",
       "        0.01078459, -0.0049027 ,  0.00941862, -0.00548418, -0.00375865,\n",
       "        0.01463018, -0.00229884,  0.01189869, -0.01217241,  0.0034869 ,\n",
       "        0.02009911,  0.00302576,  0.00205424, -0.01815282,  0.01093012,\n",
       "        0.00721059,  0.00286405,  0.00022887, -0.02084333,  0.01679887,\n",
       "       -0.00746668, -0.01487194, -0.02442184,  0.00921292, -0.00623323,\n",
       "        0.00991315,  0.00468771,  0.00204072, -0.00819424,  0.01687701,\n",
       "       -0.00923278, -0.00575411,  0.0173942 ,  0.00541877, -0.01798927,\n",
       "        0.00395949,  0.01773626,  0.01119599,  0.0043185 ,  0.01096909,\n",
       "        0.0021534 ,  0.00772907, -0.01934665, -0.01458001, -0.00320008],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "init.run()\n",
    "affine_layer1.eval({X:example_X, t:example_ys})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result shows the representation of the first example as a 100 dimention vector in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Maximum_1:0' shape=(100, 100) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_activation_layer1 = relu(affine_layer1)\n",
    "affine_activation_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0007422 , 0.00012643, 0.        , 0.01961509, 0.        ,\n",
       "       0.        , 0.00343027, 0.        , 0.00936494, 0.00826222,\n",
       "       0.        , 0.00906352, 0.        , 0.02327353, 0.00395058,\n",
       "       0.00056864, 0.02171693, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00477656, 0.        , 0.        , 0.        ,\n",
       "       0.02208962, 0.00555941, 0.        , 0.00183791, 0.        ,\n",
       "       0.01254911, 0.        , 0.02289553, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01034675, 0.00582147, 0.01361462,\n",
       "       0.        , 0.00341097, 0.01175604, 0.00232195, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00103437,\n",
       "       0.        , 0.        , 0.00276269, 0.        , 0.        ,\n",
       "       0.01078459, 0.        , 0.00941862, 0.        , 0.        ,\n",
       "       0.01463018, 0.        , 0.01189869, 0.        , 0.0034869 ,\n",
       "       0.02009911, 0.00302576, 0.00205424, 0.        , 0.01093012,\n",
       "       0.00721059, 0.00286405, 0.00022887, 0.        , 0.01679887,\n",
       "       0.        , 0.        , 0.        , 0.00921292, 0.        ,\n",
       "       0.00991315, 0.00468771, 0.00204072, 0.        , 0.01687701,\n",
       "       0.        , 0.        , 0.0173942 , 0.00541877, 0.        ,\n",
       "       0.00395949, 0.01773626, 0.01119599, 0.0043185 , 0.01096909,\n",
       "       0.0021534 , 0.00772907, 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_activation_layer1.eval({X:example_X, t:example_ys})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is after applying ReLU to the above representation. You can see that we set all the negative numbers to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = tf.Variable(tf.random_normal([hidden_size, output_size], stddev=0.01))\n",
    "b3 = tf.Variable(tf.zeros([output_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine_layer2 = affine(affine_activation_layer1, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because you have new variables, you need to initialize them.\n",
    "init = tf.global_variables_initializer()\n",
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.8602933e-04,  8.0156862e-04, -1.8080018e-04, -3.0994401e-05,\n",
       "        7.0591079e-05,  4.7835533e-04,  1.7216617e-03, -3.0160867e-04,\n",
       "        3.4927719e-04,  7.8509547e-05], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_layer2.eval({X:example_X, t:example_ys})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_centered = X - tf.reduce_max(X) # to avoid overflow\n",
    "    X_exp = tf.exp(X_centered)\n",
    "    exp_sum = tf.reduce_sum(X_exp, axis=1)\n",
    "    return tf.transpose(tf.transpose(X_exp) / exp_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_layer = softmax(affine_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09994438, 0.10005315, 0.0999549 , 0.09996988, 0.09998003,\n",
       "       0.10002081, 0.10014524, 0.09994283, 0.1000079 , 0.09998082],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_layer.eval({X:example_X, t:example_ys})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got somewhat evenly distributed probabilities over 10 digits. This is as expected because we haven't trained our model yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    return -tf.reduce_mean(tf.log(tf.reduce_sum(y * t, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cross_entropy_error(softmax_layer, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3024163"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.eval({X:example_X, t:example_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times to iterate over training data\n",
    "training_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of batches\n",
    "num_batch = int(mnist.train.num_examples/batch_size)\n",
    "num_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7b1da58754431c8f92199ae8aaae47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=550), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0001 cost= 0.801890662\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00ed7c69c984483b8774d4c6d46b10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=550), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0002 cost= 0.109185281\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for _ in tqdm_notebook(range(num_batch)):\n",
    "        train_X, train_ys = mnist.train.next_batch(batch_size)\n",
    "        trainer.run(feed_dict={X:train_X, t:train_ys})\n",
    "        avg_cost += loss.eval(feed_dict={X:train_X, t:train_ys}) / num_batch\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = mnist.test.images[:batch_size]\n",
    "test_t = mnist.test.labels[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(network, t):\n",
    "    \n",
    "    t_predict = tf.argmax(network, axis=1)\n",
    "    t_actual = tf.argmax(t, axis=1)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(t_predict, t_actual), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98000002"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(softmax_layer, t).eval(feed_dict={X:test_x, t:test_t})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 98%. Awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**dreamgonfly@gmail.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
    "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
    "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
    "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
    "\n",
    "  ga('create', 'UA-91026007-1', 'auto');\n",
    "  ga('send', 'pageview');\n",
    "\n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {
    "065d38bfc86543d6bc25b291f29108e4": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "0b05e2f081a449d189273f1ed62c90c1": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

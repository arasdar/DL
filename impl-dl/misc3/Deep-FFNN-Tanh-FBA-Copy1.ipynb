{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#         y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#         y *= 2.0 # uni-var/ std\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#             y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#             y *= 2.0 # uni-var/ std\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "#         dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "#         self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "# #         dy =  dy @ self.W_fixed[2].T # done\n",
    "#         dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= 2.0 # uni-var/ std\n",
    "#             dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "# #             dy =  dy @ self.W_fixed[2].T # done\n",
    "#             dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#         dy *= 2.0 # uni-var/ std\n",
    "#         dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3087, acc-0.0800, valid loss-2.3034, acc-0.0894, test loss-2.3037, acc-0.0935\n",
      "Iter-20, train loss-2.3167, acc-0.0400, valid loss-2.3032, acc-0.0902, test loss-2.3035, acc-0.0941\n",
      "Iter-30, train loss-2.3211, acc-0.1000, valid loss-2.3031, acc-0.0904, test loss-2.3034, acc-0.0945\n",
      "Iter-40, train loss-2.3090, acc-0.0400, valid loss-2.3029, acc-0.0900, test loss-2.3032, acc-0.0951\n",
      "Iter-50, train loss-2.2886, acc-0.1000, valid loss-2.3028, acc-0.0904, test loss-2.3031, acc-0.0958\n",
      "Iter-60, train loss-2.2755, acc-0.1600, valid loss-2.3026, acc-0.0908, test loss-2.3029, acc-0.0960\n",
      "Iter-70, train loss-2.3107, acc-0.0400, valid loss-2.3024, acc-0.0914, test loss-2.3028, acc-0.0965\n",
      "Iter-80, train loss-2.3248, acc-0.0800, valid loss-2.3023, acc-0.0920, test loss-2.3026, acc-0.0969\n",
      "Iter-90, train loss-2.2951, acc-0.1400, valid loss-2.3021, acc-0.0924, test loss-2.3025, acc-0.0971\n",
      "Iter-100, train loss-2.3158, acc-0.1200, valid loss-2.3020, acc-0.0928, test loss-2.3023, acc-0.0973\n",
      "Iter-110, train loss-2.2853, acc-0.0800, valid loss-2.3017, acc-0.0926, test loss-2.3021, acc-0.0979\n",
      "Iter-120, train loss-2.3339, acc-0.0200, valid loss-2.3015, acc-0.0936, test loss-2.3019, acc-0.0985\n",
      "Iter-130, train loss-2.3035, acc-0.0600, valid loss-2.3014, acc-0.0940, test loss-2.3018, acc-0.0986\n",
      "Iter-140, train loss-2.2974, acc-0.0800, valid loss-2.3012, acc-0.0940, test loss-2.3016, acc-0.0994\n",
      "Iter-150, train loss-2.2914, acc-0.0400, valid loss-2.3010, acc-0.0946, test loss-2.3015, acc-0.1000\n",
      "Iter-160, train loss-2.3146, acc-0.0800, valid loss-2.3009, acc-0.0946, test loss-2.3013, acc-0.0998\n",
      "Iter-170, train loss-2.3023, acc-0.1200, valid loss-2.3008, acc-0.0952, test loss-2.3012, acc-0.1000\n",
      "Iter-180, train loss-2.3064, acc-0.0800, valid loss-2.3007, acc-0.0952, test loss-2.3011, acc-0.1004\n",
      "Iter-190, train loss-2.2980, acc-0.0600, valid loss-2.3006, acc-0.0952, test loss-2.3010, acc-0.1003\n",
      "Iter-200, train loss-2.2935, acc-0.0800, valid loss-2.3004, acc-0.0950, test loss-2.3009, acc-0.0999\n",
      "Iter-210, train loss-2.3385, acc-0.0800, valid loss-2.3002, acc-0.0954, test loss-2.3007, acc-0.1002\n",
      "Iter-220, train loss-2.2961, acc-0.0800, valid loss-2.3001, acc-0.0952, test loss-2.3005, acc-0.1014\n",
      "Iter-230, train loss-2.3010, acc-0.1000, valid loss-2.2999, acc-0.0966, test loss-2.3004, acc-0.1015\n",
      "Iter-240, train loss-2.3017, acc-0.1200, valid loss-2.2998, acc-0.0970, test loss-2.3003, acc-0.1014\n",
      "Iter-250, train loss-2.3145, acc-0.1600, valid loss-2.2997, acc-0.0976, test loss-2.3001, acc-0.1022\n",
      "Iter-260, train loss-2.3161, acc-0.0000, valid loss-2.2995, acc-0.0984, test loss-2.3000, acc-0.1025\n",
      "Iter-270, train loss-2.3089, acc-0.0600, valid loss-2.2993, acc-0.0992, test loss-2.2998, acc-0.1030\n",
      "Iter-280, train loss-2.2921, acc-0.1400, valid loss-2.2992, acc-0.0990, test loss-2.2997, acc-0.1030\n",
      "Iter-290, train loss-2.2853, acc-0.1600, valid loss-2.2990, acc-0.0990, test loss-2.2996, acc-0.1034\n",
      "Iter-300, train loss-2.3082, acc-0.0800, valid loss-2.2988, acc-0.0992, test loss-2.2994, acc-0.1037\n",
      "Iter-310, train loss-2.3059, acc-0.1400, valid loss-2.2986, acc-0.1004, test loss-2.2992, acc-0.1044\n",
      "Iter-320, train loss-2.3028, acc-0.1000, valid loss-2.2985, acc-0.1010, test loss-2.2990, acc-0.1052\n",
      "Iter-330, train loss-2.2986, acc-0.1200, valid loss-2.2983, acc-0.1012, test loss-2.2989, acc-0.1053\n",
      "Iter-340, train loss-2.3267, acc-0.0200, valid loss-2.2982, acc-0.1014, test loss-2.2987, acc-0.1051\n",
      "Iter-350, train loss-2.2711, acc-0.1200, valid loss-2.2980, acc-0.1010, test loss-2.2985, acc-0.1061\n",
      "Iter-360, train loss-2.2926, acc-0.1400, valid loss-2.2978, acc-0.1008, test loss-2.2984, acc-0.1066\n",
      "Iter-370, train loss-2.2860, acc-0.1400, valid loss-2.2976, acc-0.1014, test loss-2.2982, acc-0.1073\n",
      "Iter-380, train loss-2.3043, acc-0.0800, valid loss-2.2974, acc-0.1022, test loss-2.2980, acc-0.1077\n",
      "Iter-390, train loss-2.2965, acc-0.1000, valid loss-2.2972, acc-0.1032, test loss-2.2978, acc-0.1082\n",
      "Iter-400, train loss-2.3420, acc-0.0800, valid loss-2.2970, acc-0.1038, test loss-2.2976, acc-0.1080\n",
      "Iter-410, train loss-2.3014, acc-0.0800, valid loss-2.2969, acc-0.1036, test loss-2.2975, acc-0.1089\n",
      "Iter-420, train loss-2.2828, acc-0.1000, valid loss-2.2967, acc-0.1036, test loss-2.2973, acc-0.1095\n",
      "Iter-430, train loss-2.3298, acc-0.0600, valid loss-2.2966, acc-0.1042, test loss-2.2972, acc-0.1099\n",
      "Iter-440, train loss-2.2887, acc-0.1400, valid loss-2.2964, acc-0.1042, test loss-2.2970, acc-0.1103\n",
      "Iter-450, train loss-2.3116, acc-0.1000, valid loss-2.2962, acc-0.1046, test loss-2.2969, acc-0.1106\n",
      "Iter-460, train loss-2.2823, acc-0.1800, valid loss-2.2961, acc-0.1050, test loss-2.2967, acc-0.1108\n",
      "Iter-470, train loss-2.2986, acc-0.0800, valid loss-2.2959, acc-0.1050, test loss-2.2965, acc-0.1105\n",
      "Iter-480, train loss-2.3110, acc-0.0400, valid loss-2.2957, acc-0.1058, test loss-2.2963, acc-0.1110\n",
      "Iter-490, train loss-2.2814, acc-0.1000, valid loss-2.2955, acc-0.1056, test loss-2.2962, acc-0.1116\n",
      "Iter-500, train loss-2.2997, acc-0.1400, valid loss-2.2953, acc-0.1052, test loss-2.2960, acc-0.1112\n",
      "Iter-510, train loss-2.3006, acc-0.1000, valid loss-2.2952, acc-0.1052, test loss-2.2959, acc-0.1114\n",
      "Iter-520, train loss-2.2967, acc-0.1200, valid loss-2.2951, acc-0.1052, test loss-2.2957, acc-0.1113\n",
      "Iter-530, train loss-2.3086, acc-0.0600, valid loss-2.2948, acc-0.1056, test loss-2.2955, acc-0.1120\n",
      "Iter-540, train loss-2.2803, acc-0.1800, valid loss-2.2946, acc-0.1056, test loss-2.2953, acc-0.1128\n",
      "Iter-550, train loss-2.2837, acc-0.1800, valid loss-2.2945, acc-0.1062, test loss-2.2952, acc-0.1135\n",
      "Iter-560, train loss-2.2870, acc-0.1200, valid loss-2.2943, acc-0.1072, test loss-2.2951, acc-0.1141\n",
      "Iter-570, train loss-2.2958, acc-0.0800, valid loss-2.2941, acc-0.1082, test loss-2.2949, acc-0.1148\n",
      "Iter-580, train loss-2.2945, acc-0.0800, valid loss-2.2939, acc-0.1092, test loss-2.2947, acc-0.1150\n",
      "Iter-590, train loss-2.2815, acc-0.1000, valid loss-2.2938, acc-0.1104, test loss-2.2946, acc-0.1155\n",
      "Iter-600, train loss-2.2847, acc-0.1800, valid loss-2.2936, acc-0.1112, test loss-2.2944, acc-0.1157\n",
      "Iter-610, train loss-2.3059, acc-0.1400, valid loss-2.2934, acc-0.1124, test loss-2.2942, acc-0.1159\n",
      "Iter-620, train loss-2.2914, acc-0.1000, valid loss-2.2932, acc-0.1136, test loss-2.2940, acc-0.1161\n",
      "Iter-630, train loss-2.2894, acc-0.1000, valid loss-2.2931, acc-0.1134, test loss-2.2938, acc-0.1164\n",
      "Iter-640, train loss-2.2978, acc-0.0800, valid loss-2.2928, acc-0.1138, test loss-2.2936, acc-0.1174\n",
      "Iter-650, train loss-2.3020, acc-0.0400, valid loss-2.2926, acc-0.1140, test loss-2.2934, acc-0.1176\n",
      "Iter-660, train loss-2.3051, acc-0.0400, valid loss-2.2924, acc-0.1146, test loss-2.2932, acc-0.1179\n",
      "Iter-670, train loss-2.3004, acc-0.1400, valid loss-2.2923, acc-0.1144, test loss-2.2930, acc-0.1181\n",
      "Iter-680, train loss-2.2806, acc-0.1200, valid loss-2.2920, acc-0.1154, test loss-2.2928, acc-0.1181\n",
      "Iter-690, train loss-2.2858, acc-0.1400, valid loss-2.2918, acc-0.1166, test loss-2.2926, acc-0.1189\n",
      "Iter-700, train loss-2.2899, acc-0.0800, valid loss-2.2917, acc-0.1172, test loss-2.2925, acc-0.1194\n",
      "Iter-710, train loss-2.2912, acc-0.1000, valid loss-2.2914, acc-0.1168, test loss-2.2922, acc-0.1201\n",
      "Iter-720, train loss-2.2699, acc-0.1600, valid loss-2.2912, acc-0.1182, test loss-2.2920, acc-0.1199\n",
      "Iter-730, train loss-2.2705, acc-0.1800, valid loss-2.2910, acc-0.1190, test loss-2.2918, acc-0.1203\n",
      "Iter-740, train loss-2.2843, acc-0.1200, valid loss-2.2907, acc-0.1196, test loss-2.2916, acc-0.1207\n",
      "Iter-750, train loss-2.2977, acc-0.1000, valid loss-2.2905, acc-0.1202, test loss-2.2914, acc-0.1210\n",
      "Iter-760, train loss-2.3022, acc-0.0800, valid loss-2.2903, acc-0.1204, test loss-2.2911, acc-0.1210\n",
      "Iter-770, train loss-2.2885, acc-0.1600, valid loss-2.2901, acc-0.1212, test loss-2.2909, acc-0.1220\n",
      "Iter-780, train loss-2.3183, acc-0.0600, valid loss-2.2899, acc-0.1214, test loss-2.2907, acc-0.1224\n",
      "Iter-790, train loss-2.2771, acc-0.1200, valid loss-2.2897, acc-0.1222, test loss-2.2905, acc-0.1230\n",
      "Iter-800, train loss-2.2976, acc-0.1000, valid loss-2.2895, acc-0.1222, test loss-2.2903, acc-0.1238\n",
      "Iter-810, train loss-2.2672, acc-0.1600, valid loss-2.2893, acc-0.1224, test loss-2.2901, acc-0.1249\n",
      "Iter-820, train loss-2.3045, acc-0.1200, valid loss-2.2891, acc-0.1226, test loss-2.2899, acc-0.1257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.2870, acc-0.1400, valid loss-2.2889, acc-0.1234, test loss-2.2898, acc-0.1255\n",
      "Iter-840, train loss-2.2806, acc-0.1400, valid loss-2.2886, acc-0.1250, test loss-2.2895, acc-0.1259\n",
      "Iter-850, train loss-2.3080, acc-0.0600, valid loss-2.2884, acc-0.1264, test loss-2.2893, acc-0.1264\n",
      "Iter-860, train loss-2.2623, acc-0.1400, valid loss-2.2881, acc-0.1274, test loss-2.2890, acc-0.1275\n",
      "Iter-870, train loss-2.2885, acc-0.1600, valid loss-2.2879, acc-0.1286, test loss-2.2888, acc-0.1278\n",
      "Iter-880, train loss-2.3148, acc-0.0600, valid loss-2.2876, acc-0.1300, test loss-2.2885, acc-0.1285\n",
      "Iter-890, train loss-2.3312, acc-0.0800, valid loss-2.2874, acc-0.1304, test loss-2.2883, acc-0.1289\n",
      "Iter-900, train loss-2.3034, acc-0.1400, valid loss-2.2872, acc-0.1312, test loss-2.2881, acc-0.1293\n",
      "Iter-910, train loss-2.2864, acc-0.1200, valid loss-2.2869, acc-0.1316, test loss-2.2879, acc-0.1297\n",
      "Iter-920, train loss-2.2692, acc-0.1400, valid loss-2.2867, acc-0.1320, test loss-2.2877, acc-0.1299\n",
      "Iter-930, train loss-2.3073, acc-0.2200, valid loss-2.2865, acc-0.1328, test loss-2.2875, acc-0.1303\n",
      "Iter-940, train loss-2.3002, acc-0.1600, valid loss-2.2863, acc-0.1342, test loss-2.2872, acc-0.1309\n",
      "Iter-950, train loss-2.2860, acc-0.1600, valid loss-2.2860, acc-0.1350, test loss-2.2870, acc-0.1313\n",
      "Iter-960, train loss-2.2949, acc-0.0600, valid loss-2.2858, acc-0.1352, test loss-2.2867, acc-0.1315\n",
      "Iter-970, train loss-2.2881, acc-0.1000, valid loss-2.2855, acc-0.1368, test loss-2.2865, acc-0.1324\n",
      "Iter-980, train loss-2.3042, acc-0.1000, valid loss-2.2853, acc-0.1370, test loss-2.2862, acc-0.1326\n",
      "Iter-990, train loss-2.2806, acc-0.1200, valid loss-2.2850, acc-0.1372, test loss-2.2860, acc-0.1333\n",
      "Iter-1000, train loss-2.2798, acc-0.1200, valid loss-2.2847, acc-0.1384, test loss-2.2857, acc-0.1332\n",
      "Iter-1010, train loss-2.3017, acc-0.0600, valid loss-2.2845, acc-0.1394, test loss-2.2855, acc-0.1338\n",
      "Iter-1020, train loss-2.2849, acc-0.0800, valid loss-2.2842, acc-0.1396, test loss-2.2852, acc-0.1341\n",
      "Iter-1030, train loss-2.3234, acc-0.0600, valid loss-2.2839, acc-0.1410, test loss-2.2850, acc-0.1352\n",
      "Iter-1040, train loss-2.2564, acc-0.2600, valid loss-2.2837, acc-0.1418, test loss-2.2847, acc-0.1357\n",
      "Iter-1050, train loss-2.3013, acc-0.1000, valid loss-2.2834, acc-0.1420, test loss-2.2845, acc-0.1366\n",
      "Iter-1060, train loss-2.2605, acc-0.2400, valid loss-2.2832, acc-0.1428, test loss-2.2842, acc-0.1377\n",
      "Iter-1070, train loss-2.2765, acc-0.2000, valid loss-2.2829, acc-0.1428, test loss-2.2839, acc-0.1380\n",
      "Iter-1080, train loss-2.2719, acc-0.1600, valid loss-2.2827, acc-0.1438, test loss-2.2837, acc-0.1388\n",
      "Iter-1090, train loss-2.2588, acc-0.1400, valid loss-2.2823, acc-0.1448, test loss-2.2834, acc-0.1399\n",
      "Iter-1100, train loss-2.2558, acc-0.1200, valid loss-2.2821, acc-0.1444, test loss-2.2831, acc-0.1409\n",
      "Iter-1110, train loss-2.3203, acc-0.1600, valid loss-2.2818, acc-0.1446, test loss-2.2829, acc-0.1417\n",
      "Iter-1120, train loss-2.2832, acc-0.1600, valid loss-2.2816, acc-0.1452, test loss-2.2826, acc-0.1420\n",
      "Iter-1130, train loss-2.3109, acc-0.0800, valid loss-2.2813, acc-0.1458, test loss-2.2824, acc-0.1430\n",
      "Iter-1140, train loss-2.2819, acc-0.1200, valid loss-2.2810, acc-0.1460, test loss-2.2821, acc-0.1433\n",
      "Iter-1150, train loss-2.2933, acc-0.1000, valid loss-2.2808, acc-0.1462, test loss-2.2819, acc-0.1440\n",
      "Iter-1160, train loss-2.2558, acc-0.1200, valid loss-2.2805, acc-0.1468, test loss-2.2815, acc-0.1445\n",
      "Iter-1170, train loss-2.2662, acc-0.2400, valid loss-2.2802, acc-0.1480, test loss-2.2813, acc-0.1453\n",
      "Iter-1180, train loss-2.2756, acc-0.1600, valid loss-2.2799, acc-0.1498, test loss-2.2810, acc-0.1464\n",
      "Iter-1190, train loss-2.2819, acc-0.2000, valid loss-2.2795, acc-0.1516, test loss-2.2806, acc-0.1472\n",
      "Iter-1200, train loss-2.3070, acc-0.1200, valid loss-2.2792, acc-0.1522, test loss-2.2803, acc-0.1475\n",
      "Iter-1210, train loss-2.2550, acc-0.1800, valid loss-2.2789, acc-0.1540, test loss-2.2800, acc-0.1480\n",
      "Iter-1220, train loss-2.2851, acc-0.2200, valid loss-2.2786, acc-0.1546, test loss-2.2797, acc-0.1489\n",
      "Iter-1230, train loss-2.2751, acc-0.1200, valid loss-2.2782, acc-0.1556, test loss-2.2793, acc-0.1496\n",
      "Iter-1240, train loss-2.2747, acc-0.1200, valid loss-2.2779, acc-0.1556, test loss-2.2790, acc-0.1500\n",
      "Iter-1250, train loss-2.2898, acc-0.0800, valid loss-2.2776, acc-0.1556, test loss-2.2787, acc-0.1505\n",
      "Iter-1260, train loss-2.2679, acc-0.1600, valid loss-2.2773, acc-0.1566, test loss-2.2784, acc-0.1512\n",
      "Iter-1270, train loss-2.3143, acc-0.1000, valid loss-2.2769, acc-0.1572, test loss-2.2781, acc-0.1519\n",
      "Iter-1280, train loss-2.2988, acc-0.1400, valid loss-2.2766, acc-0.1580, test loss-2.2777, acc-0.1524\n",
      "Iter-1290, train loss-2.2894, acc-0.1200, valid loss-2.2763, acc-0.1586, test loss-2.2774, acc-0.1536\n",
      "Iter-1300, train loss-2.2770, acc-0.0800, valid loss-2.2760, acc-0.1596, test loss-2.2771, acc-0.1547\n",
      "Iter-1310, train loss-2.2513, acc-0.1800, valid loss-2.2756, acc-0.1606, test loss-2.2767, acc-0.1548\n",
      "Iter-1320, train loss-2.2583, acc-0.1600, valid loss-2.2752, acc-0.1614, test loss-2.2764, acc-0.1556\n",
      "Iter-1330, train loss-2.2959, acc-0.1200, valid loss-2.2749, acc-0.1616, test loss-2.2761, acc-0.1555\n",
      "Iter-1340, train loss-2.2783, acc-0.1600, valid loss-2.2746, acc-0.1626, test loss-2.2757, acc-0.1568\n",
      "Iter-1350, train loss-2.2863, acc-0.1600, valid loss-2.2742, acc-0.1636, test loss-2.2754, acc-0.1577\n",
      "Iter-1360, train loss-2.2413, acc-0.2400, valid loss-2.2739, acc-0.1648, test loss-2.2751, acc-0.1579\n",
      "Iter-1370, train loss-2.2719, acc-0.1200, valid loss-2.2736, acc-0.1666, test loss-2.2747, acc-0.1587\n",
      "Iter-1380, train loss-2.2436, acc-0.2000, valid loss-2.2732, acc-0.1676, test loss-2.2744, acc-0.1590\n",
      "Iter-1390, train loss-2.2534, acc-0.1800, valid loss-2.2728, acc-0.1678, test loss-2.2740, acc-0.1601\n",
      "Iter-1400, train loss-2.2808, acc-0.1200, valid loss-2.2725, acc-0.1680, test loss-2.2737, acc-0.1609\n",
      "Iter-1410, train loss-2.2366, acc-0.2800, valid loss-2.2721, acc-0.1690, test loss-2.2733, acc-0.1616\n",
      "Iter-1420, train loss-2.2955, acc-0.1800, valid loss-2.2717, acc-0.1698, test loss-2.2729, acc-0.1622\n",
      "Iter-1430, train loss-2.2595, acc-0.1600, valid loss-2.2713, acc-0.1702, test loss-2.2725, acc-0.1636\n",
      "Iter-1440, train loss-2.2685, acc-0.1600, valid loss-2.2710, acc-0.1720, test loss-2.2722, acc-0.1639\n",
      "Iter-1450, train loss-2.2778, acc-0.1800, valid loss-2.2706, acc-0.1724, test loss-2.2718, acc-0.1644\n",
      "Iter-1460, train loss-2.2525, acc-0.1800, valid loss-2.2702, acc-0.1730, test loss-2.2714, acc-0.1650\n",
      "Iter-1470, train loss-2.2887, acc-0.1200, valid loss-2.2698, acc-0.1738, test loss-2.2711, acc-0.1654\n",
      "Iter-1480, train loss-2.2616, acc-0.1400, valid loss-2.2695, acc-0.1748, test loss-2.2707, acc-0.1659\n",
      "Iter-1490, train loss-2.2659, acc-0.1400, valid loss-2.2692, acc-0.1752, test loss-2.2704, acc-0.1668\n",
      "Iter-1500, train loss-2.2765, acc-0.1400, valid loss-2.2688, acc-0.1756, test loss-2.2701, acc-0.1673\n",
      "Iter-1510, train loss-2.2721, acc-0.2400, valid loss-2.2685, acc-0.1754, test loss-2.2697, acc-0.1677\n",
      "Iter-1520, train loss-2.2710, acc-0.1400, valid loss-2.2681, acc-0.1768, test loss-2.2693, acc-0.1683\n",
      "Iter-1530, train loss-2.2258, acc-0.3200, valid loss-2.2677, acc-0.1768, test loss-2.2689, acc-0.1691\n",
      "Iter-1540, train loss-2.2539, acc-0.2000, valid loss-2.2673, acc-0.1770, test loss-2.2686, acc-0.1699\n",
      "Iter-1550, train loss-2.2775, acc-0.2200, valid loss-2.2670, acc-0.1778, test loss-2.2682, acc-0.1704\n",
      "Iter-1560, train loss-2.2607, acc-0.2000, valid loss-2.2666, acc-0.1774, test loss-2.2678, acc-0.1708\n",
      "Iter-1570, train loss-2.2581, acc-0.2000, valid loss-2.2662, acc-0.1782, test loss-2.2674, acc-0.1716\n",
      "Iter-1580, train loss-2.2564, acc-0.1200, valid loss-2.2658, acc-0.1794, test loss-2.2670, acc-0.1725\n",
      "Iter-1590, train loss-2.2581, acc-0.1200, valid loss-2.2653, acc-0.1800, test loss-2.2665, acc-0.1738\n",
      "Iter-1600, train loss-2.2711, acc-0.1000, valid loss-2.2650, acc-0.1802, test loss-2.2662, acc-0.1744\n",
      "Iter-1610, train loss-2.2771, acc-0.1600, valid loss-2.2646, acc-0.1804, test loss-2.2658, acc-0.1748\n",
      "Iter-1620, train loss-2.3079, acc-0.0600, valid loss-2.2641, acc-0.1806, test loss-2.2654, acc-0.1750\n",
      "Iter-1630, train loss-2.2719, acc-0.1400, valid loss-2.2636, acc-0.1818, test loss-2.2649, acc-0.1761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-2.2542, acc-0.2600, valid loss-2.2632, acc-0.1826, test loss-2.2644, acc-0.1769\n",
      "Iter-1650, train loss-2.2474, acc-0.2000, valid loss-2.2627, acc-0.1826, test loss-2.2639, acc-0.1772\n",
      "Iter-1660, train loss-2.2462, acc-0.2000, valid loss-2.2622, acc-0.1838, test loss-2.2635, acc-0.1786\n",
      "Iter-1670, train loss-2.2654, acc-0.2200, valid loss-2.2618, acc-0.1850, test loss-2.2631, acc-0.1794\n",
      "Iter-1680, train loss-2.2792, acc-0.0600, valid loss-2.2614, acc-0.1864, test loss-2.2626, acc-0.1803\n",
      "Iter-1690, train loss-2.2890, acc-0.1400, valid loss-2.2610, acc-0.1874, test loss-2.2622, acc-0.1808\n",
      "Iter-1700, train loss-2.2561, acc-0.2800, valid loss-2.2605, acc-0.1886, test loss-2.2617, acc-0.1814\n",
      "Iter-1710, train loss-2.2712, acc-0.1600, valid loss-2.2600, acc-0.1894, test loss-2.2613, acc-0.1824\n",
      "Iter-1720, train loss-2.2627, acc-0.1600, valid loss-2.2596, acc-0.1898, test loss-2.2609, acc-0.1831\n",
      "Iter-1730, train loss-2.2527, acc-0.1600, valid loss-2.2592, acc-0.1902, test loss-2.2605, acc-0.1833\n",
      "Iter-1740, train loss-2.2240, acc-0.2600, valid loss-2.2587, acc-0.1910, test loss-2.2600, acc-0.1842\n",
      "Iter-1750, train loss-2.2708, acc-0.0800, valid loss-2.2582, acc-0.1924, test loss-2.2595, acc-0.1850\n",
      "Iter-1760, train loss-2.2748, acc-0.2000, valid loss-2.2577, acc-0.1932, test loss-2.2590, acc-0.1863\n",
      "Iter-1770, train loss-2.2630, acc-0.2400, valid loss-2.2572, acc-0.1934, test loss-2.2585, acc-0.1870\n",
      "Iter-1780, train loss-2.2708, acc-0.1800, valid loss-2.2567, acc-0.1950, test loss-2.2580, acc-0.1877\n",
      "Iter-1790, train loss-2.2463, acc-0.2800, valid loss-2.2562, acc-0.1958, test loss-2.2575, acc-0.1888\n",
      "Iter-1800, train loss-2.2648, acc-0.1600, valid loss-2.2557, acc-0.1962, test loss-2.2570, acc-0.1896\n",
      "Iter-1810, train loss-2.2578, acc-0.1600, valid loss-2.2553, acc-0.1974, test loss-2.2565, acc-0.1910\n",
      "Iter-1820, train loss-2.2838, acc-0.1000, valid loss-2.2549, acc-0.1978, test loss-2.2561, acc-0.1912\n",
      "Iter-1830, train loss-2.2497, acc-0.1600, valid loss-2.2543, acc-0.1982, test loss-2.2556, acc-0.1914\n",
      "Iter-1840, train loss-2.2947, acc-0.1800, valid loss-2.2538, acc-0.1984, test loss-2.2551, acc-0.1917\n",
      "Iter-1850, train loss-2.2539, acc-0.2000, valid loss-2.2533, acc-0.1992, test loss-2.2546, acc-0.1933\n",
      "Iter-1860, train loss-2.2571, acc-0.2000, valid loss-2.2529, acc-0.2014, test loss-2.2541, acc-0.1938\n",
      "Iter-1870, train loss-2.2742, acc-0.1600, valid loss-2.2524, acc-0.2014, test loss-2.2536, acc-0.1948\n",
      "Iter-1880, train loss-2.2541, acc-0.2600, valid loss-2.2519, acc-0.2024, test loss-2.2531, acc-0.1953\n",
      "Iter-1890, train loss-2.2308, acc-0.2400, valid loss-2.2513, acc-0.2034, test loss-2.2526, acc-0.1960\n",
      "Iter-1900, train loss-2.2473, acc-0.2000, valid loss-2.2508, acc-0.2040, test loss-2.2521, acc-0.1964\n",
      "Iter-1910, train loss-2.2728, acc-0.1400, valid loss-2.2502, acc-0.2050, test loss-2.2515, acc-0.1974\n",
      "Iter-1920, train loss-2.2728, acc-0.1600, valid loss-2.2497, acc-0.2058, test loss-2.2510, acc-0.1982\n",
      "Iter-1930, train loss-2.2333, acc-0.2400, valid loss-2.2492, acc-0.2066, test loss-2.2505, acc-0.1988\n",
      "Iter-1940, train loss-2.2325, acc-0.2400, valid loss-2.2486, acc-0.2080, test loss-2.2498, acc-0.1994\n",
      "Iter-1950, train loss-2.2105, acc-0.1800, valid loss-2.2480, acc-0.2084, test loss-2.2493, acc-0.2007\n",
      "Iter-1960, train loss-2.2358, acc-0.2200, valid loss-2.2475, acc-0.2092, test loss-2.2488, acc-0.2017\n",
      "Iter-1970, train loss-2.1978, acc-0.3000, valid loss-2.2470, acc-0.2100, test loss-2.2482, acc-0.2022\n",
      "Iter-1980, train loss-2.2261, acc-0.2800, valid loss-2.2464, acc-0.2100, test loss-2.2477, acc-0.2031\n",
      "Iter-1990, train loss-2.2728, acc-0.1600, valid loss-2.2459, acc-0.2108, test loss-2.2471, acc-0.2043\n",
      "Iter-2000, train loss-2.2183, acc-0.3400, valid loss-2.2453, acc-0.2122, test loss-2.2466, acc-0.2050\n",
      "Iter-2010, train loss-2.2544, acc-0.2600, valid loss-2.2448, acc-0.2138, test loss-2.2460, acc-0.2058\n",
      "Iter-2020, train loss-2.2159, acc-0.2800, valid loss-2.2442, acc-0.2148, test loss-2.2455, acc-0.2067\n",
      "Iter-2030, train loss-2.2450, acc-0.1800, valid loss-2.2437, acc-0.2160, test loss-2.2449, acc-0.2073\n",
      "Iter-2040, train loss-2.2440, acc-0.2000, valid loss-2.2431, acc-0.2174, test loss-2.2443, acc-0.2088\n",
      "Iter-2050, train loss-2.2404, acc-0.2200, valid loss-2.2425, acc-0.2176, test loss-2.2438, acc-0.2094\n",
      "Iter-2060, train loss-2.2269, acc-0.2400, valid loss-2.2419, acc-0.2188, test loss-2.2432, acc-0.2101\n",
      "Iter-2070, train loss-2.2653, acc-0.1000, valid loss-2.2414, acc-0.2194, test loss-2.2426, acc-0.2115\n",
      "Iter-2080, train loss-2.2640, acc-0.1200, valid loss-2.2408, acc-0.2210, test loss-2.2421, acc-0.2118\n",
      "Iter-2090, train loss-2.2699, acc-0.1200, valid loss-2.2402, acc-0.2218, test loss-2.2415, acc-0.2130\n",
      "Iter-2100, train loss-2.2296, acc-0.2000, valid loss-2.2397, acc-0.2224, test loss-2.2409, acc-0.2137\n",
      "Iter-2110, train loss-2.2322, acc-0.2000, valid loss-2.2390, acc-0.2238, test loss-2.2402, acc-0.2154\n",
      "Iter-2120, train loss-2.2410, acc-0.2000, valid loss-2.2383, acc-0.2250, test loss-2.2396, acc-0.2162\n",
      "Iter-2130, train loss-2.2684, acc-0.2000, valid loss-2.2378, acc-0.2252, test loss-2.2390, acc-0.2167\n",
      "Iter-2140, train loss-2.2254, acc-0.2400, valid loss-2.2372, acc-0.2252, test loss-2.2384, acc-0.2179\n",
      "Iter-2150, train loss-2.2414, acc-0.2000, valid loss-2.2365, acc-0.2264, test loss-2.2378, acc-0.2195\n",
      "Iter-2160, train loss-2.2871, acc-0.1800, valid loss-2.2359, acc-0.2270, test loss-2.2371, acc-0.2202\n",
      "Iter-2170, train loss-2.2761, acc-0.1000, valid loss-2.2352, acc-0.2274, test loss-2.2365, acc-0.2206\n",
      "Iter-2180, train loss-2.2313, acc-0.2800, valid loss-2.2345, acc-0.2294, test loss-2.2358, acc-0.2223\n",
      "Iter-2190, train loss-2.2323, acc-0.2000, valid loss-2.2339, acc-0.2300, test loss-2.2351, acc-0.2238\n",
      "Iter-2200, train loss-2.2005, acc-0.2600, valid loss-2.2333, acc-0.2308, test loss-2.2345, acc-0.2246\n",
      "Iter-2210, train loss-2.1839, acc-0.2800, valid loss-2.2326, acc-0.2316, test loss-2.2338, acc-0.2256\n",
      "Iter-2220, train loss-2.2441, acc-0.2600, valid loss-2.2320, acc-0.2320, test loss-2.2332, acc-0.2264\n",
      "Iter-2230, train loss-2.2147, acc-0.2600, valid loss-2.2313, acc-0.2330, test loss-2.2325, acc-0.2270\n",
      "Iter-2240, train loss-2.2163, acc-0.2800, valid loss-2.2307, acc-0.2340, test loss-2.2319, acc-0.2278\n",
      "Iter-2250, train loss-2.2150, acc-0.2400, valid loss-2.2301, acc-0.2348, test loss-2.2313, acc-0.2290\n",
      "Iter-2260, train loss-2.2183, acc-0.2600, valid loss-2.2294, acc-0.2360, test loss-2.2306, acc-0.2301\n",
      "Iter-2270, train loss-2.2415, acc-0.2400, valid loss-2.2288, acc-0.2366, test loss-2.2300, acc-0.2302\n",
      "Iter-2280, train loss-2.2173, acc-0.2800, valid loss-2.2281, acc-0.2380, test loss-2.2293, acc-0.2311\n",
      "Iter-2290, train loss-2.2450, acc-0.2600, valid loss-2.2274, acc-0.2384, test loss-2.2286, acc-0.2318\n",
      "Iter-2300, train loss-2.2276, acc-0.2400, valid loss-2.2268, acc-0.2388, test loss-2.2280, acc-0.2324\n",
      "Iter-2310, train loss-2.1915, acc-0.2800, valid loss-2.2261, acc-0.2402, test loss-2.2273, acc-0.2328\n",
      "Iter-2320, train loss-2.2128, acc-0.2800, valid loss-2.2254, acc-0.2408, test loss-2.2266, acc-0.2347\n",
      "Iter-2330, train loss-2.2118, acc-0.1000, valid loss-2.2248, acc-0.2416, test loss-2.2260, acc-0.2351\n",
      "Iter-2340, train loss-2.1946, acc-0.2600, valid loss-2.2240, acc-0.2426, test loss-2.2252, acc-0.2365\n",
      "Iter-2350, train loss-2.2293, acc-0.1800, valid loss-2.2234, acc-0.2428, test loss-2.2246, acc-0.2367\n",
      "Iter-2360, train loss-2.2238, acc-0.2200, valid loss-2.2227, acc-0.2440, test loss-2.2239, acc-0.2370\n",
      "Iter-2370, train loss-2.2296, acc-0.2600, valid loss-2.2220, acc-0.2446, test loss-2.2232, acc-0.2379\n",
      "Iter-2380, train loss-2.2402, acc-0.2200, valid loss-2.2213, acc-0.2456, test loss-2.2225, acc-0.2382\n",
      "Iter-2390, train loss-2.2055, acc-0.2400, valid loss-2.2207, acc-0.2466, test loss-2.2219, acc-0.2384\n",
      "Iter-2400, train loss-2.1738, acc-0.2400, valid loss-2.2200, acc-0.2472, test loss-2.2212, acc-0.2398\n",
      "Iter-2410, train loss-2.2004, acc-0.3400, valid loss-2.2192, acc-0.2480, test loss-2.2204, acc-0.2413\n",
      "Iter-2420, train loss-2.2512, acc-0.1800, valid loss-2.2186, acc-0.2480, test loss-2.2198, acc-0.2426\n",
      "Iter-2430, train loss-2.1800, acc-0.3000, valid loss-2.2178, acc-0.2486, test loss-2.2190, acc-0.2438\n",
      "Iter-2440, train loss-2.2045, acc-0.2800, valid loss-2.2170, acc-0.2496, test loss-2.2182, acc-0.2455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-2.2005, acc-0.3000, valid loss-2.2162, acc-0.2506, test loss-2.2174, acc-0.2459\n",
      "Iter-2460, train loss-2.2170, acc-0.2800, valid loss-2.2155, acc-0.2516, test loss-2.2167, acc-0.2467\n",
      "Iter-2470, train loss-2.2467, acc-0.1200, valid loss-2.2148, acc-0.2524, test loss-2.2160, acc-0.2472\n",
      "Iter-2480, train loss-2.2278, acc-0.2200, valid loss-2.2140, acc-0.2532, test loss-2.2152, acc-0.2478\n",
      "Iter-2490, train loss-2.2161, acc-0.2200, valid loss-2.2132, acc-0.2542, test loss-2.2144, acc-0.2489\n",
      "Iter-2500, train loss-2.2081, acc-0.2600, valid loss-2.2124, acc-0.2544, test loss-2.2136, acc-0.2497\n",
      "Iter-2510, train loss-2.1748, acc-0.3600, valid loss-2.2117, acc-0.2552, test loss-2.2128, acc-0.2500\n",
      "Iter-2520, train loss-2.2354, acc-0.1800, valid loss-2.2109, acc-0.2562, test loss-2.2120, acc-0.2510\n",
      "Iter-2530, train loss-2.1859, acc-0.3800, valid loss-2.2101, acc-0.2562, test loss-2.2113, acc-0.2515\n",
      "Iter-2540, train loss-2.1945, acc-0.2400, valid loss-2.2093, acc-0.2570, test loss-2.2105, acc-0.2526\n",
      "Iter-2550, train loss-2.2456, acc-0.1800, valid loss-2.2086, acc-0.2572, test loss-2.2097, acc-0.2532\n",
      "Iter-2560, train loss-2.2053, acc-0.2000, valid loss-2.2078, acc-0.2568, test loss-2.2089, acc-0.2542\n",
      "Iter-2570, train loss-2.1509, acc-0.3600, valid loss-2.2070, acc-0.2586, test loss-2.2081, acc-0.2547\n",
      "Iter-2580, train loss-2.1957, acc-0.3600, valid loss-2.2062, acc-0.2594, test loss-2.2073, acc-0.2557\n",
      "Iter-2590, train loss-2.2307, acc-0.1800, valid loss-2.2053, acc-0.2610, test loss-2.2064, acc-0.2576\n",
      "Iter-2600, train loss-2.2377, acc-0.2000, valid loss-2.2046, acc-0.2618, test loss-2.2057, acc-0.2583\n",
      "Iter-2610, train loss-2.2207, acc-0.2800, valid loss-2.2038, acc-0.2624, test loss-2.2049, acc-0.2600\n",
      "Iter-2620, train loss-2.1985, acc-0.3000, valid loss-2.2031, acc-0.2628, test loss-2.2042, acc-0.2612\n",
      "Iter-2630, train loss-2.2202, acc-0.2000, valid loss-2.2023, acc-0.2630, test loss-2.2034, acc-0.2624\n",
      "Iter-2640, train loss-2.1770, acc-0.3800, valid loss-2.2015, acc-0.2632, test loss-2.2025, acc-0.2636\n",
      "Iter-2650, train loss-2.2214, acc-0.2000, valid loss-2.2006, acc-0.2650, test loss-2.2017, acc-0.2645\n",
      "Iter-2660, train loss-2.1837, acc-0.3000, valid loss-2.1998, acc-0.2652, test loss-2.2009, acc-0.2651\n",
      "Iter-2670, train loss-2.1726, acc-0.3600, valid loss-2.1989, acc-0.2660, test loss-2.2000, acc-0.2657\n",
      "Iter-2680, train loss-2.1484, acc-0.3400, valid loss-2.1981, acc-0.2658, test loss-2.1992, acc-0.2662\n",
      "Iter-2690, train loss-2.2023, acc-0.2200, valid loss-2.1973, acc-0.2668, test loss-2.1983, acc-0.2674\n",
      "Iter-2700, train loss-2.1546, acc-0.3600, valid loss-2.1963, acc-0.2674, test loss-2.1974, acc-0.2687\n",
      "Iter-2710, train loss-2.2108, acc-0.2200, valid loss-2.1955, acc-0.2688, test loss-2.1966, acc-0.2697\n",
      "Iter-2720, train loss-2.1925, acc-0.2200, valid loss-2.1947, acc-0.2692, test loss-2.1957, acc-0.2701\n",
      "Iter-2730, train loss-2.1897, acc-0.3200, valid loss-2.1939, acc-0.2704, test loss-2.1949, acc-0.2708\n",
      "Iter-2740, train loss-2.2272, acc-0.1600, valid loss-2.1930, acc-0.2706, test loss-2.1941, acc-0.2713\n",
      "Iter-2750, train loss-2.1616, acc-0.3600, valid loss-2.1922, acc-0.2708, test loss-2.1932, acc-0.2721\n",
      "Iter-2760, train loss-2.1873, acc-0.2400, valid loss-2.1914, acc-0.2718, test loss-2.1924, acc-0.2733\n",
      "Iter-2770, train loss-2.1390, acc-0.4600, valid loss-2.1905, acc-0.2730, test loss-2.1915, acc-0.2740\n",
      "Iter-2780, train loss-2.2313, acc-0.2400, valid loss-2.1896, acc-0.2738, test loss-2.1906, acc-0.2747\n",
      "Iter-2790, train loss-2.2146, acc-0.2000, valid loss-2.1888, acc-0.2744, test loss-2.1898, acc-0.2753\n",
      "Iter-2800, train loss-2.2470, acc-0.2200, valid loss-2.1879, acc-0.2762, test loss-2.1889, acc-0.2755\n",
      "Iter-2810, train loss-2.2103, acc-0.1600, valid loss-2.1871, acc-0.2770, test loss-2.1881, acc-0.2760\n",
      "Iter-2820, train loss-2.2297, acc-0.1800, valid loss-2.1864, acc-0.2776, test loss-2.1874, acc-0.2765\n",
      "Iter-2830, train loss-2.2227, acc-0.2200, valid loss-2.1855, acc-0.2782, test loss-2.1865, acc-0.2766\n",
      "Iter-2840, train loss-2.1542, acc-0.3400, valid loss-2.1845, acc-0.2798, test loss-2.1856, acc-0.2779\n",
      "Iter-2850, train loss-2.2085, acc-0.1600, valid loss-2.1836, acc-0.2808, test loss-2.1846, acc-0.2785\n",
      "Iter-2860, train loss-2.1771, acc-0.2400, valid loss-2.1827, acc-0.2816, test loss-2.1837, acc-0.2790\n",
      "Iter-2870, train loss-2.1791, acc-0.2400, valid loss-2.1818, acc-0.2822, test loss-2.1828, acc-0.2798\n",
      "Iter-2880, train loss-2.1354, acc-0.3400, valid loss-2.1810, acc-0.2824, test loss-2.1819, acc-0.2798\n",
      "Iter-2890, train loss-2.1677, acc-0.3000, valid loss-2.1801, acc-0.2830, test loss-2.1811, acc-0.2807\n",
      "Iter-2900, train loss-2.1836, acc-0.2600, valid loss-2.1792, acc-0.2842, test loss-2.1801, acc-0.2817\n",
      "Iter-2910, train loss-2.1649, acc-0.3200, valid loss-2.1783, acc-0.2846, test loss-2.1792, acc-0.2826\n",
      "Iter-2920, train loss-2.1290, acc-0.3200, valid loss-2.1773, acc-0.2848, test loss-2.1783, acc-0.2833\n",
      "Iter-2930, train loss-2.2012, acc-0.3000, valid loss-2.1765, acc-0.2854, test loss-2.1775, acc-0.2841\n",
      "Iter-2940, train loss-2.1689, acc-0.3200, valid loss-2.1755, acc-0.2856, test loss-2.1764, acc-0.2847\n",
      "Iter-2950, train loss-2.1912, acc-0.2600, valid loss-2.1746, acc-0.2864, test loss-2.1755, acc-0.2858\n",
      "Iter-2960, train loss-2.2078, acc-0.2600, valid loss-2.1738, acc-0.2874, test loss-2.1747, acc-0.2869\n",
      "Iter-2970, train loss-2.1829, acc-0.2800, valid loss-2.1729, acc-0.2884, test loss-2.1738, acc-0.2875\n",
      "Iter-2980, train loss-2.1987, acc-0.2000, valid loss-2.1720, acc-0.2888, test loss-2.1729, acc-0.2878\n",
      "Iter-2990, train loss-2.1380, acc-0.4200, valid loss-2.1711, acc-0.2902, test loss-2.1721, acc-0.2886\n",
      "Iter-3000, train loss-2.2120, acc-0.1600, valid loss-2.1702, acc-0.2904, test loss-2.1712, acc-0.2895\n",
      "Iter-3010, train loss-2.1685, acc-0.1800, valid loss-2.1693, acc-0.2906, test loss-2.1702, acc-0.2904\n",
      "Iter-3020, train loss-2.1787, acc-0.3400, valid loss-2.1684, acc-0.2914, test loss-2.1694, acc-0.2913\n",
      "Iter-3030, train loss-2.1784, acc-0.3000, valid loss-2.1676, acc-0.2922, test loss-2.1685, acc-0.2917\n",
      "Iter-3040, train loss-2.2091, acc-0.2000, valid loss-2.1667, acc-0.2926, test loss-2.1677, acc-0.2923\n",
      "Iter-3050, train loss-2.1548, acc-0.2600, valid loss-2.1658, acc-0.2930, test loss-2.1667, acc-0.2931\n",
      "Iter-3060, train loss-2.2059, acc-0.2000, valid loss-2.1649, acc-0.2934, test loss-2.1658, acc-0.2938\n",
      "Iter-3070, train loss-2.1539, acc-0.2600, valid loss-2.1639, acc-0.2944, test loss-2.1648, acc-0.2938\n",
      "Iter-3080, train loss-2.1627, acc-0.3200, valid loss-2.1629, acc-0.2952, test loss-2.1638, acc-0.2948\n",
      "Iter-3090, train loss-2.1836, acc-0.2800, valid loss-2.1619, acc-0.2964, test loss-2.1629, acc-0.2961\n",
      "Iter-3100, train loss-2.1846, acc-0.2400, valid loss-2.1611, acc-0.2968, test loss-2.1620, acc-0.2966\n",
      "Iter-3110, train loss-2.1615, acc-0.3200, valid loss-2.1600, acc-0.2972, test loss-2.1610, acc-0.2973\n",
      "Iter-3120, train loss-2.1461, acc-0.3000, valid loss-2.1591, acc-0.2980, test loss-2.1600, acc-0.2984\n",
      "Iter-3130, train loss-2.1781, acc-0.2400, valid loss-2.1582, acc-0.2990, test loss-2.1591, acc-0.2994\n",
      "Iter-3140, train loss-2.1461, acc-0.3600, valid loss-2.1573, acc-0.2994, test loss-2.1582, acc-0.3006\n",
      "Iter-3150, train loss-2.1125, acc-0.3000, valid loss-2.1562, acc-0.3000, test loss-2.1571, acc-0.3022\n",
      "Iter-3160, train loss-2.1094, acc-0.3800, valid loss-2.1552, acc-0.3006, test loss-2.1561, acc-0.3031\n",
      "Iter-3170, train loss-2.1055, acc-0.4400, valid loss-2.1542, acc-0.3016, test loss-2.1551, acc-0.3029\n",
      "Iter-3180, train loss-2.1183, acc-0.3200, valid loss-2.1533, acc-0.3026, test loss-2.1542, acc-0.3038\n",
      "Iter-3190, train loss-2.1565, acc-0.3200, valid loss-2.1522, acc-0.3030, test loss-2.1531, acc-0.3049\n",
      "Iter-3200, train loss-2.1944, acc-0.2000, valid loss-2.1512, acc-0.3042, test loss-2.1521, acc-0.3057\n",
      "Iter-3210, train loss-2.2125, acc-0.1400, valid loss-2.1503, acc-0.3050, test loss-2.1512, acc-0.3063\n",
      "Iter-3220, train loss-2.1534, acc-0.2800, valid loss-2.1494, acc-0.3062, test loss-2.1503, acc-0.3073\n",
      "Iter-3230, train loss-2.1751, acc-0.2800, valid loss-2.1484, acc-0.3066, test loss-2.1493, acc-0.3080\n",
      "Iter-3240, train loss-2.1337, acc-0.3800, valid loss-2.1473, acc-0.3070, test loss-2.1482, acc-0.3086\n",
      "Iter-3250, train loss-2.1486, acc-0.3600, valid loss-2.1463, acc-0.3074, test loss-2.1471, acc-0.3095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-2.1368, acc-0.2800, valid loss-2.1453, acc-0.3086, test loss-2.1462, acc-0.3103\n",
      "Iter-3270, train loss-2.1541, acc-0.3400, valid loss-2.1443, acc-0.3098, test loss-2.1452, acc-0.3117\n",
      "Iter-3280, train loss-2.1275, acc-0.3600, valid loss-2.1433, acc-0.3110, test loss-2.1441, acc-0.3129\n",
      "Iter-3290, train loss-2.1588, acc-0.1800, valid loss-2.1423, acc-0.3130, test loss-2.1431, acc-0.3139\n",
      "Iter-3300, train loss-2.1198, acc-0.3200, valid loss-2.1414, acc-0.3132, test loss-2.1422, acc-0.3150\n",
      "Iter-3310, train loss-2.1923, acc-0.2400, valid loss-2.1404, acc-0.3144, test loss-2.1412, acc-0.3158\n",
      "Iter-3320, train loss-2.2380, acc-0.2000, valid loss-2.1394, acc-0.3150, test loss-2.1402, acc-0.3170\n",
      "Iter-3330, train loss-2.1609, acc-0.2600, valid loss-2.1384, acc-0.3166, test loss-2.1392, acc-0.3180\n",
      "Iter-3340, train loss-2.0983, acc-0.3600, valid loss-2.1374, acc-0.3172, test loss-2.1383, acc-0.3194\n",
      "Iter-3350, train loss-2.1048, acc-0.3600, valid loss-2.1364, acc-0.3184, test loss-2.1372, acc-0.3199\n",
      "Iter-3360, train loss-2.1220, acc-0.3400, valid loss-2.1353, acc-0.3190, test loss-2.1361, acc-0.3216\n",
      "Iter-3370, train loss-2.1327, acc-0.3400, valid loss-2.1343, acc-0.3208, test loss-2.1351, acc-0.3225\n",
      "Iter-3380, train loss-2.1023, acc-0.4000, valid loss-2.1333, acc-0.3220, test loss-2.1341, acc-0.3240\n",
      "Iter-3390, train loss-2.0900, acc-0.4000, valid loss-2.1322, acc-0.3234, test loss-2.1330, acc-0.3250\n",
      "Iter-3400, train loss-2.1390, acc-0.2600, valid loss-2.1312, acc-0.3244, test loss-2.1320, acc-0.3261\n",
      "Iter-3410, train loss-2.1247, acc-0.3600, valid loss-2.1302, acc-0.3258, test loss-2.1310, acc-0.3269\n",
      "Iter-3420, train loss-2.1264, acc-0.3200, valid loss-2.1292, acc-0.3272, test loss-2.1300, acc-0.3284\n",
      "Iter-3430, train loss-2.2245, acc-0.2000, valid loss-2.1281, acc-0.3292, test loss-2.1289, acc-0.3295\n",
      "Iter-3440, train loss-2.0982, acc-0.3400, valid loss-2.1271, acc-0.3308, test loss-2.1279, acc-0.3306\n",
      "Iter-3450, train loss-2.1879, acc-0.1800, valid loss-2.1261, acc-0.3326, test loss-2.1269, acc-0.3326\n",
      "Iter-3460, train loss-2.1488, acc-0.2600, valid loss-2.1250, acc-0.3348, test loss-2.1258, acc-0.3331\n",
      "Iter-3470, train loss-2.0860, acc-0.4200, valid loss-2.1239, acc-0.3362, test loss-2.1246, acc-0.3351\n",
      "Iter-3480, train loss-2.1074, acc-0.2800, valid loss-2.1229, acc-0.3366, test loss-2.1236, acc-0.3361\n",
      "Iter-3490, train loss-2.1187, acc-0.3400, valid loss-2.1219, acc-0.3374, test loss-2.1227, acc-0.3376\n",
      "Iter-3500, train loss-2.1051, acc-0.3600, valid loss-2.1208, acc-0.3392, test loss-2.1216, acc-0.3396\n",
      "Iter-3510, train loss-2.1149, acc-0.3600, valid loss-2.1198, acc-0.3408, test loss-2.1205, acc-0.3402\n",
      "Iter-3520, train loss-2.1234, acc-0.4000, valid loss-2.1186, acc-0.3412, test loss-2.1193, acc-0.3409\n",
      "Iter-3530, train loss-2.0712, acc-0.3000, valid loss-2.1174, acc-0.3426, test loss-2.1182, acc-0.3415\n",
      "Iter-3540, train loss-2.1688, acc-0.2600, valid loss-2.1164, acc-0.3442, test loss-2.1171, acc-0.3424\n",
      "Iter-3550, train loss-2.1849, acc-0.3000, valid loss-2.1153, acc-0.3446, test loss-2.1160, acc-0.3440\n",
      "Iter-3560, train loss-2.0938, acc-0.4400, valid loss-2.1142, acc-0.3450, test loss-2.1149, acc-0.3443\n",
      "Iter-3570, train loss-2.1397, acc-0.3400, valid loss-2.1131, acc-0.3462, test loss-2.1138, acc-0.3452\n",
      "Iter-3580, train loss-2.1103, acc-0.3200, valid loss-2.1120, acc-0.3480, test loss-2.1127, acc-0.3473\n",
      "Iter-3590, train loss-2.1951, acc-0.2800, valid loss-2.1109, acc-0.3490, test loss-2.1116, acc-0.3490\n",
      "Iter-3600, train loss-2.1143, acc-0.3000, valid loss-2.1097, acc-0.3494, test loss-2.1104, acc-0.3493\n",
      "Iter-3610, train loss-2.0942, acc-0.3200, valid loss-2.1086, acc-0.3494, test loss-2.1093, acc-0.3508\n",
      "Iter-3620, train loss-2.0628, acc-0.3800, valid loss-2.1076, acc-0.3524, test loss-2.1083, acc-0.3511\n",
      "Iter-3630, train loss-2.1327, acc-0.3200, valid loss-2.1066, acc-0.3532, test loss-2.1073, acc-0.3522\n",
      "Iter-3640, train loss-2.1625, acc-0.2400, valid loss-2.1056, acc-0.3548, test loss-2.1062, acc-0.3535\n",
      "Iter-3650, train loss-2.0898, acc-0.3000, valid loss-2.1045, acc-0.3558, test loss-2.1052, acc-0.3541\n",
      "Iter-3660, train loss-2.1097, acc-0.3200, valid loss-2.1034, acc-0.3566, test loss-2.1041, acc-0.3551\n",
      "Iter-3670, train loss-2.1139, acc-0.3400, valid loss-2.1023, acc-0.3580, test loss-2.1030, acc-0.3560\n",
      "Iter-3680, train loss-2.0226, acc-0.5400, valid loss-2.1012, acc-0.3588, test loss-2.1019, acc-0.3569\n",
      "Iter-3690, train loss-2.0825, acc-0.4600, valid loss-2.1001, acc-0.3594, test loss-2.1007, acc-0.3579\n",
      "Iter-3700, train loss-2.0532, acc-0.4000, valid loss-2.0989, acc-0.3604, test loss-2.0996, acc-0.3590\n",
      "Iter-3710, train loss-2.1328, acc-0.2600, valid loss-2.0979, acc-0.3616, test loss-2.0985, acc-0.3604\n",
      "Iter-3720, train loss-2.0887, acc-0.4200, valid loss-2.0968, acc-0.3626, test loss-2.0974, acc-0.3617\n",
      "Iter-3730, train loss-2.0970, acc-0.3000, valid loss-2.0957, acc-0.3630, test loss-2.0963, acc-0.3624\n",
      "Iter-3740, train loss-2.1385, acc-0.3000, valid loss-2.0946, acc-0.3630, test loss-2.0952, acc-0.3638\n",
      "Iter-3750, train loss-2.1717, acc-0.2600, valid loss-2.0935, acc-0.3634, test loss-2.0941, acc-0.3645\n",
      "Iter-3760, train loss-2.0876, acc-0.3800, valid loss-2.0923, acc-0.3646, test loss-2.0929, acc-0.3654\n",
      "Iter-3770, train loss-2.0576, acc-0.3400, valid loss-2.0911, acc-0.3658, test loss-2.0918, acc-0.3669\n",
      "Iter-3780, train loss-2.1522, acc-0.3000, valid loss-2.0901, acc-0.3664, test loss-2.0907, acc-0.3666\n",
      "Iter-3790, train loss-2.0906, acc-0.3600, valid loss-2.0889, acc-0.3664, test loss-2.0895, acc-0.3671\n",
      "Iter-3800, train loss-2.0870, acc-0.4000, valid loss-2.0878, acc-0.3664, test loss-2.0884, acc-0.3678\n",
      "Iter-3810, train loss-2.0301, acc-0.4400, valid loss-2.0867, acc-0.3676, test loss-2.0872, acc-0.3691\n",
      "Iter-3820, train loss-2.0963, acc-0.3600, valid loss-2.0855, acc-0.3676, test loss-2.0860, acc-0.3699\n",
      "Iter-3830, train loss-2.0546, acc-0.4000, valid loss-2.0842, acc-0.3684, test loss-2.0848, acc-0.3707\n",
      "Iter-3840, train loss-2.0497, acc-0.4000, valid loss-2.0831, acc-0.3682, test loss-2.0836, acc-0.3714\n",
      "Iter-3850, train loss-2.1011, acc-0.4400, valid loss-2.0819, acc-0.3684, test loss-2.0824, acc-0.3725\n",
      "Iter-3860, train loss-2.0730, acc-0.3600, valid loss-2.0808, acc-0.3692, test loss-2.0814, acc-0.3735\n",
      "Iter-3870, train loss-2.0719, acc-0.3600, valid loss-2.0797, acc-0.3698, test loss-2.0802, acc-0.3742\n",
      "Iter-3880, train loss-2.0373, acc-0.4000, valid loss-2.0784, acc-0.3708, test loss-2.0789, acc-0.3758\n",
      "Iter-3890, train loss-2.1495, acc-0.2200, valid loss-2.0772, acc-0.3724, test loss-2.0777, acc-0.3773\n",
      "Iter-3900, train loss-2.0918, acc-0.3800, valid loss-2.0761, acc-0.3740, test loss-2.0766, acc-0.3774\n",
      "Iter-3910, train loss-2.0560, acc-0.3600, valid loss-2.0749, acc-0.3748, test loss-2.0754, acc-0.3787\n",
      "Iter-3920, train loss-2.1302, acc-0.3200, valid loss-2.0737, acc-0.3746, test loss-2.0742, acc-0.3795\n",
      "Iter-3930, train loss-2.0657, acc-0.4600, valid loss-2.0726, acc-0.3756, test loss-2.0731, acc-0.3797\n",
      "Iter-3940, train loss-2.1273, acc-0.3200, valid loss-2.0714, acc-0.3760, test loss-2.0719, acc-0.3804\n",
      "Iter-3950, train loss-2.1720, acc-0.2400, valid loss-2.0703, acc-0.3772, test loss-2.0707, acc-0.3821\n",
      "Iter-3960, train loss-2.0951, acc-0.3800, valid loss-2.0691, acc-0.3776, test loss-2.0696, acc-0.3831\n",
      "Iter-3970, train loss-2.0692, acc-0.4000, valid loss-2.0680, acc-0.3776, test loss-2.0685, acc-0.3832\n",
      "Iter-3980, train loss-2.0222, acc-0.4000, valid loss-2.0668, acc-0.3782, test loss-2.0672, acc-0.3839\n",
      "Iter-3990, train loss-2.0094, acc-0.3200, valid loss-2.0656, acc-0.3790, test loss-2.0660, acc-0.3845\n",
      "Iter-4000, train loss-2.0580, acc-0.3200, valid loss-2.0645, acc-0.3790, test loss-2.0649, acc-0.3847\n",
      "Iter-4010, train loss-2.1160, acc-0.3800, valid loss-2.0633, acc-0.3794, test loss-2.0638, acc-0.3852\n",
      "Iter-4020, train loss-2.0809, acc-0.4200, valid loss-2.0621, acc-0.3794, test loss-2.0626, acc-0.3853\n",
      "Iter-4030, train loss-2.0344, acc-0.4000, valid loss-2.0610, acc-0.3802, test loss-2.0614, acc-0.3866\n",
      "Iter-4040, train loss-2.0528, acc-0.3800, valid loss-2.0598, acc-0.3804, test loss-2.0602, acc-0.3876\n",
      "Iter-4050, train loss-2.0206, acc-0.5000, valid loss-2.0587, acc-0.3814, test loss-2.0591, acc-0.3875\n",
      "Iter-4060, train loss-2.0102, acc-0.4400, valid loss-2.0575, acc-0.3812, test loss-2.0580, acc-0.3877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-2.0351, acc-0.4800, valid loss-2.0564, acc-0.3814, test loss-2.0569, acc-0.3880\n",
      "Iter-4080, train loss-2.1190, acc-0.3200, valid loss-2.0552, acc-0.3816, test loss-2.0557, acc-0.3884\n",
      "Iter-4090, train loss-2.0532, acc-0.3400, valid loss-2.0540, acc-0.3814, test loss-2.0545, acc-0.3882\n",
      "Iter-4100, train loss-2.0818, acc-0.3600, valid loss-2.0528, acc-0.3824, test loss-2.0533, acc-0.3898\n",
      "Iter-4110, train loss-2.0366, acc-0.4200, valid loss-2.0516, acc-0.3834, test loss-2.0521, acc-0.3901\n",
      "Iter-4120, train loss-2.0911, acc-0.3400, valid loss-2.0505, acc-0.3832, test loss-2.0509, acc-0.3903\n",
      "Iter-4130, train loss-2.0465, acc-0.3600, valid loss-2.0493, acc-0.3848, test loss-2.0497, acc-0.3921\n",
      "Iter-4140, train loss-2.0225, acc-0.3600, valid loss-2.0481, acc-0.3852, test loss-2.0485, acc-0.3924\n",
      "Iter-4150, train loss-2.0431, acc-0.3800, valid loss-2.0468, acc-0.3856, test loss-2.0473, acc-0.3926\n",
      "Iter-4160, train loss-2.0044, acc-0.4400, valid loss-2.0457, acc-0.3862, test loss-2.0461, acc-0.3933\n",
      "Iter-4170, train loss-2.0425, acc-0.3600, valid loss-2.0445, acc-0.3868, test loss-2.0449, acc-0.3942\n",
      "Iter-4180, train loss-1.9996, acc-0.4600, valid loss-2.0433, acc-0.3866, test loss-2.0437, acc-0.3946\n",
      "Iter-4190, train loss-2.1092, acc-0.3200, valid loss-2.0421, acc-0.3870, test loss-2.0425, acc-0.3957\n",
      "Iter-4200, train loss-2.0543, acc-0.3200, valid loss-2.0410, acc-0.3870, test loss-2.0414, acc-0.3961\n",
      "Iter-4210, train loss-2.0546, acc-0.3600, valid loss-2.0397, acc-0.3868, test loss-2.0401, acc-0.3964\n",
      "Iter-4220, train loss-2.0116, acc-0.3800, valid loss-2.0385, acc-0.3866, test loss-2.0389, acc-0.3958\n",
      "Iter-4230, train loss-2.0183, acc-0.4200, valid loss-2.0373, acc-0.3866, test loss-2.0376, acc-0.3955\n",
      "Iter-4240, train loss-2.0584, acc-0.3200, valid loss-2.0360, acc-0.3872, test loss-2.0364, acc-0.3956\n",
      "Iter-4250, train loss-2.0104, acc-0.5000, valid loss-2.0348, acc-0.3872, test loss-2.0352, acc-0.3959\n",
      "Iter-4260, train loss-2.0123, acc-0.3600, valid loss-2.0336, acc-0.3882, test loss-2.0340, acc-0.3972\n",
      "Iter-4270, train loss-2.0052, acc-0.5400, valid loss-2.0324, acc-0.3876, test loss-2.0328, acc-0.3973\n",
      "Iter-4280, train loss-2.0725, acc-0.3800, valid loss-2.0313, acc-0.3878, test loss-2.0316, acc-0.3976\n",
      "Iter-4290, train loss-2.0658, acc-0.3600, valid loss-2.0301, acc-0.3884, test loss-2.0305, acc-0.3977\n",
      "Iter-4300, train loss-1.9964, acc-0.4200, valid loss-2.0290, acc-0.3896, test loss-2.0293, acc-0.3978\n",
      "Iter-4310, train loss-2.0544, acc-0.2600, valid loss-2.0278, acc-0.3898, test loss-2.0281, acc-0.3978\n",
      "Iter-4320, train loss-1.9527, acc-0.4800, valid loss-2.0266, acc-0.3906, test loss-2.0269, acc-0.3982\n",
      "Iter-4330, train loss-2.0369, acc-0.4000, valid loss-2.0253, acc-0.3914, test loss-2.0256, acc-0.3998\n",
      "Iter-4340, train loss-1.9760, acc-0.4600, valid loss-2.0242, acc-0.3928, test loss-2.0245, acc-0.4003\n",
      "Iter-4350, train loss-2.0262, acc-0.3600, valid loss-2.0229, acc-0.3934, test loss-2.0232, acc-0.4004\n",
      "Iter-4360, train loss-2.0982, acc-0.3200, valid loss-2.0218, acc-0.3936, test loss-2.0221, acc-0.4003\n",
      "Iter-4370, train loss-2.0455, acc-0.3600, valid loss-2.0207, acc-0.3940, test loss-2.0210, acc-0.4002\n",
      "Iter-4380, train loss-2.0051, acc-0.3800, valid loss-2.0195, acc-0.3948, test loss-2.0198, acc-0.4020\n",
      "Iter-4390, train loss-1.9756, acc-0.4800, valid loss-2.0182, acc-0.3956, test loss-2.0185, acc-0.4027\n",
      "Iter-4400, train loss-1.9442, acc-0.5600, valid loss-2.0169, acc-0.3962, test loss-2.0172, acc-0.4032\n",
      "Iter-4410, train loss-2.0068, acc-0.4400, valid loss-2.0157, acc-0.3968, test loss-2.0160, acc-0.4035\n",
      "Iter-4420, train loss-2.1177, acc-0.3000, valid loss-2.0145, acc-0.3962, test loss-2.0147, acc-0.4037\n",
      "Iter-4430, train loss-2.0737, acc-0.3000, valid loss-2.0133, acc-0.3958, test loss-2.0135, acc-0.4039\n",
      "Iter-4440, train loss-1.9453, acc-0.4200, valid loss-2.0120, acc-0.3970, test loss-2.0122, acc-0.4044\n",
      "Iter-4450, train loss-2.0190, acc-0.4400, valid loss-2.0108, acc-0.3966, test loss-2.0111, acc-0.4047\n",
      "Iter-4460, train loss-1.9694, acc-0.4200, valid loss-2.0095, acc-0.3966, test loss-2.0098, acc-0.4047\n",
      "Iter-4470, train loss-1.9898, acc-0.3600, valid loss-2.0083, acc-0.3970, test loss-2.0086, acc-0.4048\n",
      "Iter-4480, train loss-2.0223, acc-0.4600, valid loss-2.0070, acc-0.3972, test loss-2.0072, acc-0.4047\n",
      "Iter-4490, train loss-1.9881, acc-0.4000, valid loss-2.0057, acc-0.3972, test loss-2.0059, acc-0.4047\n",
      "Iter-4500, train loss-1.9800, acc-0.4400, valid loss-2.0045, acc-0.3972, test loss-2.0047, acc-0.4050\n",
      "Iter-4510, train loss-2.0105, acc-0.3600, valid loss-2.0033, acc-0.3982, test loss-2.0034, acc-0.4050\n",
      "Iter-4520, train loss-2.0516, acc-0.3400, valid loss-2.0020, acc-0.3986, test loss-2.0022, acc-0.4057\n",
      "Iter-4530, train loss-1.9273, acc-0.5200, valid loss-2.0009, acc-0.3988, test loss-2.0011, acc-0.4056\n",
      "Iter-4540, train loss-1.9556, acc-0.4000, valid loss-1.9996, acc-0.3992, test loss-1.9998, acc-0.4069\n",
      "Iter-4550, train loss-2.0356, acc-0.3200, valid loss-1.9984, acc-0.3996, test loss-1.9986, acc-0.4069\n",
      "Iter-4560, train loss-1.9707, acc-0.3600, valid loss-1.9971, acc-0.3994, test loss-1.9973, acc-0.4074\n",
      "Iter-4570, train loss-2.0047, acc-0.3200, valid loss-1.9959, acc-0.3996, test loss-1.9961, acc-0.4083\n",
      "Iter-4580, train loss-2.0340, acc-0.4400, valid loss-1.9947, acc-0.3998, test loss-1.9948, acc-0.4084\n",
      "Iter-4590, train loss-2.0799, acc-0.2600, valid loss-1.9936, acc-0.4000, test loss-1.9938, acc-0.4085\n",
      "Iter-4600, train loss-1.9579, acc-0.5000, valid loss-1.9924, acc-0.4008, test loss-1.9925, acc-0.4088\n",
      "Iter-4610, train loss-2.0147, acc-0.4400, valid loss-1.9912, acc-0.4012, test loss-1.9913, acc-0.4090\n",
      "Iter-4620, train loss-1.8517, acc-0.5200, valid loss-1.9898, acc-0.4018, test loss-1.9899, acc-0.4096\n",
      "Iter-4630, train loss-2.0082, acc-0.4400, valid loss-1.9886, acc-0.4016, test loss-1.9888, acc-0.4100\n",
      "Iter-4640, train loss-2.0562, acc-0.3600, valid loss-1.9874, acc-0.4020, test loss-1.9875, acc-0.4100\n",
      "Iter-4650, train loss-1.9651, acc-0.4200, valid loss-1.9862, acc-0.4020, test loss-1.9863, acc-0.4105\n",
      "Iter-4660, train loss-1.9773, acc-0.3800, valid loss-1.9850, acc-0.4022, test loss-1.9852, acc-0.4109\n",
      "Iter-4670, train loss-1.9928, acc-0.3200, valid loss-1.9838, acc-0.4038, test loss-1.9839, acc-0.4115\n",
      "Iter-4680, train loss-1.9298, acc-0.3600, valid loss-1.9824, acc-0.4036, test loss-1.9826, acc-0.4117\n",
      "Iter-4690, train loss-1.9788, acc-0.4200, valid loss-1.9812, acc-0.4038, test loss-1.9813, acc-0.4116\n",
      "Iter-4700, train loss-1.9666, acc-0.3800, valid loss-1.9798, acc-0.4044, test loss-1.9799, acc-0.4128\n",
      "Iter-4710, train loss-1.9801, acc-0.3400, valid loss-1.9787, acc-0.4046, test loss-1.9788, acc-0.4129\n",
      "Iter-4720, train loss-1.9793, acc-0.4600, valid loss-1.9775, acc-0.4052, test loss-1.9776, acc-0.4139\n",
      "Iter-4730, train loss-2.0486, acc-0.3400, valid loss-1.9762, acc-0.4048, test loss-1.9763, acc-0.4149\n",
      "Iter-4740, train loss-1.9408, acc-0.3800, valid loss-1.9749, acc-0.4054, test loss-1.9750, acc-0.4158\n",
      "Iter-4750, train loss-1.9870, acc-0.3800, valid loss-1.9736, acc-0.4056, test loss-1.9737, acc-0.4163\n",
      "Iter-4760, train loss-1.9259, acc-0.3600, valid loss-1.9723, acc-0.4062, test loss-1.9724, acc-0.4167\n",
      "Iter-4770, train loss-1.9598, acc-0.4400, valid loss-1.9711, acc-0.4062, test loss-1.9712, acc-0.4172\n",
      "Iter-4780, train loss-1.9092, acc-0.4200, valid loss-1.9698, acc-0.4064, test loss-1.9698, acc-0.4172\n",
      "Iter-4790, train loss-1.9518, acc-0.4000, valid loss-1.9686, acc-0.4072, test loss-1.9686, acc-0.4178\n",
      "Iter-4800, train loss-1.9641, acc-0.4400, valid loss-1.9674, acc-0.4076, test loss-1.9674, acc-0.4181\n",
      "Iter-4810, train loss-1.9819, acc-0.4200, valid loss-1.9662, acc-0.4088, test loss-1.9662, acc-0.4190\n",
      "Iter-4820, train loss-2.0116, acc-0.4000, valid loss-1.9650, acc-0.4092, test loss-1.9651, acc-0.4194\n",
      "Iter-4830, train loss-1.9768, acc-0.3400, valid loss-1.9639, acc-0.4092, test loss-1.9640, acc-0.4198\n",
      "Iter-4840, train loss-1.9497, acc-0.4400, valid loss-1.9626, acc-0.4102, test loss-1.9627, acc-0.4204\n",
      "Iter-4850, train loss-1.9106, acc-0.4800, valid loss-1.9613, acc-0.4104, test loss-1.9614, acc-0.4213\n",
      "Iter-4860, train loss-1.9170, acc-0.5000, valid loss-1.9600, acc-0.4104, test loss-1.9601, acc-0.4220\n",
      "Iter-4870, train loss-1.8606, acc-0.5400, valid loss-1.9587, acc-0.4110, test loss-1.9588, acc-0.4225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-1.9674, acc-0.3600, valid loss-1.9575, acc-0.4118, test loss-1.9576, acc-0.4230\n",
      "Iter-4890, train loss-1.9626, acc-0.3600, valid loss-1.9562, acc-0.4120, test loss-1.9562, acc-0.4229\n",
      "Iter-4900, train loss-1.9353, acc-0.4600, valid loss-1.9548, acc-0.4120, test loss-1.9549, acc-0.4228\n",
      "Iter-4910, train loss-1.9590, acc-0.4600, valid loss-1.9535, acc-0.4120, test loss-1.9536, acc-0.4238\n",
      "Iter-4920, train loss-1.9642, acc-0.4200, valid loss-1.9522, acc-0.4132, test loss-1.9523, acc-0.4244\n",
      "Iter-4930, train loss-1.9430, acc-0.4800, valid loss-1.9511, acc-0.4130, test loss-1.9511, acc-0.4242\n",
      "Iter-4940, train loss-2.0134, acc-0.3000, valid loss-1.9498, acc-0.4132, test loss-1.9498, acc-0.4241\n",
      "Iter-4950, train loss-2.0539, acc-0.2600, valid loss-1.9486, acc-0.4138, test loss-1.9487, acc-0.4243\n",
      "Iter-4960, train loss-1.8335, acc-0.4800, valid loss-1.9473, acc-0.4146, test loss-1.9473, acc-0.4246\n",
      "Iter-4970, train loss-1.9934, acc-0.3200, valid loss-1.9460, acc-0.4148, test loss-1.9460, acc-0.4254\n",
      "Iter-4980, train loss-1.9433, acc-0.4000, valid loss-1.9447, acc-0.4154, test loss-1.9447, acc-0.4263\n",
      "Iter-4990, train loss-2.0643, acc-0.3200, valid loss-1.9434, acc-0.4156, test loss-1.9434, acc-0.4269\n",
      "Iter-5000, train loss-1.9162, acc-0.4200, valid loss-1.9421, acc-0.4168, test loss-1.9421, acc-0.4277\n",
      "Iter-5010, train loss-1.9453, acc-0.4800, valid loss-1.9408, acc-0.4172, test loss-1.9408, acc-0.4277\n",
      "Iter-5020, train loss-1.9691, acc-0.4600, valid loss-1.9395, acc-0.4180, test loss-1.9394, acc-0.4280\n",
      "Iter-5030, train loss-1.9034, acc-0.4400, valid loss-1.9382, acc-0.4186, test loss-1.9382, acc-0.4284\n",
      "Iter-5040, train loss-1.8867, acc-0.4800, valid loss-1.9369, acc-0.4184, test loss-1.9368, acc-0.4290\n",
      "Iter-5050, train loss-1.9602, acc-0.4000, valid loss-1.9357, acc-0.4186, test loss-1.9356, acc-0.4295\n",
      "Iter-5060, train loss-1.9847, acc-0.4000, valid loss-1.9343, acc-0.4200, test loss-1.9342, acc-0.4302\n",
      "Iter-5070, train loss-1.8950, acc-0.4200, valid loss-1.9330, acc-0.4206, test loss-1.9329, acc-0.4305\n",
      "Iter-5080, train loss-1.8893, acc-0.4400, valid loss-1.9316, acc-0.4206, test loss-1.9315, acc-0.4310\n",
      "Iter-5090, train loss-1.9529, acc-0.4000, valid loss-1.9303, acc-0.4208, test loss-1.9302, acc-0.4310\n",
      "Iter-5100, train loss-1.9703, acc-0.3800, valid loss-1.9290, acc-0.4214, test loss-1.9289, acc-0.4315\n",
      "Iter-5110, train loss-1.8939, acc-0.5200, valid loss-1.9277, acc-0.4218, test loss-1.9276, acc-0.4315\n",
      "Iter-5120, train loss-1.8747, acc-0.4200, valid loss-1.9263, acc-0.4220, test loss-1.9262, acc-0.4318\n",
      "Iter-5130, train loss-1.9075, acc-0.3800, valid loss-1.9251, acc-0.4224, test loss-1.9249, acc-0.4320\n",
      "Iter-5140, train loss-1.8743, acc-0.4200, valid loss-1.9238, acc-0.4234, test loss-1.9237, acc-0.4326\n",
      "Iter-5150, train loss-1.8145, acc-0.5600, valid loss-1.9225, acc-0.4248, test loss-1.9223, acc-0.4327\n",
      "Iter-5160, train loss-1.9275, acc-0.4400, valid loss-1.9212, acc-0.4252, test loss-1.9210, acc-0.4331\n",
      "Iter-5170, train loss-1.9298, acc-0.4000, valid loss-1.9198, acc-0.4256, test loss-1.9197, acc-0.4335\n",
      "Iter-5180, train loss-1.9368, acc-0.4000, valid loss-1.9186, acc-0.4262, test loss-1.9184, acc-0.4337\n",
      "Iter-5190, train loss-1.9641, acc-0.3800, valid loss-1.9173, acc-0.4268, test loss-1.9171, acc-0.4337\n",
      "Iter-5200, train loss-1.8497, acc-0.4600, valid loss-1.9161, acc-0.4274, test loss-1.9158, acc-0.4335\n",
      "Iter-5210, train loss-1.8310, acc-0.5400, valid loss-1.9147, acc-0.4278, test loss-1.9145, acc-0.4341\n",
      "Iter-5220, train loss-1.8897, acc-0.4600, valid loss-1.9135, acc-0.4278, test loss-1.9133, acc-0.4346\n",
      "Iter-5230, train loss-1.9007, acc-0.4600, valid loss-1.9123, acc-0.4280, test loss-1.9121, acc-0.4350\n",
      "Iter-5240, train loss-1.9209, acc-0.4000, valid loss-1.9110, acc-0.4292, test loss-1.9108, acc-0.4354\n",
      "Iter-5250, train loss-1.9140, acc-0.4000, valid loss-1.9098, acc-0.4296, test loss-1.9095, acc-0.4354\n",
      "Iter-5260, train loss-1.9383, acc-0.3800, valid loss-1.9085, acc-0.4298, test loss-1.9082, acc-0.4355\n",
      "Iter-5270, train loss-1.9476, acc-0.5000, valid loss-1.9072, acc-0.4304, test loss-1.9070, acc-0.4355\n",
      "Iter-5280, train loss-1.8618, acc-0.4600, valid loss-1.9059, acc-0.4310, test loss-1.9057, acc-0.4359\n",
      "Iter-5290, train loss-1.9481, acc-0.4200, valid loss-1.9046, acc-0.4318, test loss-1.9044, acc-0.4365\n",
      "Iter-5300, train loss-1.9209, acc-0.3600, valid loss-1.9033, acc-0.4322, test loss-1.9031, acc-0.4370\n",
      "Iter-5310, train loss-1.8594, acc-0.5000, valid loss-1.9021, acc-0.4322, test loss-1.9018, acc-0.4371\n",
      "Iter-5320, train loss-1.9212, acc-0.4200, valid loss-1.9008, acc-0.4328, test loss-1.9006, acc-0.4380\n",
      "Iter-5330, train loss-1.8700, acc-0.5400, valid loss-1.8996, acc-0.4330, test loss-1.8993, acc-0.4380\n",
      "Iter-5340, train loss-1.8279, acc-0.5400, valid loss-1.8982, acc-0.4332, test loss-1.8980, acc-0.4387\n",
      "Iter-5350, train loss-1.8914, acc-0.4200, valid loss-1.8970, acc-0.4330, test loss-1.8967, acc-0.4388\n",
      "Iter-5360, train loss-1.9064, acc-0.3800, valid loss-1.8958, acc-0.4330, test loss-1.8955, acc-0.4389\n",
      "Iter-5370, train loss-1.8735, acc-0.4400, valid loss-1.8945, acc-0.4330, test loss-1.8942, acc-0.4387\n",
      "Iter-5380, train loss-1.8520, acc-0.4600, valid loss-1.8933, acc-0.4332, test loss-1.8930, acc-0.4389\n",
      "Iter-5390, train loss-1.9508, acc-0.3800, valid loss-1.8920, acc-0.4332, test loss-1.8917, acc-0.4392\n",
      "Iter-5400, train loss-1.8657, acc-0.4800, valid loss-1.8908, acc-0.4336, test loss-1.8904, acc-0.4393\n",
      "Iter-5410, train loss-1.9140, acc-0.3200, valid loss-1.8895, acc-0.4344, test loss-1.8892, acc-0.4397\n",
      "Iter-5420, train loss-1.9379, acc-0.4200, valid loss-1.8885, acc-0.4348, test loss-1.8881, acc-0.4396\n",
      "Iter-5430, train loss-1.9028, acc-0.4200, valid loss-1.8872, acc-0.4356, test loss-1.8868, acc-0.4399\n",
      "Iter-5440, train loss-1.9186, acc-0.4000, valid loss-1.8858, acc-0.4366, test loss-1.8855, acc-0.4399\n",
      "Iter-5450, train loss-1.8519, acc-0.4800, valid loss-1.8846, acc-0.4374, test loss-1.8843, acc-0.4405\n",
      "Iter-5460, train loss-1.8872, acc-0.3600, valid loss-1.8832, acc-0.4380, test loss-1.8829, acc-0.4410\n",
      "Iter-5470, train loss-1.8782, acc-0.5200, valid loss-1.8818, acc-0.4380, test loss-1.8815, acc-0.4416\n",
      "Iter-5480, train loss-1.8313, acc-0.4600, valid loss-1.8806, acc-0.4386, test loss-1.8802, acc-0.4421\n",
      "Iter-5490, train loss-1.7832, acc-0.5600, valid loss-1.8794, acc-0.4388, test loss-1.8790, acc-0.4421\n",
      "Iter-5500, train loss-1.8794, acc-0.4200, valid loss-1.8782, acc-0.4396, test loss-1.8778, acc-0.4424\n",
      "Iter-5510, train loss-1.9097, acc-0.5000, valid loss-1.8768, acc-0.4400, test loss-1.8765, acc-0.4427\n",
      "Iter-5520, train loss-1.8881, acc-0.3800, valid loss-1.8757, acc-0.4402, test loss-1.8753, acc-0.4431\n",
      "Iter-5530, train loss-1.8920, acc-0.4600, valid loss-1.8744, acc-0.4406, test loss-1.8740, acc-0.4432\n",
      "Iter-5540, train loss-1.8678, acc-0.4200, valid loss-1.8731, acc-0.4408, test loss-1.8727, acc-0.4434\n",
      "Iter-5550, train loss-1.9125, acc-0.4600, valid loss-1.8718, acc-0.4412, test loss-1.8714, acc-0.4439\n",
      "Iter-5560, train loss-1.7998, acc-0.5600, valid loss-1.8706, acc-0.4406, test loss-1.8702, acc-0.4440\n",
      "Iter-5570, train loss-1.8952, acc-0.4000, valid loss-1.8693, acc-0.4416, test loss-1.8689, acc-0.4443\n",
      "Iter-5580, train loss-1.8908, acc-0.3400, valid loss-1.8681, acc-0.4418, test loss-1.8677, acc-0.4446\n",
      "Iter-5590, train loss-1.8135, acc-0.4400, valid loss-1.8668, acc-0.4426, test loss-1.8664, acc-0.4450\n",
      "Iter-5600, train loss-1.7701, acc-0.5400, valid loss-1.8656, acc-0.4434, test loss-1.8651, acc-0.4454\n",
      "Iter-5610, train loss-1.8567, acc-0.4600, valid loss-1.8643, acc-0.4430, test loss-1.8638, acc-0.4458\n",
      "Iter-5620, train loss-1.7770, acc-0.5400, valid loss-1.8630, acc-0.4440, test loss-1.8626, acc-0.4462\n",
      "Iter-5630, train loss-1.7991, acc-0.4600, valid loss-1.8618, acc-0.4448, test loss-1.8613, acc-0.4464\n",
      "Iter-5640, train loss-1.7865, acc-0.4800, valid loss-1.8605, acc-0.4450, test loss-1.8600, acc-0.4468\n",
      "Iter-5650, train loss-1.8953, acc-0.3800, valid loss-1.8592, acc-0.4454, test loss-1.8587, acc-0.4474\n",
      "Iter-5660, train loss-1.8275, acc-0.4800, valid loss-1.8579, acc-0.4454, test loss-1.8574, acc-0.4476\n",
      "Iter-5670, train loss-1.8668, acc-0.4800, valid loss-1.8566, acc-0.4464, test loss-1.8561, acc-0.4479\n",
      "Iter-5680, train loss-1.7938, acc-0.5600, valid loss-1.8554, acc-0.4466, test loss-1.8549, acc-0.4482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-1.8626, acc-0.5000, valid loss-1.8542, acc-0.4468, test loss-1.8537, acc-0.4486\n",
      "Iter-5700, train loss-1.8950, acc-0.3800, valid loss-1.8530, acc-0.4468, test loss-1.8525, acc-0.4490\n",
      "Iter-5710, train loss-1.8647, acc-0.4200, valid loss-1.8517, acc-0.4466, test loss-1.8512, acc-0.4494\n",
      "Iter-5720, train loss-1.7402, acc-0.5200, valid loss-1.8505, acc-0.4464, test loss-1.8499, acc-0.4495\n",
      "Iter-5730, train loss-1.8920, acc-0.5000, valid loss-1.8491, acc-0.4466, test loss-1.8486, acc-0.4501\n",
      "Iter-5740, train loss-1.9451, acc-0.3000, valid loss-1.8480, acc-0.4468, test loss-1.8474, acc-0.4503\n",
      "Iter-5750, train loss-1.8445, acc-0.4800, valid loss-1.8468, acc-0.4466, test loss-1.8462, acc-0.4505\n",
      "Iter-5760, train loss-1.9211, acc-0.3400, valid loss-1.8456, acc-0.4470, test loss-1.8450, acc-0.4509\n",
      "Iter-5770, train loss-1.8676, acc-0.5200, valid loss-1.8443, acc-0.4470, test loss-1.8438, acc-0.4513\n",
      "Iter-5780, train loss-1.6691, acc-0.6200, valid loss-1.8430, acc-0.4474, test loss-1.8424, acc-0.4515\n",
      "Iter-5790, train loss-1.8095, acc-0.4800, valid loss-1.8417, acc-0.4480, test loss-1.8411, acc-0.4516\n",
      "Iter-5800, train loss-1.7988, acc-0.4200, valid loss-1.8404, acc-0.4486, test loss-1.8398, acc-0.4523\n",
      "Iter-5810, train loss-1.7962, acc-0.4600, valid loss-1.8391, acc-0.4488, test loss-1.8384, acc-0.4525\n",
      "Iter-5820, train loss-1.8772, acc-0.4200, valid loss-1.8377, acc-0.4492, test loss-1.8371, acc-0.4531\n",
      "Iter-5830, train loss-1.7606, acc-0.5600, valid loss-1.8364, acc-0.4494, test loss-1.8358, acc-0.4531\n",
      "Iter-5840, train loss-1.8301, acc-0.5400, valid loss-1.8351, acc-0.4498, test loss-1.8344, acc-0.4536\n",
      "Iter-5850, train loss-1.8376, acc-0.4800, valid loss-1.8339, acc-0.4498, test loss-1.8332, acc-0.4536\n",
      "Iter-5860, train loss-1.7214, acc-0.5600, valid loss-1.8325, acc-0.4504, test loss-1.8319, acc-0.4538\n",
      "Iter-5870, train loss-1.8523, acc-0.5000, valid loss-1.8312, acc-0.4510, test loss-1.8306, acc-0.4539\n",
      "Iter-5880, train loss-1.8432, acc-0.4600, valid loss-1.8299, acc-0.4516, test loss-1.8292, acc-0.4540\n",
      "Iter-5890, train loss-1.8041, acc-0.4800, valid loss-1.8286, acc-0.4518, test loss-1.8279, acc-0.4543\n",
      "Iter-5900, train loss-1.8861, acc-0.4000, valid loss-1.8272, acc-0.4518, test loss-1.8266, acc-0.4550\n",
      "Iter-5910, train loss-1.7648, acc-0.5600, valid loss-1.8259, acc-0.4520, test loss-1.8252, acc-0.4554\n",
      "Iter-5920, train loss-1.8172, acc-0.4600, valid loss-1.8246, acc-0.4522, test loss-1.8239, acc-0.4556\n",
      "Iter-5930, train loss-1.8612, acc-0.5000, valid loss-1.8232, acc-0.4520, test loss-1.8226, acc-0.4562\n",
      "Iter-5940, train loss-1.7090, acc-0.4800, valid loss-1.8219, acc-0.4520, test loss-1.8212, acc-0.4563\n",
      "Iter-5950, train loss-1.8866, acc-0.4400, valid loss-1.8206, acc-0.4526, test loss-1.8200, acc-0.4567\n",
      "Iter-5960, train loss-1.7771, acc-0.4600, valid loss-1.8194, acc-0.4530, test loss-1.8187, acc-0.4573\n",
      "Iter-5970, train loss-1.7715, acc-0.5400, valid loss-1.8181, acc-0.4542, test loss-1.8174, acc-0.4574\n",
      "Iter-5980, train loss-1.8063, acc-0.4200, valid loss-1.8168, acc-0.4542, test loss-1.8161, acc-0.4577\n",
      "Iter-5990, train loss-1.7812, acc-0.5000, valid loss-1.8155, acc-0.4548, test loss-1.8148, acc-0.4581\n",
      "Iter-6000, train loss-1.8039, acc-0.5000, valid loss-1.8143, acc-0.4548, test loss-1.8135, acc-0.4587\n",
      "Iter-6010, train loss-1.7796, acc-0.4600, valid loss-1.8130, acc-0.4556, test loss-1.8123, acc-0.4588\n",
      "Iter-6020, train loss-1.7599, acc-0.5200, valid loss-1.8118, acc-0.4560, test loss-1.8110, acc-0.4593\n",
      "Iter-6030, train loss-1.7319, acc-0.5600, valid loss-1.8105, acc-0.4564, test loss-1.8097, acc-0.4591\n",
      "Iter-6040, train loss-1.7746, acc-0.5400, valid loss-1.8092, acc-0.4568, test loss-1.8084, acc-0.4588\n",
      "Iter-6050, train loss-1.8878, acc-0.4400, valid loss-1.8079, acc-0.4572, test loss-1.8071, acc-0.4593\n",
      "Iter-6060, train loss-1.8040, acc-0.5200, valid loss-1.8067, acc-0.4574, test loss-1.8059, acc-0.4598\n",
      "Iter-6070, train loss-1.8534, acc-0.4000, valid loss-1.8054, acc-0.4576, test loss-1.8045, acc-0.4606\n",
      "Iter-6080, train loss-1.6964, acc-0.4800, valid loss-1.8041, acc-0.4578, test loss-1.8032, acc-0.4610\n",
      "Iter-6090, train loss-1.7560, acc-0.5800, valid loss-1.8029, acc-0.4582, test loss-1.8020, acc-0.4608\n",
      "Iter-6100, train loss-1.8031, acc-0.4200, valid loss-1.8016, acc-0.4584, test loss-1.8007, acc-0.4616\n",
      "Iter-6110, train loss-1.7642, acc-0.5200, valid loss-1.8003, acc-0.4588, test loss-1.7995, acc-0.4617\n",
      "Iter-6120, train loss-1.7563, acc-0.5000, valid loss-1.7990, acc-0.4590, test loss-1.7982, acc-0.4621\n",
      "Iter-6130, train loss-1.7524, acc-0.4800, valid loss-1.7977, acc-0.4596, test loss-1.7969, acc-0.4627\n",
      "Iter-6140, train loss-1.7900, acc-0.5200, valid loss-1.7965, acc-0.4600, test loss-1.7956, acc-0.4630\n",
      "Iter-6150, train loss-1.7498, acc-0.4400, valid loss-1.7952, acc-0.4604, test loss-1.7944, acc-0.4632\n",
      "Iter-6160, train loss-1.7665, acc-0.4800, valid loss-1.7941, acc-0.4604, test loss-1.7932, acc-0.4635\n",
      "Iter-6170, train loss-1.7650, acc-0.4600, valid loss-1.7928, acc-0.4604, test loss-1.7920, acc-0.4645\n",
      "Iter-6180, train loss-1.7738, acc-0.5200, valid loss-1.7914, acc-0.4606, test loss-1.7905, acc-0.4645\n",
      "Iter-6190, train loss-1.7041, acc-0.5000, valid loss-1.7902, acc-0.4606, test loss-1.7893, acc-0.4649\n",
      "Iter-6200, train loss-1.8940, acc-0.3800, valid loss-1.7890, acc-0.4612, test loss-1.7881, acc-0.4652\n",
      "Iter-6210, train loss-1.7209, acc-0.5800, valid loss-1.7877, acc-0.4614, test loss-1.7868, acc-0.4656\n",
      "Iter-6220, train loss-1.8385, acc-0.5000, valid loss-1.7864, acc-0.4616, test loss-1.7856, acc-0.4661\n",
      "Iter-6230, train loss-1.7514, acc-0.5400, valid loss-1.7852, acc-0.4618, test loss-1.7843, acc-0.4663\n",
      "Iter-6240, train loss-1.7656, acc-0.4400, valid loss-1.7839, acc-0.4622, test loss-1.7829, acc-0.4667\n",
      "Iter-6250, train loss-1.8025, acc-0.3800, valid loss-1.7826, acc-0.4622, test loss-1.7817, acc-0.4671\n",
      "Iter-6260, train loss-1.7590, acc-0.4400, valid loss-1.7814, acc-0.4624, test loss-1.7805, acc-0.4671\n",
      "Iter-6270, train loss-1.7987, acc-0.4200, valid loss-1.7801, acc-0.4626, test loss-1.7792, acc-0.4670\n",
      "Iter-6280, train loss-1.7545, acc-0.4400, valid loss-1.7789, acc-0.4630, test loss-1.7780, acc-0.4673\n",
      "Iter-6290, train loss-1.7794, acc-0.4600, valid loss-1.7776, acc-0.4634, test loss-1.7767, acc-0.4676\n",
      "Iter-6300, train loss-1.7775, acc-0.5000, valid loss-1.7763, acc-0.4634, test loss-1.7754, acc-0.4682\n",
      "Iter-6310, train loss-1.7487, acc-0.5800, valid loss-1.7751, acc-0.4638, test loss-1.7742, acc-0.4682\n",
      "Iter-6320, train loss-1.7139, acc-0.4400, valid loss-1.7738, acc-0.4642, test loss-1.7729, acc-0.4689\n",
      "Iter-6330, train loss-1.7542, acc-0.5000, valid loss-1.7726, acc-0.4648, test loss-1.7716, acc-0.4689\n",
      "Iter-6340, train loss-1.7870, acc-0.4600, valid loss-1.7713, acc-0.4648, test loss-1.7704, acc-0.4694\n",
      "Iter-6350, train loss-1.8309, acc-0.4800, valid loss-1.7700, acc-0.4650, test loss-1.7690, acc-0.4695\n",
      "Iter-6360, train loss-1.7249, acc-0.4800, valid loss-1.7687, acc-0.4658, test loss-1.7678, acc-0.4700\n",
      "Iter-6370, train loss-1.8172, acc-0.4200, valid loss-1.7675, acc-0.4664, test loss-1.7665, acc-0.4705\n",
      "Iter-6380, train loss-1.7330, acc-0.5000, valid loss-1.7661, acc-0.4670, test loss-1.7652, acc-0.4709\n",
      "Iter-6390, train loss-1.7651, acc-0.4400, valid loss-1.7649, acc-0.4674, test loss-1.7639, acc-0.4707\n",
      "Iter-6400, train loss-1.8006, acc-0.4600, valid loss-1.7635, acc-0.4678, test loss-1.7625, acc-0.4713\n",
      "Iter-6410, train loss-1.8186, acc-0.4000, valid loss-1.7622, acc-0.4686, test loss-1.7611, acc-0.4715\n",
      "Iter-6420, train loss-1.7410, acc-0.5200, valid loss-1.7609, acc-0.4684, test loss-1.7599, acc-0.4718\n",
      "Iter-6430, train loss-1.7380, acc-0.4800, valid loss-1.7597, acc-0.4692, test loss-1.7587, acc-0.4725\n",
      "Iter-6440, train loss-1.7761, acc-0.4600, valid loss-1.7584, acc-0.4702, test loss-1.7573, acc-0.4728\n",
      "Iter-6450, train loss-1.7822, acc-0.3800, valid loss-1.7570, acc-0.4710, test loss-1.7559, acc-0.4732\n",
      "Iter-6460, train loss-1.6207, acc-0.5800, valid loss-1.7558, acc-0.4712, test loss-1.7547, acc-0.4737\n",
      "Iter-6470, train loss-1.8584, acc-0.4200, valid loss-1.7546, acc-0.4710, test loss-1.7534, acc-0.4737\n",
      "Iter-6480, train loss-1.7738, acc-0.4400, valid loss-1.7532, acc-0.4718, test loss-1.7521, acc-0.4746\n",
      "Iter-6490, train loss-1.6919, acc-0.4800, valid loss-1.7520, acc-0.4724, test loss-1.7508, acc-0.4753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-1.6973, acc-0.5600, valid loss-1.7507, acc-0.4732, test loss-1.7496, acc-0.4761\n",
      "Iter-6510, train loss-1.8736, acc-0.4000, valid loss-1.7495, acc-0.4742, test loss-1.7484, acc-0.4762\n",
      "Iter-6520, train loss-1.8376, acc-0.3400, valid loss-1.7482, acc-0.4742, test loss-1.7471, acc-0.4764\n",
      "Iter-6530, train loss-1.7338, acc-0.5400, valid loss-1.7470, acc-0.4746, test loss-1.7458, acc-0.4768\n",
      "Iter-6540, train loss-1.5938, acc-0.6200, valid loss-1.7457, acc-0.4750, test loss-1.7445, acc-0.4771\n",
      "Iter-6550, train loss-1.7404, acc-0.4800, valid loss-1.7444, acc-0.4750, test loss-1.7433, acc-0.4772\n",
      "Iter-6560, train loss-1.7732, acc-0.5000, valid loss-1.7432, acc-0.4752, test loss-1.7420, acc-0.4775\n",
      "Iter-6570, train loss-1.6677, acc-0.5200, valid loss-1.7419, acc-0.4752, test loss-1.7408, acc-0.4775\n",
      "Iter-6580, train loss-1.7445, acc-0.5400, valid loss-1.7406, acc-0.4758, test loss-1.7395, acc-0.4778\n",
      "Iter-6590, train loss-1.6115, acc-0.5800, valid loss-1.7393, acc-0.4762, test loss-1.7382, acc-0.4781\n",
      "Iter-6600, train loss-1.6395, acc-0.6000, valid loss-1.7381, acc-0.4764, test loss-1.7369, acc-0.4785\n",
      "Iter-6610, train loss-1.9306, acc-0.3800, valid loss-1.7368, acc-0.4768, test loss-1.7356, acc-0.4788\n",
      "Iter-6620, train loss-1.6688, acc-0.5200, valid loss-1.7354, acc-0.4768, test loss-1.7342, acc-0.4792\n",
      "Iter-6630, train loss-1.6534, acc-0.4400, valid loss-1.7342, acc-0.4772, test loss-1.7330, acc-0.4794\n",
      "Iter-6640, train loss-1.7978, acc-0.4200, valid loss-1.7329, acc-0.4778, test loss-1.7317, acc-0.4798\n",
      "Iter-6650, train loss-1.6740, acc-0.5600, valid loss-1.7317, acc-0.4776, test loss-1.7305, acc-0.4803\n",
      "Iter-6660, train loss-1.7028, acc-0.4800, valid loss-1.7304, acc-0.4778, test loss-1.7292, acc-0.4807\n",
      "Iter-6670, train loss-1.7027, acc-0.5400, valid loss-1.7291, acc-0.4784, test loss-1.7278, acc-0.4812\n",
      "Iter-6680, train loss-1.7380, acc-0.5000, valid loss-1.7278, acc-0.4788, test loss-1.7265, acc-0.4814\n",
      "Iter-6690, train loss-1.6198, acc-0.5600, valid loss-1.7265, acc-0.4788, test loss-1.7252, acc-0.4814\n",
      "Iter-6700, train loss-1.6869, acc-0.5200, valid loss-1.7252, acc-0.4796, test loss-1.7239, acc-0.4814\n",
      "Iter-6710, train loss-1.8944, acc-0.3200, valid loss-1.7239, acc-0.4802, test loss-1.7226, acc-0.4819\n",
      "Iter-6720, train loss-1.7695, acc-0.3400, valid loss-1.7226, acc-0.4806, test loss-1.7213, acc-0.4823\n",
      "Iter-6730, train loss-1.7101, acc-0.4400, valid loss-1.7214, acc-0.4806, test loss-1.7200, acc-0.4828\n",
      "Iter-6740, train loss-1.7204, acc-0.5400, valid loss-1.7201, acc-0.4812, test loss-1.7187, acc-0.4833\n",
      "Iter-6750, train loss-1.7736, acc-0.4200, valid loss-1.7188, acc-0.4818, test loss-1.7175, acc-0.4839\n",
      "Iter-6760, train loss-1.7889, acc-0.5000, valid loss-1.7176, acc-0.4820, test loss-1.7162, acc-0.4845\n",
      "Iter-6770, train loss-1.7548, acc-0.4600, valid loss-1.7163, acc-0.4818, test loss-1.7149, acc-0.4851\n",
      "Iter-6780, train loss-1.6547, acc-0.4800, valid loss-1.7150, acc-0.4814, test loss-1.7136, acc-0.4851\n",
      "Iter-6790, train loss-1.7779, acc-0.3800, valid loss-1.7137, acc-0.4822, test loss-1.7123, acc-0.4852\n",
      "Iter-6800, train loss-1.7355, acc-0.4600, valid loss-1.7124, acc-0.4824, test loss-1.7110, acc-0.4852\n",
      "Iter-6810, train loss-1.7359, acc-0.4200, valid loss-1.7111, acc-0.4826, test loss-1.7097, acc-0.4855\n",
      "Iter-6820, train loss-1.8112, acc-0.3800, valid loss-1.7099, acc-0.4828, test loss-1.7085, acc-0.4860\n",
      "Iter-6830, train loss-1.7086, acc-0.4800, valid loss-1.7086, acc-0.4834, test loss-1.7072, acc-0.4863\n",
      "Iter-6840, train loss-1.5199, acc-0.6200, valid loss-1.7072, acc-0.4836, test loss-1.7058, acc-0.4866\n",
      "Iter-6850, train loss-1.6646, acc-0.5600, valid loss-1.7060, acc-0.4838, test loss-1.7045, acc-0.4869\n",
      "Iter-6860, train loss-1.6980, acc-0.5200, valid loss-1.7048, acc-0.4840, test loss-1.7033, acc-0.4877\n",
      "Iter-6870, train loss-1.6348, acc-0.5800, valid loss-1.7035, acc-0.4844, test loss-1.7020, acc-0.4879\n",
      "Iter-6880, train loss-1.7037, acc-0.6000, valid loss-1.7023, acc-0.4844, test loss-1.7009, acc-0.4880\n",
      "Iter-6890, train loss-1.8580, acc-0.4400, valid loss-1.7011, acc-0.4844, test loss-1.6997, acc-0.4883\n",
      "Iter-6900, train loss-1.6243, acc-0.5600, valid loss-1.6999, acc-0.4852, test loss-1.6985, acc-0.4887\n",
      "Iter-6910, train loss-1.5485, acc-0.7400, valid loss-1.6986, acc-0.4858, test loss-1.6971, acc-0.4892\n",
      "Iter-6920, train loss-1.6856, acc-0.5200, valid loss-1.6974, acc-0.4862, test loss-1.6959, acc-0.4895\n",
      "Iter-6930, train loss-1.6583, acc-0.5400, valid loss-1.6960, acc-0.4874, test loss-1.6945, acc-0.4903\n",
      "Iter-6940, train loss-1.7033, acc-0.5400, valid loss-1.6948, acc-0.4874, test loss-1.6933, acc-0.4911\n",
      "Iter-6950, train loss-1.7081, acc-0.5000, valid loss-1.6935, acc-0.4878, test loss-1.6919, acc-0.4916\n",
      "Iter-6960, train loss-1.6723, acc-0.5600, valid loss-1.6923, acc-0.4878, test loss-1.6908, acc-0.4918\n",
      "Iter-6970, train loss-1.5366, acc-0.6000, valid loss-1.6909, acc-0.4882, test loss-1.6894, acc-0.4921\n",
      "Iter-6980, train loss-1.7743, acc-0.4200, valid loss-1.6897, acc-0.4886, test loss-1.6881, acc-0.4924\n",
      "Iter-6990, train loss-1.8004, acc-0.4200, valid loss-1.6884, acc-0.4884, test loss-1.6869, acc-0.4924\n",
      "Iter-7000, train loss-1.6735, acc-0.5400, valid loss-1.6872, acc-0.4886, test loss-1.6856, acc-0.4931\n",
      "Iter-7010, train loss-1.7152, acc-0.4400, valid loss-1.6858, acc-0.4900, test loss-1.6843, acc-0.4939\n",
      "Iter-7020, train loss-1.6709, acc-0.4000, valid loss-1.6846, acc-0.4902, test loss-1.6830, acc-0.4944\n",
      "Iter-7030, train loss-1.6377, acc-0.5600, valid loss-1.6834, acc-0.4904, test loss-1.6819, acc-0.4948\n",
      "Iter-7040, train loss-1.7512, acc-0.3800, valid loss-1.6821, acc-0.4906, test loss-1.6806, acc-0.4953\n",
      "Iter-7050, train loss-1.5046, acc-0.5600, valid loss-1.6808, acc-0.4908, test loss-1.6792, acc-0.4961\n",
      "Iter-7060, train loss-1.6223, acc-0.5000, valid loss-1.6796, acc-0.4916, test loss-1.6780, acc-0.4963\n",
      "Iter-7070, train loss-1.6228, acc-0.5600, valid loss-1.6783, acc-0.4922, test loss-1.6767, acc-0.4967\n",
      "Iter-7080, train loss-1.6465, acc-0.5000, valid loss-1.6770, acc-0.4926, test loss-1.6753, acc-0.4971\n",
      "Iter-7090, train loss-1.6228, acc-0.5400, valid loss-1.6758, acc-0.4928, test loss-1.6742, acc-0.4978\n",
      "Iter-7100, train loss-1.7463, acc-0.5400, valid loss-1.6745, acc-0.4936, test loss-1.6728, acc-0.4980\n",
      "Iter-7110, train loss-1.8127, acc-0.4200, valid loss-1.6731, acc-0.4938, test loss-1.6715, acc-0.4979\n",
      "Iter-7120, train loss-1.7537, acc-0.5000, valid loss-1.6719, acc-0.4942, test loss-1.6703, acc-0.4983\n",
      "Iter-7130, train loss-1.6594, acc-0.4800, valid loss-1.6707, acc-0.4954, test loss-1.6691, acc-0.4993\n",
      "Iter-7140, train loss-1.6436, acc-0.5600, valid loss-1.6694, acc-0.4956, test loss-1.6678, acc-0.4999\n",
      "Iter-7150, train loss-1.7457, acc-0.5200, valid loss-1.6682, acc-0.4962, test loss-1.6665, acc-0.5004\n",
      "Iter-7160, train loss-1.6287, acc-0.5600, valid loss-1.6670, acc-0.4970, test loss-1.6653, acc-0.5010\n",
      "Iter-7170, train loss-1.6486, acc-0.5000, valid loss-1.6657, acc-0.4976, test loss-1.6639, acc-0.5017\n",
      "Iter-7180, train loss-1.6598, acc-0.5800, valid loss-1.6644, acc-0.4978, test loss-1.6627, acc-0.5020\n",
      "Iter-7190, train loss-1.5514, acc-0.5600, valid loss-1.6632, acc-0.4982, test loss-1.6614, acc-0.5026\n",
      "Iter-7200, train loss-1.6903, acc-0.4200, valid loss-1.6619, acc-0.4986, test loss-1.6601, acc-0.5032\n",
      "Iter-7210, train loss-1.7602, acc-0.4800, valid loss-1.6607, acc-0.4988, test loss-1.6590, acc-0.5037\n",
      "Iter-7220, train loss-1.7399, acc-0.4200, valid loss-1.6595, acc-0.4994, test loss-1.6577, acc-0.5041\n",
      "Iter-7230, train loss-1.7714, acc-0.5000, valid loss-1.6583, acc-0.4998, test loss-1.6565, acc-0.5045\n",
      "Iter-7240, train loss-1.5606, acc-0.5200, valid loss-1.6570, acc-0.5006, test loss-1.6553, acc-0.5053\n",
      "Iter-7250, train loss-1.7169, acc-0.5200, valid loss-1.6558, acc-0.5010, test loss-1.6540, acc-0.5052\n",
      "Iter-7260, train loss-1.7571, acc-0.3800, valid loss-1.6547, acc-0.5012, test loss-1.6529, acc-0.5056\n",
      "Iter-7270, train loss-1.6720, acc-0.5200, valid loss-1.6535, acc-0.5010, test loss-1.6517, acc-0.5057\n",
      "Iter-7280, train loss-1.6112, acc-0.5200, valid loss-1.6522, acc-0.5020, test loss-1.6504, acc-0.5061\n",
      "Iter-7290, train loss-1.7408, acc-0.4200, valid loss-1.6510, acc-0.5022, test loss-1.6491, acc-0.5067\n",
      "Iter-7300, train loss-1.7457, acc-0.4800, valid loss-1.6497, acc-0.5026, test loss-1.6478, acc-0.5073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-1.6243, acc-0.4600, valid loss-1.6484, acc-0.5032, test loss-1.6466, acc-0.5076\n",
      "Iter-7320, train loss-1.6647, acc-0.5600, valid loss-1.6471, acc-0.5034, test loss-1.6453, acc-0.5076\n",
      "Iter-7330, train loss-1.6617, acc-0.5000, valid loss-1.6458, acc-0.5034, test loss-1.6440, acc-0.5087\n",
      "Iter-7340, train loss-1.6907, acc-0.5000, valid loss-1.6446, acc-0.5038, test loss-1.6428, acc-0.5089\n",
      "Iter-7350, train loss-1.7082, acc-0.4800, valid loss-1.6435, acc-0.5038, test loss-1.6416, acc-0.5091\n",
      "Iter-7360, train loss-1.6345, acc-0.5400, valid loss-1.6422, acc-0.5046, test loss-1.6403, acc-0.5095\n",
      "Iter-7370, train loss-1.6199, acc-0.5400, valid loss-1.6410, acc-0.5054, test loss-1.6390, acc-0.5106\n",
      "Iter-7380, train loss-1.6240, acc-0.5200, valid loss-1.6397, acc-0.5058, test loss-1.6378, acc-0.5109\n",
      "Iter-7390, train loss-1.4962, acc-0.6200, valid loss-1.6385, acc-0.5058, test loss-1.6366, acc-0.5113\n",
      "Iter-7400, train loss-1.5206, acc-0.6000, valid loss-1.6373, acc-0.5058, test loss-1.6354, acc-0.5114\n",
      "Iter-7410, train loss-1.6422, acc-0.6200, valid loss-1.6360, acc-0.5060, test loss-1.6341, acc-0.5116\n",
      "Iter-7420, train loss-1.6751, acc-0.4600, valid loss-1.6349, acc-0.5066, test loss-1.6330, acc-0.5122\n",
      "Iter-7430, train loss-1.5695, acc-0.5800, valid loss-1.6336, acc-0.5076, test loss-1.6317, acc-0.5127\n",
      "Iter-7440, train loss-1.6477, acc-0.4800, valid loss-1.6324, acc-0.5078, test loss-1.6305, acc-0.5130\n",
      "Iter-7450, train loss-1.5719, acc-0.5200, valid loss-1.6312, acc-0.5086, test loss-1.6292, acc-0.5137\n",
      "Iter-7460, train loss-1.7006, acc-0.4400, valid loss-1.6299, acc-0.5090, test loss-1.6279, acc-0.5142\n",
      "Iter-7470, train loss-1.5521, acc-0.5800, valid loss-1.6285, acc-0.5096, test loss-1.6266, acc-0.5146\n",
      "Iter-7480, train loss-1.7391, acc-0.3600, valid loss-1.6273, acc-0.5100, test loss-1.6253, acc-0.5151\n",
      "Iter-7490, train loss-1.6415, acc-0.4400, valid loss-1.6260, acc-0.5100, test loss-1.6241, acc-0.5157\n",
      "Iter-7500, train loss-1.7368, acc-0.4600, valid loss-1.6248, acc-0.5110, test loss-1.6228, acc-0.5168\n",
      "Iter-7510, train loss-1.5956, acc-0.5600, valid loss-1.6236, acc-0.5112, test loss-1.6216, acc-0.5170\n",
      "Iter-7520, train loss-1.6083, acc-0.4400, valid loss-1.6222, acc-0.5118, test loss-1.6202, acc-0.5173\n",
      "Iter-7530, train loss-1.6096, acc-0.5400, valid loss-1.6210, acc-0.5118, test loss-1.6190, acc-0.5177\n",
      "Iter-7540, train loss-1.7360, acc-0.4600, valid loss-1.6198, acc-0.5124, test loss-1.6178, acc-0.5187\n",
      "Iter-7550, train loss-1.6879, acc-0.4400, valid loss-1.6186, acc-0.5130, test loss-1.6166, acc-0.5193\n",
      "Iter-7560, train loss-1.6182, acc-0.4800, valid loss-1.6173, acc-0.5136, test loss-1.6153, acc-0.5202\n",
      "Iter-7570, train loss-1.5958, acc-0.5600, valid loss-1.6160, acc-0.5142, test loss-1.6139, acc-0.5205\n",
      "Iter-7580, train loss-1.5494, acc-0.5600, valid loss-1.6147, acc-0.5154, test loss-1.6126, acc-0.5211\n",
      "Iter-7590, train loss-1.5827, acc-0.5600, valid loss-1.6135, acc-0.5154, test loss-1.6115, acc-0.5215\n",
      "Iter-7600, train loss-1.6919, acc-0.4800, valid loss-1.6122, acc-0.5170, test loss-1.6102, acc-0.5223\n",
      "Iter-7610, train loss-1.6024, acc-0.4800, valid loss-1.6110, acc-0.5172, test loss-1.6089, acc-0.5230\n",
      "Iter-7620, train loss-1.6008, acc-0.4600, valid loss-1.6096, acc-0.5180, test loss-1.6075, acc-0.5233\n",
      "Iter-7630, train loss-1.5912, acc-0.5200, valid loss-1.6084, acc-0.5192, test loss-1.6063, acc-0.5242\n",
      "Iter-7640, train loss-1.6842, acc-0.5800, valid loss-1.6071, acc-0.5204, test loss-1.6050, acc-0.5247\n",
      "Iter-7650, train loss-1.6100, acc-0.4600, valid loss-1.6059, acc-0.5206, test loss-1.6038, acc-0.5250\n",
      "Iter-7660, train loss-1.5240, acc-0.5600, valid loss-1.6047, acc-0.5204, test loss-1.6026, acc-0.5254\n",
      "Iter-7670, train loss-1.6500, acc-0.4800, valid loss-1.6034, acc-0.5204, test loss-1.6013, acc-0.5259\n",
      "Iter-7680, train loss-1.5344, acc-0.5400, valid loss-1.6022, acc-0.5210, test loss-1.6000, acc-0.5264\n",
      "Iter-7690, train loss-1.6218, acc-0.5200, valid loss-1.6010, acc-0.5208, test loss-1.5988, acc-0.5263\n",
      "Iter-7700, train loss-1.4917, acc-0.5400, valid loss-1.5999, acc-0.5216, test loss-1.5977, acc-0.5264\n",
      "Iter-7710, train loss-1.6691, acc-0.4400, valid loss-1.5987, acc-0.5222, test loss-1.5965, acc-0.5267\n",
      "Iter-7720, train loss-1.7012, acc-0.5400, valid loss-1.5974, acc-0.5230, test loss-1.5952, acc-0.5272\n",
      "Iter-7730, train loss-1.5553, acc-0.5600, valid loss-1.5961, acc-0.5236, test loss-1.5939, acc-0.5278\n",
      "Iter-7740, train loss-1.5661, acc-0.5200, valid loss-1.5949, acc-0.5244, test loss-1.5926, acc-0.5287\n",
      "Iter-7750, train loss-1.6588, acc-0.4600, valid loss-1.5936, acc-0.5246, test loss-1.5913, acc-0.5291\n",
      "Iter-7760, train loss-1.6263, acc-0.5600, valid loss-1.5924, acc-0.5248, test loss-1.5902, acc-0.5298\n",
      "Iter-7770, train loss-1.5835, acc-0.5400, valid loss-1.5913, acc-0.5258, test loss-1.5890, acc-0.5302\n",
      "Iter-7780, train loss-1.6047, acc-0.5200, valid loss-1.5900, acc-0.5272, test loss-1.5877, acc-0.5311\n",
      "Iter-7790, train loss-1.7107, acc-0.3600, valid loss-1.5888, acc-0.5278, test loss-1.5864, acc-0.5317\n",
      "Iter-7800, train loss-1.6403, acc-0.5000, valid loss-1.5875, acc-0.5294, test loss-1.5852, acc-0.5319\n",
      "Iter-7810, train loss-1.6047, acc-0.5400, valid loss-1.5864, acc-0.5296, test loss-1.5841, acc-0.5325\n",
      "Iter-7820, train loss-1.4547, acc-0.7000, valid loss-1.5852, acc-0.5300, test loss-1.5828, acc-0.5326\n",
      "Iter-7830, train loss-1.5530, acc-0.5200, valid loss-1.5840, acc-0.5304, test loss-1.5817, acc-0.5329\n",
      "Iter-7840, train loss-1.7490, acc-0.5000, valid loss-1.5829, acc-0.5310, test loss-1.5805, acc-0.5332\n",
      "Iter-7850, train loss-1.5592, acc-0.6600, valid loss-1.5816, acc-0.5318, test loss-1.5793, acc-0.5334\n",
      "Iter-7860, train loss-1.7287, acc-0.4000, valid loss-1.5805, acc-0.5328, test loss-1.5781, acc-0.5337\n",
      "Iter-7870, train loss-1.7012, acc-0.5400, valid loss-1.5794, acc-0.5336, test loss-1.5770, acc-0.5344\n",
      "Iter-7880, train loss-1.6032, acc-0.5000, valid loss-1.5782, acc-0.5334, test loss-1.5758, acc-0.5347\n",
      "Iter-7890, train loss-1.5678, acc-0.4600, valid loss-1.5770, acc-0.5346, test loss-1.5746, acc-0.5358\n",
      "Iter-7900, train loss-1.5850, acc-0.5200, valid loss-1.5758, acc-0.5346, test loss-1.5733, acc-0.5366\n",
      "Iter-7910, train loss-1.5244, acc-0.5600, valid loss-1.5746, acc-0.5354, test loss-1.5721, acc-0.5371\n",
      "Iter-7920, train loss-1.5549, acc-0.5200, valid loss-1.5734, acc-0.5356, test loss-1.5709, acc-0.5374\n",
      "Iter-7930, train loss-1.5399, acc-0.6200, valid loss-1.5722, acc-0.5358, test loss-1.5697, acc-0.5383\n",
      "Iter-7940, train loss-1.5251, acc-0.5200, valid loss-1.5710, acc-0.5358, test loss-1.5685, acc-0.5389\n",
      "Iter-7950, train loss-1.6120, acc-0.5200, valid loss-1.5697, acc-0.5362, test loss-1.5673, acc-0.5393\n",
      "Iter-7960, train loss-1.5976, acc-0.5200, valid loss-1.5686, acc-0.5368, test loss-1.5661, acc-0.5398\n",
      "Iter-7970, train loss-1.5517, acc-0.5600, valid loss-1.5673, acc-0.5376, test loss-1.5648, acc-0.5399\n",
      "Iter-7980, train loss-1.4548, acc-0.6200, valid loss-1.5661, acc-0.5382, test loss-1.5636, acc-0.5406\n",
      "Iter-7990, train loss-1.7282, acc-0.4000, valid loss-1.5650, acc-0.5388, test loss-1.5625, acc-0.5410\n",
      "Iter-8000, train loss-1.6137, acc-0.4000, valid loss-1.5638, acc-0.5392, test loss-1.5613, acc-0.5415\n",
      "Iter-8010, train loss-1.6277, acc-0.5200, valid loss-1.5627, acc-0.5400, test loss-1.5602, acc-0.5420\n",
      "Iter-8020, train loss-1.6669, acc-0.4200, valid loss-1.5615, acc-0.5402, test loss-1.5590, acc-0.5425\n",
      "Iter-8030, train loss-1.7085, acc-0.3800, valid loss-1.5604, acc-0.5410, test loss-1.5579, acc-0.5432\n",
      "Iter-8040, train loss-1.4208, acc-0.6800, valid loss-1.5592, acc-0.5414, test loss-1.5567, acc-0.5434\n",
      "Iter-8050, train loss-1.5736, acc-0.4600, valid loss-1.5580, acc-0.5416, test loss-1.5555, acc-0.5436\n",
      "Iter-8060, train loss-1.5901, acc-0.5600, valid loss-1.5567, acc-0.5418, test loss-1.5542, acc-0.5443\n",
      "Iter-8070, train loss-1.5069, acc-0.6400, valid loss-1.5555, acc-0.5418, test loss-1.5530, acc-0.5448\n",
      "Iter-8080, train loss-1.4679, acc-0.6000, valid loss-1.5543, acc-0.5420, test loss-1.5518, acc-0.5451\n",
      "Iter-8090, train loss-1.6376, acc-0.5200, valid loss-1.5531, acc-0.5428, test loss-1.5506, acc-0.5461\n",
      "Iter-8100, train loss-1.7208, acc-0.5200, valid loss-1.5519, acc-0.5432, test loss-1.5494, acc-0.5470\n",
      "Iter-8110, train loss-1.4249, acc-0.6400, valid loss-1.5508, acc-0.5438, test loss-1.5483, acc-0.5470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-1.6825, acc-0.4000, valid loss-1.5496, acc-0.5442, test loss-1.5472, acc-0.5478\n",
      "Iter-8130, train loss-1.5298, acc-0.5600, valid loss-1.5485, acc-0.5450, test loss-1.5459, acc-0.5486\n",
      "Iter-8140, train loss-1.4855, acc-0.6000, valid loss-1.5473, acc-0.5452, test loss-1.5448, acc-0.5488\n",
      "Iter-8150, train loss-1.6370, acc-0.3600, valid loss-1.5462, acc-0.5452, test loss-1.5436, acc-0.5491\n",
      "Iter-8160, train loss-1.6334, acc-0.4800, valid loss-1.5450, acc-0.5456, test loss-1.5425, acc-0.5495\n",
      "Iter-8170, train loss-1.4916, acc-0.5000, valid loss-1.5438, acc-0.5464, test loss-1.5412, acc-0.5505\n",
      "Iter-8180, train loss-1.5454, acc-0.5600, valid loss-1.5426, acc-0.5480, test loss-1.5400, acc-0.5514\n",
      "Iter-8190, train loss-1.6018, acc-0.5200, valid loss-1.5415, acc-0.5488, test loss-1.5389, acc-0.5520\n",
      "Iter-8200, train loss-1.6867, acc-0.4200, valid loss-1.5404, acc-0.5492, test loss-1.5377, acc-0.5519\n",
      "Iter-8210, train loss-1.7330, acc-0.3600, valid loss-1.5392, acc-0.5500, test loss-1.5366, acc-0.5519\n",
      "Iter-8220, train loss-1.5391, acc-0.6000, valid loss-1.5380, acc-0.5504, test loss-1.5354, acc-0.5525\n",
      "Iter-8230, train loss-1.4740, acc-0.5600, valid loss-1.5369, acc-0.5510, test loss-1.5342, acc-0.5534\n",
      "Iter-8240, train loss-1.4290, acc-0.6800, valid loss-1.5357, acc-0.5522, test loss-1.5330, acc-0.5534\n",
      "Iter-8250, train loss-1.4734, acc-0.6000, valid loss-1.5346, acc-0.5530, test loss-1.5319, acc-0.5542\n",
      "Iter-8260, train loss-1.5087, acc-0.4800, valid loss-1.5334, acc-0.5536, test loss-1.5307, acc-0.5546\n",
      "Iter-8270, train loss-1.6092, acc-0.5400, valid loss-1.5323, acc-0.5538, test loss-1.5296, acc-0.5554\n",
      "Iter-8280, train loss-1.4939, acc-0.5200, valid loss-1.5311, acc-0.5540, test loss-1.5285, acc-0.5557\n",
      "Iter-8290, train loss-1.4535, acc-0.6800, valid loss-1.5300, acc-0.5544, test loss-1.5273, acc-0.5561\n",
      "Iter-8300, train loss-1.4599, acc-0.6800, valid loss-1.5288, acc-0.5548, test loss-1.5262, acc-0.5565\n",
      "Iter-8310, train loss-1.6064, acc-0.4600, valid loss-1.5278, acc-0.5550, test loss-1.5251, acc-0.5569\n",
      "Iter-8320, train loss-1.4006, acc-0.6800, valid loss-1.5265, acc-0.5562, test loss-1.5239, acc-0.5575\n",
      "Iter-8330, train loss-1.6196, acc-0.5800, valid loss-1.5254, acc-0.5566, test loss-1.5227, acc-0.5579\n",
      "Iter-8340, train loss-1.5469, acc-0.5400, valid loss-1.5242, acc-0.5574, test loss-1.5215, acc-0.5584\n",
      "Iter-8350, train loss-1.5541, acc-0.5400, valid loss-1.5231, acc-0.5582, test loss-1.5204, acc-0.5590\n",
      "Iter-8360, train loss-1.6076, acc-0.4400, valid loss-1.5219, acc-0.5584, test loss-1.5192, acc-0.5593\n",
      "Iter-8370, train loss-1.5325, acc-0.5600, valid loss-1.5208, acc-0.5588, test loss-1.5180, acc-0.5600\n",
      "Iter-8380, train loss-1.5503, acc-0.5600, valid loss-1.5197, acc-0.5600, test loss-1.5169, acc-0.5605\n",
      "Iter-8390, train loss-1.4645, acc-0.5800, valid loss-1.5184, acc-0.5608, test loss-1.5157, acc-0.5610\n",
      "Iter-8400, train loss-1.6547, acc-0.4800, valid loss-1.5173, acc-0.5610, test loss-1.5145, acc-0.5618\n",
      "Iter-8410, train loss-1.6128, acc-0.5200, valid loss-1.5162, acc-0.5612, test loss-1.5134, acc-0.5619\n",
      "Iter-8420, train loss-1.5634, acc-0.6000, valid loss-1.5149, acc-0.5614, test loss-1.5121, acc-0.5621\n",
      "Iter-8430, train loss-1.6475, acc-0.5400, valid loss-1.5138, acc-0.5622, test loss-1.5110, acc-0.5624\n",
      "Iter-8440, train loss-1.4796, acc-0.4800, valid loss-1.5126, acc-0.5628, test loss-1.5098, acc-0.5623\n",
      "Iter-8450, train loss-1.5664, acc-0.5400, valid loss-1.5115, acc-0.5632, test loss-1.5086, acc-0.5630\n",
      "Iter-8460, train loss-1.6867, acc-0.5000, valid loss-1.5103, acc-0.5638, test loss-1.5074, acc-0.5632\n",
      "Iter-8470, train loss-1.4384, acc-0.6600, valid loss-1.5092, acc-0.5646, test loss-1.5063, acc-0.5633\n",
      "Iter-8480, train loss-1.3821, acc-0.7000, valid loss-1.5081, acc-0.5650, test loss-1.5052, acc-0.5638\n",
      "Iter-8490, train loss-1.4703, acc-0.5400, valid loss-1.5070, acc-0.5650, test loss-1.5041, acc-0.5640\n",
      "Iter-8500, train loss-1.5515, acc-0.5200, valid loss-1.5058, acc-0.5654, test loss-1.5029, acc-0.5645\n",
      "Iter-8510, train loss-1.6623, acc-0.4000, valid loss-1.5046, acc-0.5658, test loss-1.5017, acc-0.5650\n",
      "Iter-8520, train loss-1.5214, acc-0.5800, valid loss-1.5034, acc-0.5672, test loss-1.5005, acc-0.5658\n",
      "Iter-8530, train loss-1.5124, acc-0.5400, valid loss-1.5023, acc-0.5678, test loss-1.4993, acc-0.5666\n",
      "Iter-8540, train loss-1.5525, acc-0.5400, valid loss-1.5012, acc-0.5680, test loss-1.4982, acc-0.5674\n",
      "Iter-8550, train loss-1.5232, acc-0.5800, valid loss-1.5001, acc-0.5684, test loss-1.4971, acc-0.5676\n",
      "Iter-8560, train loss-1.4269, acc-0.6000, valid loss-1.4989, acc-0.5686, test loss-1.4959, acc-0.5685\n",
      "Iter-8570, train loss-1.3593, acc-0.6600, valid loss-1.4977, acc-0.5692, test loss-1.4948, acc-0.5686\n",
      "Iter-8580, train loss-1.5111, acc-0.5200, valid loss-1.4966, acc-0.5694, test loss-1.4936, acc-0.5695\n",
      "Iter-8590, train loss-1.4700, acc-0.5200, valid loss-1.4954, acc-0.5698, test loss-1.4924, acc-0.5699\n",
      "Iter-8600, train loss-1.3931, acc-0.6400, valid loss-1.4943, acc-0.5702, test loss-1.4913, acc-0.5704\n",
      "Iter-8610, train loss-1.5119, acc-0.6200, valid loss-1.4931, acc-0.5704, test loss-1.4901, acc-0.5709\n",
      "Iter-8620, train loss-1.5346, acc-0.6600, valid loss-1.4919, acc-0.5714, test loss-1.4889, acc-0.5713\n",
      "Iter-8630, train loss-1.5769, acc-0.5600, valid loss-1.4908, acc-0.5712, test loss-1.4878, acc-0.5716\n",
      "Iter-8640, train loss-1.4758, acc-0.7000, valid loss-1.4897, acc-0.5718, test loss-1.4867, acc-0.5721\n",
      "Iter-8650, train loss-1.5000, acc-0.6000, valid loss-1.4886, acc-0.5726, test loss-1.4856, acc-0.5732\n",
      "Iter-8660, train loss-1.5196, acc-0.5600, valid loss-1.4875, acc-0.5736, test loss-1.4845, acc-0.5741\n",
      "Iter-8670, train loss-1.4324, acc-0.5800, valid loss-1.4863, acc-0.5752, test loss-1.4833, acc-0.5744\n",
      "Iter-8680, train loss-1.4428, acc-0.5600, valid loss-1.4852, acc-0.5758, test loss-1.4822, acc-0.5745\n",
      "Iter-8690, train loss-1.6355, acc-0.5400, valid loss-1.4841, acc-0.5758, test loss-1.4811, acc-0.5749\n",
      "Iter-8700, train loss-1.5349, acc-0.4800, valid loss-1.4830, acc-0.5766, test loss-1.4799, acc-0.5753\n",
      "Iter-8710, train loss-1.3966, acc-0.6600, valid loss-1.4818, acc-0.5774, test loss-1.4788, acc-0.5756\n",
      "Iter-8720, train loss-1.4434, acc-0.5800, valid loss-1.4807, acc-0.5778, test loss-1.4776, acc-0.5757\n",
      "Iter-8730, train loss-1.4661, acc-0.6000, valid loss-1.4795, acc-0.5784, test loss-1.4765, acc-0.5762\n",
      "Iter-8740, train loss-1.4967, acc-0.6000, valid loss-1.4784, acc-0.5780, test loss-1.4753, acc-0.5762\n",
      "Iter-8750, train loss-1.3612, acc-0.6800, valid loss-1.4772, acc-0.5786, test loss-1.4742, acc-0.5768\n",
      "Iter-8760, train loss-1.5359, acc-0.5400, valid loss-1.4761, acc-0.5798, test loss-1.4731, acc-0.5772\n",
      "Iter-8770, train loss-1.5245, acc-0.5200, valid loss-1.4750, acc-0.5808, test loss-1.4720, acc-0.5781\n",
      "Iter-8780, train loss-1.4807, acc-0.6000, valid loss-1.4738, acc-0.5820, test loss-1.4707, acc-0.5786\n",
      "Iter-8790, train loss-1.6058, acc-0.4600, valid loss-1.4727, acc-0.5824, test loss-1.4697, acc-0.5794\n",
      "Iter-8800, train loss-1.5460, acc-0.5200, valid loss-1.4716, acc-0.5834, test loss-1.4685, acc-0.5800\n",
      "Iter-8810, train loss-1.4851, acc-0.6200, valid loss-1.4705, acc-0.5840, test loss-1.4674, acc-0.5806\n",
      "Iter-8820, train loss-1.4753, acc-0.5600, valid loss-1.4694, acc-0.5838, test loss-1.4663, acc-0.5807\n",
      "Iter-8830, train loss-1.4714, acc-0.5600, valid loss-1.4682, acc-0.5838, test loss-1.4652, acc-0.5821\n",
      "Iter-8840, train loss-1.4064, acc-0.6400, valid loss-1.4671, acc-0.5844, test loss-1.4640, acc-0.5830\n",
      "Iter-8850, train loss-1.5568, acc-0.5200, valid loss-1.4659, acc-0.5844, test loss-1.4629, acc-0.5832\n",
      "Iter-8860, train loss-1.3295, acc-0.7200, valid loss-1.4648, acc-0.5846, test loss-1.4617, acc-0.5837\n",
      "Iter-8870, train loss-1.4964, acc-0.5800, valid loss-1.4638, acc-0.5852, test loss-1.4607, acc-0.5838\n",
      "Iter-8880, train loss-1.3359, acc-0.6800, valid loss-1.4626, acc-0.5856, test loss-1.4595, acc-0.5841\n",
      "Iter-8890, train loss-1.4351, acc-0.5200, valid loss-1.4615, acc-0.5866, test loss-1.4585, acc-0.5843\n",
      "Iter-8900, train loss-1.4946, acc-0.5600, valid loss-1.4604, acc-0.5868, test loss-1.4573, acc-0.5849\n",
      "Iter-8910, train loss-1.4409, acc-0.6200, valid loss-1.4594, acc-0.5866, test loss-1.4563, acc-0.5847\n",
      "Iter-8920, train loss-1.3616, acc-0.6800, valid loss-1.4583, acc-0.5872, test loss-1.4552, acc-0.5856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-1.5391, acc-0.4800, valid loss-1.4572, acc-0.5878, test loss-1.4541, acc-0.5863\n",
      "Iter-8940, train loss-1.3577, acc-0.6400, valid loss-1.4561, acc-0.5886, test loss-1.4530, acc-0.5862\n",
      "Iter-8950, train loss-1.3477, acc-0.6400, valid loss-1.4550, acc-0.5890, test loss-1.4519, acc-0.5873\n",
      "Iter-8960, train loss-1.4633, acc-0.5200, valid loss-1.4539, acc-0.5896, test loss-1.4508, acc-0.5875\n",
      "Iter-8970, train loss-1.4899, acc-0.5400, valid loss-1.4528, acc-0.5896, test loss-1.4497, acc-0.5878\n",
      "Iter-8980, train loss-1.4737, acc-0.6600, valid loss-1.4517, acc-0.5896, test loss-1.4485, acc-0.5883\n",
      "Iter-8990, train loss-1.3584, acc-0.6000, valid loss-1.4505, acc-0.5904, test loss-1.4473, acc-0.5889\n",
      "Iter-9000, train loss-1.4973, acc-0.5800, valid loss-1.4494, acc-0.5918, test loss-1.4462, acc-0.5894\n",
      "Iter-9010, train loss-1.3122, acc-0.6600, valid loss-1.4483, acc-0.5918, test loss-1.4451, acc-0.5900\n",
      "Iter-9020, train loss-1.5333, acc-0.6000, valid loss-1.4472, acc-0.5918, test loss-1.4441, acc-0.5903\n",
      "Iter-9030, train loss-1.4513, acc-0.5400, valid loss-1.4462, acc-0.5924, test loss-1.4430, acc-0.5908\n",
      "Iter-9040, train loss-1.5282, acc-0.5200, valid loss-1.4450, acc-0.5928, test loss-1.4418, acc-0.5920\n",
      "Iter-9050, train loss-1.4065, acc-0.6200, valid loss-1.4439, acc-0.5934, test loss-1.4407, acc-0.5923\n",
      "Iter-9060, train loss-1.5569, acc-0.5400, valid loss-1.4428, acc-0.5944, test loss-1.4396, acc-0.5927\n",
      "Iter-9070, train loss-1.4912, acc-0.5600, valid loss-1.4418, acc-0.5950, test loss-1.4386, acc-0.5927\n",
      "Iter-9080, train loss-1.5321, acc-0.6000, valid loss-1.4408, acc-0.5948, test loss-1.4376, acc-0.5935\n",
      "Iter-9090, train loss-1.3993, acc-0.5400, valid loss-1.4397, acc-0.5950, test loss-1.4365, acc-0.5940\n",
      "Iter-9100, train loss-1.4312, acc-0.6000, valid loss-1.4387, acc-0.5954, test loss-1.4355, acc-0.5940\n",
      "Iter-9110, train loss-1.3746, acc-0.6200, valid loss-1.4377, acc-0.5952, test loss-1.4345, acc-0.5945\n",
      "Iter-9120, train loss-1.4158, acc-0.6000, valid loss-1.4366, acc-0.5952, test loss-1.4334, acc-0.5949\n",
      "Iter-9130, train loss-1.4065, acc-0.6000, valid loss-1.4355, acc-0.5956, test loss-1.4323, acc-0.5953\n",
      "Iter-9140, train loss-1.3162, acc-0.7000, valid loss-1.4344, acc-0.5956, test loss-1.4312, acc-0.5963\n",
      "Iter-9150, train loss-1.4377, acc-0.6600, valid loss-1.4333, acc-0.5958, test loss-1.4301, acc-0.5971\n",
      "Iter-9160, train loss-1.2602, acc-0.7000, valid loss-1.4323, acc-0.5958, test loss-1.4291, acc-0.5977\n",
      "Iter-9170, train loss-1.4875, acc-0.5200, valid loss-1.4312, acc-0.5968, test loss-1.4280, acc-0.5982\n",
      "Iter-9180, train loss-1.4223, acc-0.6200, valid loss-1.4301, acc-0.5976, test loss-1.4269, acc-0.5984\n",
      "Iter-9190, train loss-1.2685, acc-0.6600, valid loss-1.4290, acc-0.5978, test loss-1.4258, acc-0.5990\n",
      "Iter-9200, train loss-1.4493, acc-0.6000, valid loss-1.4280, acc-0.5980, test loss-1.4247, acc-0.5993\n",
      "Iter-9210, train loss-1.4268, acc-0.6200, valid loss-1.4269, acc-0.5998, test loss-1.4236, acc-0.6001\n",
      "Iter-9220, train loss-1.3430, acc-0.6200, valid loss-1.4259, acc-0.6008, test loss-1.4226, acc-0.6010\n",
      "Iter-9230, train loss-1.3697, acc-0.5600, valid loss-1.4248, acc-0.6012, test loss-1.4215, acc-0.6015\n",
      "Iter-9240, train loss-1.3381, acc-0.7400, valid loss-1.4237, acc-0.6016, test loss-1.4204, acc-0.6020\n",
      "Iter-9250, train loss-1.3606, acc-0.6000, valid loss-1.4226, acc-0.6022, test loss-1.4193, acc-0.6029\n",
      "Iter-9260, train loss-1.4348, acc-0.7000, valid loss-1.4216, acc-0.6024, test loss-1.4183, acc-0.6030\n",
      "Iter-9270, train loss-1.5220, acc-0.4800, valid loss-1.4206, acc-0.6026, test loss-1.4173, acc-0.6035\n",
      "Iter-9280, train loss-1.3492, acc-0.6000, valid loss-1.4196, acc-0.6026, test loss-1.4162, acc-0.6047\n",
      "Iter-9290, train loss-1.3950, acc-0.6000, valid loss-1.4186, acc-0.6028, test loss-1.4152, acc-0.6052\n",
      "Iter-9300, train loss-1.5736, acc-0.5400, valid loss-1.4176, acc-0.6036, test loss-1.4143, acc-0.6055\n",
      "Iter-9310, train loss-1.5217, acc-0.5400, valid loss-1.4164, acc-0.6040, test loss-1.4132, acc-0.6059\n",
      "Iter-9320, train loss-1.3513, acc-0.6200, valid loss-1.4153, acc-0.6052, test loss-1.4120, acc-0.6070\n",
      "Iter-9330, train loss-1.3613, acc-0.6200, valid loss-1.4143, acc-0.6058, test loss-1.4110, acc-0.6079\n",
      "Iter-9340, train loss-1.3424, acc-0.6800, valid loss-1.4132, acc-0.6058, test loss-1.4099, acc-0.6080\n",
      "Iter-9350, train loss-1.4625, acc-0.5600, valid loss-1.4122, acc-0.6070, test loss-1.4089, acc-0.6090\n",
      "Iter-9360, train loss-1.3449, acc-0.5200, valid loss-1.4111, acc-0.6078, test loss-1.4078, acc-0.6099\n",
      "Iter-9370, train loss-1.5416, acc-0.5400, valid loss-1.4100, acc-0.6082, test loss-1.4067, acc-0.6101\n",
      "Iter-9380, train loss-1.4308, acc-0.5600, valid loss-1.4090, acc-0.6084, test loss-1.4056, acc-0.6109\n",
      "Iter-9390, train loss-1.3586, acc-0.6800, valid loss-1.4080, acc-0.6086, test loss-1.4047, acc-0.6116\n",
      "Iter-9400, train loss-1.4772, acc-0.5000, valid loss-1.4070, acc-0.6088, test loss-1.4036, acc-0.6122\n",
      "Iter-9410, train loss-1.3164, acc-0.7000, valid loss-1.4059, acc-0.6094, test loss-1.4026, acc-0.6129\n",
      "Iter-9420, train loss-1.4005, acc-0.5800, valid loss-1.4049, acc-0.6094, test loss-1.4016, acc-0.6133\n",
      "Iter-9430, train loss-1.3825, acc-0.6200, valid loss-1.4039, acc-0.6100, test loss-1.4006, acc-0.6135\n",
      "Iter-9440, train loss-1.3442, acc-0.6800, valid loss-1.4029, acc-0.6102, test loss-1.3996, acc-0.6139\n",
      "Iter-9450, train loss-1.5305, acc-0.5600, valid loss-1.4020, acc-0.6116, test loss-1.3986, acc-0.6143\n",
      "Iter-9460, train loss-1.4118, acc-0.6400, valid loss-1.4009, acc-0.6126, test loss-1.3976, acc-0.6148\n",
      "Iter-9470, train loss-1.3684, acc-0.6800, valid loss-1.4000, acc-0.6128, test loss-1.3966, acc-0.6153\n",
      "Iter-9480, train loss-1.3817, acc-0.6400, valid loss-1.3989, acc-0.6130, test loss-1.3956, acc-0.6155\n",
      "Iter-9490, train loss-1.2699, acc-0.7200, valid loss-1.3979, acc-0.6134, test loss-1.3946, acc-0.6158\n",
      "Iter-9500, train loss-1.3601, acc-0.6400, valid loss-1.3968, acc-0.6140, test loss-1.3935, acc-0.6164\n",
      "Iter-9510, train loss-1.3412, acc-0.6200, valid loss-1.3959, acc-0.6142, test loss-1.3925, acc-0.6169\n",
      "Iter-9520, train loss-1.4327, acc-0.5200, valid loss-1.3949, acc-0.6144, test loss-1.3916, acc-0.6175\n",
      "Iter-9530, train loss-1.3252, acc-0.7800, valid loss-1.3939, acc-0.6144, test loss-1.3906, acc-0.6183\n",
      "Iter-9540, train loss-1.5133, acc-0.5800, valid loss-1.3929, acc-0.6152, test loss-1.3896, acc-0.6184\n",
      "Iter-9550, train loss-1.4253, acc-0.5800, valid loss-1.3918, acc-0.6160, test loss-1.3885, acc-0.6193\n",
      "Iter-9560, train loss-1.3709, acc-0.6200, valid loss-1.3908, acc-0.6166, test loss-1.3875, acc-0.6196\n",
      "Iter-9570, train loss-1.4873, acc-0.6200, valid loss-1.3898, acc-0.6170, test loss-1.3864, acc-0.6198\n",
      "Iter-9580, train loss-1.3928, acc-0.6000, valid loss-1.3888, acc-0.6176, test loss-1.3854, acc-0.6199\n",
      "Iter-9590, train loss-1.4026, acc-0.6000, valid loss-1.3878, acc-0.6182, test loss-1.3844, acc-0.6203\n",
      "Iter-9600, train loss-1.2837, acc-0.6800, valid loss-1.3868, acc-0.6186, test loss-1.3834, acc-0.6207\n",
      "Iter-9610, train loss-1.4498, acc-0.5600, valid loss-1.3858, acc-0.6192, test loss-1.3824, acc-0.6215\n",
      "Iter-9620, train loss-1.4450, acc-0.5200, valid loss-1.3848, acc-0.6196, test loss-1.3814, acc-0.6218\n",
      "Iter-9630, train loss-1.3996, acc-0.6600, valid loss-1.3838, acc-0.6198, test loss-1.3804, acc-0.6225\n",
      "Iter-9640, train loss-1.2795, acc-0.7600, valid loss-1.3828, acc-0.6200, test loss-1.3793, acc-0.6228\n",
      "Iter-9650, train loss-1.4266, acc-0.7000, valid loss-1.3818, acc-0.6204, test loss-1.3783, acc-0.6230\n",
      "Iter-9660, train loss-1.4335, acc-0.5800, valid loss-1.3808, acc-0.6204, test loss-1.3774, acc-0.6233\n",
      "Iter-9670, train loss-1.5314, acc-0.5600, valid loss-1.3799, acc-0.6214, test loss-1.3764, acc-0.6235\n",
      "Iter-9680, train loss-1.3292, acc-0.6600, valid loss-1.3789, acc-0.6220, test loss-1.3754, acc-0.6242\n",
      "Iter-9690, train loss-1.3443, acc-0.6600, valid loss-1.3779, acc-0.6226, test loss-1.3744, acc-0.6251\n",
      "Iter-9700, train loss-1.3899, acc-0.5800, valid loss-1.3768, acc-0.6230, test loss-1.3734, acc-0.6253\n",
      "Iter-9710, train loss-1.2461, acc-0.7600, valid loss-1.3758, acc-0.6232, test loss-1.3724, acc-0.6254\n",
      "Iter-9720, train loss-1.3550, acc-0.6800, valid loss-1.3749, acc-0.6236, test loss-1.3715, acc-0.6258\n",
      "Iter-9730, train loss-1.3429, acc-0.6600, valid loss-1.3739, acc-0.6240, test loss-1.3705, acc-0.6257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-1.4151, acc-0.5600, valid loss-1.3729, acc-0.6254, test loss-1.3695, acc-0.6262\n",
      "Iter-9750, train loss-1.4121, acc-0.6600, valid loss-1.3720, acc-0.6260, test loss-1.3685, acc-0.6266\n",
      "Iter-9760, train loss-1.3076, acc-0.6400, valid loss-1.3710, acc-0.6268, test loss-1.3676, acc-0.6272\n",
      "Iter-9770, train loss-1.4431, acc-0.5600, valid loss-1.3700, acc-0.6264, test loss-1.3666, acc-0.6276\n",
      "Iter-9780, train loss-1.3828, acc-0.5600, valid loss-1.3690, acc-0.6268, test loss-1.3656, acc-0.6282\n",
      "Iter-9790, train loss-1.4310, acc-0.5600, valid loss-1.3680, acc-0.6268, test loss-1.3646, acc-0.6285\n",
      "Iter-9800, train loss-1.2968, acc-0.7000, valid loss-1.3670, acc-0.6276, test loss-1.3636, acc-0.6286\n",
      "Iter-9810, train loss-1.1958, acc-0.7000, valid loss-1.3660, acc-0.6280, test loss-1.3626, acc-0.6291\n",
      "Iter-9820, train loss-1.3550, acc-0.6400, valid loss-1.3650, acc-0.6292, test loss-1.3616, acc-0.6297\n",
      "Iter-9830, train loss-1.4228, acc-0.5600, valid loss-1.3641, acc-0.6292, test loss-1.3606, acc-0.6299\n",
      "Iter-9840, train loss-1.5233, acc-0.4400, valid loss-1.3631, acc-0.6298, test loss-1.3596, acc-0.6304\n",
      "Iter-9850, train loss-1.3897, acc-0.6200, valid loss-1.3621, acc-0.6298, test loss-1.3586, acc-0.6308\n",
      "Iter-9860, train loss-1.3360, acc-0.7000, valid loss-1.3611, acc-0.6306, test loss-1.3577, acc-0.6311\n",
      "Iter-9870, train loss-1.2961, acc-0.6600, valid loss-1.3601, acc-0.6318, test loss-1.3566, acc-0.6314\n",
      "Iter-9880, train loss-1.3663, acc-0.6200, valid loss-1.3591, acc-0.6326, test loss-1.3556, acc-0.6322\n",
      "Iter-9890, train loss-1.4167, acc-0.5600, valid loss-1.3581, acc-0.6330, test loss-1.3547, acc-0.6324\n",
      "Iter-9900, train loss-1.3884, acc-0.5600, valid loss-1.3572, acc-0.6332, test loss-1.3537, acc-0.6330\n",
      "Iter-9910, train loss-1.3009, acc-0.6200, valid loss-1.3563, acc-0.6332, test loss-1.3528, acc-0.6336\n",
      "Iter-9920, train loss-1.3757, acc-0.5800, valid loss-1.3553, acc-0.6336, test loss-1.3519, acc-0.6343\n",
      "Iter-9930, train loss-1.5914, acc-0.4400, valid loss-1.3543, acc-0.6344, test loss-1.3509, acc-0.6346\n",
      "Iter-9940, train loss-1.3919, acc-0.6200, valid loss-1.3533, acc-0.6354, test loss-1.3499, acc-0.6353\n",
      "Iter-9950, train loss-1.4574, acc-0.5400, valid loss-1.3523, acc-0.6356, test loss-1.3489, acc-0.6357\n",
      "Iter-9960, train loss-1.4257, acc-0.5000, valid loss-1.3514, acc-0.6362, test loss-1.3479, acc-0.6366\n",
      "Iter-9970, train loss-1.4207, acc-0.5400, valid loss-1.3504, acc-0.6362, test loss-1.3469, acc-0.6370\n",
      "Iter-9980, train loss-1.3490, acc-0.6600, valid loss-1.3494, acc-0.6370, test loss-1.3460, acc-0.6376\n",
      "Iter-9990, train loss-1.3640, acc-0.6600, valid loss-1.3484, acc-0.6378, test loss-1.3450, acc-0.6379\n",
      "Iter-10000, train loss-1.3265, acc-0.6600, valid loss-1.3475, acc-0.6382, test loss-1.3440, acc-0.6383\n",
      "Iter-10010, train loss-1.3147, acc-0.7200, valid loss-1.3465, acc-0.6388, test loss-1.3431, acc-0.6392\n",
      "Iter-10020, train loss-1.5493, acc-0.4600, valid loss-1.3456, acc-0.6388, test loss-1.3421, acc-0.6391\n",
      "Iter-10030, train loss-1.4405, acc-0.6400, valid loss-1.3447, acc-0.6392, test loss-1.3412, acc-0.6398\n",
      "Iter-10040, train loss-1.3013, acc-0.6400, valid loss-1.3438, acc-0.6396, test loss-1.3403, acc-0.6407\n",
      "Iter-10050, train loss-1.3513, acc-0.6000, valid loss-1.3428, acc-0.6402, test loss-1.3394, acc-0.6406\n",
      "Iter-10060, train loss-1.2173, acc-0.7200, valid loss-1.3419, acc-0.6404, test loss-1.3384, acc-0.6411\n",
      "Iter-10070, train loss-1.4892, acc-0.4800, valid loss-1.3410, acc-0.6406, test loss-1.3375, acc-0.6415\n",
      "Iter-10080, train loss-1.3330, acc-0.6600, valid loss-1.3399, acc-0.6414, test loss-1.3365, acc-0.6422\n",
      "Iter-10090, train loss-1.3604, acc-0.6600, valid loss-1.3390, acc-0.6418, test loss-1.3355, acc-0.6430\n",
      "Iter-10100, train loss-1.3702, acc-0.6400, valid loss-1.3381, acc-0.6418, test loss-1.3346, acc-0.6435\n",
      "Iter-10110, train loss-1.2179, acc-0.7400, valid loss-1.3372, acc-0.6418, test loss-1.3337, acc-0.6434\n",
      "Iter-10120, train loss-1.3415, acc-0.7000, valid loss-1.3362, acc-0.6420, test loss-1.3327, acc-0.6436\n",
      "Iter-10130, train loss-1.3299, acc-0.6000, valid loss-1.3353, acc-0.6420, test loss-1.3318, acc-0.6438\n",
      "Iter-10140, train loss-1.3732, acc-0.6800, valid loss-1.3343, acc-0.6422, test loss-1.3308, acc-0.6444\n",
      "Iter-10150, train loss-1.3025, acc-0.7000, valid loss-1.3334, acc-0.6426, test loss-1.3299, acc-0.6447\n",
      "Iter-10160, train loss-1.1355, acc-0.6800, valid loss-1.3324, acc-0.6440, test loss-1.3289, acc-0.6449\n",
      "Iter-10170, train loss-1.5585, acc-0.5000, valid loss-1.3314, acc-0.6446, test loss-1.3280, acc-0.6449\n",
      "Iter-10180, train loss-1.2350, acc-0.7000, valid loss-1.3305, acc-0.6446, test loss-1.3271, acc-0.6452\n",
      "Iter-10190, train loss-1.5165, acc-0.5000, valid loss-1.3295, acc-0.6446, test loss-1.3261, acc-0.6453\n",
      "Iter-10200, train loss-1.2629, acc-0.6600, valid loss-1.3286, acc-0.6444, test loss-1.3252, acc-0.6458\n",
      "Iter-10210, train loss-1.4066, acc-0.7000, valid loss-1.3277, acc-0.6452, test loss-1.3243, acc-0.6459\n",
      "Iter-10220, train loss-1.1637, acc-0.7800, valid loss-1.3268, acc-0.6452, test loss-1.3234, acc-0.6463\n",
      "Iter-10230, train loss-1.4244, acc-0.5600, valid loss-1.3259, acc-0.6458, test loss-1.3225, acc-0.6470\n",
      "Iter-10240, train loss-1.2871, acc-0.6600, valid loss-1.3249, acc-0.6470, test loss-1.3215, acc-0.6479\n",
      "Iter-10250, train loss-1.2836, acc-0.6600, valid loss-1.3240, acc-0.6474, test loss-1.3205, acc-0.6486\n",
      "Iter-10260, train loss-1.3456, acc-0.6200, valid loss-1.3230, acc-0.6478, test loss-1.3196, acc-0.6490\n",
      "Iter-10270, train loss-1.3342, acc-0.6400, valid loss-1.3221, acc-0.6478, test loss-1.3187, acc-0.6494\n",
      "Iter-10280, train loss-1.2217, acc-0.6800, valid loss-1.3212, acc-0.6480, test loss-1.3178, acc-0.6501\n",
      "Iter-10290, train loss-1.4255, acc-0.5400, valid loss-1.3203, acc-0.6484, test loss-1.3169, acc-0.6500\n",
      "Iter-10300, train loss-1.2695, acc-0.7200, valid loss-1.3193, acc-0.6492, test loss-1.3159, acc-0.6505\n",
      "Iter-10310, train loss-1.3506, acc-0.5600, valid loss-1.3184, acc-0.6498, test loss-1.3150, acc-0.6509\n",
      "Iter-10320, train loss-1.2626, acc-0.7400, valid loss-1.3175, acc-0.6502, test loss-1.3141, acc-0.6513\n",
      "Iter-10330, train loss-1.4469, acc-0.6200, valid loss-1.3166, acc-0.6504, test loss-1.3132, acc-0.6515\n",
      "Iter-10340, train loss-1.2314, acc-0.6800, valid loss-1.3156, acc-0.6516, test loss-1.3122, acc-0.6521\n",
      "Iter-10350, train loss-1.2649, acc-0.6800, valid loss-1.3147, acc-0.6516, test loss-1.3113, acc-0.6526\n",
      "Iter-10360, train loss-1.3545, acc-0.6200, valid loss-1.3137, acc-0.6524, test loss-1.3103, acc-0.6534\n",
      "Iter-10370, train loss-1.1555, acc-0.7200, valid loss-1.3128, acc-0.6526, test loss-1.3094, acc-0.6541\n",
      "Iter-10380, train loss-1.3356, acc-0.5200, valid loss-1.3119, acc-0.6532, test loss-1.3085, acc-0.6547\n",
      "Iter-10390, train loss-1.2431, acc-0.7000, valid loss-1.3110, acc-0.6542, test loss-1.3076, acc-0.6551\n",
      "Iter-10400, train loss-1.2226, acc-0.6600, valid loss-1.3101, acc-0.6546, test loss-1.3067, acc-0.6560\n",
      "Iter-10410, train loss-1.2661, acc-0.6600, valid loss-1.3091, acc-0.6552, test loss-1.3057, acc-0.6568\n",
      "Iter-10420, train loss-1.3582, acc-0.6800, valid loss-1.3081, acc-0.6558, test loss-1.3047, acc-0.6571\n",
      "Iter-10430, train loss-1.1966, acc-0.7200, valid loss-1.3072, acc-0.6558, test loss-1.3038, acc-0.6574\n",
      "Iter-10440, train loss-1.3156, acc-0.6200, valid loss-1.3063, acc-0.6562, test loss-1.3029, acc-0.6578\n",
      "Iter-10450, train loss-1.3587, acc-0.6200, valid loss-1.3054, acc-0.6566, test loss-1.3020, acc-0.6579\n",
      "Iter-10460, train loss-1.2975, acc-0.6800, valid loss-1.3045, acc-0.6572, test loss-1.3011, acc-0.6584\n",
      "Iter-10470, train loss-1.2276, acc-0.6600, valid loss-1.3036, acc-0.6572, test loss-1.3002, acc-0.6587\n",
      "Iter-10480, train loss-1.3190, acc-0.6800, valid loss-1.3027, acc-0.6574, test loss-1.2993, acc-0.6592\n",
      "Iter-10490, train loss-1.4584, acc-0.4800, valid loss-1.3017, acc-0.6574, test loss-1.2983, acc-0.6596\n",
      "Iter-10500, train loss-1.2332, acc-0.7000, valid loss-1.3008, acc-0.6578, test loss-1.2974, acc-0.6598\n",
      "Iter-10510, train loss-1.1287, acc-0.7800, valid loss-1.3000, acc-0.6584, test loss-1.2966, acc-0.6597\n",
      "Iter-10520, train loss-1.3389, acc-0.6200, valid loss-1.2991, acc-0.6584, test loss-1.2957, acc-0.6604\n",
      "Iter-10530, train loss-1.2869, acc-0.7400, valid loss-1.2982, acc-0.6588, test loss-1.2948, acc-0.6604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10540, train loss-1.1486, acc-0.7600, valid loss-1.2973, acc-0.6592, test loss-1.2939, acc-0.6607\n",
      "Iter-10550, train loss-1.1927, acc-0.7200, valid loss-1.2964, acc-0.6598, test loss-1.2930, acc-0.6611\n",
      "Iter-10560, train loss-1.2256, acc-0.7000, valid loss-1.2955, acc-0.6598, test loss-1.2921, acc-0.6615\n",
      "Iter-10570, train loss-1.3535, acc-0.6200, valid loss-1.2946, acc-0.6600, test loss-1.2911, acc-0.6618\n",
      "Iter-10580, train loss-1.4112, acc-0.6800, valid loss-1.2936, acc-0.6602, test loss-1.2902, acc-0.6624\n",
      "Iter-10590, train loss-1.2759, acc-0.6400, valid loss-1.2928, acc-0.6604, test loss-1.2893, acc-0.6625\n",
      "Iter-10600, train loss-1.2615, acc-0.7000, valid loss-1.2919, acc-0.6604, test loss-1.2885, acc-0.6629\n",
      "Iter-10610, train loss-1.2293, acc-0.6600, valid loss-1.2910, acc-0.6608, test loss-1.2876, acc-0.6636\n",
      "Iter-10620, train loss-1.4199, acc-0.5600, valid loss-1.2901, acc-0.6606, test loss-1.2867, acc-0.6637\n",
      "Iter-10630, train loss-1.3337, acc-0.6800, valid loss-1.2892, acc-0.6606, test loss-1.2858, acc-0.6640\n",
      "Iter-10640, train loss-1.1251, acc-0.7200, valid loss-1.2883, acc-0.6608, test loss-1.2849, acc-0.6647\n",
      "Iter-10650, train loss-1.2216, acc-0.7200, valid loss-1.2874, acc-0.6608, test loss-1.2841, acc-0.6647\n",
      "Iter-10660, train loss-1.2368, acc-0.6200, valid loss-1.2865, acc-0.6610, test loss-1.2831, acc-0.6658\n",
      "Iter-10670, train loss-1.3111, acc-0.6600, valid loss-1.2856, acc-0.6618, test loss-1.2822, acc-0.6668\n",
      "Iter-10680, train loss-1.2831, acc-0.6400, valid loss-1.2847, acc-0.6620, test loss-1.2813, acc-0.6670\n",
      "Iter-10690, train loss-1.3524, acc-0.6400, valid loss-1.2839, acc-0.6626, test loss-1.2805, acc-0.6675\n",
      "Iter-10700, train loss-1.2494, acc-0.7400, valid loss-1.2830, acc-0.6630, test loss-1.2796, acc-0.6679\n",
      "Iter-10710, train loss-1.3085, acc-0.6800, valid loss-1.2821, acc-0.6632, test loss-1.2788, acc-0.6681\n",
      "Iter-10720, train loss-1.2038, acc-0.7000, valid loss-1.2813, acc-0.6636, test loss-1.2780, acc-0.6684\n",
      "Iter-10730, train loss-1.2766, acc-0.6400, valid loss-1.2804, acc-0.6636, test loss-1.2771, acc-0.6689\n",
      "Iter-10740, train loss-1.3085, acc-0.6400, valid loss-1.2796, acc-0.6642, test loss-1.2763, acc-0.6691\n",
      "Iter-10750, train loss-1.2000, acc-0.7000, valid loss-1.2787, acc-0.6638, test loss-1.2754, acc-0.6691\n",
      "Iter-10760, train loss-1.4599, acc-0.6200, valid loss-1.2779, acc-0.6638, test loss-1.2746, acc-0.6696\n",
      "Iter-10770, train loss-1.2764, acc-0.6600, valid loss-1.2770, acc-0.6648, test loss-1.2737, acc-0.6697\n",
      "Iter-10780, train loss-1.4168, acc-0.5600, valid loss-1.2762, acc-0.6646, test loss-1.2729, acc-0.6701\n",
      "Iter-10790, train loss-1.2791, acc-0.6800, valid loss-1.2752, acc-0.6652, test loss-1.2719, acc-0.6711\n",
      "Iter-10800, train loss-1.3567, acc-0.6200, valid loss-1.2744, acc-0.6656, test loss-1.2711, acc-0.6715\n",
      "Iter-10810, train loss-1.1835, acc-0.7600, valid loss-1.2735, acc-0.6662, test loss-1.2702, acc-0.6716\n",
      "Iter-10820, train loss-1.3596, acc-0.5800, valid loss-1.2726, acc-0.6668, test loss-1.2693, acc-0.6724\n",
      "Iter-10830, train loss-1.2558, acc-0.6400, valid loss-1.2718, acc-0.6672, test loss-1.2685, acc-0.6726\n",
      "Iter-10840, train loss-1.3126, acc-0.6800, valid loss-1.2708, acc-0.6676, test loss-1.2676, acc-0.6729\n",
      "Iter-10850, train loss-1.3729, acc-0.6800, valid loss-1.2700, acc-0.6680, test loss-1.2667, acc-0.6730\n",
      "Iter-10860, train loss-1.2924, acc-0.6400, valid loss-1.2691, acc-0.6686, test loss-1.2659, acc-0.6740\n",
      "Iter-10870, train loss-1.2790, acc-0.6600, valid loss-1.2683, acc-0.6688, test loss-1.2650, acc-0.6743\n",
      "Iter-10880, train loss-1.2710, acc-0.6800, valid loss-1.2674, acc-0.6694, test loss-1.2642, acc-0.6745\n",
      "Iter-10890, train loss-1.3054, acc-0.6200, valid loss-1.2665, acc-0.6696, test loss-1.2633, acc-0.6748\n",
      "Iter-10900, train loss-1.3135, acc-0.6600, valid loss-1.2657, acc-0.6696, test loss-1.2624, acc-0.6750\n",
      "Iter-10910, train loss-1.3494, acc-0.6400, valid loss-1.2648, acc-0.6698, test loss-1.2616, acc-0.6750\n",
      "Iter-10920, train loss-1.2510, acc-0.7200, valid loss-1.2640, acc-0.6696, test loss-1.2608, acc-0.6752\n",
      "Iter-10930, train loss-1.4114, acc-0.6600, valid loss-1.2631, acc-0.6700, test loss-1.2599, acc-0.6754\n",
      "Iter-10940, train loss-1.2388, acc-0.7000, valid loss-1.2622, acc-0.6706, test loss-1.2591, acc-0.6759\n",
      "Iter-10950, train loss-1.2401, acc-0.6200, valid loss-1.2613, acc-0.6706, test loss-1.2582, acc-0.6762\n",
      "Iter-10960, train loss-1.3519, acc-0.6600, valid loss-1.2606, acc-0.6708, test loss-1.2574, acc-0.6768\n",
      "Iter-10970, train loss-1.2393, acc-0.6400, valid loss-1.2597, acc-0.6708, test loss-1.2565, acc-0.6773\n",
      "Iter-10980, train loss-1.3578, acc-0.5000, valid loss-1.2588, acc-0.6710, test loss-1.2557, acc-0.6778\n",
      "Iter-10990, train loss-1.3832, acc-0.6400, valid loss-1.2580, acc-0.6710, test loss-1.2548, acc-0.6782\n",
      "Iter-11000, train loss-1.1847, acc-0.7200, valid loss-1.2572, acc-0.6712, test loss-1.2540, acc-0.6786\n",
      "Iter-11010, train loss-1.2562, acc-0.6600, valid loss-1.2563, acc-0.6714, test loss-1.2532, acc-0.6787\n",
      "Iter-11020, train loss-1.2070, acc-0.7000, valid loss-1.2554, acc-0.6722, test loss-1.2523, acc-0.6791\n",
      "Iter-11030, train loss-1.3272, acc-0.6200, valid loss-1.2546, acc-0.6728, test loss-1.2515, acc-0.6793\n",
      "Iter-11040, train loss-1.2860, acc-0.6000, valid loss-1.2537, acc-0.6738, test loss-1.2506, acc-0.6793\n",
      "Iter-11050, train loss-1.1329, acc-0.6000, valid loss-1.2529, acc-0.6744, test loss-1.2497, acc-0.6792\n",
      "Iter-11060, train loss-1.3755, acc-0.5600, valid loss-1.2520, acc-0.6744, test loss-1.2488, acc-0.6795\n",
      "Iter-11070, train loss-1.1073, acc-0.7800, valid loss-1.2512, acc-0.6750, test loss-1.2480, acc-0.6797\n",
      "Iter-11080, train loss-1.1922, acc-0.7000, valid loss-1.2503, acc-0.6750, test loss-1.2471, acc-0.6802\n",
      "Iter-11090, train loss-1.1334, acc-0.7200, valid loss-1.2494, acc-0.6762, test loss-1.2463, acc-0.6803\n",
      "Iter-11100, train loss-1.2337, acc-0.7000, valid loss-1.2486, acc-0.6762, test loss-1.2455, acc-0.6807\n",
      "Iter-11110, train loss-1.3295, acc-0.6400, valid loss-1.2477, acc-0.6764, test loss-1.2446, acc-0.6816\n",
      "Iter-11120, train loss-1.1083, acc-0.7600, valid loss-1.2469, acc-0.6770, test loss-1.2438, acc-0.6815\n",
      "Iter-11130, train loss-1.4078, acc-0.5800, valid loss-1.2461, acc-0.6772, test loss-1.2430, acc-0.6817\n",
      "Iter-11140, train loss-1.3058, acc-0.6000, valid loss-1.2453, acc-0.6774, test loss-1.2422, acc-0.6822\n",
      "Iter-11150, train loss-1.2951, acc-0.6400, valid loss-1.2444, acc-0.6774, test loss-1.2413, acc-0.6826\n",
      "Iter-11160, train loss-1.1855, acc-0.6800, valid loss-1.2436, acc-0.6780, test loss-1.2405, acc-0.6831\n",
      "Iter-11170, train loss-1.1786, acc-0.7200, valid loss-1.2428, acc-0.6782, test loss-1.2397, acc-0.6833\n",
      "Iter-11180, train loss-1.1506, acc-0.6800, valid loss-1.2420, acc-0.6786, test loss-1.2389, acc-0.6837\n",
      "Iter-11190, train loss-1.1145, acc-0.7400, valid loss-1.2411, acc-0.6792, test loss-1.2381, acc-0.6842\n",
      "Iter-11200, train loss-1.3591, acc-0.6400, valid loss-1.2403, acc-0.6796, test loss-1.2372, acc-0.6844\n",
      "Iter-11210, train loss-1.1592, acc-0.7600, valid loss-1.2394, acc-0.6802, test loss-1.2364, acc-0.6848\n",
      "Iter-11220, train loss-1.3102, acc-0.6000, valid loss-1.2387, acc-0.6802, test loss-1.2356, acc-0.6851\n",
      "Iter-11230, train loss-1.2694, acc-0.6400, valid loss-1.2378, acc-0.6808, test loss-1.2348, acc-0.6860\n",
      "Iter-11240, train loss-1.3102, acc-0.6400, valid loss-1.2370, acc-0.6808, test loss-1.2340, acc-0.6862\n",
      "Iter-11250, train loss-1.2425, acc-0.6800, valid loss-1.2361, acc-0.6812, test loss-1.2331, acc-0.6865\n",
      "Iter-11260, train loss-1.2351, acc-0.7600, valid loss-1.2353, acc-0.6816, test loss-1.2323, acc-0.6870\n",
      "Iter-11270, train loss-1.1919, acc-0.7000, valid loss-1.2345, acc-0.6820, test loss-1.2315, acc-0.6872\n",
      "Iter-11280, train loss-1.1589, acc-0.6800, valid loss-1.2337, acc-0.6828, test loss-1.2307, acc-0.6873\n",
      "Iter-11290, train loss-1.2460, acc-0.5800, valid loss-1.2329, acc-0.6830, test loss-1.2299, acc-0.6878\n",
      "Iter-11300, train loss-1.2349, acc-0.7400, valid loss-1.2320, acc-0.6824, test loss-1.2290, acc-0.6882\n",
      "Iter-11310, train loss-1.2549, acc-0.6800, valid loss-1.2312, acc-0.6828, test loss-1.2282, acc-0.6883\n",
      "Iter-11320, train loss-1.2134, acc-0.7400, valid loss-1.2304, acc-0.6830, test loss-1.2273, acc-0.6889\n",
      "Iter-11330, train loss-1.3278, acc-0.6400, valid loss-1.2295, acc-0.6832, test loss-1.2265, acc-0.6896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11340, train loss-1.2039, acc-0.7000, valid loss-1.2287, acc-0.6840, test loss-1.2257, acc-0.6900\n",
      "Iter-11350, train loss-1.2052, acc-0.6800, valid loss-1.2279, acc-0.6842, test loss-1.2249, acc-0.6901\n",
      "Iter-11360, train loss-1.2345, acc-0.6200, valid loss-1.2271, acc-0.6846, test loss-1.2241, acc-0.6906\n",
      "Iter-11370, train loss-1.2446, acc-0.7000, valid loss-1.2263, acc-0.6848, test loss-1.2233, acc-0.6910\n",
      "Iter-11380, train loss-1.1660, acc-0.7000, valid loss-1.2255, acc-0.6846, test loss-1.2225, acc-0.6917\n",
      "Iter-11390, train loss-1.4222, acc-0.6000, valid loss-1.2247, acc-0.6848, test loss-1.2217, acc-0.6921\n",
      "Iter-11400, train loss-1.2451, acc-0.6400, valid loss-1.2239, acc-0.6852, test loss-1.2209, acc-0.6924\n",
      "Iter-11410, train loss-1.2739, acc-0.6400, valid loss-1.2231, acc-0.6854, test loss-1.2201, acc-0.6928\n",
      "Iter-11420, train loss-1.3266, acc-0.6000, valid loss-1.2223, acc-0.6858, test loss-1.2192, acc-0.6928\n",
      "Iter-11430, train loss-1.2370, acc-0.6400, valid loss-1.2215, acc-0.6860, test loss-1.2184, acc-0.6930\n",
      "Iter-11440, train loss-1.2560, acc-0.6400, valid loss-1.2207, acc-0.6864, test loss-1.2177, acc-0.6938\n",
      "Iter-11450, train loss-1.1633, acc-0.7400, valid loss-1.2200, acc-0.6870, test loss-1.2169, acc-0.6943\n",
      "Iter-11460, train loss-1.2554, acc-0.6600, valid loss-1.2192, acc-0.6868, test loss-1.2162, acc-0.6943\n",
      "Iter-11470, train loss-1.1639, acc-0.7400, valid loss-1.2184, acc-0.6870, test loss-1.2154, acc-0.6951\n",
      "Iter-11480, train loss-1.2274, acc-0.6800, valid loss-1.2176, acc-0.6874, test loss-1.2146, acc-0.6956\n",
      "Iter-11490, train loss-1.1430, acc-0.7200, valid loss-1.2168, acc-0.6876, test loss-1.2138, acc-0.6955\n",
      "Iter-11500, train loss-1.1224, acc-0.7200, valid loss-1.2160, acc-0.6878, test loss-1.2130, acc-0.6960\n",
      "Iter-11510, train loss-1.2666, acc-0.6600, valid loss-1.2152, acc-0.6884, test loss-1.2122, acc-0.6965\n",
      "Iter-11520, train loss-1.0192, acc-0.7600, valid loss-1.2144, acc-0.6888, test loss-1.2114, acc-0.6966\n",
      "Iter-11530, train loss-1.1627, acc-0.6600, valid loss-1.2136, acc-0.6890, test loss-1.2106, acc-0.6973\n",
      "Iter-11540, train loss-1.3211, acc-0.6600, valid loss-1.2129, acc-0.6894, test loss-1.2098, acc-0.6974\n",
      "Iter-11550, train loss-1.1542, acc-0.7400, valid loss-1.2121, acc-0.6894, test loss-1.2091, acc-0.6981\n",
      "Iter-11560, train loss-1.1916, acc-0.6800, valid loss-1.2113, acc-0.6900, test loss-1.2082, acc-0.6984\n",
      "Iter-11570, train loss-1.0952, acc-0.7400, valid loss-1.2104, acc-0.6900, test loss-1.2074, acc-0.6988\n",
      "Iter-11580, train loss-1.2962, acc-0.6600, valid loss-1.2096, acc-0.6900, test loss-1.2066, acc-0.6990\n",
      "Iter-11590, train loss-1.2115, acc-0.6800, valid loss-1.2089, acc-0.6898, test loss-1.2058, acc-0.6993\n",
      "Iter-11600, train loss-1.1748, acc-0.6800, valid loss-1.2081, acc-0.6900, test loss-1.2051, acc-0.6993\n",
      "Iter-11610, train loss-1.1329, acc-0.7000, valid loss-1.2073, acc-0.6902, test loss-1.2042, acc-0.6998\n",
      "Iter-11620, train loss-1.2159, acc-0.6400, valid loss-1.2065, acc-0.6910, test loss-1.2034, acc-0.6998\n",
      "Iter-11630, train loss-1.1848, acc-0.6800, valid loss-1.2057, acc-0.6914, test loss-1.2026, acc-0.6999\n",
      "Iter-11640, train loss-1.2369, acc-0.7800, valid loss-1.2049, acc-0.6920, test loss-1.2019, acc-0.7002\n",
      "Iter-11650, train loss-1.2639, acc-0.6200, valid loss-1.2042, acc-0.6922, test loss-1.2012, acc-0.7003\n",
      "Iter-11660, train loss-1.0455, acc-0.7600, valid loss-1.2034, acc-0.6922, test loss-1.2004, acc-0.7004\n",
      "Iter-11670, train loss-1.1469, acc-0.7200, valid loss-1.2026, acc-0.6926, test loss-1.1996, acc-0.7009\n",
      "Iter-11680, train loss-1.2492, acc-0.7000, valid loss-1.2018, acc-0.6928, test loss-1.1988, acc-0.7011\n",
      "Iter-11690, train loss-1.2441, acc-0.7200, valid loss-1.2010, acc-0.6930, test loss-1.1980, acc-0.7014\n",
      "Iter-11700, train loss-1.2743, acc-0.6600, valid loss-1.2002, acc-0.6932, test loss-1.1972, acc-0.7015\n",
      "Iter-11710, train loss-1.1547, acc-0.7400, valid loss-1.1994, acc-0.6934, test loss-1.1964, acc-0.7017\n",
      "Iter-11720, train loss-1.2077, acc-0.6200, valid loss-1.1986, acc-0.6940, test loss-1.1957, acc-0.7018\n",
      "Iter-11730, train loss-1.2586, acc-0.6600, valid loss-1.1979, acc-0.6944, test loss-1.1950, acc-0.7023\n",
      "Iter-11740, train loss-1.1630, acc-0.7200, valid loss-1.1971, acc-0.6942, test loss-1.1942, acc-0.7027\n",
      "Iter-11750, train loss-1.1586, acc-0.7200, valid loss-1.1963, acc-0.6946, test loss-1.1934, acc-0.7031\n",
      "Iter-11760, train loss-1.1006, acc-0.8200, valid loss-1.1954, acc-0.6946, test loss-1.1925, acc-0.7031\n",
      "Iter-11770, train loss-1.2277, acc-0.6800, valid loss-1.1947, acc-0.6950, test loss-1.1918, acc-0.7032\n",
      "Iter-11780, train loss-1.3492, acc-0.6200, valid loss-1.1940, acc-0.6952, test loss-1.1911, acc-0.7032\n",
      "Iter-11790, train loss-1.0699, acc-0.7200, valid loss-1.1932, acc-0.6954, test loss-1.1903, acc-0.7031\n",
      "Iter-11800, train loss-1.2176, acc-0.6800, valid loss-1.1924, acc-0.6954, test loss-1.1895, acc-0.7038\n",
      "Iter-11810, train loss-1.2009, acc-0.6800, valid loss-1.1916, acc-0.6956, test loss-1.1887, acc-0.7042\n",
      "Iter-11820, train loss-1.1384, acc-0.7000, valid loss-1.1908, acc-0.6962, test loss-1.1879, acc-0.7045\n",
      "Iter-11830, train loss-1.1982, acc-0.6800, valid loss-1.1900, acc-0.6968, test loss-1.1872, acc-0.7046\n",
      "Iter-11840, train loss-1.3073, acc-0.6400, valid loss-1.1893, acc-0.6970, test loss-1.1864, acc-0.7053\n",
      "Iter-11850, train loss-1.1441, acc-0.7600, valid loss-1.1885, acc-0.6970, test loss-1.1857, acc-0.7057\n",
      "Iter-11860, train loss-1.3285, acc-0.6200, valid loss-1.1878, acc-0.6976, test loss-1.1849, acc-0.7059\n",
      "Iter-11870, train loss-1.3128, acc-0.6600, valid loss-1.1871, acc-0.6976, test loss-1.1842, acc-0.7060\n",
      "Iter-11880, train loss-1.0734, acc-0.7000, valid loss-1.1863, acc-0.6976, test loss-1.1835, acc-0.7062\n",
      "Iter-11890, train loss-1.3178, acc-0.6600, valid loss-1.1855, acc-0.6980, test loss-1.1827, acc-0.7063\n",
      "Iter-11900, train loss-1.2682, acc-0.6600, valid loss-1.1848, acc-0.6982, test loss-1.1819, acc-0.7065\n",
      "Iter-11910, train loss-1.1372, acc-0.7000, valid loss-1.1840, acc-0.6984, test loss-1.1811, acc-0.7068\n",
      "Iter-11920, train loss-1.0886, acc-0.7800, valid loss-1.1832, acc-0.6984, test loss-1.1803, acc-0.7070\n",
      "Iter-11930, train loss-1.3090, acc-0.6600, valid loss-1.1824, acc-0.6982, test loss-1.1796, acc-0.7073\n",
      "Iter-11940, train loss-1.1946, acc-0.6800, valid loss-1.1816, acc-0.6984, test loss-1.1788, acc-0.7073\n",
      "Iter-11950, train loss-1.2202, acc-0.6400, valid loss-1.1809, acc-0.6988, test loss-1.1781, acc-0.7079\n",
      "Iter-11960, train loss-1.2613, acc-0.7200, valid loss-1.1801, acc-0.6994, test loss-1.1773, acc-0.7084\n",
      "Iter-11970, train loss-1.1236, acc-0.7200, valid loss-1.1793, acc-0.7000, test loss-1.1765, acc-0.7087\n",
      "Iter-11980, train loss-1.3307, acc-0.6200, valid loss-1.1786, acc-0.7000, test loss-1.1758, acc-0.7086\n",
      "Iter-11990, train loss-1.1269, acc-0.7200, valid loss-1.1778, acc-0.7004, test loss-1.1750, acc-0.7089\n",
      "Iter-12000, train loss-1.2446, acc-0.6400, valid loss-1.1771, acc-0.7002, test loss-1.1743, acc-0.7095\n",
      "Iter-12010, train loss-1.0686, acc-0.7400, valid loss-1.1763, acc-0.7006, test loss-1.1735, acc-0.7097\n",
      "Iter-12020, train loss-1.3192, acc-0.6200, valid loss-1.1755, acc-0.7012, test loss-1.1727, acc-0.7098\n",
      "Iter-12030, train loss-1.1676, acc-0.7200, valid loss-1.1747, acc-0.7012, test loss-1.1720, acc-0.7100\n",
      "Iter-12040, train loss-1.1439, acc-0.7200, valid loss-1.1740, acc-0.7018, test loss-1.1713, acc-0.7102\n",
      "Iter-12050, train loss-1.0752, acc-0.7800, valid loss-1.1733, acc-0.7022, test loss-1.1706, acc-0.7103\n",
      "Iter-12060, train loss-1.1594, acc-0.7600, valid loss-1.1725, acc-0.7028, test loss-1.1698, acc-0.7107\n",
      "Iter-12070, train loss-1.1643, acc-0.6800, valid loss-1.1717, acc-0.7030, test loss-1.1690, acc-0.7108\n",
      "Iter-12080, train loss-1.2528, acc-0.6800, valid loss-1.1710, acc-0.7026, test loss-1.1683, acc-0.7113\n",
      "Iter-12090, train loss-1.0543, acc-0.7200, valid loss-1.1702, acc-0.7038, test loss-1.1675, acc-0.7113\n",
      "Iter-12100, train loss-1.1896, acc-0.6400, valid loss-1.1694, acc-0.7044, test loss-1.1667, acc-0.7116\n",
      "Iter-12110, train loss-1.1227, acc-0.6800, valid loss-1.1687, acc-0.7044, test loss-1.1660, acc-0.7116\n",
      "Iter-12120, train loss-1.2399, acc-0.6800, valid loss-1.1680, acc-0.7048, test loss-1.1653, acc-0.7121\n",
      "Iter-12130, train loss-1.1619, acc-0.6400, valid loss-1.1673, acc-0.7054, test loss-1.1646, acc-0.7128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-12140, train loss-1.0862, acc-0.7200, valid loss-1.1665, acc-0.7056, test loss-1.1639, acc-0.7135\n",
      "Iter-12150, train loss-1.1775, acc-0.7200, valid loss-1.1657, acc-0.7058, test loss-1.1631, acc-0.7142\n",
      "Iter-12160, train loss-1.1992, acc-0.6200, valid loss-1.1650, acc-0.7054, test loss-1.1623, acc-0.7139\n",
      "Iter-12170, train loss-1.0745, acc-0.8400, valid loss-1.1643, acc-0.7060, test loss-1.1616, acc-0.7139\n",
      "Iter-12180, train loss-1.3207, acc-0.5600, valid loss-1.1635, acc-0.7062, test loss-1.1609, acc-0.7143\n",
      "Iter-12190, train loss-1.0330, acc-0.8000, valid loss-1.1628, acc-0.7064, test loss-1.1602, acc-0.7142\n",
      "Iter-12200, train loss-1.0903, acc-0.7400, valid loss-1.1621, acc-0.7066, test loss-1.1595, acc-0.7144\n",
      "Iter-12210, train loss-1.0573, acc-0.7400, valid loss-1.1614, acc-0.7066, test loss-1.1587, acc-0.7149\n",
      "Iter-12220, train loss-1.2172, acc-0.7200, valid loss-1.1605, acc-0.7068, test loss-1.1580, acc-0.7151\n",
      "Iter-12230, train loss-1.2937, acc-0.5000, valid loss-1.1599, acc-0.7066, test loss-1.1573, acc-0.7151\n",
      "Iter-12240, train loss-1.1806, acc-0.6600, valid loss-1.1591, acc-0.7070, test loss-1.1565, acc-0.7155\n",
      "Iter-12250, train loss-1.1016, acc-0.7400, valid loss-1.1583, acc-0.7072, test loss-1.1558, acc-0.7161\n",
      "Iter-12260, train loss-1.1791, acc-0.7200, valid loss-1.1576, acc-0.7074, test loss-1.1550, acc-0.7159\n",
      "Iter-12270, train loss-1.1321, acc-0.6400, valid loss-1.1569, acc-0.7076, test loss-1.1544, acc-0.7160\n",
      "Iter-12280, train loss-1.0641, acc-0.7400, valid loss-1.1562, acc-0.7086, test loss-1.1536, acc-0.7160\n",
      "Iter-12290, train loss-1.1924, acc-0.7000, valid loss-1.1555, acc-0.7084, test loss-1.1529, acc-0.7162\n",
      "Iter-12300, train loss-1.0841, acc-0.8200, valid loss-1.1547, acc-0.7090, test loss-1.1521, acc-0.7165\n",
      "Iter-12310, train loss-1.0611, acc-0.7600, valid loss-1.1540, acc-0.7092, test loss-1.1514, acc-0.7166\n",
      "Iter-12320, train loss-1.1384, acc-0.7000, valid loss-1.1533, acc-0.7094, test loss-1.1508, acc-0.7168\n",
      "Iter-12330, train loss-0.9436, acc-0.8400, valid loss-1.1525, acc-0.7098, test loss-1.1500, acc-0.7173\n",
      "Iter-12340, train loss-1.0642, acc-0.7800, valid loss-1.1518, acc-0.7102, test loss-1.1493, acc-0.7175\n",
      "Iter-12350, train loss-1.1837, acc-0.6800, valid loss-1.1511, acc-0.7104, test loss-1.1485, acc-0.7181\n",
      "Iter-12360, train loss-1.0663, acc-0.7200, valid loss-1.1503, acc-0.7104, test loss-1.1478, acc-0.7181\n",
      "Iter-12370, train loss-1.2160, acc-0.6800, valid loss-1.1496, acc-0.7102, test loss-1.1471, acc-0.7181\n",
      "Iter-12380, train loss-1.2019, acc-0.6200, valid loss-1.1489, acc-0.7110, test loss-1.1464, acc-0.7184\n",
      "Iter-12390, train loss-1.2378, acc-0.6600, valid loss-1.1481, acc-0.7104, test loss-1.1456, acc-0.7183\n",
      "Iter-12400, train loss-1.0208, acc-0.7600, valid loss-1.1474, acc-0.7112, test loss-1.1449, acc-0.7183\n",
      "Iter-12410, train loss-1.3022, acc-0.6000, valid loss-1.1467, acc-0.7116, test loss-1.1442, acc-0.7186\n",
      "Iter-12420, train loss-1.0902, acc-0.7000, valid loss-1.1460, acc-0.7110, test loss-1.1434, acc-0.7184\n",
      "Iter-12430, train loss-1.2501, acc-0.7400, valid loss-1.1452, acc-0.7114, test loss-1.1427, acc-0.7185\n",
      "Iter-12440, train loss-1.1440, acc-0.6200, valid loss-1.1445, acc-0.7120, test loss-1.1420, acc-0.7185\n",
      "Iter-12450, train loss-1.0193, acc-0.7600, valid loss-1.1438, acc-0.7122, test loss-1.1413, acc-0.7187\n",
      "Iter-12460, train loss-1.1156, acc-0.7200, valid loss-1.1431, acc-0.7122, test loss-1.1406, acc-0.7190\n",
      "Iter-12470, train loss-1.2041, acc-0.7400, valid loss-1.1424, acc-0.7128, test loss-1.1399, acc-0.7192\n",
      "Iter-12480, train loss-1.2234, acc-0.6600, valid loss-1.1416, acc-0.7136, test loss-1.1392, acc-0.7194\n",
      "Iter-12490, train loss-1.1340, acc-0.7800, valid loss-1.1409, acc-0.7138, test loss-1.1384, acc-0.7196\n",
      "Iter-12500, train loss-1.1680, acc-0.7000, valid loss-1.1402, acc-0.7138, test loss-1.1377, acc-0.7197\n",
      "Iter-12510, train loss-1.1672, acc-0.7200, valid loss-1.1395, acc-0.7140, test loss-1.1370, acc-0.7198\n",
      "Iter-12520, train loss-1.0107, acc-0.8200, valid loss-1.1387, acc-0.7142, test loss-1.1362, acc-0.7198\n",
      "Iter-12530, train loss-1.1882, acc-0.6800, valid loss-1.1380, acc-0.7142, test loss-1.1355, acc-0.7203\n",
      "Iter-12540, train loss-1.1457, acc-0.7200, valid loss-1.1372, acc-0.7144, test loss-1.1348, acc-0.7205\n",
      "Iter-12550, train loss-1.0161, acc-0.8000, valid loss-1.1365, acc-0.7146, test loss-1.1340, acc-0.7205\n",
      "Iter-12560, train loss-1.1498, acc-0.7000, valid loss-1.1358, acc-0.7146, test loss-1.1333, acc-0.7207\n",
      "Iter-12570, train loss-1.1733, acc-0.7200, valid loss-1.1350, acc-0.7148, test loss-1.1326, acc-0.7208\n",
      "Iter-12580, train loss-1.2305, acc-0.6200, valid loss-1.1343, acc-0.7156, test loss-1.1319, acc-0.7211\n",
      "Iter-12590, train loss-0.9617, acc-0.7600, valid loss-1.1336, acc-0.7158, test loss-1.1312, acc-0.7214\n",
      "Iter-12600, train loss-1.1460, acc-0.6600, valid loss-1.1329, acc-0.7162, test loss-1.1305, acc-0.7216\n",
      "Iter-12610, train loss-1.1199, acc-0.7600, valid loss-1.1322, acc-0.7162, test loss-1.1298, acc-0.7218\n",
      "Iter-12620, train loss-1.0940, acc-0.7400, valid loss-1.1315, acc-0.7164, test loss-1.1291, acc-0.7218\n",
      "Iter-12630, train loss-1.2368, acc-0.6600, valid loss-1.1309, acc-0.7166, test loss-1.1285, acc-0.7224\n",
      "Iter-12640, train loss-1.1134, acc-0.7000, valid loss-1.1302, acc-0.7166, test loss-1.1278, acc-0.7228\n",
      "Iter-12650, train loss-1.0523, acc-0.8000, valid loss-1.1294, acc-0.7168, test loss-1.1271, acc-0.7228\n",
      "Iter-12660, train loss-1.1477, acc-0.6600, valid loss-1.1287, acc-0.7176, test loss-1.1264, acc-0.7231\n",
      "Iter-12670, train loss-1.1285, acc-0.7200, valid loss-1.1281, acc-0.7176, test loss-1.1257, acc-0.7230\n",
      "Iter-12680, train loss-1.1917, acc-0.7000, valid loss-1.1274, acc-0.7184, test loss-1.1250, acc-0.7228\n",
      "Iter-12690, train loss-1.2060, acc-0.7200, valid loss-1.1267, acc-0.7184, test loss-1.1244, acc-0.7230\n",
      "Iter-12700, train loss-1.0697, acc-0.7600, valid loss-1.1259, acc-0.7188, test loss-1.1236, acc-0.7231\n",
      "Iter-12710, train loss-1.3496, acc-0.6000, valid loss-1.1253, acc-0.7192, test loss-1.1230, acc-0.7231\n",
      "Iter-12720, train loss-1.1002, acc-0.7000, valid loss-1.1245, acc-0.7194, test loss-1.1223, acc-0.7233\n",
      "Iter-12730, train loss-1.2090, acc-0.6800, valid loss-1.1238, acc-0.7196, test loss-1.1215, acc-0.7236\n",
      "Iter-12740, train loss-0.9843, acc-0.7800, valid loss-1.1231, acc-0.7200, test loss-1.1208, acc-0.7237\n",
      "Iter-12750, train loss-1.2050, acc-0.6800, valid loss-1.1224, acc-0.7204, test loss-1.1201, acc-0.7240\n",
      "Iter-12760, train loss-1.1219, acc-0.7600, valid loss-1.1217, acc-0.7210, test loss-1.1195, acc-0.7240\n",
      "Iter-12770, train loss-1.3071, acc-0.6200, valid loss-1.1211, acc-0.7210, test loss-1.1188, acc-0.7243\n",
      "Iter-12780, train loss-1.1651, acc-0.7000, valid loss-1.1203, acc-0.7214, test loss-1.1181, acc-0.7246\n",
      "Iter-12790, train loss-1.1425, acc-0.6800, valid loss-1.1197, acc-0.7214, test loss-1.1174, acc-0.7246\n",
      "Iter-12800, train loss-1.1029, acc-0.6800, valid loss-1.1189, acc-0.7216, test loss-1.1167, acc-0.7249\n",
      "Iter-12810, train loss-1.1358, acc-0.7200, valid loss-1.1182, acc-0.7218, test loss-1.1160, acc-0.7250\n",
      "Iter-12820, train loss-1.0925, acc-0.7800, valid loss-1.1176, acc-0.7218, test loss-1.1154, acc-0.7257\n",
      "Iter-12830, train loss-0.9090, acc-0.8400, valid loss-1.1169, acc-0.7218, test loss-1.1147, acc-0.7258\n",
      "Iter-12840, train loss-1.1083, acc-0.6000, valid loss-1.1162, acc-0.7222, test loss-1.1140, acc-0.7259\n",
      "Iter-12850, train loss-1.1623, acc-0.6800, valid loss-1.1155, acc-0.7226, test loss-1.1133, acc-0.7261\n",
      "Iter-12860, train loss-1.1107, acc-0.7200, valid loss-1.1149, acc-0.7230, test loss-1.1126, acc-0.7264\n",
      "Iter-12870, train loss-1.2373, acc-0.6400, valid loss-1.1142, acc-0.7230, test loss-1.1120, acc-0.7263\n",
      "Iter-12880, train loss-1.0258, acc-0.7600, valid loss-1.1135, acc-0.7228, test loss-1.1113, acc-0.7265\n",
      "Iter-12890, train loss-0.9784, acc-0.7800, valid loss-1.1128, acc-0.7232, test loss-1.1106, acc-0.7270\n",
      "Iter-12900, train loss-1.2036, acc-0.6200, valid loss-1.1121, acc-0.7232, test loss-1.1099, acc-0.7269\n",
      "Iter-12910, train loss-1.0061, acc-0.7800, valid loss-1.1115, acc-0.7234, test loss-1.1093, acc-0.7271\n",
      "Iter-12920, train loss-1.0805, acc-0.7400, valid loss-1.1108, acc-0.7232, test loss-1.1086, acc-0.7273\n",
      "Iter-12930, train loss-1.2344, acc-0.6200, valid loss-1.1101, acc-0.7234, test loss-1.1079, acc-0.7271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-12940, train loss-1.1705, acc-0.7200, valid loss-1.1095, acc-0.7234, test loss-1.1073, acc-0.7270\n",
      "Iter-12950, train loss-1.2272, acc-0.6400, valid loss-1.1088, acc-0.7236, test loss-1.1066, acc-0.7273\n",
      "Iter-12960, train loss-1.3007, acc-0.6600, valid loss-1.1082, acc-0.7236, test loss-1.1060, acc-0.7276\n",
      "Iter-12970, train loss-1.0632, acc-0.7000, valid loss-1.1075, acc-0.7236, test loss-1.1053, acc-0.7275\n",
      "Iter-12980, train loss-1.1919, acc-0.6800, valid loss-1.1068, acc-0.7240, test loss-1.1046, acc-0.7272\n",
      "Iter-12990, train loss-1.0421, acc-0.6800, valid loss-1.1062, acc-0.7248, test loss-1.1039, acc-0.7274\n",
      "Iter-13000, train loss-1.1806, acc-0.7200, valid loss-1.1055, acc-0.7250, test loss-1.1033, acc-0.7274\n",
      "Iter-13010, train loss-1.1559, acc-0.7200, valid loss-1.1049, acc-0.7252, test loss-1.1027, acc-0.7278\n",
      "Iter-13020, train loss-1.1552, acc-0.7000, valid loss-1.1043, acc-0.7250, test loss-1.1021, acc-0.7278\n",
      "Iter-13030, train loss-1.1023, acc-0.7800, valid loss-1.1036, acc-0.7254, test loss-1.1014, acc-0.7285\n",
      "Iter-13040, train loss-1.0688, acc-0.7200, valid loss-1.1029, acc-0.7252, test loss-1.1007, acc-0.7289\n",
      "Iter-13050, train loss-1.0395, acc-0.7800, valid loss-1.1022, acc-0.7252, test loss-1.1001, acc-0.7296\n",
      "Iter-13060, train loss-1.0767, acc-0.7400, valid loss-1.1016, acc-0.7256, test loss-1.0994, acc-0.7296\n",
      "Iter-13070, train loss-1.0942, acc-0.7600, valid loss-1.1010, acc-0.7258, test loss-1.0988, acc-0.7298\n",
      "Iter-13080, train loss-1.0655, acc-0.7000, valid loss-1.1003, acc-0.7258, test loss-1.0981, acc-0.7300\n",
      "Iter-13090, train loss-1.1424, acc-0.7000, valid loss-1.0996, acc-0.7264, test loss-1.0974, acc-0.7298\n",
      "Iter-13100, train loss-1.1398, acc-0.6200, valid loss-1.0989, acc-0.7266, test loss-1.0967, acc-0.7300\n",
      "Iter-13110, train loss-1.1028, acc-0.7200, valid loss-1.0982, acc-0.7266, test loss-1.0961, acc-0.7305\n",
      "Iter-13120, train loss-1.0630, acc-0.7400, valid loss-1.0976, acc-0.7266, test loss-1.0954, acc-0.7306\n",
      "Iter-13130, train loss-1.1951, acc-0.6800, valid loss-1.0969, acc-0.7272, test loss-1.0948, acc-0.7310\n",
      "Iter-13140, train loss-1.1185, acc-0.8200, valid loss-1.0963, acc-0.7272, test loss-1.0942, acc-0.7308\n",
      "Iter-13150, train loss-1.1467, acc-0.6200, valid loss-1.0956, acc-0.7274, test loss-1.0935, acc-0.7308\n",
      "Iter-13160, train loss-1.1760, acc-0.6400, valid loss-1.0950, acc-0.7280, test loss-1.0929, acc-0.7310\n",
      "Iter-13170, train loss-1.2507, acc-0.6000, valid loss-1.0943, acc-0.7280, test loss-1.0922, acc-0.7311\n",
      "Iter-13180, train loss-1.1119, acc-0.6800, valid loss-1.0937, acc-0.7286, test loss-1.0916, acc-0.7312\n",
      "Iter-13190, train loss-1.2546, acc-0.6600, valid loss-1.0931, acc-0.7288, test loss-1.0910, acc-0.7312\n",
      "Iter-13200, train loss-1.1005, acc-0.7000, valid loss-1.0924, acc-0.7286, test loss-1.0903, acc-0.7314\n",
      "Iter-13210, train loss-1.0517, acc-0.6800, valid loss-1.0918, acc-0.7292, test loss-1.0897, acc-0.7319\n",
      "Iter-13220, train loss-1.0841, acc-0.7200, valid loss-1.0911, acc-0.7294, test loss-1.0891, acc-0.7318\n",
      "Iter-13230, train loss-0.9088, acc-0.8600, valid loss-1.0905, acc-0.7296, test loss-1.0884, acc-0.7319\n",
      "Iter-13240, train loss-1.1144, acc-0.6800, valid loss-1.0899, acc-0.7300, test loss-1.0878, acc-0.7320\n",
      "Iter-13250, train loss-1.0131, acc-0.8400, valid loss-1.0892, acc-0.7302, test loss-1.0872, acc-0.7324\n",
      "Iter-13260, train loss-1.2166, acc-0.7400, valid loss-1.0886, acc-0.7306, test loss-1.0865, acc-0.7325\n",
      "Iter-13270, train loss-1.1581, acc-0.6600, valid loss-1.0879, acc-0.7310, test loss-1.0859, acc-0.7326\n",
      "Iter-13280, train loss-1.1473, acc-0.7000, valid loss-1.0872, acc-0.7312, test loss-1.0852, acc-0.7329\n",
      "Iter-13290, train loss-1.1264, acc-0.7400, valid loss-1.0866, acc-0.7310, test loss-1.0846, acc-0.7330\n",
      "Iter-13300, train loss-1.1153, acc-0.7800, valid loss-1.0860, acc-0.7318, test loss-1.0839, acc-0.7330\n",
      "Iter-13310, train loss-1.0827, acc-0.8000, valid loss-1.0853, acc-0.7324, test loss-1.0833, acc-0.7330\n",
      "Iter-13320, train loss-1.2316, acc-0.7200, valid loss-1.0847, acc-0.7326, test loss-1.0826, acc-0.7331\n",
      "Iter-13330, train loss-0.9169, acc-0.7800, valid loss-1.0840, acc-0.7332, test loss-1.0820, acc-0.7335\n",
      "Iter-13340, train loss-1.1799, acc-0.6800, valid loss-1.0834, acc-0.7332, test loss-1.0814, acc-0.7336\n",
      "Iter-13350, train loss-1.0368, acc-0.6600, valid loss-1.0827, acc-0.7334, test loss-1.0808, acc-0.7334\n",
      "Iter-13360, train loss-1.0925, acc-0.6800, valid loss-1.0821, acc-0.7332, test loss-1.0801, acc-0.7334\n",
      "Iter-13370, train loss-1.0865, acc-0.7600, valid loss-1.0815, acc-0.7334, test loss-1.0795, acc-0.7335\n",
      "Iter-13380, train loss-0.9920, acc-0.8000, valid loss-1.0809, acc-0.7338, test loss-1.0789, acc-0.7337\n",
      "Iter-13390, train loss-1.0159, acc-0.8000, valid loss-1.0802, acc-0.7340, test loss-1.0782, acc-0.7340\n",
      "Iter-13400, train loss-1.0998, acc-0.7200, valid loss-1.0795, acc-0.7340, test loss-1.0776, acc-0.7344\n",
      "Iter-13410, train loss-1.1439, acc-0.6800, valid loss-1.0789, acc-0.7340, test loss-1.0770, acc-0.7349\n",
      "Iter-13420, train loss-1.1845, acc-0.7000, valid loss-1.0782, acc-0.7340, test loss-1.0763, acc-0.7352\n",
      "Iter-13430, train loss-1.1877, acc-0.7400, valid loss-1.0776, acc-0.7344, test loss-1.0756, acc-0.7353\n",
      "Iter-13440, train loss-1.1653, acc-0.6800, valid loss-1.0769, acc-0.7344, test loss-1.0750, acc-0.7358\n",
      "Iter-13450, train loss-1.1492, acc-0.7200, valid loss-1.0763, acc-0.7340, test loss-1.0743, acc-0.7360\n",
      "Iter-13460, train loss-1.1628, acc-0.6600, valid loss-1.0756, acc-0.7344, test loss-1.0737, acc-0.7361\n",
      "Iter-13470, train loss-1.0473, acc-0.7600, valid loss-1.0750, acc-0.7344, test loss-1.0730, acc-0.7361\n",
      "Iter-13480, train loss-1.1301, acc-0.7200, valid loss-1.0743, acc-0.7346, test loss-1.0724, acc-0.7361\n",
      "Iter-13490, train loss-1.0073, acc-0.7200, valid loss-1.0737, acc-0.7350, test loss-1.0717, acc-0.7370\n",
      "Iter-13500, train loss-0.9816, acc-0.7600, valid loss-1.0730, acc-0.7354, test loss-1.0711, acc-0.7368\n",
      "Iter-13510, train loss-1.0269, acc-0.7800, valid loss-1.0724, acc-0.7354, test loss-1.0705, acc-0.7372\n",
      "Iter-13520, train loss-1.2252, acc-0.6200, valid loss-1.0718, acc-0.7358, test loss-1.0699, acc-0.7373\n",
      "Iter-13530, train loss-1.2008, acc-0.6400, valid loss-1.0711, acc-0.7358, test loss-1.0692, acc-0.7379\n",
      "Iter-13540, train loss-1.1649, acc-0.7800, valid loss-1.0705, acc-0.7362, test loss-1.0686, acc-0.7382\n",
      "Iter-13550, train loss-1.1859, acc-0.6600, valid loss-1.0698, acc-0.7364, test loss-1.0680, acc-0.7381\n",
      "Iter-13560, train loss-0.9147, acc-0.8600, valid loss-1.0692, acc-0.7366, test loss-1.0673, acc-0.7383\n",
      "Iter-13570, train loss-1.1060, acc-0.6400, valid loss-1.0686, acc-0.7372, test loss-1.0668, acc-0.7384\n",
      "Iter-13580, train loss-1.0273, acc-0.7200, valid loss-1.0680, acc-0.7370, test loss-1.0661, acc-0.7387\n",
      "Iter-13590, train loss-0.9808, acc-0.7400, valid loss-1.0673, acc-0.7372, test loss-1.0655, acc-0.7385\n",
      "Iter-13600, train loss-1.0217, acc-0.7800, valid loss-1.0666, acc-0.7374, test loss-1.0648, acc-0.7387\n",
      "Iter-13610, train loss-1.1522, acc-0.6200, valid loss-1.0660, acc-0.7374, test loss-1.0642, acc-0.7388\n",
      "Iter-13620, train loss-1.0411, acc-0.8200, valid loss-1.0653, acc-0.7380, test loss-1.0635, acc-0.7393\n",
      "Iter-13630, train loss-1.0782, acc-0.7000, valid loss-1.0647, acc-0.7380, test loss-1.0629, acc-0.7395\n",
      "Iter-13640, train loss-1.2333, acc-0.6600, valid loss-1.0641, acc-0.7382, test loss-1.0623, acc-0.7396\n",
      "Iter-13650, train loss-1.1253, acc-0.6600, valid loss-1.0635, acc-0.7382, test loss-1.0616, acc-0.7398\n",
      "Iter-13660, train loss-0.9739, acc-0.8400, valid loss-1.0629, acc-0.7382, test loss-1.0611, acc-0.7400\n",
      "Iter-13670, train loss-1.0881, acc-0.7600, valid loss-1.0623, acc-0.7382, test loss-1.0605, acc-0.7398\n",
      "Iter-13680, train loss-1.0822, acc-0.6800, valid loss-1.0617, acc-0.7384, test loss-1.0599, acc-0.7400\n",
      "Iter-13690, train loss-0.8698, acc-0.8200, valid loss-1.0611, acc-0.7382, test loss-1.0593, acc-0.7403\n",
      "Iter-13700, train loss-0.9514, acc-0.7800, valid loss-1.0605, acc-0.7384, test loss-1.0587, acc-0.7403\n",
      "Iter-13710, train loss-1.1494, acc-0.6600, valid loss-1.0599, acc-0.7384, test loss-1.0581, acc-0.7402\n",
      "Iter-13720, train loss-1.2816, acc-0.6400, valid loss-1.0592, acc-0.7384, test loss-1.0574, acc-0.7407\n",
      "Iter-13730, train loss-0.9954, acc-0.7400, valid loss-1.0586, acc-0.7384, test loss-1.0568, acc-0.7407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13740, train loss-0.9678, acc-0.7600, valid loss-1.0580, acc-0.7382, test loss-1.0562, acc-0.7409\n",
      "Iter-13750, train loss-1.1445, acc-0.6600, valid loss-1.0574, acc-0.7384, test loss-1.0556, acc-0.7414\n",
      "Iter-13760, train loss-1.2173, acc-0.6400, valid loss-1.0568, acc-0.7384, test loss-1.0550, acc-0.7417\n",
      "Iter-13770, train loss-0.9840, acc-0.7800, valid loss-1.0562, acc-0.7382, test loss-1.0544, acc-0.7415\n",
      "Iter-13780, train loss-1.1272, acc-0.7400, valid loss-1.0555, acc-0.7384, test loss-1.0538, acc-0.7414\n",
      "Iter-13790, train loss-1.1550, acc-0.7000, valid loss-1.0549, acc-0.7388, test loss-1.0531, acc-0.7418\n",
      "Iter-13800, train loss-1.2764, acc-0.5800, valid loss-1.0543, acc-0.7392, test loss-1.0525, acc-0.7419\n",
      "Iter-13810, train loss-1.0879, acc-0.6600, valid loss-1.0537, acc-0.7390, test loss-1.0519, acc-0.7420\n",
      "Iter-13820, train loss-0.9556, acc-0.7800, valid loss-1.0530, acc-0.7394, test loss-1.0513, acc-0.7421\n",
      "Iter-13830, train loss-1.0901, acc-0.7000, valid loss-1.0524, acc-0.7394, test loss-1.0507, acc-0.7426\n",
      "Iter-13840, train loss-1.1302, acc-0.6400, valid loss-1.0519, acc-0.7398, test loss-1.0501, acc-0.7424\n",
      "Iter-13850, train loss-1.1397, acc-0.6400, valid loss-1.0513, acc-0.7400, test loss-1.0496, acc-0.7426\n",
      "Iter-13860, train loss-1.0172, acc-0.7600, valid loss-1.0507, acc-0.7400, test loss-1.0490, acc-0.7427\n",
      "Iter-13870, train loss-0.9584, acc-0.8600, valid loss-1.0501, acc-0.7404, test loss-1.0484, acc-0.7428\n",
      "Iter-13880, train loss-1.1233, acc-0.6600, valid loss-1.0495, acc-0.7406, test loss-1.0478, acc-0.7430\n",
      "Iter-13890, train loss-1.1012, acc-0.7400, valid loss-1.0489, acc-0.7406, test loss-1.0472, acc-0.7432\n",
      "Iter-13900, train loss-1.0389, acc-0.7200, valid loss-1.0483, acc-0.7410, test loss-1.0466, acc-0.7438\n",
      "Iter-13910, train loss-1.1363, acc-0.7600, valid loss-1.0477, acc-0.7410, test loss-1.0460, acc-0.7441\n",
      "Iter-13920, train loss-0.9164, acc-0.8000, valid loss-1.0471, acc-0.7410, test loss-1.0454, acc-0.7443\n",
      "Iter-13930, train loss-1.2480, acc-0.6400, valid loss-1.0465, acc-0.7412, test loss-1.0448, acc-0.7442\n",
      "Iter-13940, train loss-1.0450, acc-0.7200, valid loss-1.0459, acc-0.7412, test loss-1.0442, acc-0.7444\n",
      "Iter-13950, train loss-1.0109, acc-0.7400, valid loss-1.0453, acc-0.7412, test loss-1.0436, acc-0.7446\n",
      "Iter-13960, train loss-0.9639, acc-0.8400, valid loss-1.0447, acc-0.7408, test loss-1.0430, acc-0.7446\n",
      "Iter-13970, train loss-1.2313, acc-0.6600, valid loss-1.0441, acc-0.7410, test loss-1.0424, acc-0.7449\n",
      "Iter-13980, train loss-1.0031, acc-0.8000, valid loss-1.0435, acc-0.7416, test loss-1.0419, acc-0.7451\n",
      "Iter-13990, train loss-1.0163, acc-0.7600, valid loss-1.0430, acc-0.7420, test loss-1.0413, acc-0.7457\n",
      "Iter-14000, train loss-1.2029, acc-0.6400, valid loss-1.0424, acc-0.7422, test loss-1.0407, acc-0.7461\n",
      "Iter-14010, train loss-1.1373, acc-0.7000, valid loss-1.0418, acc-0.7424, test loss-1.0402, acc-0.7463\n",
      "Iter-14020, train loss-1.1849, acc-0.6400, valid loss-1.0412, acc-0.7422, test loss-1.0396, acc-0.7465\n",
      "Iter-14030, train loss-1.1525, acc-0.6800, valid loss-1.0407, acc-0.7426, test loss-1.0390, acc-0.7468\n",
      "Iter-14040, train loss-1.1192, acc-0.7000, valid loss-1.0401, acc-0.7430, test loss-1.0384, acc-0.7473\n",
      "Iter-14050, train loss-1.2104, acc-0.7000, valid loss-1.0395, acc-0.7428, test loss-1.0379, acc-0.7473\n",
      "Iter-14060, train loss-0.9963, acc-0.8200, valid loss-1.0390, acc-0.7428, test loss-1.0374, acc-0.7470\n",
      "Iter-14070, train loss-1.2586, acc-0.6400, valid loss-1.0384, acc-0.7428, test loss-1.0368, acc-0.7469\n",
      "Iter-14080, train loss-1.0151, acc-0.7400, valid loss-1.0378, acc-0.7428, test loss-1.0362, acc-0.7471\n",
      "Iter-14090, train loss-1.0072, acc-0.7600, valid loss-1.0371, acc-0.7428, test loss-1.0356, acc-0.7472\n",
      "Iter-14100, train loss-1.0480, acc-0.7400, valid loss-1.0365, acc-0.7428, test loss-1.0350, acc-0.7473\n",
      "Iter-14110, train loss-1.1722, acc-0.7000, valid loss-1.0360, acc-0.7426, test loss-1.0344, acc-0.7474\n",
      "Iter-14120, train loss-1.0678, acc-0.7400, valid loss-1.0354, acc-0.7426, test loss-1.0338, acc-0.7480\n",
      "Iter-14130, train loss-1.1508, acc-0.6600, valid loss-1.0348, acc-0.7430, test loss-1.0332, acc-0.7479\n",
      "Iter-14140, train loss-0.9986, acc-0.7600, valid loss-1.0342, acc-0.7430, test loss-1.0327, acc-0.7480\n",
      "Iter-14150, train loss-1.0486, acc-0.7000, valid loss-1.0336, acc-0.7436, test loss-1.0321, acc-0.7481\n",
      "Iter-14160, train loss-0.9172, acc-0.8200, valid loss-1.0330, acc-0.7434, test loss-1.0315, acc-0.7481\n",
      "Iter-14170, train loss-0.9804, acc-0.7800, valid loss-1.0324, acc-0.7434, test loss-1.0310, acc-0.7480\n",
      "Iter-14180, train loss-0.8366, acc-0.8400, valid loss-1.0318, acc-0.7436, test loss-1.0304, acc-0.7481\n",
      "Iter-14190, train loss-1.1051, acc-0.7400, valid loss-1.0313, acc-0.7436, test loss-1.0298, acc-0.7481\n",
      "Iter-14200, train loss-0.9862, acc-0.7600, valid loss-1.0307, acc-0.7438, test loss-1.0292, acc-0.7482\n",
      "Iter-14210, train loss-0.9578, acc-0.7800, valid loss-1.0301, acc-0.7446, test loss-1.0287, acc-0.7484\n",
      "Iter-14220, train loss-1.0777, acc-0.8000, valid loss-1.0296, acc-0.7448, test loss-1.0281, acc-0.7484\n",
      "Iter-14230, train loss-0.9526, acc-0.7400, valid loss-1.0290, acc-0.7450, test loss-1.0275, acc-0.7484\n",
      "Iter-14240, train loss-0.9860, acc-0.7200, valid loss-1.0284, acc-0.7450, test loss-1.0270, acc-0.7485\n",
      "Iter-14250, train loss-1.1946, acc-0.6000, valid loss-1.0278, acc-0.7458, test loss-1.0264, acc-0.7488\n",
      "Iter-14260, train loss-0.9316, acc-0.7800, valid loss-1.0273, acc-0.7460, test loss-1.0259, acc-0.7491\n",
      "Iter-14270, train loss-0.9516, acc-0.8200, valid loss-1.0267, acc-0.7462, test loss-1.0253, acc-0.7491\n",
      "Iter-14280, train loss-1.1476, acc-0.6400, valid loss-1.0262, acc-0.7462, test loss-1.0248, acc-0.7491\n",
      "Iter-14290, train loss-1.0537, acc-0.7200, valid loss-1.0257, acc-0.7466, test loss-1.0242, acc-0.7493\n",
      "Iter-14300, train loss-0.9671, acc-0.8200, valid loss-1.0251, acc-0.7462, test loss-1.0236, acc-0.7495\n",
      "Iter-14310, train loss-1.1155, acc-0.6800, valid loss-1.0245, acc-0.7468, test loss-1.0231, acc-0.7497\n",
      "Iter-14320, train loss-0.9566, acc-0.8000, valid loss-1.0240, acc-0.7466, test loss-1.0225, acc-0.7503\n",
      "Iter-14330, train loss-0.8897, acc-0.7800, valid loss-1.0233, acc-0.7468, test loss-1.0219, acc-0.7503\n",
      "Iter-14340, train loss-1.0429, acc-0.7800, valid loss-1.0228, acc-0.7470, test loss-1.0213, acc-0.7503\n",
      "Iter-14350, train loss-1.0527, acc-0.7400, valid loss-1.0222, acc-0.7474, test loss-1.0208, acc-0.7501\n",
      "Iter-14360, train loss-1.0776, acc-0.7000, valid loss-1.0216, acc-0.7472, test loss-1.0203, acc-0.7507\n",
      "Iter-14370, train loss-0.9519, acc-0.8400, valid loss-1.0210, acc-0.7474, test loss-1.0197, acc-0.7506\n",
      "Iter-14380, train loss-0.8776, acc-0.7600, valid loss-1.0205, acc-0.7472, test loss-1.0191, acc-0.7507\n",
      "Iter-14390, train loss-1.0682, acc-0.6800, valid loss-1.0199, acc-0.7480, test loss-1.0186, acc-0.7511\n",
      "Iter-14400, train loss-1.0068, acc-0.7400, valid loss-1.0193, acc-0.7478, test loss-1.0180, acc-0.7513\n",
      "Iter-14410, train loss-1.0192, acc-0.7400, valid loss-1.0188, acc-0.7478, test loss-1.0174, acc-0.7513\n",
      "Iter-14420, train loss-0.9886, acc-0.6800, valid loss-1.0182, acc-0.7482, test loss-1.0168, acc-0.7516\n",
      "Iter-14430, train loss-1.1011, acc-0.7200, valid loss-1.0176, acc-0.7484, test loss-1.0163, acc-0.7514\n",
      "Iter-14440, train loss-1.0064, acc-0.7600, valid loss-1.0170, acc-0.7488, test loss-1.0157, acc-0.7520\n",
      "Iter-14450, train loss-0.9698, acc-0.7600, valid loss-1.0164, acc-0.7488, test loss-1.0151, acc-0.7521\n",
      "Iter-14460, train loss-1.0717, acc-0.7400, valid loss-1.0159, acc-0.7486, test loss-1.0146, acc-0.7522\n",
      "Iter-14470, train loss-0.9799, acc-0.8000, valid loss-1.0153, acc-0.7492, test loss-1.0140, acc-0.7523\n",
      "Iter-14480, train loss-1.0078, acc-0.8000, valid loss-1.0148, acc-0.7494, test loss-1.0135, acc-0.7521\n",
      "Iter-14490, train loss-0.9869, acc-0.7400, valid loss-1.0142, acc-0.7492, test loss-1.0129, acc-0.7522\n",
      "Iter-14500, train loss-0.8773, acc-0.8400, valid loss-1.0136, acc-0.7492, test loss-1.0124, acc-0.7522\n",
      "Iter-14510, train loss-0.8792, acc-0.7400, valid loss-1.0131, acc-0.7492, test loss-1.0118, acc-0.7523\n",
      "Iter-14520, train loss-1.0515, acc-0.7600, valid loss-1.0125, acc-0.7494, test loss-1.0113, acc-0.7527\n",
      "Iter-14530, train loss-0.9901, acc-0.7400, valid loss-1.0120, acc-0.7496, test loss-1.0108, acc-0.7527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-14540, train loss-1.0953, acc-0.7000, valid loss-1.0115, acc-0.7498, test loss-1.0102, acc-0.7527\n",
      "Iter-14550, train loss-1.1074, acc-0.6600, valid loss-1.0109, acc-0.7502, test loss-1.0097, acc-0.7528\n",
      "Iter-14560, train loss-0.7980, acc-0.9000, valid loss-1.0103, acc-0.7502, test loss-1.0091, acc-0.7528\n",
      "Iter-14570, train loss-0.9687, acc-0.7400, valid loss-1.0097, acc-0.7504, test loss-1.0085, acc-0.7530\n",
      "Iter-14580, train loss-1.0682, acc-0.7000, valid loss-1.0092, acc-0.7506, test loss-1.0079, acc-0.7533\n",
      "Iter-14590, train loss-0.9354, acc-0.8200, valid loss-1.0086, acc-0.7510, test loss-1.0074, acc-0.7534\n",
      "Iter-14600, train loss-1.0232, acc-0.7200, valid loss-1.0081, acc-0.7510, test loss-1.0068, acc-0.7535\n",
      "Iter-14610, train loss-1.0025, acc-0.7200, valid loss-1.0075, acc-0.7510, test loss-1.0063, acc-0.7539\n",
      "Iter-14620, train loss-1.0389, acc-0.7400, valid loss-1.0070, acc-0.7512, test loss-1.0058, acc-0.7541\n",
      "Iter-14630, train loss-1.0682, acc-0.6800, valid loss-1.0065, acc-0.7514, test loss-1.0053, acc-0.7541\n",
      "Iter-14640, train loss-0.9874, acc-0.7400, valid loss-1.0059, acc-0.7514, test loss-1.0047, acc-0.7541\n",
      "Iter-14650, train loss-1.0736, acc-0.7000, valid loss-1.0054, acc-0.7514, test loss-1.0042, acc-0.7541\n",
      "Iter-14660, train loss-0.9404, acc-0.7800, valid loss-1.0048, acc-0.7514, test loss-1.0036, acc-0.7543\n",
      "Iter-14670, train loss-0.9253, acc-0.7800, valid loss-1.0043, acc-0.7516, test loss-1.0031, acc-0.7545\n",
      "Iter-14680, train loss-0.8282, acc-0.8400, valid loss-1.0037, acc-0.7514, test loss-1.0025, acc-0.7546\n",
      "Iter-14690, train loss-1.0512, acc-0.7000, valid loss-1.0032, acc-0.7516, test loss-1.0020, acc-0.7546\n",
      "Iter-14700, train loss-0.9263, acc-0.8200, valid loss-1.0027, acc-0.7512, test loss-1.0015, acc-0.7548\n",
      "Iter-14710, train loss-1.0708, acc-0.7200, valid loss-1.0021, acc-0.7518, test loss-1.0009, acc-0.7548\n",
      "Iter-14720, train loss-0.9941, acc-0.7200, valid loss-1.0016, acc-0.7522, test loss-1.0004, acc-0.7552\n",
      "Iter-14730, train loss-0.9535, acc-0.7800, valid loss-1.0010, acc-0.7522, test loss-0.9998, acc-0.7553\n",
      "Iter-14740, train loss-0.9984, acc-0.7600, valid loss-1.0005, acc-0.7522, test loss-0.9993, acc-0.7549\n",
      "Iter-14750, train loss-0.9148, acc-0.8200, valid loss-0.9999, acc-0.7522, test loss-0.9987, acc-0.7551\n",
      "Iter-14760, train loss-0.9797, acc-0.7200, valid loss-0.9993, acc-0.7526, test loss-0.9982, acc-0.7556\n",
      "Iter-14770, train loss-0.8944, acc-0.8600, valid loss-0.9988, acc-0.7526, test loss-0.9976, acc-0.7556\n",
      "Iter-14780, train loss-0.9467, acc-0.7200, valid loss-0.9982, acc-0.7524, test loss-0.9971, acc-0.7560\n",
      "Iter-14790, train loss-0.9753, acc-0.7600, valid loss-0.9977, acc-0.7526, test loss-0.9965, acc-0.7559\n",
      "Iter-14800, train loss-1.0615, acc-0.7400, valid loss-0.9971, acc-0.7526, test loss-0.9960, acc-0.7562\n",
      "Iter-14810, train loss-1.1178, acc-0.7200, valid loss-0.9966, acc-0.7528, test loss-0.9955, acc-0.7564\n",
      "Iter-14820, train loss-1.0011, acc-0.7600, valid loss-0.9962, acc-0.7530, test loss-0.9950, acc-0.7568\n",
      "Iter-14830, train loss-0.8344, acc-0.9200, valid loss-0.9956, acc-0.7530, test loss-0.9944, acc-0.7566\n",
      "Iter-14840, train loss-1.0836, acc-0.7200, valid loss-0.9951, acc-0.7532, test loss-0.9939, acc-0.7564\n",
      "Iter-14850, train loss-0.8594, acc-0.8000, valid loss-0.9946, acc-0.7532, test loss-0.9934, acc-0.7568\n",
      "Iter-14860, train loss-0.9899, acc-0.7800, valid loss-0.9940, acc-0.7534, test loss-0.9929, acc-0.7567\n",
      "Iter-14870, train loss-1.1655, acc-0.6600, valid loss-0.9935, acc-0.7540, test loss-0.9924, acc-0.7568\n",
      "Iter-14880, train loss-0.9571, acc-0.7200, valid loss-0.9930, acc-0.7540, test loss-0.9919, acc-0.7570\n",
      "Iter-14890, train loss-1.0118, acc-0.7200, valid loss-0.9925, acc-0.7538, test loss-0.9913, acc-0.7567\n",
      "Iter-14900, train loss-0.9820, acc-0.7800, valid loss-0.9919, acc-0.7538, test loss-0.9908, acc-0.7570\n",
      "Iter-14910, train loss-1.1252, acc-0.5600, valid loss-0.9914, acc-0.7538, test loss-0.9903, acc-0.7570\n",
      "Iter-14920, train loss-0.9218, acc-0.8200, valid loss-0.9909, acc-0.7536, test loss-0.9898, acc-0.7571\n",
      "Iter-14930, train loss-1.0591, acc-0.7600, valid loss-0.9903, acc-0.7546, test loss-0.9892, acc-0.7570\n",
      "Iter-14940, train loss-0.9777, acc-0.8200, valid loss-0.9898, acc-0.7546, test loss-0.9887, acc-0.7573\n",
      "Iter-14950, train loss-1.0301, acc-0.7600, valid loss-0.9892, acc-0.7548, test loss-0.9882, acc-0.7576\n",
      "Iter-14960, train loss-0.9560, acc-0.8400, valid loss-0.9887, acc-0.7548, test loss-0.9877, acc-0.7577\n",
      "Iter-14970, train loss-0.9572, acc-0.8200, valid loss-0.9882, acc-0.7552, test loss-0.9871, acc-0.7579\n",
      "Iter-14980, train loss-1.1297, acc-0.7400, valid loss-0.9877, acc-0.7548, test loss-0.9866, acc-0.7583\n",
      "Iter-14990, train loss-1.0222, acc-0.7000, valid loss-0.9871, acc-0.7546, test loss-0.9861, acc-0.7586\n",
      "Iter-15000, train loss-1.0733, acc-0.6400, valid loss-0.9866, acc-0.7552, test loss-0.9856, acc-0.7587\n",
      "Iter-15010, train loss-0.9348, acc-0.7600, valid loss-0.9861, acc-0.7556, test loss-0.9850, acc-0.7589\n",
      "Iter-15020, train loss-0.8641, acc-0.8000, valid loss-0.9856, acc-0.7554, test loss-0.9845, acc-0.7589\n",
      "Iter-15030, train loss-1.1243, acc-0.7000, valid loss-0.9850, acc-0.7554, test loss-0.9840, acc-0.7589\n",
      "Iter-15040, train loss-0.8396, acc-0.8000, valid loss-0.9845, acc-0.7554, test loss-0.9834, acc-0.7593\n",
      "Iter-15050, train loss-0.9947, acc-0.7800, valid loss-0.9840, acc-0.7560, test loss-0.9829, acc-0.7592\n",
      "Iter-15060, train loss-1.1937, acc-0.7000, valid loss-0.9835, acc-0.7558, test loss-0.9825, acc-0.7599\n",
      "Iter-15070, train loss-1.0629, acc-0.7000, valid loss-0.9830, acc-0.7558, test loss-0.9820, acc-0.7600\n",
      "Iter-15080, train loss-0.9125, acc-0.8200, valid loss-0.9825, acc-0.7558, test loss-0.9815, acc-0.7601\n",
      "Iter-15090, train loss-1.1053, acc-0.7800, valid loss-0.9820, acc-0.7562, test loss-0.9810, acc-0.7601\n",
      "Iter-15100, train loss-0.9267, acc-0.8000, valid loss-0.9815, acc-0.7562, test loss-0.9804, acc-0.7602\n",
      "Iter-15110, train loss-1.1237, acc-0.7600, valid loss-0.9809, acc-0.7564, test loss-0.9799, acc-0.7602\n",
      "Iter-15120, train loss-1.1408, acc-0.7000, valid loss-0.9804, acc-0.7568, test loss-0.9794, acc-0.7605\n",
      "Iter-15130, train loss-0.9720, acc-0.7800, valid loss-0.9799, acc-0.7568, test loss-0.9788, acc-0.7607\n",
      "Iter-15140, train loss-0.9699, acc-0.8200, valid loss-0.9794, acc-0.7570, test loss-0.9783, acc-0.7610\n",
      "Iter-15150, train loss-0.8986, acc-0.8400, valid loss-0.9789, acc-0.7570, test loss-0.9778, acc-0.7610\n",
      "Iter-15160, train loss-1.0188, acc-0.7600, valid loss-0.9783, acc-0.7570, test loss-0.9773, acc-0.7611\n",
      "Iter-15170, train loss-1.0698, acc-0.6400, valid loss-0.9778, acc-0.7574, test loss-0.9768, acc-0.7611\n",
      "Iter-15180, train loss-1.0476, acc-0.6000, valid loss-0.9773, acc-0.7570, test loss-0.9762, acc-0.7614\n",
      "Iter-15190, train loss-1.1159, acc-0.6800, valid loss-0.9768, acc-0.7572, test loss-0.9757, acc-0.7613\n",
      "Iter-15200, train loss-1.1037, acc-0.6600, valid loss-0.9763, acc-0.7572, test loss-0.9752, acc-0.7614\n",
      "Iter-15210, train loss-1.0272, acc-0.7400, valid loss-0.9757, acc-0.7574, test loss-0.9746, acc-0.7614\n",
      "Iter-15220, train loss-0.9725, acc-0.7200, valid loss-0.9752, acc-0.7574, test loss-0.9741, acc-0.7619\n",
      "Iter-15230, train loss-0.9352, acc-0.7400, valid loss-0.9747, acc-0.7574, test loss-0.9736, acc-0.7621\n",
      "Iter-15240, train loss-0.9779, acc-0.7200, valid loss-0.9742, acc-0.7574, test loss-0.9731, acc-0.7622\n",
      "Iter-15250, train loss-0.9253, acc-0.8000, valid loss-0.9737, acc-0.7574, test loss-0.9726, acc-0.7624\n",
      "Iter-15260, train loss-0.9364, acc-0.7600, valid loss-0.9732, acc-0.7574, test loss-0.9721, acc-0.7627\n",
      "Iter-15270, train loss-1.0959, acc-0.7000, valid loss-0.9726, acc-0.7576, test loss-0.9715, acc-0.7630\n",
      "Iter-15280, train loss-0.9774, acc-0.7400, valid loss-0.9721, acc-0.7576, test loss-0.9710, acc-0.7630\n",
      "Iter-15290, train loss-1.0035, acc-0.7200, valid loss-0.9716, acc-0.7576, test loss-0.9705, acc-0.7633\n",
      "Iter-15300, train loss-0.9779, acc-0.7600, valid loss-0.9711, acc-0.7580, test loss-0.9700, acc-0.7634\n",
      "Iter-15310, train loss-0.9355, acc-0.7600, valid loss-0.9706, acc-0.7580, test loss-0.9695, acc-0.7633\n",
      "Iter-15320, train loss-1.1627, acc-0.7200, valid loss-0.9701, acc-0.7582, test loss-0.9690, acc-0.7634\n",
      "Iter-15330, train loss-0.9856, acc-0.7000, valid loss-0.9696, acc-0.7584, test loss-0.9686, acc-0.7634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-15340, train loss-0.8894, acc-0.8000, valid loss-0.9691, acc-0.7586, test loss-0.9681, acc-0.7636\n",
      "Iter-15350, train loss-0.9678, acc-0.7600, valid loss-0.9685, acc-0.7586, test loss-0.9675, acc-0.7638\n",
      "Iter-15360, train loss-0.9828, acc-0.7800, valid loss-0.9680, acc-0.7588, test loss-0.9670, acc-0.7638\n",
      "Iter-15370, train loss-0.8814, acc-0.7600, valid loss-0.9675, acc-0.7590, test loss-0.9665, acc-0.7641\n",
      "Iter-15380, train loss-1.1720, acc-0.6600, valid loss-0.9670, acc-0.7590, test loss-0.9660, acc-0.7643\n",
      "Iter-15390, train loss-0.9684, acc-0.7600, valid loss-0.9666, acc-0.7590, test loss-0.9656, acc-0.7646\n",
      "Iter-15400, train loss-0.9388, acc-0.8200, valid loss-0.9660, acc-0.7590, test loss-0.9651, acc-0.7645\n",
      "Iter-15410, train loss-0.8169, acc-0.8600, valid loss-0.9655, acc-0.7590, test loss-0.9646, acc-0.7644\n",
      "Iter-15420, train loss-0.9487, acc-0.7200, valid loss-0.9650, acc-0.7588, test loss-0.9641, acc-0.7646\n",
      "Iter-15430, train loss-0.8449, acc-0.8200, valid loss-0.9645, acc-0.7590, test loss-0.9636, acc-0.7646\n",
      "Iter-15440, train loss-0.9318, acc-0.7400, valid loss-0.9640, acc-0.7590, test loss-0.9631, acc-0.7647\n",
      "Iter-15450, train loss-0.9526, acc-0.8000, valid loss-0.9635, acc-0.7592, test loss-0.9626, acc-0.7650\n",
      "Iter-15460, train loss-0.9037, acc-0.8200, valid loss-0.9631, acc-0.7592, test loss-0.9621, acc-0.7648\n",
      "Iter-15470, train loss-1.0130, acc-0.6600, valid loss-0.9626, acc-0.7592, test loss-0.9617, acc-0.7648\n",
      "Iter-15480, train loss-0.9259, acc-0.8000, valid loss-0.9621, acc-0.7592, test loss-0.9611, acc-0.7648\n",
      "Iter-15490, train loss-1.0190, acc-0.7400, valid loss-0.9616, acc-0.7596, test loss-0.9606, acc-0.7649\n",
      "Iter-15500, train loss-0.7922, acc-0.8800, valid loss-0.9610, acc-0.7596, test loss-0.9602, acc-0.7649\n",
      "Iter-15510, train loss-0.9045, acc-0.7600, valid loss-0.9605, acc-0.7598, test loss-0.9597, acc-0.7650\n",
      "Iter-15520, train loss-0.9810, acc-0.7600, valid loss-0.9600, acc-0.7596, test loss-0.9591, acc-0.7650\n",
      "Iter-15530, train loss-0.8811, acc-0.7800, valid loss-0.9595, acc-0.7598, test loss-0.9586, acc-0.7650\n",
      "Iter-15540, train loss-0.7359, acc-0.9000, valid loss-0.9590, acc-0.7602, test loss-0.9581, acc-0.7653\n",
      "Iter-15550, train loss-0.8084, acc-0.8000, valid loss-0.9585, acc-0.7604, test loss-0.9576, acc-0.7657\n",
      "Iter-15560, train loss-0.9958, acc-0.7800, valid loss-0.9580, acc-0.7604, test loss-0.9571, acc-0.7659\n",
      "Iter-15570, train loss-0.8727, acc-0.8200, valid loss-0.9575, acc-0.7606, test loss-0.9567, acc-0.7659\n",
      "Iter-15580, train loss-0.9472, acc-0.7400, valid loss-0.9570, acc-0.7608, test loss-0.9562, acc-0.7660\n",
      "Iter-15590, train loss-0.8666, acc-0.8000, valid loss-0.9566, acc-0.7608, test loss-0.9557, acc-0.7661\n",
      "Iter-15600, train loss-0.9911, acc-0.7400, valid loss-0.9561, acc-0.7608, test loss-0.9552, acc-0.7661\n",
      "Iter-15610, train loss-0.9801, acc-0.7800, valid loss-0.9556, acc-0.7612, test loss-0.9547, acc-0.7660\n",
      "Iter-15620, train loss-0.9685, acc-0.7600, valid loss-0.9551, acc-0.7612, test loss-0.9542, acc-0.7660\n",
      "Iter-15630, train loss-0.9066, acc-0.7800, valid loss-0.9546, acc-0.7614, test loss-0.9537, acc-0.7661\n",
      "Iter-15640, train loss-0.9559, acc-0.7400, valid loss-0.9540, acc-0.7614, test loss-0.9532, acc-0.7662\n",
      "Iter-15650, train loss-0.9939, acc-0.6600, valid loss-0.9536, acc-0.7618, test loss-0.9527, acc-0.7664\n",
      "Iter-15660, train loss-0.9857, acc-0.7200, valid loss-0.9531, acc-0.7622, test loss-0.9522, acc-0.7665\n",
      "Iter-15670, train loss-0.8220, acc-0.8400, valid loss-0.9526, acc-0.7622, test loss-0.9517, acc-0.7668\n",
      "Iter-15680, train loss-1.0011, acc-0.7000, valid loss-0.9521, acc-0.7622, test loss-0.9513, acc-0.7670\n",
      "Iter-15690, train loss-0.9558, acc-0.8200, valid loss-0.9516, acc-0.7622, test loss-0.9508, acc-0.7669\n",
      "Iter-15700, train loss-0.9766, acc-0.7600, valid loss-0.9512, acc-0.7624, test loss-0.9503, acc-0.7672\n",
      "Iter-15710, train loss-0.8810, acc-0.7600, valid loss-0.9507, acc-0.7630, test loss-0.9498, acc-0.7674\n",
      "Iter-15720, train loss-0.8999, acc-0.7400, valid loss-0.9502, acc-0.7628, test loss-0.9493, acc-0.7675\n",
      "Iter-15730, train loss-1.1368, acc-0.6600, valid loss-0.9497, acc-0.7628, test loss-0.9489, acc-0.7678\n",
      "Iter-15740, train loss-1.0319, acc-0.6600, valid loss-0.9492, acc-0.7628, test loss-0.9483, acc-0.7680\n",
      "Iter-15750, train loss-0.7615, acc-0.8200, valid loss-0.9487, acc-0.7626, test loss-0.9479, acc-0.7681\n",
      "Iter-15760, train loss-1.0585, acc-0.7400, valid loss-0.9482, acc-0.7626, test loss-0.9474, acc-0.7678\n",
      "Iter-15770, train loss-1.0058, acc-0.8200, valid loss-0.9477, acc-0.7628, test loss-0.9469, acc-0.7682\n",
      "Iter-15780, train loss-0.9183, acc-0.7800, valid loss-0.9473, acc-0.7630, test loss-0.9465, acc-0.7679\n",
      "Iter-15790, train loss-0.9010, acc-0.8400, valid loss-0.9468, acc-0.7632, test loss-0.9460, acc-0.7679\n",
      "Iter-15800, train loss-0.9470, acc-0.7600, valid loss-0.9463, acc-0.7634, test loss-0.9455, acc-0.7679\n",
      "Iter-15810, train loss-1.0597, acc-0.7200, valid loss-0.9458, acc-0.7634, test loss-0.9450, acc-0.7682\n",
      "Iter-15820, train loss-0.9446, acc-0.8200, valid loss-0.9453, acc-0.7632, test loss-0.9445, acc-0.7681\n",
      "Iter-15830, train loss-1.0049, acc-0.7600, valid loss-0.9448, acc-0.7636, test loss-0.9441, acc-0.7680\n",
      "Iter-15840, train loss-1.1420, acc-0.6200, valid loss-0.9444, acc-0.7638, test loss-0.9436, acc-0.7681\n",
      "Iter-15850, train loss-0.7799, acc-0.8600, valid loss-0.9439, acc-0.7640, test loss-0.9431, acc-0.7684\n",
      "Iter-15860, train loss-1.3467, acc-0.5200, valid loss-0.9434, acc-0.7642, test loss-0.9426, acc-0.7686\n",
      "Iter-15870, train loss-1.0391, acc-0.7200, valid loss-0.9429, acc-0.7646, test loss-0.9421, acc-0.7688\n",
      "Iter-15880, train loss-1.0678, acc-0.6400, valid loss-0.9424, acc-0.7646, test loss-0.9417, acc-0.7689\n",
      "Iter-15890, train loss-0.9858, acc-0.7600, valid loss-0.9419, acc-0.7646, test loss-0.9412, acc-0.7688\n",
      "Iter-15900, train loss-0.9200, acc-0.7800, valid loss-0.9415, acc-0.7646, test loss-0.9407, acc-0.7690\n",
      "Iter-15910, train loss-0.8642, acc-0.8400, valid loss-0.9410, acc-0.7646, test loss-0.9402, acc-0.7688\n",
      "Iter-15920, train loss-0.9981, acc-0.6600, valid loss-0.9405, acc-0.7650, test loss-0.9398, acc-0.7690\n",
      "Iter-15930, train loss-0.8334, acc-0.7800, valid loss-0.9400, acc-0.7650, test loss-0.9393, acc-0.7690\n",
      "Iter-15940, train loss-0.8548, acc-0.7800, valid loss-0.9395, acc-0.7650, test loss-0.9388, acc-0.7696\n",
      "Iter-15950, train loss-0.9368, acc-0.7800, valid loss-0.9391, acc-0.7650, test loss-0.9384, acc-0.7696\n",
      "Iter-15960, train loss-0.8225, acc-0.8000, valid loss-0.9386, acc-0.7648, test loss-0.9380, acc-0.7696\n",
      "Iter-15970, train loss-1.0995, acc-0.6800, valid loss-0.9381, acc-0.7652, test loss-0.9375, acc-0.7700\n",
      "Iter-15980, train loss-1.0315, acc-0.7000, valid loss-0.9377, acc-0.7656, test loss-0.9370, acc-0.7702\n",
      "Iter-15990, train loss-0.8034, acc-0.8000, valid loss-0.9372, acc-0.7658, test loss-0.9365, acc-0.7703\n",
      "Iter-16000, train loss-0.8288, acc-0.8200, valid loss-0.9367, acc-0.7658, test loss-0.9360, acc-0.7704\n",
      "Iter-16010, train loss-0.8323, acc-0.7600, valid loss-0.9362, acc-0.7658, test loss-0.9356, acc-0.7707\n",
      "Iter-16020, train loss-0.9468, acc-0.7800, valid loss-0.9357, acc-0.7658, test loss-0.9351, acc-0.7708\n",
      "Iter-16030, train loss-1.0764, acc-0.7400, valid loss-0.9353, acc-0.7658, test loss-0.9347, acc-0.7707\n",
      "Iter-16040, train loss-0.7770, acc-0.8600, valid loss-0.9347, acc-0.7658, test loss-0.9342, acc-0.7708\n",
      "Iter-16050, train loss-0.9680, acc-0.7400, valid loss-0.9343, acc-0.7660, test loss-0.9337, acc-0.7710\n",
      "Iter-16060, train loss-0.9935, acc-0.7000, valid loss-0.9338, acc-0.7662, test loss-0.9332, acc-0.7712\n",
      "Iter-16070, train loss-0.7809, acc-0.8600, valid loss-0.9333, acc-0.7664, test loss-0.9327, acc-0.7713\n",
      "Iter-16080, train loss-0.8755, acc-0.7800, valid loss-0.9328, acc-0.7664, test loss-0.9323, acc-0.7712\n",
      "Iter-16090, train loss-0.9567, acc-0.7400, valid loss-0.9324, acc-0.7668, test loss-0.9318, acc-0.7713\n",
      "Iter-16100, train loss-0.8572, acc-0.7800, valid loss-0.9319, acc-0.7670, test loss-0.9314, acc-0.7713\n",
      "Iter-16110, train loss-0.9560, acc-0.7600, valid loss-0.9315, acc-0.7668, test loss-0.9310, acc-0.7715\n",
      "Iter-16120, train loss-1.0641, acc-0.7600, valid loss-0.9310, acc-0.7666, test loss-0.9305, acc-0.7719\n",
      "Iter-16130, train loss-1.0481, acc-0.7000, valid loss-0.9306, acc-0.7664, test loss-0.9301, acc-0.7720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-16140, train loss-0.9645, acc-0.7400, valid loss-0.9302, acc-0.7662, test loss-0.9296, acc-0.7720\n",
      "Iter-16150, train loss-0.9927, acc-0.7200, valid loss-0.9297, acc-0.7668, test loss-0.9292, acc-0.7721\n",
      "Iter-16160, train loss-0.8578, acc-0.8400, valid loss-0.9292, acc-0.7670, test loss-0.9287, acc-0.7721\n",
      "Iter-16170, train loss-0.9029, acc-0.8200, valid loss-0.9287, acc-0.7678, test loss-0.9282, acc-0.7723\n",
      "Iter-16180, train loss-0.9494, acc-0.8400, valid loss-0.9283, acc-0.7678, test loss-0.9278, acc-0.7724\n",
      "Iter-16190, train loss-0.8236, acc-0.8200, valid loss-0.9278, acc-0.7680, test loss-0.9273, acc-0.7721\n",
      "Iter-16200, train loss-0.8995, acc-0.7600, valid loss-0.9274, acc-0.7680, test loss-0.9269, acc-0.7724\n",
      "Iter-16210, train loss-0.8115, acc-0.7600, valid loss-0.9269, acc-0.7686, test loss-0.9264, acc-0.7729\n",
      "Iter-16220, train loss-0.9234, acc-0.7400, valid loss-0.9265, acc-0.7686, test loss-0.9260, acc-0.7729\n",
      "Iter-16230, train loss-0.9301, acc-0.8400, valid loss-0.9260, acc-0.7686, test loss-0.9255, acc-0.7729\n",
      "Iter-16240, train loss-0.7129, acc-0.8800, valid loss-0.9255, acc-0.7688, test loss-0.9251, acc-0.7729\n",
      "Iter-16250, train loss-0.9585, acc-0.7200, valid loss-0.9250, acc-0.7688, test loss-0.9246, acc-0.7732\n",
      "Iter-16260, train loss-0.9664, acc-0.7600, valid loss-0.9246, acc-0.7692, test loss-0.9241, acc-0.7731\n",
      "Iter-16270, train loss-0.9273, acc-0.8400, valid loss-0.9241, acc-0.7696, test loss-0.9237, acc-0.7728\n",
      "Iter-16280, train loss-0.8193, acc-0.8200, valid loss-0.9236, acc-0.7696, test loss-0.9232, acc-0.7729\n",
      "Iter-16290, train loss-0.9058, acc-0.8200, valid loss-0.9232, acc-0.7698, test loss-0.9228, acc-0.7728\n",
      "Iter-16300, train loss-0.9527, acc-0.7800, valid loss-0.9228, acc-0.7702, test loss-0.9223, acc-0.7729\n",
      "Iter-16310, train loss-0.8172, acc-0.8000, valid loss-0.9223, acc-0.7702, test loss-0.9219, acc-0.7729\n",
      "Iter-16320, train loss-0.8681, acc-0.8000, valid loss-0.9219, acc-0.7702, test loss-0.9215, acc-0.7731\n",
      "Iter-16330, train loss-0.7395, acc-0.9400, valid loss-0.9214, acc-0.7706, test loss-0.9210, acc-0.7734\n",
      "Iter-16340, train loss-0.8972, acc-0.8400, valid loss-0.9210, acc-0.7708, test loss-0.9205, acc-0.7734\n",
      "Iter-16350, train loss-1.1283, acc-0.6200, valid loss-0.9205, acc-0.7710, test loss-0.9201, acc-0.7734\n",
      "Iter-16360, train loss-0.8981, acc-0.8400, valid loss-0.9201, acc-0.7712, test loss-0.9197, acc-0.7734\n",
      "Iter-16370, train loss-0.7967, acc-0.7800, valid loss-0.9196, acc-0.7712, test loss-0.9192, acc-0.7737\n",
      "Iter-16380, train loss-0.9029, acc-0.8000, valid loss-0.9192, acc-0.7714, test loss-0.9188, acc-0.7737\n",
      "Iter-16390, train loss-0.8105, acc-0.7800, valid loss-0.9187, acc-0.7716, test loss-0.9183, acc-0.7736\n",
      "Iter-16400, train loss-0.8502, acc-0.8600, valid loss-0.9183, acc-0.7716, test loss-0.9179, acc-0.7733\n",
      "Iter-16410, train loss-0.8983, acc-0.7400, valid loss-0.9178, acc-0.7716, test loss-0.9175, acc-0.7735\n",
      "Iter-16420, train loss-1.0967, acc-0.6400, valid loss-0.9174, acc-0.7718, test loss-0.9170, acc-0.7737\n",
      "Iter-16430, train loss-0.9802, acc-0.7400, valid loss-0.9170, acc-0.7716, test loss-0.9166, acc-0.7738\n",
      "Iter-16440, train loss-0.9448, acc-0.7400, valid loss-0.9166, acc-0.7718, test loss-0.9162, acc-0.7739\n",
      "Iter-16450, train loss-1.1142, acc-0.6400, valid loss-0.9161, acc-0.7722, test loss-0.9157, acc-0.7737\n",
      "Iter-16460, train loss-0.9827, acc-0.7000, valid loss-0.9157, acc-0.7726, test loss-0.9153, acc-0.7741\n",
      "Iter-16470, train loss-0.8795, acc-0.8000, valid loss-0.9152, acc-0.7724, test loss-0.9148, acc-0.7742\n",
      "Iter-16480, train loss-0.9370, acc-0.7600, valid loss-0.9148, acc-0.7726, test loss-0.9144, acc-0.7741\n",
      "Iter-16490, train loss-0.9079, acc-0.8200, valid loss-0.9143, acc-0.7728, test loss-0.9140, acc-0.7742\n",
      "Iter-16500, train loss-0.9844, acc-0.8400, valid loss-0.9139, acc-0.7728, test loss-0.9136, acc-0.7746\n",
      "Iter-16510, train loss-1.0271, acc-0.7400, valid loss-0.9135, acc-0.7726, test loss-0.9131, acc-0.7746\n",
      "Iter-16520, train loss-0.8463, acc-0.7800, valid loss-0.9130, acc-0.7732, test loss-0.9127, acc-0.7747\n",
      "Iter-16530, train loss-0.8967, acc-0.7800, valid loss-0.9126, acc-0.7732, test loss-0.9122, acc-0.7747\n",
      "Iter-16540, train loss-0.9827, acc-0.8400, valid loss-0.9122, acc-0.7732, test loss-0.9118, acc-0.7748\n",
      "Iter-16550, train loss-0.9462, acc-0.7400, valid loss-0.9117, acc-0.7732, test loss-0.9114, acc-0.7748\n",
      "Iter-16560, train loss-0.9447, acc-0.7200, valid loss-0.9113, acc-0.7732, test loss-0.9110, acc-0.7749\n",
      "Iter-16570, train loss-0.8759, acc-0.8400, valid loss-0.9108, acc-0.7732, test loss-0.9105, acc-0.7750\n",
      "Iter-16580, train loss-0.8393, acc-0.8800, valid loss-0.9104, acc-0.7736, test loss-0.9101, acc-0.7751\n",
      "Iter-16590, train loss-0.8774, acc-0.8000, valid loss-0.9100, acc-0.7732, test loss-0.9097, acc-0.7751\n",
      "Iter-16600, train loss-0.9730, acc-0.7400, valid loss-0.9096, acc-0.7736, test loss-0.9092, acc-0.7752\n",
      "Iter-16610, train loss-0.8648, acc-0.8200, valid loss-0.9091, acc-0.7736, test loss-0.9088, acc-0.7752\n",
      "Iter-16620, train loss-0.9751, acc-0.7800, valid loss-0.9087, acc-0.7736, test loss-0.9084, acc-0.7754\n",
      "Iter-16630, train loss-0.9340, acc-0.7600, valid loss-0.9083, acc-0.7738, test loss-0.9080, acc-0.7756\n",
      "Iter-16640, train loss-0.7631, acc-0.8400, valid loss-0.9078, acc-0.7740, test loss-0.9076, acc-0.7756\n",
      "Iter-16650, train loss-0.7783, acc-0.9000, valid loss-0.9074, acc-0.7742, test loss-0.9071, acc-0.7758\n",
      "Iter-16660, train loss-0.8950, acc-0.8000, valid loss-0.9070, acc-0.7748, test loss-0.9067, acc-0.7760\n",
      "Iter-16670, train loss-1.0127, acc-0.7400, valid loss-0.9065, acc-0.7746, test loss-0.9062, acc-0.7762\n",
      "Iter-16680, train loss-0.8591, acc-0.8200, valid loss-0.9061, acc-0.7750, test loss-0.9058, acc-0.7762\n",
      "Iter-16690, train loss-0.7983, acc-0.8600, valid loss-0.9056, acc-0.7746, test loss-0.9054, acc-0.7762\n",
      "Iter-16700, train loss-0.9376, acc-0.7400, valid loss-0.9052, acc-0.7748, test loss-0.9050, acc-0.7763\n",
      "Iter-16710, train loss-0.9618, acc-0.7600, valid loss-0.9048, acc-0.7750, test loss-0.9046, acc-0.7762\n",
      "Iter-16720, train loss-1.0243, acc-0.6600, valid loss-0.9044, acc-0.7756, test loss-0.9041, acc-0.7769\n",
      "Iter-16730, train loss-0.9464, acc-0.7000, valid loss-0.9039, acc-0.7750, test loss-0.9037, acc-0.7766\n",
      "Iter-16740, train loss-0.9897, acc-0.7200, valid loss-0.9035, acc-0.7752, test loss-0.9033, acc-0.7767\n",
      "Iter-16750, train loss-0.8170, acc-0.7600, valid loss-0.9031, acc-0.7754, test loss-0.9029, acc-0.7769\n",
      "Iter-16760, train loss-0.8109, acc-0.8600, valid loss-0.9027, acc-0.7754, test loss-0.9024, acc-0.7769\n",
      "Iter-16770, train loss-1.1216, acc-0.6400, valid loss-0.9023, acc-0.7756, test loss-0.9020, acc-0.7770\n",
      "Iter-16780, train loss-0.9834, acc-0.7000, valid loss-0.9019, acc-0.7758, test loss-0.9016, acc-0.7775\n",
      "Iter-16790, train loss-0.9683, acc-0.7600, valid loss-0.9014, acc-0.7762, test loss-0.9012, acc-0.7774\n",
      "Iter-16800, train loss-0.9111, acc-0.7800, valid loss-0.9010, acc-0.7762, test loss-0.9008, acc-0.7777\n",
      "Iter-16810, train loss-0.9992, acc-0.7600, valid loss-0.9006, acc-0.7764, test loss-0.9004, acc-0.7779\n",
      "Iter-16820, train loss-0.9341, acc-0.7000, valid loss-0.9001, acc-0.7762, test loss-0.8999, acc-0.7779\n",
      "Iter-16830, train loss-1.0476, acc-0.6600, valid loss-0.8997, acc-0.7764, test loss-0.8995, acc-0.7778\n",
      "Iter-16840, train loss-0.8809, acc-0.7800, valid loss-0.8993, acc-0.7770, test loss-0.8991, acc-0.7779\n",
      "Iter-16850, train loss-0.8586, acc-0.8000, valid loss-0.8989, acc-0.7770, test loss-0.8987, acc-0.7781\n",
      "Iter-16860, train loss-0.7747, acc-0.8200, valid loss-0.8985, acc-0.7768, test loss-0.8982, acc-0.7783\n",
      "Iter-16870, train loss-0.8844, acc-0.8200, valid loss-0.8981, acc-0.7770, test loss-0.8978, acc-0.7786\n",
      "Iter-16880, train loss-1.0549, acc-0.7000, valid loss-0.8976, acc-0.7772, test loss-0.8974, acc-0.7785\n",
      "Iter-16890, train loss-0.9893, acc-0.7400, valid loss-0.8972, acc-0.7774, test loss-0.8970, acc-0.7787\n",
      "Iter-16900, train loss-0.6956, acc-0.9000, valid loss-0.8968, acc-0.7776, test loss-0.8965, acc-0.7788\n",
      "Iter-16910, train loss-0.9618, acc-0.7600, valid loss-0.8963, acc-0.7784, test loss-0.8961, acc-0.7791\n",
      "Iter-16920, train loss-1.0351, acc-0.7200, valid loss-0.8959, acc-0.7786, test loss-0.8957, acc-0.7790\n",
      "Iter-16930, train loss-0.9794, acc-0.7800, valid loss-0.8955, acc-0.7780, test loss-0.8953, acc-0.7791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-16940, train loss-0.8753, acc-0.7600, valid loss-0.8951, acc-0.7786, test loss-0.8949, acc-0.7793\n",
      "Iter-16950, train loss-0.9289, acc-0.7200, valid loss-0.8947, acc-0.7786, test loss-0.8945, acc-0.7794\n",
      "Iter-16960, train loss-0.8331, acc-0.8000, valid loss-0.8943, acc-0.7786, test loss-0.8941, acc-0.7793\n",
      "Iter-16970, train loss-0.8231, acc-0.8000, valid loss-0.8938, acc-0.7788, test loss-0.8936, acc-0.7794\n",
      "Iter-16980, train loss-0.7578, acc-0.8400, valid loss-0.8934, acc-0.7790, test loss-0.8933, acc-0.7796\n",
      "Iter-16990, train loss-0.9986, acc-0.7600, valid loss-0.8930, acc-0.7788, test loss-0.8928, acc-0.7795\n",
      "Iter-17000, train loss-0.8537, acc-0.7400, valid loss-0.8926, acc-0.7790, test loss-0.8924, acc-0.7795\n",
      "Iter-17010, train loss-0.8792, acc-0.8000, valid loss-0.8922, acc-0.7790, test loss-0.8920, acc-0.7797\n",
      "Iter-17020, train loss-0.8853, acc-0.6800, valid loss-0.8918, acc-0.7790, test loss-0.8916, acc-0.7795\n",
      "Iter-17030, train loss-0.8645, acc-0.8600, valid loss-0.8914, acc-0.7784, test loss-0.8912, acc-0.7794\n",
      "Iter-17040, train loss-1.1061, acc-0.6800, valid loss-0.8909, acc-0.7784, test loss-0.8908, acc-0.7798\n",
      "Iter-17050, train loss-1.0959, acc-0.7000, valid loss-0.8905, acc-0.7790, test loss-0.8903, acc-0.7800\n",
      "Iter-17060, train loss-0.9046, acc-0.7800, valid loss-0.8901, acc-0.7790, test loss-0.8899, acc-0.7803\n",
      "Iter-17070, train loss-0.8738, acc-0.7800, valid loss-0.8897, acc-0.7788, test loss-0.8895, acc-0.7805\n",
      "Iter-17080, train loss-0.8461, acc-0.8200, valid loss-0.8892, acc-0.7790, test loss-0.8891, acc-0.7805\n",
      "Iter-17090, train loss-0.8234, acc-0.7800, valid loss-0.8888, acc-0.7790, test loss-0.8887, acc-0.7807\n",
      "Iter-17100, train loss-1.0769, acc-0.7400, valid loss-0.8884, acc-0.7790, test loss-0.8883, acc-0.7805\n",
      "Iter-17110, train loss-0.9296, acc-0.7800, valid loss-0.8880, acc-0.7792, test loss-0.8879, acc-0.7807\n",
      "Iter-17120, train loss-0.7715, acc-0.8600, valid loss-0.8876, acc-0.7792, test loss-0.8874, acc-0.7809\n",
      "Iter-17130, train loss-0.8128, acc-0.8000, valid loss-0.8871, acc-0.7790, test loss-0.8870, acc-0.7811\n",
      "Iter-17140, train loss-0.8781, acc-0.7800, valid loss-0.8867, acc-0.7790, test loss-0.8866, acc-0.7810\n",
      "Iter-17150, train loss-0.8885, acc-0.7600, valid loss-0.8863, acc-0.7790, test loss-0.8862, acc-0.7808\n",
      "Iter-17160, train loss-0.8312, acc-0.8400, valid loss-0.8859, acc-0.7792, test loss-0.8858, acc-0.7810\n",
      "Iter-17170, train loss-1.1600, acc-0.6200, valid loss-0.8855, acc-0.7794, test loss-0.8854, acc-0.7812\n",
      "Iter-17180, train loss-0.9758, acc-0.7000, valid loss-0.8851, acc-0.7796, test loss-0.8851, acc-0.7811\n",
      "Iter-17190, train loss-0.7936, acc-0.8000, valid loss-0.8847, acc-0.7798, test loss-0.8847, acc-0.7812\n",
      "Iter-17200, train loss-0.9328, acc-0.7600, valid loss-0.8843, acc-0.7798, test loss-0.8843, acc-0.7812\n",
      "Iter-17210, train loss-0.9496, acc-0.7200, valid loss-0.8839, acc-0.7800, test loss-0.8839, acc-0.7813\n",
      "Iter-17220, train loss-0.9330, acc-0.7600, valid loss-0.8835, acc-0.7800, test loss-0.8835, acc-0.7816\n",
      "Iter-17230, train loss-1.0254, acc-0.7200, valid loss-0.8831, acc-0.7798, test loss-0.8831, acc-0.7817\n",
      "Iter-17240, train loss-0.8772, acc-0.8400, valid loss-0.8827, acc-0.7798, test loss-0.8827, acc-0.7818\n",
      "Iter-17250, train loss-0.8733, acc-0.7400, valid loss-0.8823, acc-0.7800, test loss-0.8823, acc-0.7819\n",
      "Iter-17260, train loss-0.7513, acc-0.8600, valid loss-0.8819, acc-0.7802, test loss-0.8819, acc-0.7821\n",
      "Iter-17270, train loss-0.8283, acc-0.7600, valid loss-0.8814, acc-0.7802, test loss-0.8814, acc-0.7819\n",
      "Iter-17280, train loss-0.6983, acc-0.8600, valid loss-0.8811, acc-0.7806, test loss-0.8811, acc-0.7820\n",
      "Iter-17290, train loss-0.9522, acc-0.6800, valid loss-0.8807, acc-0.7802, test loss-0.8807, acc-0.7820\n",
      "Iter-17300, train loss-0.8374, acc-0.8200, valid loss-0.8803, acc-0.7804, test loss-0.8803, acc-0.7820\n",
      "Iter-17310, train loss-0.7921, acc-0.7600, valid loss-0.8799, acc-0.7806, test loss-0.8799, acc-0.7821\n",
      "Iter-17320, train loss-0.8635, acc-0.8200, valid loss-0.8795, acc-0.7808, test loss-0.8795, acc-0.7822\n",
      "Iter-17330, train loss-0.7557, acc-0.8400, valid loss-0.8790, acc-0.7810, test loss-0.8791, acc-0.7823\n",
      "Iter-17340, train loss-1.0773, acc-0.6400, valid loss-0.8786, acc-0.7820, test loss-0.8786, acc-0.7824\n",
      "Iter-17350, train loss-0.8410, acc-0.8800, valid loss-0.8782, acc-0.7820, test loss-0.8782, acc-0.7824\n",
      "Iter-17360, train loss-1.0560, acc-0.7600, valid loss-0.8778, acc-0.7822, test loss-0.8779, acc-0.7825\n",
      "Iter-17370, train loss-0.8209, acc-0.7600, valid loss-0.8774, acc-0.7822, test loss-0.8774, acc-0.7827\n",
      "Iter-17380, train loss-0.9793, acc-0.7200, valid loss-0.8770, acc-0.7828, test loss-0.8770, acc-0.7828\n",
      "Iter-17390, train loss-0.8889, acc-0.7800, valid loss-0.8766, acc-0.7830, test loss-0.8766, acc-0.7828\n",
      "Iter-17400, train loss-0.7016, acc-0.9200, valid loss-0.8762, acc-0.7830, test loss-0.8762, acc-0.7831\n",
      "Iter-17410, train loss-0.9248, acc-0.6800, valid loss-0.8757, acc-0.7828, test loss-0.8758, acc-0.7832\n",
      "Iter-17420, train loss-0.7210, acc-0.9200, valid loss-0.8753, acc-0.7826, test loss-0.8754, acc-0.7830\n",
      "Iter-17430, train loss-0.8517, acc-0.7200, valid loss-0.8750, acc-0.7828, test loss-0.8750, acc-0.7833\n",
      "Iter-17440, train loss-0.9429, acc-0.7800, valid loss-0.8746, acc-0.7830, test loss-0.8747, acc-0.7833\n",
      "Iter-17450, train loss-0.9987, acc-0.7200, valid loss-0.8742, acc-0.7830, test loss-0.8743, acc-0.7836\n",
      "Iter-17460, train loss-0.8020, acc-0.7600, valid loss-0.8738, acc-0.7832, test loss-0.8739, acc-0.7837\n",
      "Iter-17470, train loss-0.6950, acc-0.8800, valid loss-0.8735, acc-0.7832, test loss-0.8736, acc-0.7840\n",
      "Iter-17480, train loss-0.9055, acc-0.7600, valid loss-0.8731, acc-0.7834, test loss-0.8732, acc-0.7841\n",
      "Iter-17490, train loss-0.7589, acc-0.8600, valid loss-0.8727, acc-0.7836, test loss-0.8728, acc-0.7841\n",
      "Iter-17500, train loss-0.9808, acc-0.6400, valid loss-0.8724, acc-0.7840, test loss-0.8725, acc-0.7841\n",
      "Iter-17510, train loss-0.8206, acc-0.7800, valid loss-0.8720, acc-0.7840, test loss-0.8720, acc-0.7842\n",
      "Iter-17520, train loss-0.9094, acc-0.8200, valid loss-0.8716, acc-0.7840, test loss-0.8716, acc-0.7840\n",
      "Iter-17530, train loss-1.1326, acc-0.7200, valid loss-0.8712, acc-0.7842, test loss-0.8713, acc-0.7843\n",
      "Iter-17540, train loss-0.8820, acc-0.7400, valid loss-0.8708, acc-0.7842, test loss-0.8709, acc-0.7845\n",
      "Iter-17550, train loss-0.8714, acc-0.8200, valid loss-0.8704, acc-0.7844, test loss-0.8705, acc-0.7847\n",
      "Iter-17560, train loss-0.7432, acc-0.8200, valid loss-0.8700, acc-0.7844, test loss-0.8701, acc-0.7847\n",
      "Iter-17570, train loss-0.8277, acc-0.8000, valid loss-0.8697, acc-0.7844, test loss-0.8698, acc-0.7847\n",
      "Iter-17580, train loss-0.8346, acc-0.8200, valid loss-0.8693, acc-0.7842, test loss-0.8694, acc-0.7847\n",
      "Iter-17590, train loss-0.8782, acc-0.7800, valid loss-0.8689, acc-0.7844, test loss-0.8690, acc-0.7847\n",
      "Iter-17600, train loss-0.7497, acc-0.8400, valid loss-0.8685, acc-0.7844, test loss-0.8686, acc-0.7849\n",
      "Iter-17610, train loss-1.1230, acc-0.6800, valid loss-0.8681, acc-0.7838, test loss-0.8683, acc-0.7850\n",
      "Iter-17620, train loss-0.7948, acc-0.8000, valid loss-0.8678, acc-0.7836, test loss-0.8679, acc-0.7850\n",
      "Iter-17630, train loss-0.9921, acc-0.6800, valid loss-0.8674, acc-0.7838, test loss-0.8675, acc-0.7850\n",
      "Iter-17640, train loss-0.7662, acc-0.8800, valid loss-0.8670, acc-0.7838, test loss-0.8672, acc-0.7851\n",
      "Iter-17650, train loss-0.8660, acc-0.8000, valid loss-0.8666, acc-0.7836, test loss-0.8668, acc-0.7852\n",
      "Iter-17660, train loss-0.8613, acc-0.8000, valid loss-0.8662, acc-0.7838, test loss-0.8664, acc-0.7855\n",
      "Iter-17670, train loss-0.9608, acc-0.7600, valid loss-0.8658, acc-0.7836, test loss-0.8660, acc-0.7858\n",
      "Iter-17680, train loss-0.8995, acc-0.7200, valid loss-0.8654, acc-0.7840, test loss-0.8656, acc-0.7859\n",
      "Iter-17690, train loss-0.8713, acc-0.7600, valid loss-0.8651, acc-0.7842, test loss-0.8652, acc-0.7857\n",
      "Iter-17700, train loss-1.0750, acc-0.6400, valid loss-0.8646, acc-0.7844, test loss-0.8648, acc-0.7858\n",
      "Iter-17710, train loss-0.8471, acc-0.7400, valid loss-0.8642, acc-0.7844, test loss-0.8644, acc-0.7859\n",
      "Iter-17720, train loss-0.8650, acc-0.7800, valid loss-0.8638, acc-0.7848, test loss-0.8640, acc-0.7860\n",
      "Iter-17730, train loss-0.8414, acc-0.8200, valid loss-0.8635, acc-0.7850, test loss-0.8637, acc-0.7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-17740, train loss-0.7998, acc-0.8400, valid loss-0.8631, acc-0.7854, test loss-0.8633, acc-0.7863\n",
      "Iter-17750, train loss-0.8680, acc-0.8000, valid loss-0.8627, acc-0.7854, test loss-0.8629, acc-0.7866\n",
      "Iter-17760, train loss-0.8949, acc-0.7000, valid loss-0.8623, acc-0.7860, test loss-0.8625, acc-0.7871\n",
      "Iter-17770, train loss-0.8127, acc-0.8400, valid loss-0.8619, acc-0.7862, test loss-0.8621, acc-0.7870\n",
      "Iter-17780, train loss-0.8748, acc-0.8200, valid loss-0.8615, acc-0.7860, test loss-0.8617, acc-0.7872\n",
      "Iter-17790, train loss-0.8378, acc-0.7600, valid loss-0.8611, acc-0.7860, test loss-0.8613, acc-0.7871\n",
      "Iter-17800, train loss-0.8654, acc-0.7600, valid loss-0.8607, acc-0.7860, test loss-0.8610, acc-0.7875\n",
      "Iter-17810, train loss-0.9437, acc-0.7400, valid loss-0.8604, acc-0.7862, test loss-0.8606, acc-0.7876\n",
      "Iter-17820, train loss-0.9139, acc-0.8000, valid loss-0.8600, acc-0.7864, test loss-0.8603, acc-0.7874\n",
      "Iter-17830, train loss-0.8975, acc-0.8000, valid loss-0.8596, acc-0.7864, test loss-0.8599, acc-0.7871\n",
      "Iter-17840, train loss-0.7884, acc-0.8200, valid loss-0.8592, acc-0.7866, test loss-0.8595, acc-0.7872\n",
      "Iter-17850, train loss-1.0042, acc-0.7600, valid loss-0.8589, acc-0.7868, test loss-0.8591, acc-0.7872\n",
      "Iter-17860, train loss-0.8826, acc-0.7800, valid loss-0.8585, acc-0.7870, test loss-0.8587, acc-0.7873\n",
      "Iter-17870, train loss-0.7820, acc-0.8000, valid loss-0.8581, acc-0.7874, test loss-0.8584, acc-0.7875\n",
      "Iter-17880, train loss-1.0125, acc-0.7200, valid loss-0.8577, acc-0.7872, test loss-0.8580, acc-0.7875\n",
      "Iter-17890, train loss-0.9643, acc-0.6600, valid loss-0.8573, acc-0.7878, test loss-0.8576, acc-0.7879\n",
      "Iter-17900, train loss-0.9388, acc-0.8000, valid loss-0.8570, acc-0.7876, test loss-0.8572, acc-0.7878\n",
      "Iter-17910, train loss-1.1025, acc-0.6600, valid loss-0.8566, acc-0.7880, test loss-0.8569, acc-0.7879\n",
      "Iter-17920, train loss-0.7957, acc-0.8200, valid loss-0.8562, acc-0.7884, test loss-0.8565, acc-0.7883\n",
      "Iter-17930, train loss-0.8991, acc-0.8000, valid loss-0.8558, acc-0.7884, test loss-0.8561, acc-0.7884\n",
      "Iter-17940, train loss-0.8945, acc-0.7600, valid loss-0.8554, acc-0.7882, test loss-0.8557, acc-0.7885\n",
      "Iter-17950, train loss-0.9669, acc-0.7000, valid loss-0.8550, acc-0.7882, test loss-0.8553, acc-0.7885\n",
      "Iter-17960, train loss-0.7808, acc-0.8600, valid loss-0.8546, acc-0.7884, test loss-0.8549, acc-0.7888\n",
      "Iter-17970, train loss-0.7778, acc-0.8000, valid loss-0.8542, acc-0.7884, test loss-0.8546, acc-0.7890\n",
      "Iter-17980, train loss-0.8292, acc-0.8000, valid loss-0.8539, acc-0.7884, test loss-0.8542, acc-0.7893\n",
      "Iter-17990, train loss-0.8581, acc-0.7400, valid loss-0.8535, acc-0.7884, test loss-0.8538, acc-0.7893\n",
      "Iter-18000, train loss-0.7624, acc-0.8000, valid loss-0.8531, acc-0.7886, test loss-0.8535, acc-0.7892\n",
      "Iter-18010, train loss-0.8067, acc-0.7600, valid loss-0.8527, acc-0.7886, test loss-0.8531, acc-0.7894\n",
      "Iter-18020, train loss-0.9013, acc-0.7600, valid loss-0.8523, acc-0.7882, test loss-0.8527, acc-0.7894\n",
      "Iter-18030, train loss-0.7932, acc-0.8200, valid loss-0.8520, acc-0.7884, test loss-0.8524, acc-0.7894\n",
      "Iter-18040, train loss-0.8948, acc-0.7400, valid loss-0.8516, acc-0.7884, test loss-0.8520, acc-0.7897\n",
      "Iter-18050, train loss-0.7847, acc-0.7800, valid loss-0.8512, acc-0.7886, test loss-0.8517, acc-0.7896\n",
      "Iter-18060, train loss-0.7857, acc-0.8400, valid loss-0.8509, acc-0.7884, test loss-0.8513, acc-0.7896\n",
      "Iter-18070, train loss-0.7883, acc-0.7400, valid loss-0.8505, acc-0.7886, test loss-0.8509, acc-0.7897\n",
      "Iter-18080, train loss-0.7692, acc-0.8600, valid loss-0.8502, acc-0.7886, test loss-0.8506, acc-0.7899\n",
      "Iter-18090, train loss-0.9264, acc-0.7800, valid loss-0.8498, acc-0.7884, test loss-0.8502, acc-0.7900\n",
      "Iter-18100, train loss-0.9237, acc-0.8000, valid loss-0.8495, acc-0.7884, test loss-0.8499, acc-0.7900\n",
      "Iter-18110, train loss-0.6433, acc-0.8800, valid loss-0.8491, acc-0.7888, test loss-0.8495, acc-0.7904\n",
      "Iter-18120, train loss-0.8535, acc-0.7400, valid loss-0.8487, acc-0.7890, test loss-0.8492, acc-0.7906\n",
      "Iter-18130, train loss-0.7633, acc-0.8000, valid loss-0.8484, acc-0.7888, test loss-0.8488, acc-0.7906\n",
      "Iter-18140, train loss-0.8460, acc-0.7600, valid loss-0.8480, acc-0.7890, test loss-0.8485, acc-0.7907\n",
      "Iter-18150, train loss-0.8186, acc-0.7800, valid loss-0.8477, acc-0.7886, test loss-0.8481, acc-0.7906\n",
      "Iter-18160, train loss-0.8279, acc-0.7800, valid loss-0.8473, acc-0.7886, test loss-0.8478, acc-0.7908\n",
      "Iter-18170, train loss-1.0031, acc-0.7400, valid loss-0.8470, acc-0.7888, test loss-0.8474, acc-0.7908\n",
      "Iter-18180, train loss-0.8824, acc-0.7400, valid loss-0.8466, acc-0.7888, test loss-0.8470, acc-0.7910\n",
      "Iter-18190, train loss-0.8089, acc-0.7800, valid loss-0.8462, acc-0.7890, test loss-0.8467, acc-0.7911\n",
      "Iter-18200, train loss-1.0231, acc-0.7200, valid loss-0.8459, acc-0.7888, test loss-0.8463, acc-0.7909\n",
      "Iter-18210, train loss-0.8078, acc-0.8000, valid loss-0.8455, acc-0.7890, test loss-0.8460, acc-0.7907\n",
      "Iter-18220, train loss-0.8428, acc-0.7600, valid loss-0.8451, acc-0.7892, test loss-0.8456, acc-0.7909\n",
      "Iter-18230, train loss-0.7963, acc-0.7600, valid loss-0.8448, acc-0.7890, test loss-0.8453, acc-0.7909\n",
      "Iter-18240, train loss-0.8769, acc-0.7600, valid loss-0.8444, acc-0.7890, test loss-0.8449, acc-0.7908\n",
      "Iter-18250, train loss-0.9474, acc-0.7400, valid loss-0.8441, acc-0.7890, test loss-0.8446, acc-0.7909\n",
      "Iter-18260, train loss-0.8005, acc-0.8200, valid loss-0.8437, acc-0.7892, test loss-0.8442, acc-0.7909\n",
      "Iter-18270, train loss-0.7254, acc-0.8800, valid loss-0.8434, acc-0.7890, test loss-0.8439, acc-0.7910\n",
      "Iter-18280, train loss-0.9121, acc-0.7400, valid loss-0.8430, acc-0.7890, test loss-0.8435, acc-0.7909\n",
      "Iter-18290, train loss-0.7591, acc-0.8000, valid loss-0.8427, acc-0.7892, test loss-0.8432, acc-0.7913\n",
      "Iter-18300, train loss-0.8930, acc-0.8200, valid loss-0.8423, acc-0.7896, test loss-0.8428, acc-0.7917\n",
      "Iter-18310, train loss-0.8759, acc-0.7400, valid loss-0.8420, acc-0.7894, test loss-0.8425, acc-0.7918\n",
      "Iter-18320, train loss-0.8120, acc-0.8000, valid loss-0.8416, acc-0.7894, test loss-0.8422, acc-0.7920\n",
      "Iter-18330, train loss-0.7473, acc-0.8400, valid loss-0.8413, acc-0.7894, test loss-0.8418, acc-0.7920\n",
      "Iter-18340, train loss-0.8760, acc-0.7600, valid loss-0.8410, acc-0.7896, test loss-0.8415, acc-0.7919\n",
      "Iter-18350, train loss-0.8274, acc-0.7400, valid loss-0.8406, acc-0.7896, test loss-0.8411, acc-0.7917\n",
      "Iter-18360, train loss-0.8997, acc-0.7600, valid loss-0.8402, acc-0.7894, test loss-0.8407, acc-0.7923\n",
      "Iter-18370, train loss-0.6467, acc-0.9200, valid loss-0.8399, acc-0.7894, test loss-0.8404, acc-0.7921\n",
      "Iter-18380, train loss-0.7951, acc-0.8600, valid loss-0.8395, acc-0.7894, test loss-0.8400, acc-0.7920\n",
      "Iter-18390, train loss-0.9336, acc-0.7600, valid loss-0.8392, acc-0.7898, test loss-0.8397, acc-0.7921\n",
      "Iter-18400, train loss-0.7890, acc-0.7800, valid loss-0.8388, acc-0.7900, test loss-0.8394, acc-0.7923\n",
      "Iter-18410, train loss-0.9120, acc-0.7800, valid loss-0.8385, acc-0.7900, test loss-0.8390, acc-0.7925\n",
      "Iter-18420, train loss-0.9666, acc-0.8000, valid loss-0.8381, acc-0.7902, test loss-0.8387, acc-0.7927\n",
      "Iter-18430, train loss-0.7304, acc-0.8200, valid loss-0.8378, acc-0.7904, test loss-0.8383, acc-0.7928\n",
      "Iter-18440, train loss-0.9566, acc-0.7000, valid loss-0.8374, acc-0.7904, test loss-0.8380, acc-0.7927\n",
      "Iter-18450, train loss-0.8164, acc-0.8400, valid loss-0.8371, acc-0.7906, test loss-0.8376, acc-0.7928\n",
      "Iter-18460, train loss-0.8943, acc-0.7800, valid loss-0.8367, acc-0.7904, test loss-0.8373, acc-0.7928\n",
      "Iter-18470, train loss-1.0707, acc-0.7000, valid loss-0.8364, acc-0.7906, test loss-0.8370, acc-0.7931\n",
      "Iter-18480, train loss-0.7930, acc-0.7800, valid loss-0.8361, acc-0.7906, test loss-0.8366, acc-0.7929\n",
      "Iter-18490, train loss-0.9135, acc-0.8200, valid loss-0.8357, acc-0.7902, test loss-0.8362, acc-0.7928\n",
      "Iter-18500, train loss-0.9110, acc-0.8000, valid loss-0.8353, acc-0.7904, test loss-0.8359, acc-0.7928\n",
      "Iter-18510, train loss-0.8640, acc-0.7600, valid loss-0.8350, acc-0.7910, test loss-0.8356, acc-0.7929\n",
      "Iter-18520, train loss-0.8310, acc-0.7800, valid loss-0.8347, acc-0.7910, test loss-0.8352, acc-0.7932\n",
      "Iter-18530, train loss-1.1013, acc-0.6600, valid loss-0.8343, acc-0.7912, test loss-0.8349, acc-0.7933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-18540, train loss-0.8818, acc-0.7800, valid loss-0.8340, acc-0.7912, test loss-0.8346, acc-0.7933\n",
      "Iter-18550, train loss-0.7633, acc-0.8800, valid loss-0.8336, acc-0.7912, test loss-0.8342, acc-0.7933\n",
      "Iter-18560, train loss-0.8554, acc-0.7600, valid loss-0.8333, acc-0.7924, test loss-0.8339, acc-0.7931\n",
      "Iter-18570, train loss-0.9690, acc-0.7000, valid loss-0.8330, acc-0.7928, test loss-0.8336, acc-0.7931\n",
      "Iter-18580, train loss-0.8288, acc-0.8000, valid loss-0.8326, acc-0.7928, test loss-0.8333, acc-0.7936\n",
      "Iter-18590, train loss-0.8469, acc-0.7600, valid loss-0.8323, acc-0.7926, test loss-0.8329, acc-0.7933\n",
      "Iter-18600, train loss-0.8577, acc-0.7600, valid loss-0.8320, acc-0.7928, test loss-0.8326, acc-0.7936\n",
      "Iter-18610, train loss-0.6933, acc-0.8600, valid loss-0.8316, acc-0.7928, test loss-0.8322, acc-0.7936\n",
      "Iter-18620, train loss-0.8658, acc-0.7800, valid loss-0.8313, acc-0.7928, test loss-0.8319, acc-0.7940\n",
      "Iter-18630, train loss-0.8004, acc-0.8200, valid loss-0.8309, acc-0.7932, test loss-0.8315, acc-0.7940\n",
      "Iter-18640, train loss-0.8931, acc-0.8400, valid loss-0.8305, acc-0.7932, test loss-0.8312, acc-0.7940\n",
      "Iter-18650, train loss-0.8456, acc-0.7600, valid loss-0.8302, acc-0.7932, test loss-0.8308, acc-0.7939\n",
      "Iter-18660, train loss-0.8461, acc-0.7400, valid loss-0.8299, acc-0.7930, test loss-0.8305, acc-0.7940\n",
      "Iter-18670, train loss-0.9490, acc-0.8000, valid loss-0.8295, acc-0.7932, test loss-0.8301, acc-0.7941\n",
      "Iter-18680, train loss-0.7714, acc-0.8200, valid loss-0.8292, acc-0.7936, test loss-0.8298, acc-0.7940\n",
      "Iter-18690, train loss-0.9300, acc-0.7400, valid loss-0.8288, acc-0.7936, test loss-0.8295, acc-0.7941\n",
      "Iter-18700, train loss-0.7830, acc-0.8200, valid loss-0.8285, acc-0.7940, test loss-0.8291, acc-0.7942\n",
      "Iter-18710, train loss-0.7345, acc-0.8000, valid loss-0.8282, acc-0.7940, test loss-0.8288, acc-0.7941\n",
      "Iter-18720, train loss-0.6058, acc-0.9200, valid loss-0.8278, acc-0.7940, test loss-0.8285, acc-0.7941\n",
      "Iter-18730, train loss-0.7683, acc-0.7800, valid loss-0.8275, acc-0.7944, test loss-0.8281, acc-0.7941\n",
      "Iter-18740, train loss-0.8301, acc-0.7800, valid loss-0.8271, acc-0.7944, test loss-0.8277, acc-0.7942\n",
      "Iter-18750, train loss-0.7014, acc-0.8200, valid loss-0.8267, acc-0.7944, test loss-0.8274, acc-0.7942\n",
      "Iter-18760, train loss-0.8005, acc-0.8200, valid loss-0.8264, acc-0.7944, test loss-0.8270, acc-0.7948\n",
      "Iter-18770, train loss-0.8359, acc-0.7600, valid loss-0.8261, acc-0.7944, test loss-0.8267, acc-0.7947\n",
      "Iter-18780, train loss-0.8541, acc-0.8000, valid loss-0.8257, acc-0.7944, test loss-0.8263, acc-0.7948\n",
      "Iter-18790, train loss-0.7831, acc-0.8400, valid loss-0.8253, acc-0.7944, test loss-0.8260, acc-0.7948\n",
      "Iter-18800, train loss-0.8177, acc-0.8000, valid loss-0.8250, acc-0.7944, test loss-0.8257, acc-0.7949\n",
      "Iter-18810, train loss-0.8061, acc-0.8400, valid loss-0.8246, acc-0.7944, test loss-0.8253, acc-0.7950\n",
      "Iter-18820, train loss-0.9934, acc-0.7000, valid loss-0.8243, acc-0.7944, test loss-0.8250, acc-0.7951\n",
      "Iter-18830, train loss-0.9117, acc-0.8000, valid loss-0.8239, acc-0.7944, test loss-0.8246, acc-0.7954\n",
      "Iter-18840, train loss-0.8280, acc-0.7600, valid loss-0.8236, acc-0.7944, test loss-0.8243, acc-0.7954\n",
      "Iter-18850, train loss-0.8389, acc-0.7400, valid loss-0.8233, acc-0.7946, test loss-0.8240, acc-0.7954\n",
      "Iter-18860, train loss-1.0850, acc-0.6200, valid loss-0.8230, acc-0.7946, test loss-0.8236, acc-0.7956\n",
      "Iter-18870, train loss-0.7968, acc-0.8400, valid loss-0.8226, acc-0.7946, test loss-0.8233, acc-0.7955\n",
      "Iter-18880, train loss-0.9645, acc-0.7400, valid loss-0.8223, acc-0.7946, test loss-0.8230, acc-0.7958\n",
      "Iter-18890, train loss-0.8376, acc-0.8400, valid loss-0.8219, acc-0.7946, test loss-0.8227, acc-0.7961\n",
      "Iter-18900, train loss-0.8948, acc-0.7400, valid loss-0.8216, acc-0.7948, test loss-0.8223, acc-0.7960\n",
      "Iter-18910, train loss-0.7573, acc-0.7800, valid loss-0.8212, acc-0.7952, test loss-0.8220, acc-0.7962\n",
      "Iter-18920, train loss-0.8135, acc-0.8000, valid loss-0.8209, acc-0.7958, test loss-0.8216, acc-0.7963\n",
      "Iter-18930, train loss-0.7826, acc-0.8200, valid loss-0.8205, acc-0.7956, test loss-0.8213, acc-0.7966\n",
      "Iter-18940, train loss-0.7600, acc-0.8400, valid loss-0.8202, acc-0.7956, test loss-0.8210, acc-0.7966\n",
      "Iter-18950, train loss-0.7398, acc-0.8000, valid loss-0.8198, acc-0.7956, test loss-0.8206, acc-0.7968\n",
      "Iter-18960, train loss-0.8195, acc-0.8000, valid loss-0.8195, acc-0.7958, test loss-0.8203, acc-0.7970\n",
      "Iter-18970, train loss-0.8389, acc-0.7800, valid loss-0.8192, acc-0.7958, test loss-0.8200, acc-0.7970\n",
      "Iter-18980, train loss-1.0113, acc-0.7200, valid loss-0.8189, acc-0.7964, test loss-0.8197, acc-0.7969\n",
      "Iter-18990, train loss-0.7742, acc-0.7800, valid loss-0.8185, acc-0.7964, test loss-0.8193, acc-0.7971\n",
      "Iter-19000, train loss-0.6341, acc-0.8000, valid loss-0.8182, acc-0.7964, test loss-0.8190, acc-0.7972\n",
      "Iter-19010, train loss-0.9247, acc-0.7200, valid loss-0.8178, acc-0.7964, test loss-0.8187, acc-0.7970\n",
      "Iter-19020, train loss-0.8251, acc-0.8400, valid loss-0.8175, acc-0.7964, test loss-0.8184, acc-0.7971\n",
      "Iter-19030, train loss-0.8217, acc-0.7800, valid loss-0.8172, acc-0.7964, test loss-0.8180, acc-0.7971\n",
      "Iter-19040, train loss-1.0173, acc-0.6800, valid loss-0.8168, acc-0.7966, test loss-0.8177, acc-0.7972\n",
      "Iter-19050, train loss-0.8716, acc-0.7800, valid loss-0.8165, acc-0.7968, test loss-0.8173, acc-0.7973\n",
      "Iter-19060, train loss-0.8332, acc-0.8000, valid loss-0.8162, acc-0.7970, test loss-0.8170, acc-0.7974\n",
      "Iter-19070, train loss-0.9768, acc-0.6600, valid loss-0.8159, acc-0.7970, test loss-0.8167, acc-0.7972\n",
      "Iter-19080, train loss-0.6933, acc-0.8800, valid loss-0.8155, acc-0.7972, test loss-0.8164, acc-0.7972\n",
      "Iter-19090, train loss-0.8217, acc-0.7800, valid loss-0.8152, acc-0.7970, test loss-0.8161, acc-0.7973\n",
      "Iter-19100, train loss-0.8543, acc-0.7600, valid loss-0.8149, acc-0.7970, test loss-0.8157, acc-0.7977\n",
      "Iter-19110, train loss-0.9780, acc-0.7200, valid loss-0.8146, acc-0.7976, test loss-0.8154, acc-0.7979\n",
      "Iter-19120, train loss-0.6928, acc-0.8200, valid loss-0.8142, acc-0.7976, test loss-0.8151, acc-0.7979\n",
      "Iter-19130, train loss-0.9539, acc-0.7600, valid loss-0.8139, acc-0.7974, test loss-0.8148, acc-0.7979\n",
      "Iter-19140, train loss-0.8573, acc-0.8200, valid loss-0.8136, acc-0.7974, test loss-0.8144, acc-0.7980\n",
      "Iter-19150, train loss-1.0300, acc-0.7200, valid loss-0.8133, acc-0.7972, test loss-0.8141, acc-0.7980\n",
      "Iter-19160, train loss-0.6673, acc-0.8800, valid loss-0.8129, acc-0.7972, test loss-0.8138, acc-0.7983\n",
      "Iter-19170, train loss-1.0634, acc-0.6600, valid loss-0.8126, acc-0.7974, test loss-0.8135, acc-0.7982\n",
      "Iter-19180, train loss-0.7446, acc-0.8800, valid loss-0.8123, acc-0.7972, test loss-0.8132, acc-0.7984\n",
      "Iter-19190, train loss-0.8116, acc-0.8600, valid loss-0.8120, acc-0.7974, test loss-0.8129, acc-0.7984\n",
      "Iter-19200, train loss-0.8995, acc-0.7400, valid loss-0.8117, acc-0.7974, test loss-0.8126, acc-0.7985\n",
      "Iter-19210, train loss-0.7812, acc-0.8600, valid loss-0.8114, acc-0.7970, test loss-0.8122, acc-0.7987\n",
      "Iter-19220, train loss-0.9266, acc-0.7200, valid loss-0.8111, acc-0.7972, test loss-0.8119, acc-0.7988\n",
      "Iter-19230, train loss-0.8427, acc-0.7800, valid loss-0.8107, acc-0.7974, test loss-0.8116, acc-0.7988\n",
      "Iter-19240, train loss-0.8569, acc-0.7800, valid loss-0.8104, acc-0.7970, test loss-0.8113, acc-0.7988\n",
      "Iter-19250, train loss-0.8846, acc-0.6800, valid loss-0.8101, acc-0.7972, test loss-0.8109, acc-0.7988\n",
      "Iter-19260, train loss-0.7992, acc-0.8400, valid loss-0.8097, acc-0.7976, test loss-0.8106, acc-0.7989\n",
      "Iter-19270, train loss-0.6701, acc-0.9400, valid loss-0.8094, acc-0.7982, test loss-0.8103, acc-0.7989\n",
      "Iter-19280, train loss-0.7831, acc-0.8200, valid loss-0.8091, acc-0.7986, test loss-0.8100, acc-0.7989\n",
      "Iter-19290, train loss-0.7013, acc-0.8600, valid loss-0.8087, acc-0.7986, test loss-0.8096, acc-0.7991\n",
      "Iter-19300, train loss-0.9880, acc-0.6800, valid loss-0.8084, acc-0.7986, test loss-0.8093, acc-0.7991\n",
      "Iter-19310, train loss-0.8923, acc-0.8000, valid loss-0.8080, acc-0.7986, test loss-0.8089, acc-0.7992\n",
      "Iter-19320, train loss-0.9358, acc-0.8000, valid loss-0.8077, acc-0.7988, test loss-0.8087, acc-0.7992\n",
      "Iter-19330, train loss-0.8347, acc-0.8000, valid loss-0.8074, acc-0.7988, test loss-0.8083, acc-0.7993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-19340, train loss-0.8534, acc-0.8000, valid loss-0.8071, acc-0.7992, test loss-0.8080, acc-0.7997\n",
      "Iter-19350, train loss-0.8961, acc-0.8000, valid loss-0.8068, acc-0.7992, test loss-0.8077, acc-0.7996\n",
      "Iter-19360, train loss-0.9694, acc-0.7200, valid loss-0.8065, acc-0.7990, test loss-0.8074, acc-0.7998\n",
      "Iter-19370, train loss-0.8938, acc-0.7600, valid loss-0.8062, acc-0.7992, test loss-0.8071, acc-0.7998\n",
      "Iter-19380, train loss-0.6950, acc-0.8400, valid loss-0.8058, acc-0.7996, test loss-0.8068, acc-0.7998\n",
      "Iter-19390, train loss-0.8781, acc-0.8000, valid loss-0.8055, acc-0.7996, test loss-0.8065, acc-0.7999\n",
      "Iter-19400, train loss-0.7455, acc-0.8200, valid loss-0.8052, acc-0.7996, test loss-0.8061, acc-0.8001\n",
      "Iter-19410, train loss-0.6249, acc-0.9200, valid loss-0.8049, acc-0.7996, test loss-0.8058, acc-0.8003\n",
      "Iter-19420, train loss-0.6877, acc-0.8400, valid loss-0.8046, acc-0.7998, test loss-0.8055, acc-0.8003\n",
      "Iter-19430, train loss-0.8199, acc-0.8400, valid loss-0.8042, acc-0.7996, test loss-0.8052, acc-0.8002\n",
      "Iter-19440, train loss-0.8677, acc-0.7400, valid loss-0.8039, acc-0.7998, test loss-0.8049, acc-0.8004\n",
      "Iter-19450, train loss-1.0315, acc-0.6800, valid loss-0.8036, acc-0.8002, test loss-0.8046, acc-0.8004\n",
      "Iter-19460, train loss-0.6928, acc-0.8400, valid loss-0.8034, acc-0.8002, test loss-0.8043, acc-0.8004\n",
      "Iter-19470, train loss-0.9063, acc-0.7200, valid loss-0.8031, acc-0.8000, test loss-0.8040, acc-0.8004\n",
      "Iter-19480, train loss-0.8381, acc-0.8200, valid loss-0.8028, acc-0.8000, test loss-0.8037, acc-0.8003\n",
      "Iter-19490, train loss-0.7121, acc-0.8800, valid loss-0.8025, acc-0.8004, test loss-0.8034, acc-0.8005\n",
      "Iter-19500, train loss-0.7761, acc-0.7800, valid loss-0.8022, acc-0.8004, test loss-0.8031, acc-0.8005\n",
      "Iter-19510, train loss-0.7873, acc-0.7600, valid loss-0.8019, acc-0.8004, test loss-0.8028, acc-0.8008\n",
      "Iter-19520, train loss-0.7685, acc-0.8200, valid loss-0.8015, acc-0.8004, test loss-0.8024, acc-0.8009\n",
      "Iter-19530, train loss-0.6757, acc-0.8200, valid loss-0.8012, acc-0.8004, test loss-0.8021, acc-0.8007\n",
      "Iter-19540, train loss-0.6898, acc-0.8800, valid loss-0.8009, acc-0.8004, test loss-0.8018, acc-0.8007\n",
      "Iter-19550, train loss-0.9285, acc-0.7800, valid loss-0.8006, acc-0.8006, test loss-0.8015, acc-0.8007\n",
      "Iter-19560, train loss-0.8853, acc-0.7400, valid loss-0.8003, acc-0.8004, test loss-0.8012, acc-0.8008\n",
      "Iter-19570, train loss-0.6928, acc-0.8200, valid loss-0.8000, acc-0.8004, test loss-0.8009, acc-0.8010\n",
      "Iter-19580, train loss-0.7754, acc-0.8400, valid loss-0.7997, acc-0.8006, test loss-0.8006, acc-0.8008\n",
      "Iter-19590, train loss-0.7042, acc-0.8600, valid loss-0.7994, acc-0.8006, test loss-0.8003, acc-0.8013\n",
      "Iter-19600, train loss-0.7201, acc-0.8200, valid loss-0.7991, acc-0.8004, test loss-0.8000, acc-0.8011\n",
      "Iter-19610, train loss-0.7949, acc-0.7400, valid loss-0.7988, acc-0.8008, test loss-0.7997, acc-0.8012\n",
      "Iter-19620, train loss-0.7164, acc-0.8400, valid loss-0.7985, acc-0.8004, test loss-0.7994, acc-0.8013\n",
      "Iter-19630, train loss-0.8145, acc-0.8200, valid loss-0.7982, acc-0.8006, test loss-0.7991, acc-0.8012\n",
      "Iter-19640, train loss-0.8938, acc-0.7400, valid loss-0.7979, acc-0.8006, test loss-0.7988, acc-0.8012\n",
      "Iter-19650, train loss-0.7102, acc-0.8400, valid loss-0.7976, acc-0.8008, test loss-0.7985, acc-0.8012\n",
      "Iter-19660, train loss-0.8029, acc-0.8200, valid loss-0.7972, acc-0.8006, test loss-0.7982, acc-0.8015\n",
      "Iter-19670, train loss-0.8079, acc-0.8000, valid loss-0.7969, acc-0.8010, test loss-0.7979, acc-0.8015\n",
      "Iter-19680, train loss-0.6082, acc-0.8800, valid loss-0.7966, acc-0.8012, test loss-0.7976, acc-0.8018\n",
      "Iter-19690, train loss-0.7239, acc-0.8000, valid loss-0.7963, acc-0.8012, test loss-0.7973, acc-0.8021\n",
      "Iter-19700, train loss-0.8338, acc-0.7400, valid loss-0.7961, acc-0.8012, test loss-0.7970, acc-0.8022\n",
      "Iter-19710, train loss-0.9348, acc-0.7000, valid loss-0.7957, acc-0.8012, test loss-0.7966, acc-0.8022\n",
      "Iter-19720, train loss-0.9888, acc-0.6600, valid loss-0.7954, acc-0.8012, test loss-0.7963, acc-0.8022\n",
      "Iter-19730, train loss-0.8353, acc-0.7800, valid loss-0.7951, acc-0.8012, test loss-0.7961, acc-0.8023\n",
      "Iter-19740, train loss-0.9011, acc-0.7800, valid loss-0.7948, acc-0.8016, test loss-0.7958, acc-0.8025\n",
      "Iter-19750, train loss-0.5922, acc-0.8800, valid loss-0.7945, acc-0.8016, test loss-0.7955, acc-0.8025\n",
      "Iter-19760, train loss-0.7680, acc-0.8000, valid loss-0.7942, acc-0.8016, test loss-0.7952, acc-0.8026\n",
      "Iter-19770, train loss-0.6531, acc-0.8200, valid loss-0.7939, acc-0.8018, test loss-0.7949, acc-0.8027\n",
      "Iter-19780, train loss-1.0734, acc-0.6800, valid loss-0.7937, acc-0.8018, test loss-0.7946, acc-0.8029\n",
      "Iter-19790, train loss-0.6714, acc-0.8000, valid loss-0.7934, acc-0.8016, test loss-0.7943, acc-0.8028\n",
      "Iter-19800, train loss-0.7127, acc-0.8800, valid loss-0.7930, acc-0.8020, test loss-0.7940, acc-0.8029\n",
      "Iter-19810, train loss-0.7375, acc-0.8800, valid loss-0.7927, acc-0.8020, test loss-0.7937, acc-0.8026\n",
      "Iter-19820, train loss-0.7282, acc-0.8400, valid loss-0.7924, acc-0.8020, test loss-0.7934, acc-0.8024\n",
      "Iter-19830, train loss-0.7678, acc-0.8200, valid loss-0.7921, acc-0.8022, test loss-0.7931, acc-0.8023\n",
      "Iter-19840, train loss-0.8505, acc-0.8000, valid loss-0.7918, acc-0.8022, test loss-0.7928, acc-0.8023\n",
      "Iter-19850, train loss-0.9453, acc-0.7400, valid loss-0.7915, acc-0.8022, test loss-0.7925, acc-0.8024\n",
      "Iter-19860, train loss-0.7271, acc-0.8200, valid loss-0.7912, acc-0.8024, test loss-0.7922, acc-0.8023\n",
      "Iter-19870, train loss-0.8777, acc-0.8200, valid loss-0.7910, acc-0.8026, test loss-0.7919, acc-0.8023\n",
      "Iter-19880, train loss-0.8377, acc-0.7600, valid loss-0.7906, acc-0.8024, test loss-0.7916, acc-0.8021\n",
      "Iter-19890, train loss-0.8151, acc-0.8200, valid loss-0.7903, acc-0.8024, test loss-0.7913, acc-0.8022\n",
      "Iter-19900, train loss-0.8375, acc-0.8400, valid loss-0.7901, acc-0.8026, test loss-0.7911, acc-0.8023\n",
      "Iter-19910, train loss-0.7822, acc-0.8200, valid loss-0.7898, acc-0.8026, test loss-0.7908, acc-0.8022\n",
      "Iter-19920, train loss-0.8886, acc-0.7800, valid loss-0.7895, acc-0.8026, test loss-0.7905, acc-0.8022\n",
      "Iter-19930, train loss-0.8997, acc-0.7400, valid loss-0.7892, acc-0.8026, test loss-0.7902, acc-0.8023\n",
      "Iter-19940, train loss-0.9988, acc-0.6800, valid loss-0.7889, acc-0.8030, test loss-0.7899, acc-0.8024\n",
      "Iter-19950, train loss-0.6406, acc-0.8800, valid loss-0.7886, acc-0.8030, test loss-0.7896, acc-0.8026\n",
      "Iter-19960, train loss-0.6839, acc-0.8400, valid loss-0.7883, acc-0.8036, test loss-0.7894, acc-0.8026\n",
      "Iter-19970, train loss-0.6735, acc-0.8400, valid loss-0.7880, acc-0.8036, test loss-0.7890, acc-0.8025\n",
      "Iter-19980, train loss-0.8622, acc-0.7800, valid loss-0.7877, acc-0.8036, test loss-0.7888, acc-0.8027\n",
      "Iter-19990, train loss-0.7645, acc-0.8200, valid loss-0.7874, acc-0.8036, test loss-0.7885, acc-0.8028\n",
      "Iter-20000, train loss-1.0292, acc-0.7400, valid loss-0.7871, acc-0.8036, test loss-0.7882, acc-0.8031\n",
      "Iter-20010, train loss-0.9421, acc-0.7800, valid loss-0.7868, acc-0.8034, test loss-0.7879, acc-0.8030\n",
      "Iter-20020, train loss-0.8311, acc-0.8000, valid loss-0.7866, acc-0.8036, test loss-0.7876, acc-0.8031\n",
      "Iter-20030, train loss-0.7638, acc-0.8000, valid loss-0.7862, acc-0.8032, test loss-0.7873, acc-0.8032\n",
      "Iter-20040, train loss-0.7407, acc-0.7600, valid loss-0.7860, acc-0.8036, test loss-0.7870, acc-0.8032\n",
      "Iter-20050, train loss-0.9931, acc-0.7400, valid loss-0.7857, acc-0.8034, test loss-0.7867, acc-0.8031\n",
      "Iter-20060, train loss-0.8717, acc-0.7600, valid loss-0.7854, acc-0.8034, test loss-0.7865, acc-0.8032\n",
      "Iter-20070, train loss-0.8449, acc-0.7600, valid loss-0.7851, acc-0.8032, test loss-0.7862, acc-0.8030\n",
      "Iter-20080, train loss-1.0969, acc-0.6400, valid loss-0.7848, acc-0.8034, test loss-0.7859, acc-0.8027\n",
      "Iter-20090, train loss-0.8201, acc-0.8000, valid loss-0.7845, acc-0.8042, test loss-0.7856, acc-0.8029\n",
      "Iter-20100, train loss-0.9382, acc-0.7000, valid loss-0.7842, acc-0.8042, test loss-0.7853, acc-0.8031\n",
      "Iter-20110, train loss-0.8413, acc-0.8000, valid loss-0.7839, acc-0.8044, test loss-0.7850, acc-0.8029\n",
      "Iter-20120, train loss-0.8647, acc-0.8000, valid loss-0.7836, acc-0.8046, test loss-0.7847, acc-0.8033\n",
      "Iter-20130, train loss-0.8910, acc-0.7600, valid loss-0.7833, acc-0.8046, test loss-0.7844, acc-0.8032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-20140, train loss-1.0299, acc-0.7000, valid loss-0.7831, acc-0.8046, test loss-0.7842, acc-0.8032\n",
      "Iter-20150, train loss-0.7316, acc-0.8200, valid loss-0.7828, acc-0.8044, test loss-0.7839, acc-0.8032\n",
      "Iter-20160, train loss-0.7821, acc-0.7800, valid loss-0.7825, acc-0.8042, test loss-0.7836, acc-0.8034\n",
      "Iter-20170, train loss-0.8678, acc-0.8000, valid loss-0.7822, acc-0.8044, test loss-0.7833, acc-0.8035\n",
      "Iter-20180, train loss-0.7827, acc-0.8200, valid loss-0.7819, acc-0.8044, test loss-0.7830, acc-0.8035\n",
      "Iter-20190, train loss-0.8462, acc-0.7800, valid loss-0.7816, acc-0.8046, test loss-0.7827, acc-0.8037\n",
      "Iter-20200, train loss-0.7797, acc-0.8600, valid loss-0.7814, acc-0.8046, test loss-0.7825, acc-0.8037\n",
      "Iter-20210, train loss-1.0220, acc-0.6400, valid loss-0.7811, acc-0.8048, test loss-0.7822, acc-0.8038\n",
      "Iter-20220, train loss-0.8505, acc-0.7600, valid loss-0.7808, acc-0.8046, test loss-0.7819, acc-0.8040\n",
      "Iter-20230, train loss-0.7486, acc-0.7400, valid loss-0.7805, acc-0.8046, test loss-0.7816, acc-0.8040\n",
      "Iter-20240, train loss-0.8486, acc-0.8000, valid loss-0.7802, acc-0.8046, test loss-0.7813, acc-0.8040\n",
      "Iter-20250, train loss-0.7257, acc-0.8600, valid loss-0.7799, acc-0.8048, test loss-0.7811, acc-0.8038\n",
      "Iter-20260, train loss-0.8490, acc-0.7600, valid loss-0.7796, acc-0.8050, test loss-0.7808, acc-0.8042\n",
      "Iter-20270, train loss-0.9154, acc-0.7800, valid loss-0.7793, acc-0.8050, test loss-0.7805, acc-0.8044\n",
      "Iter-20280, train loss-0.8150, acc-0.7800, valid loss-0.7791, acc-0.8050, test loss-0.7802, acc-0.8044\n",
      "Iter-20290, train loss-0.8026, acc-0.7800, valid loss-0.7788, acc-0.8052, test loss-0.7799, acc-0.8043\n",
      "Iter-20300, train loss-0.9141, acc-0.6800, valid loss-0.7785, acc-0.8052, test loss-0.7796, acc-0.8045\n",
      "Iter-20310, train loss-0.7690, acc-0.8000, valid loss-0.7782, acc-0.8054, test loss-0.7793, acc-0.8047\n",
      "Iter-20320, train loss-0.9456, acc-0.7400, valid loss-0.7780, acc-0.8054, test loss-0.7791, acc-0.8047\n",
      "Iter-20330, train loss-0.7491, acc-0.8000, valid loss-0.7777, acc-0.8052, test loss-0.7788, acc-0.8047\n",
      "Iter-20340, train loss-0.9961, acc-0.7200, valid loss-0.7774, acc-0.8052, test loss-0.7785, acc-0.8048\n",
      "Iter-20350, train loss-0.9442, acc-0.7400, valid loss-0.7771, acc-0.8052, test loss-0.7782, acc-0.8049\n",
      "Iter-20360, train loss-0.8968, acc-0.7600, valid loss-0.7768, acc-0.8052, test loss-0.7780, acc-0.8049\n",
      "Iter-20370, train loss-0.6624, acc-0.9200, valid loss-0.7765, acc-0.8056, test loss-0.7777, acc-0.8049\n",
      "Iter-20380, train loss-0.8974, acc-0.7200, valid loss-0.7763, acc-0.8056, test loss-0.7774, acc-0.8049\n",
      "Iter-20390, train loss-0.7425, acc-0.8200, valid loss-0.7760, acc-0.8058, test loss-0.7771, acc-0.8052\n",
      "Iter-20400, train loss-0.7249, acc-0.8200, valid loss-0.7757, acc-0.8062, test loss-0.7769, acc-0.8052\n",
      "Iter-20410, train loss-0.8278, acc-0.7600, valid loss-0.7754, acc-0.8062, test loss-0.7766, acc-0.8052\n",
      "Iter-20420, train loss-0.7798, acc-0.7800, valid loss-0.7752, acc-0.8062, test loss-0.7764, acc-0.8056\n",
      "Iter-20430, train loss-0.7408, acc-0.8200, valid loss-0.7749, acc-0.8064, test loss-0.7761, acc-0.8056\n",
      "Iter-20440, train loss-0.7747, acc-0.8400, valid loss-0.7746, acc-0.8066, test loss-0.7758, acc-0.8058\n",
      "Iter-20450, train loss-0.6308, acc-0.8800, valid loss-0.7743, acc-0.8066, test loss-0.7755, acc-0.8057\n",
      "Iter-20460, train loss-0.7089, acc-0.9000, valid loss-0.7740, acc-0.8068, test loss-0.7752, acc-0.8057\n",
      "Iter-20470, train loss-0.7612, acc-0.7600, valid loss-0.7738, acc-0.8068, test loss-0.7750, acc-0.8058\n",
      "Iter-20480, train loss-0.6801, acc-0.8600, valid loss-0.7735, acc-0.8068, test loss-0.7747, acc-0.8062\n",
      "Iter-20490, train loss-0.7746, acc-0.8000, valid loss-0.7732, acc-0.8070, test loss-0.7744, acc-0.8063\n",
      "Iter-20500, train loss-0.7892, acc-0.8200, valid loss-0.7729, acc-0.8070, test loss-0.7742, acc-0.8065\n",
      "Iter-20510, train loss-0.7148, acc-0.8000, valid loss-0.7726, acc-0.8070, test loss-0.7739, acc-0.8067\n",
      "Iter-20520, train loss-0.7515, acc-0.8400, valid loss-0.7723, acc-0.8070, test loss-0.7736, acc-0.8066\n",
      "Iter-20530, train loss-0.7687, acc-0.7800, valid loss-0.7720, acc-0.8072, test loss-0.7733, acc-0.8068\n",
      "Iter-20540, train loss-0.8344, acc-0.7200, valid loss-0.7717, acc-0.8074, test loss-0.7730, acc-0.8070\n",
      "Iter-20550, train loss-0.8885, acc-0.7200, valid loss-0.7715, acc-0.8076, test loss-0.7728, acc-0.8070\n",
      "Iter-20560, train loss-0.5299, acc-0.9600, valid loss-0.7712, acc-0.8076, test loss-0.7725, acc-0.8071\n",
      "Iter-20570, train loss-0.7275, acc-0.8200, valid loss-0.7710, acc-0.8076, test loss-0.7723, acc-0.8072\n",
      "Iter-20580, train loss-0.8035, acc-0.8200, valid loss-0.7707, acc-0.8076, test loss-0.7720, acc-0.8073\n",
      "Iter-20590, train loss-0.6318, acc-0.8600, valid loss-0.7704, acc-0.8076, test loss-0.7717, acc-0.8074\n",
      "Iter-20600, train loss-0.6210, acc-0.8600, valid loss-0.7702, acc-0.8076, test loss-0.7715, acc-0.8075\n",
      "Iter-20610, train loss-0.8590, acc-0.7600, valid loss-0.7699, acc-0.8078, test loss-0.7712, acc-0.8073\n",
      "Iter-20620, train loss-0.8180, acc-0.7800, valid loss-0.7696, acc-0.8078, test loss-0.7709, acc-0.8074\n",
      "Iter-20630, train loss-0.7072, acc-0.8400, valid loss-0.7693, acc-0.8078, test loss-0.7707, acc-0.8074\n",
      "Iter-20640, train loss-0.6962, acc-0.8000, valid loss-0.7690, acc-0.8084, test loss-0.7704, acc-0.8077\n",
      "Iter-20650, train loss-0.8451, acc-0.7800, valid loss-0.7688, acc-0.8084, test loss-0.7701, acc-0.8076\n",
      "Iter-20660, train loss-0.7180, acc-0.8400, valid loss-0.7685, acc-0.8084, test loss-0.7698, acc-0.8078\n",
      "Iter-20670, train loss-0.7910, acc-0.8000, valid loss-0.7682, acc-0.8084, test loss-0.7695, acc-0.8079\n",
      "Iter-20680, train loss-0.7679, acc-0.7800, valid loss-0.7679, acc-0.8086, test loss-0.7692, acc-0.8077\n",
      "Iter-20690, train loss-0.7108, acc-0.8400, valid loss-0.7676, acc-0.8088, test loss-0.7690, acc-0.8079\n",
      "Iter-20700, train loss-0.7665, acc-0.8200, valid loss-0.7674, acc-0.8090, test loss-0.7687, acc-0.8078\n",
      "Iter-20710, train loss-0.7898, acc-0.7600, valid loss-0.7671, acc-0.8090, test loss-0.7684, acc-0.8080\n",
      "Iter-20720, train loss-0.7268, acc-0.8400, valid loss-0.7668, acc-0.8090, test loss-0.7682, acc-0.8077\n",
      "Iter-20730, train loss-0.8561, acc-0.8200, valid loss-0.7666, acc-0.8088, test loss-0.7679, acc-0.8081\n",
      "Iter-20740, train loss-0.8904, acc-0.7200, valid loss-0.7663, acc-0.8088, test loss-0.7677, acc-0.8082\n",
      "Iter-20750, train loss-0.7832, acc-0.7400, valid loss-0.7661, acc-0.8088, test loss-0.7674, acc-0.8081\n",
      "Iter-20760, train loss-0.7236, acc-0.8600, valid loss-0.7658, acc-0.8088, test loss-0.7672, acc-0.8081\n",
      "Iter-20770, train loss-0.8544, acc-0.7800, valid loss-0.7655, acc-0.8088, test loss-0.7669, acc-0.8082\n",
      "Iter-20780, train loss-0.6982, acc-0.8400, valid loss-0.7653, acc-0.8090, test loss-0.7666, acc-0.8082\n",
      "Iter-20790, train loss-0.8283, acc-0.7200, valid loss-0.7650, acc-0.8094, test loss-0.7663, acc-0.8083\n",
      "Iter-20800, train loss-0.8729, acc-0.7600, valid loss-0.7647, acc-0.8094, test loss-0.7661, acc-0.8080\n",
      "Iter-20810, train loss-0.6319, acc-0.8400, valid loss-0.7644, acc-0.8094, test loss-0.7658, acc-0.8083\n",
      "Iter-20820, train loss-0.6849, acc-0.7800, valid loss-0.7642, acc-0.8098, test loss-0.7655, acc-0.8082\n",
      "Iter-20830, train loss-0.7668, acc-0.8400, valid loss-0.7639, acc-0.8100, test loss-0.7653, acc-0.8083\n",
      "Iter-20840, train loss-0.8989, acc-0.7400, valid loss-0.7636, acc-0.8102, test loss-0.7650, acc-0.8082\n",
      "Iter-20850, train loss-0.7341, acc-0.8200, valid loss-0.7634, acc-0.8102, test loss-0.7648, acc-0.8083\n",
      "Iter-20860, train loss-0.8807, acc-0.8200, valid loss-0.7631, acc-0.8102, test loss-0.7645, acc-0.8083\n",
      "Iter-20870, train loss-0.6439, acc-0.8200, valid loss-0.7628, acc-0.8100, test loss-0.7642, acc-0.8082\n",
      "Iter-20880, train loss-0.7490, acc-0.8400, valid loss-0.7626, acc-0.8102, test loss-0.7640, acc-0.8082\n",
      "Iter-20890, train loss-0.7811, acc-0.8200, valid loss-0.7623, acc-0.8100, test loss-0.7637, acc-0.8083\n",
      "Iter-20900, train loss-0.6067, acc-0.9200, valid loss-0.7620, acc-0.8102, test loss-0.7635, acc-0.8084\n",
      "Iter-20910, train loss-0.7489, acc-0.8400, valid loss-0.7618, acc-0.8100, test loss-0.7632, acc-0.8085\n",
      "Iter-20920, train loss-0.8475, acc-0.7200, valid loss-0.7615, acc-0.8108, test loss-0.7630, acc-0.8085\n",
      "Iter-20930, train loss-1.0802, acc-0.7400, valid loss-0.7612, acc-0.8106, test loss-0.7627, acc-0.8087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-20940, train loss-0.7000, acc-0.8600, valid loss-0.7610, acc-0.8108, test loss-0.7625, acc-0.8087\n",
      "Iter-20950, train loss-0.6707, acc-0.8400, valid loss-0.7608, acc-0.8106, test loss-0.7622, acc-0.8086\n",
      "Iter-20960, train loss-0.7892, acc-0.7800, valid loss-0.7605, acc-0.8108, test loss-0.7620, acc-0.8088\n",
      "Iter-20970, train loss-0.8391, acc-0.7400, valid loss-0.7602, acc-0.8108, test loss-0.7617, acc-0.8085\n",
      "Iter-20980, train loss-0.7948, acc-0.8200, valid loss-0.7599, acc-0.8110, test loss-0.7614, acc-0.8087\n",
      "Iter-20990, train loss-0.9254, acc-0.6800, valid loss-0.7597, acc-0.8110, test loss-0.7612, acc-0.8086\n",
      "Iter-21000, train loss-0.9971, acc-0.8000, valid loss-0.7594, acc-0.8110, test loss-0.7609, acc-0.8086\n",
      "Iter-21010, train loss-0.8821, acc-0.8200, valid loss-0.7592, acc-0.8114, test loss-0.7607, acc-0.8086\n",
      "Iter-21020, train loss-0.7843, acc-0.8200, valid loss-0.7589, acc-0.8112, test loss-0.7604, acc-0.8088\n",
      "Iter-21030, train loss-0.7903, acc-0.8600, valid loss-0.7587, acc-0.8114, test loss-0.7602, acc-0.8087\n",
      "Iter-21040, train loss-0.8205, acc-0.7400, valid loss-0.7584, acc-0.8116, test loss-0.7599, acc-0.8087\n",
      "Iter-21050, train loss-0.7352, acc-0.8000, valid loss-0.7581, acc-0.8116, test loss-0.7597, acc-0.8086\n",
      "Iter-21060, train loss-0.7820, acc-0.7400, valid loss-0.7579, acc-0.8116, test loss-0.7594, acc-0.8086\n",
      "Iter-21070, train loss-0.6624, acc-0.8200, valid loss-0.7576, acc-0.8114, test loss-0.7592, acc-0.8086\n",
      "Iter-21080, train loss-0.8809, acc-0.7800, valid loss-0.7574, acc-0.8114, test loss-0.7589, acc-0.8087\n",
      "Iter-21090, train loss-0.6450, acc-0.8600, valid loss-0.7571, acc-0.8114, test loss-0.7587, acc-0.8087\n",
      "Iter-21100, train loss-0.8284, acc-0.7400, valid loss-0.7568, acc-0.8116, test loss-0.7584, acc-0.8090\n",
      "Iter-21110, train loss-0.7070, acc-0.9000, valid loss-0.7566, acc-0.8116, test loss-0.7581, acc-0.8089\n",
      "Iter-21120, train loss-0.8231, acc-0.7800, valid loss-0.7563, acc-0.8114, test loss-0.7579, acc-0.8089\n",
      "Iter-21130, train loss-0.9127, acc-0.7600, valid loss-0.7561, acc-0.8116, test loss-0.7576, acc-0.8088\n",
      "Iter-21140, train loss-0.6597, acc-0.8600, valid loss-0.7558, acc-0.8116, test loss-0.7574, acc-0.8087\n",
      "Iter-21150, train loss-0.8155, acc-0.8400, valid loss-0.7556, acc-0.8120, test loss-0.7571, acc-0.8088\n",
      "Iter-21160, train loss-0.7412, acc-0.8800, valid loss-0.7553, acc-0.8120, test loss-0.7569, acc-0.8089\n",
      "Iter-21170, train loss-0.6938, acc-0.8400, valid loss-0.7551, acc-0.8122, test loss-0.7566, acc-0.8091\n",
      "Iter-21180, train loss-0.6812, acc-0.8200, valid loss-0.7548, acc-0.8120, test loss-0.7564, acc-0.8093\n",
      "Iter-21190, train loss-0.6590, acc-0.8200, valid loss-0.7546, acc-0.8118, test loss-0.7562, acc-0.8094\n",
      "Iter-21200, train loss-0.8940, acc-0.7200, valid loss-0.7544, acc-0.8118, test loss-0.7559, acc-0.8096\n",
      "Iter-21210, train loss-0.7242, acc-0.8400, valid loss-0.7541, acc-0.8120, test loss-0.7556, acc-0.8095\n",
      "Iter-21220, train loss-0.8723, acc-0.7800, valid loss-0.7538, acc-0.8124, test loss-0.7554, acc-0.8099\n",
      "Iter-21230, train loss-0.6699, acc-0.8600, valid loss-0.7536, acc-0.8122, test loss-0.7551, acc-0.8098\n",
      "Iter-21240, train loss-0.8185, acc-0.7400, valid loss-0.7533, acc-0.8120, test loss-0.7549, acc-0.8098\n",
      "Iter-21250, train loss-0.8389, acc-0.7200, valid loss-0.7531, acc-0.8122, test loss-0.7547, acc-0.8099\n",
      "Iter-21260, train loss-0.8416, acc-0.7600, valid loss-0.7529, acc-0.8124, test loss-0.7544, acc-0.8098\n",
      "Iter-21270, train loss-0.9912, acc-0.6800, valid loss-0.7526, acc-0.8126, test loss-0.7541, acc-0.8101\n",
      "Iter-21280, train loss-0.7814, acc-0.8000, valid loss-0.7524, acc-0.8124, test loss-0.7539, acc-0.8100\n",
      "Iter-21290, train loss-0.6827, acc-0.7800, valid loss-0.7521, acc-0.8124, test loss-0.7537, acc-0.8100\n",
      "Iter-21300, train loss-0.7569, acc-0.8400, valid loss-0.7518, acc-0.8124, test loss-0.7534, acc-0.8098\n",
      "Iter-21310, train loss-0.8115, acc-0.7600, valid loss-0.7516, acc-0.8124, test loss-0.7531, acc-0.8099\n",
      "Iter-21320, train loss-0.7842, acc-0.7800, valid loss-0.7514, acc-0.8124, test loss-0.7529, acc-0.8099\n",
      "Iter-21330, train loss-0.6456, acc-0.8600, valid loss-0.7511, acc-0.8124, test loss-0.7527, acc-0.8099\n",
      "Iter-21340, train loss-0.9065, acc-0.7200, valid loss-0.7509, acc-0.8124, test loss-0.7525, acc-0.8101\n",
      "Iter-21350, train loss-0.6915, acc-0.8200, valid loss-0.7506, acc-0.8122, test loss-0.7522, acc-0.8101\n",
      "Iter-21360, train loss-0.6209, acc-0.8400, valid loss-0.7504, acc-0.8122, test loss-0.7520, acc-0.8100\n",
      "Iter-21370, train loss-0.7311, acc-0.7600, valid loss-0.7501, acc-0.8122, test loss-0.7517, acc-0.8102\n",
      "Iter-21380, train loss-0.6961, acc-0.8200, valid loss-0.7499, acc-0.8124, test loss-0.7514, acc-0.8102\n",
      "Iter-21390, train loss-0.6169, acc-0.8600, valid loss-0.7496, acc-0.8124, test loss-0.7512, acc-0.8103\n",
      "Iter-21400, train loss-0.8732, acc-0.7400, valid loss-0.7494, acc-0.8124, test loss-0.7510, acc-0.8103\n",
      "Iter-21410, train loss-0.5816, acc-0.8600, valid loss-0.7491, acc-0.8124, test loss-0.7507, acc-0.8105\n",
      "Iter-21420, train loss-0.5994, acc-0.8400, valid loss-0.7489, acc-0.8124, test loss-0.7505, acc-0.8106\n",
      "Iter-21430, train loss-0.7077, acc-0.8800, valid loss-0.7486, acc-0.8124, test loss-0.7502, acc-0.8108\n",
      "Iter-21440, train loss-0.8171, acc-0.8600, valid loss-0.7484, acc-0.8126, test loss-0.7500, acc-0.8110\n",
      "Iter-21450, train loss-0.8129, acc-0.8200, valid loss-0.7481, acc-0.8126, test loss-0.7497, acc-0.8109\n",
      "Iter-21460, train loss-0.8407, acc-0.7400, valid loss-0.7478, acc-0.8126, test loss-0.7495, acc-0.8110\n",
      "Iter-21470, train loss-0.7892, acc-0.7600, valid loss-0.7476, acc-0.8126, test loss-0.7492, acc-0.8111\n",
      "Iter-21480, train loss-0.6919, acc-0.8000, valid loss-0.7473, acc-0.8124, test loss-0.7490, acc-0.8113\n",
      "Iter-21490, train loss-0.7414, acc-0.7600, valid loss-0.7471, acc-0.8128, test loss-0.7487, acc-0.8112\n",
      "Iter-21500, train loss-0.8879, acc-0.7000, valid loss-0.7469, acc-0.8126, test loss-0.7485, acc-0.8115\n",
      "Iter-21510, train loss-0.6899, acc-0.8600, valid loss-0.7466, acc-0.8130, test loss-0.7483, acc-0.8116\n",
      "Iter-21520, train loss-0.7139, acc-0.8400, valid loss-0.7464, acc-0.8132, test loss-0.7480, acc-0.8116\n",
      "Iter-21530, train loss-0.7239, acc-0.8600, valid loss-0.7461, acc-0.8134, test loss-0.7478, acc-0.8116\n",
      "Iter-21540, train loss-0.7109, acc-0.8000, valid loss-0.7458, acc-0.8134, test loss-0.7475, acc-0.8116\n",
      "Iter-21550, train loss-0.7634, acc-0.8200, valid loss-0.7456, acc-0.8134, test loss-0.7473, acc-0.8119\n",
      "Iter-21560, train loss-0.7587, acc-0.7800, valid loss-0.7453, acc-0.8134, test loss-0.7470, acc-0.8116\n",
      "Iter-21570, train loss-0.6420, acc-0.9000, valid loss-0.7451, acc-0.8134, test loss-0.7468, acc-0.8116\n",
      "Iter-21580, train loss-0.6256, acc-0.8400, valid loss-0.7448, acc-0.8134, test loss-0.7465, acc-0.8118\n",
      "Iter-21590, train loss-0.7303, acc-0.8000, valid loss-0.7446, acc-0.8134, test loss-0.7463, acc-0.8121\n",
      "Iter-21600, train loss-0.6314, acc-0.8800, valid loss-0.7443, acc-0.8134, test loss-0.7460, acc-0.8121\n",
      "Iter-21610, train loss-0.7536, acc-0.7800, valid loss-0.7441, acc-0.8134, test loss-0.7458, acc-0.8121\n",
      "Iter-21620, train loss-0.7307, acc-0.7800, valid loss-0.7438, acc-0.8134, test loss-0.7455, acc-0.8120\n",
      "Iter-21630, train loss-0.8502, acc-0.7800, valid loss-0.7436, acc-0.8134, test loss-0.7453, acc-0.8121\n",
      "Iter-21640, train loss-0.6168, acc-0.8400, valid loss-0.7433, acc-0.8134, test loss-0.7451, acc-0.8121\n",
      "Iter-21650, train loss-0.9184, acc-0.6800, valid loss-0.7431, acc-0.8132, test loss-0.7448, acc-0.8121\n",
      "Iter-21660, train loss-0.7534, acc-0.8400, valid loss-0.7428, acc-0.8132, test loss-0.7446, acc-0.8125\n",
      "Iter-21670, train loss-0.7146, acc-0.8200, valid loss-0.7425, acc-0.8134, test loss-0.7443, acc-0.8126\n",
      "Iter-21680, train loss-0.5513, acc-0.9200, valid loss-0.7423, acc-0.8132, test loss-0.7441, acc-0.8129\n",
      "Iter-21690, train loss-0.6880, acc-0.8600, valid loss-0.7421, acc-0.8134, test loss-0.7438, acc-0.8128\n",
      "Iter-21700, train loss-0.5831, acc-0.8800, valid loss-0.7418, acc-0.8134, test loss-0.7436, acc-0.8130\n",
      "Iter-21710, train loss-0.8225, acc-0.7800, valid loss-0.7415, acc-0.8136, test loss-0.7433, acc-0.8133\n",
      "Iter-21720, train loss-0.7413, acc-0.8200, valid loss-0.7413, acc-0.8136, test loss-0.7431, acc-0.8137\n",
      "Iter-21730, train loss-0.6514, acc-0.8400, valid loss-0.7410, acc-0.8138, test loss-0.7428, acc-0.8137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-21740, train loss-0.8469, acc-0.7600, valid loss-0.7408, acc-0.8138, test loss-0.7426, acc-0.8134\n",
      "Iter-21750, train loss-0.6563, acc-0.8600, valid loss-0.7405, acc-0.8138, test loss-0.7424, acc-0.8135\n",
      "Iter-21760, train loss-0.5872, acc-0.9000, valid loss-0.7403, acc-0.8140, test loss-0.7421, acc-0.8137\n",
      "Iter-21770, train loss-0.6371, acc-0.8600, valid loss-0.7400, acc-0.8138, test loss-0.7419, acc-0.8139\n",
      "Iter-21780, train loss-0.7442, acc-0.8000, valid loss-0.7398, acc-0.8140, test loss-0.7416, acc-0.8138\n",
      "Iter-21790, train loss-0.7261, acc-0.8200, valid loss-0.7396, acc-0.8142, test loss-0.7414, acc-0.8138\n",
      "Iter-21800, train loss-0.8945, acc-0.6800, valid loss-0.7394, acc-0.8142, test loss-0.7412, acc-0.8141\n",
      "Iter-21810, train loss-0.6854, acc-0.8200, valid loss-0.7391, acc-0.8142, test loss-0.7409, acc-0.8141\n",
      "Iter-21820, train loss-0.9487, acc-0.6800, valid loss-0.7389, acc-0.8144, test loss-0.7407, acc-0.8141\n",
      "Iter-21830, train loss-0.6437, acc-0.8600, valid loss-0.7387, acc-0.8144, test loss-0.7405, acc-0.8143\n",
      "Iter-21840, train loss-0.8397, acc-0.7000, valid loss-0.7384, acc-0.8142, test loss-0.7402, acc-0.8142\n",
      "Iter-21850, train loss-0.7264, acc-0.8000, valid loss-0.7382, acc-0.8144, test loss-0.7400, acc-0.8142\n",
      "Iter-21860, train loss-0.8118, acc-0.7200, valid loss-0.7379, acc-0.8144, test loss-0.7398, acc-0.8143\n",
      "Iter-21870, train loss-0.7002, acc-0.8600, valid loss-0.7377, acc-0.8146, test loss-0.7395, acc-0.8144\n",
      "Iter-21880, train loss-0.7123, acc-0.8600, valid loss-0.7375, acc-0.8146, test loss-0.7393, acc-0.8146\n",
      "Iter-21890, train loss-0.7171, acc-0.7800, valid loss-0.7373, acc-0.8146, test loss-0.7391, acc-0.8146\n",
      "Iter-21900, train loss-0.4968, acc-0.9200, valid loss-0.7370, acc-0.8148, test loss-0.7388, acc-0.8146\n",
      "Iter-21910, train loss-0.7525, acc-0.8200, valid loss-0.7368, acc-0.8150, test loss-0.7386, acc-0.8147\n",
      "Iter-21920, train loss-0.6929, acc-0.8200, valid loss-0.7365, acc-0.8150, test loss-0.7384, acc-0.8146\n",
      "Iter-21930, train loss-0.6677, acc-0.7600, valid loss-0.7363, acc-0.8150, test loss-0.7381, acc-0.8149\n",
      "Iter-21940, train loss-0.7484, acc-0.8200, valid loss-0.7361, acc-0.8152, test loss-0.7379, acc-0.8150\n",
      "Iter-21950, train loss-0.6238, acc-0.8200, valid loss-0.7359, acc-0.8154, test loss-0.7377, acc-0.8149\n",
      "Iter-21960, train loss-0.7797, acc-0.7600, valid loss-0.7356, acc-0.8152, test loss-0.7374, acc-0.8150\n",
      "Iter-21970, train loss-0.7337, acc-0.8200, valid loss-0.7354, acc-0.8152, test loss-0.7372, acc-0.8151\n",
      "Iter-21980, train loss-0.7275, acc-0.8400, valid loss-0.7351, acc-0.8156, test loss-0.7370, acc-0.8151\n",
      "Iter-21990, train loss-0.6239, acc-0.8600, valid loss-0.7349, acc-0.8154, test loss-0.7368, acc-0.8150\n",
      "Iter-22000, train loss-0.7391, acc-0.8400, valid loss-0.7347, acc-0.8154, test loss-0.7365, acc-0.8150\n",
      "Iter-22010, train loss-0.6818, acc-0.8200, valid loss-0.7344, acc-0.8154, test loss-0.7363, acc-0.8153\n",
      "Iter-22020, train loss-0.7392, acc-0.8400, valid loss-0.7342, acc-0.8150, test loss-0.7361, acc-0.8153\n",
      "Iter-22030, train loss-0.9234, acc-0.7600, valid loss-0.7340, acc-0.8152, test loss-0.7359, acc-0.8154\n",
      "Iter-22040, train loss-0.7435, acc-0.8200, valid loss-0.7337, acc-0.8152, test loss-0.7356, acc-0.8153\n",
      "Iter-22050, train loss-0.8630, acc-0.8000, valid loss-0.7335, acc-0.8158, test loss-0.7354, acc-0.8156\n",
      "Iter-22060, train loss-0.8879, acc-0.6600, valid loss-0.7332, acc-0.8158, test loss-0.7351, acc-0.8156\n",
      "Iter-22070, train loss-0.6559, acc-0.9000, valid loss-0.7330, acc-0.8160, test loss-0.7349, acc-0.8157\n",
      "Iter-22080, train loss-0.6394, acc-0.8600, valid loss-0.7327, acc-0.8162, test loss-0.7346, acc-0.8157\n",
      "Iter-22090, train loss-0.7297, acc-0.8200, valid loss-0.7325, acc-0.8162, test loss-0.7344, acc-0.8157\n",
      "Iter-22100, train loss-0.8098, acc-0.7800, valid loss-0.7323, acc-0.8164, test loss-0.7342, acc-0.8156\n",
      "Iter-22110, train loss-0.6301, acc-0.8400, valid loss-0.7320, acc-0.8168, test loss-0.7340, acc-0.8158\n",
      "Iter-22120, train loss-0.8110, acc-0.8600, valid loss-0.7318, acc-0.8170, test loss-0.7337, acc-0.8160\n",
      "Iter-22130, train loss-0.8357, acc-0.7600, valid loss-0.7316, acc-0.8170, test loss-0.7335, acc-0.8159\n",
      "Iter-22140, train loss-0.7172, acc-0.8600, valid loss-0.7314, acc-0.8170, test loss-0.7333, acc-0.8159\n",
      "Iter-22150, train loss-0.6488, acc-0.8200, valid loss-0.7312, acc-0.8172, test loss-0.7331, acc-0.8159\n",
      "Iter-22160, train loss-0.7356, acc-0.7800, valid loss-0.7310, acc-0.8172, test loss-0.7329, acc-0.8159\n",
      "Iter-22170, train loss-0.7260, acc-0.7600, valid loss-0.7307, acc-0.8172, test loss-0.7327, acc-0.8159\n",
      "Iter-22180, train loss-0.7190, acc-0.8400, valid loss-0.7305, acc-0.8176, test loss-0.7324, acc-0.8159\n",
      "Iter-22190, train loss-0.7318, acc-0.7600, valid loss-0.7303, acc-0.8174, test loss-0.7322, acc-0.8161\n",
      "Iter-22200, train loss-0.8267, acc-0.7600, valid loss-0.7301, acc-0.8176, test loss-0.7320, acc-0.8161\n",
      "Iter-22210, train loss-0.6829, acc-0.8200, valid loss-0.7298, acc-0.8172, test loss-0.7317, acc-0.8162\n",
      "Iter-22220, train loss-0.7316, acc-0.8400, valid loss-0.7296, acc-0.8174, test loss-0.7315, acc-0.8166\n",
      "Iter-22230, train loss-0.8490, acc-0.7200, valid loss-0.7294, acc-0.8172, test loss-0.7313, acc-0.8165\n",
      "Iter-22240, train loss-1.0621, acc-0.7000, valid loss-0.7291, acc-0.8174, test loss-0.7311, acc-0.8165\n",
      "Iter-22250, train loss-0.6816, acc-0.8400, valid loss-0.7289, acc-0.8174, test loss-0.7308, acc-0.8164\n",
      "Iter-22260, train loss-0.8457, acc-0.7600, valid loss-0.7286, acc-0.8172, test loss-0.7306, acc-0.8166\n",
      "Iter-22270, train loss-0.6978, acc-0.8400, valid loss-0.7284, acc-0.8172, test loss-0.7303, acc-0.8168\n",
      "Iter-22280, train loss-0.7685, acc-0.7400, valid loss-0.7282, acc-0.8172, test loss-0.7301, acc-0.8167\n",
      "Iter-22290, train loss-0.8819, acc-0.7800, valid loss-0.7280, acc-0.8172, test loss-0.7299, acc-0.8166\n",
      "Iter-22300, train loss-0.7480, acc-0.7800, valid loss-0.7277, acc-0.8172, test loss-0.7297, acc-0.8167\n",
      "Iter-22310, train loss-0.6490, acc-0.9000, valid loss-0.7275, acc-0.8172, test loss-0.7294, acc-0.8169\n",
      "Iter-22320, train loss-0.8772, acc-0.7600, valid loss-0.7272, acc-0.8172, test loss-0.7292, acc-0.8170\n",
      "Iter-22330, train loss-0.7754, acc-0.8200, valid loss-0.7270, acc-0.8172, test loss-0.7290, acc-0.8169\n",
      "Iter-22340, train loss-0.7537, acc-0.7800, valid loss-0.7268, acc-0.8174, test loss-0.7288, acc-0.8169\n",
      "Iter-22350, train loss-0.5810, acc-0.9400, valid loss-0.7266, acc-0.8174, test loss-0.7286, acc-0.8170\n",
      "Iter-22360, train loss-0.5490, acc-0.9200, valid loss-0.7264, acc-0.8176, test loss-0.7284, acc-0.8172\n",
      "Iter-22370, train loss-0.7242, acc-0.8400, valid loss-0.7261, acc-0.8176, test loss-0.7281, acc-0.8173\n",
      "Iter-22380, train loss-0.6905, acc-0.8600, valid loss-0.7259, acc-0.8174, test loss-0.7279, acc-0.8173\n",
      "Iter-22390, train loss-0.8602, acc-0.7400, valid loss-0.7257, acc-0.8178, test loss-0.7277, acc-0.8172\n",
      "Iter-22400, train loss-0.5870, acc-0.8800, valid loss-0.7255, acc-0.8178, test loss-0.7275, acc-0.8174\n",
      "Iter-22410, train loss-0.8565, acc-0.7000, valid loss-0.7252, acc-0.8180, test loss-0.7273, acc-0.8174\n",
      "Iter-22420, train loss-0.8280, acc-0.8200, valid loss-0.7250, acc-0.8180, test loss-0.7271, acc-0.8175\n",
      "Iter-22430, train loss-0.6911, acc-0.8400, valid loss-0.7247, acc-0.8180, test loss-0.7268, acc-0.8176\n",
      "Iter-22440, train loss-0.7153, acc-0.8200, valid loss-0.7245, acc-0.8180, test loss-0.7266, acc-0.8174\n",
      "Iter-22450, train loss-0.7124, acc-0.7800, valid loss-0.7243, acc-0.8180, test loss-0.7264, acc-0.8174\n",
      "Iter-22460, train loss-0.5038, acc-0.9400, valid loss-0.7241, acc-0.8180, test loss-0.7261, acc-0.8175\n",
      "Iter-22470, train loss-0.9143, acc-0.8000, valid loss-0.7239, acc-0.8180, test loss-0.7259, acc-0.8174\n",
      "Iter-22480, train loss-0.7499, acc-0.8200, valid loss-0.7236, acc-0.8180, test loss-0.7257, acc-0.8176\n",
      "Iter-22490, train loss-0.8157, acc-0.8000, valid loss-0.7234, acc-0.8178, test loss-0.7255, acc-0.8178\n",
      "Iter-22500, train loss-0.7159, acc-0.8000, valid loss-0.7232, acc-0.8178, test loss-0.7253, acc-0.8177\n",
      "Iter-22510, train loss-0.6369, acc-0.9200, valid loss-0.7230, acc-0.8178, test loss-0.7250, acc-0.8179\n",
      "Iter-22520, train loss-0.8977, acc-0.7400, valid loss-0.7227, acc-0.8176, test loss-0.7248, acc-0.8180\n",
      "Iter-22530, train loss-0.6563, acc-0.8400, valid loss-0.7225, acc-0.8178, test loss-0.7246, acc-0.8181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22540, train loss-0.8976, acc-0.7200, valid loss-0.7223, acc-0.8178, test loss-0.7244, acc-0.8181\n",
      "Iter-22550, train loss-0.6774, acc-0.8200, valid loss-0.7221, acc-0.8178, test loss-0.7242, acc-0.8180\n",
      "Iter-22560, train loss-0.6741, acc-0.8600, valid loss-0.7219, acc-0.8178, test loss-0.7239, acc-0.8181\n",
      "Iter-22570, train loss-0.7536, acc-0.7800, valid loss-0.7217, acc-0.8180, test loss-0.7237, acc-0.8183\n",
      "Iter-22580, train loss-0.6237, acc-0.8800, valid loss-0.7214, acc-0.8180, test loss-0.7235, acc-0.8182\n",
      "Iter-22590, train loss-0.7263, acc-0.8400, valid loss-0.7212, acc-0.8182, test loss-0.7233, acc-0.8181\n",
      "Iter-22600, train loss-0.8803, acc-0.7200, valid loss-0.7210, acc-0.8182, test loss-0.7231, acc-0.8183\n",
      "Iter-22610, train loss-0.7997, acc-0.7800, valid loss-0.7208, acc-0.8182, test loss-0.7229, acc-0.8183\n",
      "Iter-22620, train loss-0.5806, acc-0.9000, valid loss-0.7206, acc-0.8180, test loss-0.7226, acc-0.8182\n",
      "Iter-22630, train loss-0.6514, acc-0.8800, valid loss-0.7204, acc-0.8180, test loss-0.7224, acc-0.8181\n",
      "Iter-22640, train loss-0.8693, acc-0.7400, valid loss-0.7201, acc-0.8182, test loss-0.7222, acc-0.8181\n",
      "Iter-22650, train loss-0.7803, acc-0.8000, valid loss-0.7199, acc-0.8182, test loss-0.7220, acc-0.8180\n",
      "Iter-22660, train loss-0.7717, acc-0.8200, valid loss-0.7197, acc-0.8182, test loss-0.7218, acc-0.8180\n",
      "Iter-22670, train loss-0.7399, acc-0.8200, valid loss-0.7195, acc-0.8182, test loss-0.7215, acc-0.8180\n",
      "Iter-22680, train loss-0.7131, acc-0.7800, valid loss-0.7193, acc-0.8182, test loss-0.7213, acc-0.8180\n",
      "Iter-22690, train loss-0.6453, acc-0.8600, valid loss-0.7191, acc-0.8182, test loss-0.7211, acc-0.8180\n",
      "Iter-22700, train loss-0.8398, acc-0.7800, valid loss-0.7188, acc-0.8184, test loss-0.7209, acc-0.8181\n",
      "Iter-22710, train loss-0.8029, acc-0.8000, valid loss-0.7186, acc-0.8184, test loss-0.7207, acc-0.8182\n",
      "Iter-22720, train loss-0.7530, acc-0.7800, valid loss-0.7184, acc-0.8182, test loss-0.7204, acc-0.8183\n",
      "Iter-22730, train loss-0.6017, acc-0.8600, valid loss-0.7182, acc-0.8188, test loss-0.7202, acc-0.8181\n",
      "Iter-22740, train loss-0.7318, acc-0.8400, valid loss-0.7179, acc-0.8186, test loss-0.7200, acc-0.8180\n",
      "Iter-22750, train loss-0.7825, acc-0.8400, valid loss-0.7177, acc-0.8186, test loss-0.7198, acc-0.8181\n",
      "Iter-22760, train loss-0.9177, acc-0.6800, valid loss-0.7175, acc-0.8184, test loss-0.7196, acc-0.8182\n",
      "Iter-22770, train loss-0.6708, acc-0.8800, valid loss-0.7173, acc-0.8184, test loss-0.7194, acc-0.8182\n",
      "Iter-22780, train loss-0.5604, acc-0.9200, valid loss-0.7171, acc-0.8184, test loss-0.7192, acc-0.8182\n",
      "Iter-22790, train loss-0.7271, acc-0.8400, valid loss-0.7169, acc-0.8186, test loss-0.7190, acc-0.8184\n",
      "Iter-22800, train loss-0.9236, acc-0.7800, valid loss-0.7167, acc-0.8186, test loss-0.7187, acc-0.8185\n",
      "Iter-22810, train loss-0.7337, acc-0.8200, valid loss-0.7164, acc-0.8186, test loss-0.7185, acc-0.8186\n",
      "Iter-22820, train loss-0.8206, acc-0.8000, valid loss-0.7162, acc-0.8188, test loss-0.7183, acc-0.8188\n",
      "Iter-22830, train loss-0.6871, acc-0.8400, valid loss-0.7160, acc-0.8188, test loss-0.7181, acc-0.8186\n",
      "Iter-22840, train loss-0.8496, acc-0.8200, valid loss-0.7158, acc-0.8188, test loss-0.7179, acc-0.8188\n",
      "Iter-22850, train loss-0.6239, acc-0.9400, valid loss-0.7155, acc-0.8188, test loss-0.7177, acc-0.8190\n",
      "Iter-22860, train loss-0.8083, acc-0.8400, valid loss-0.7153, acc-0.8190, test loss-0.7174, acc-0.8192\n",
      "Iter-22870, train loss-0.8126, acc-0.8200, valid loss-0.7151, acc-0.8188, test loss-0.7172, acc-0.8192\n",
      "Iter-22880, train loss-0.8924, acc-0.6800, valid loss-0.7149, acc-0.8188, test loss-0.7170, acc-0.8192\n",
      "Iter-22890, train loss-0.6960, acc-0.8400, valid loss-0.7147, acc-0.8188, test loss-0.7168, acc-0.8190\n",
      "Iter-22900, train loss-0.8016, acc-0.7800, valid loss-0.7144, acc-0.8188, test loss-0.7166, acc-0.8191\n",
      "Iter-22910, train loss-0.5692, acc-0.8800, valid loss-0.7142, acc-0.8190, test loss-0.7163, acc-0.8192\n",
      "Iter-22920, train loss-0.7953, acc-0.8000, valid loss-0.7140, acc-0.8192, test loss-0.7161, acc-0.8190\n",
      "Iter-22930, train loss-0.7334, acc-0.8200, valid loss-0.7138, acc-0.8194, test loss-0.7159, acc-0.8192\n",
      "Iter-22940, train loss-0.5536, acc-0.9600, valid loss-0.7136, acc-0.8194, test loss-0.7157, acc-0.8191\n",
      "Iter-22950, train loss-0.7142, acc-0.7800, valid loss-0.7134, acc-0.8192, test loss-0.7155, acc-0.8192\n",
      "Iter-22960, train loss-0.8802, acc-0.7600, valid loss-0.7132, acc-0.8192, test loss-0.7153, acc-0.8193\n",
      "Iter-22970, train loss-0.7947, acc-0.7600, valid loss-0.7130, acc-0.8192, test loss-0.7151, acc-0.8193\n",
      "Iter-22980, train loss-0.8087, acc-0.8200, valid loss-0.7128, acc-0.8192, test loss-0.7149, acc-0.8191\n",
      "Iter-22990, train loss-0.6688, acc-0.8600, valid loss-0.7125, acc-0.8192, test loss-0.7147, acc-0.8194\n",
      "Iter-23000, train loss-0.6717, acc-0.8200, valid loss-0.7123, acc-0.8192, test loss-0.7145, acc-0.8196\n",
      "Iter-23010, train loss-0.7557, acc-0.8200, valid loss-0.7121, acc-0.8192, test loss-0.7143, acc-0.8198\n",
      "Iter-23020, train loss-0.8163, acc-0.7200, valid loss-0.7119, acc-0.8192, test loss-0.7140, acc-0.8199\n",
      "Iter-23030, train loss-0.7767, acc-0.7800, valid loss-0.7117, acc-0.8192, test loss-0.7138, acc-0.8201\n",
      "Iter-23040, train loss-0.7672, acc-0.8400, valid loss-0.7115, acc-0.8192, test loss-0.7136, acc-0.8200\n",
      "Iter-23050, train loss-0.8476, acc-0.7400, valid loss-0.7113, acc-0.8192, test loss-0.7134, acc-0.8202\n",
      "Iter-23060, train loss-0.7911, acc-0.7600, valid loss-0.7111, acc-0.8194, test loss-0.7132, acc-0.8203\n",
      "Iter-23070, train loss-0.6638, acc-0.8400, valid loss-0.7108, acc-0.8194, test loss-0.7130, acc-0.8203\n",
      "Iter-23080, train loss-0.5979, acc-0.9000, valid loss-0.7106, acc-0.8194, test loss-0.7128, acc-0.8205\n",
      "Iter-23090, train loss-0.6533, acc-0.8600, valid loss-0.7104, acc-0.8194, test loss-0.7126, acc-0.8205\n",
      "Iter-23100, train loss-0.7643, acc-0.8000, valid loss-0.7102, acc-0.8194, test loss-0.7124, acc-0.8204\n",
      "Iter-23110, train loss-0.5512, acc-0.9200, valid loss-0.7100, acc-0.8192, test loss-0.7122, acc-0.8204\n",
      "Iter-23120, train loss-0.9373, acc-0.7400, valid loss-0.7098, acc-0.8194, test loss-0.7120, acc-0.8206\n",
      "Iter-23130, train loss-0.6224, acc-0.8200, valid loss-0.7096, acc-0.8198, test loss-0.7118, acc-0.8206\n",
      "Iter-23140, train loss-0.7030, acc-0.8600, valid loss-0.7094, acc-0.8198, test loss-0.7116, acc-0.8209\n",
      "Iter-23150, train loss-0.8734, acc-0.7600, valid loss-0.7092, acc-0.8198, test loss-0.7114, acc-0.8210\n",
      "Iter-23160, train loss-0.6078, acc-0.8800, valid loss-0.7090, acc-0.8202, test loss-0.7112, acc-0.8211\n",
      "Iter-23170, train loss-0.6076, acc-0.9000, valid loss-0.7088, acc-0.8200, test loss-0.7110, acc-0.8212\n",
      "Iter-23180, train loss-0.8243, acc-0.8400, valid loss-0.7086, acc-0.8200, test loss-0.7108, acc-0.8211\n",
      "Iter-23190, train loss-0.8204, acc-0.7600, valid loss-0.7084, acc-0.8198, test loss-0.7106, acc-0.8211\n",
      "Iter-23200, train loss-0.7272, acc-0.7800, valid loss-0.7082, acc-0.8198, test loss-0.7104, acc-0.8212\n",
      "Iter-23210, train loss-0.6034, acc-0.8400, valid loss-0.7080, acc-0.8198, test loss-0.7102, acc-0.8213\n",
      "Iter-23220, train loss-0.8211, acc-0.7400, valid loss-0.7078, acc-0.8200, test loss-0.7100, acc-0.8213\n",
      "Iter-23230, train loss-0.8948, acc-0.8200, valid loss-0.7076, acc-0.8202, test loss-0.7098, acc-0.8215\n",
      "Iter-23240, train loss-0.7611, acc-0.7600, valid loss-0.7074, acc-0.8200, test loss-0.7096, acc-0.8216\n",
      "Iter-23250, train loss-0.7394, acc-0.8400, valid loss-0.7072, acc-0.8200, test loss-0.7094, acc-0.8216\n",
      "Iter-23260, train loss-0.8787, acc-0.8000, valid loss-0.7070, acc-0.8202, test loss-0.7092, acc-0.8218\n",
      "Iter-23270, train loss-0.6545, acc-0.7800, valid loss-0.7068, acc-0.8202, test loss-0.7090, acc-0.8216\n",
      "Iter-23280, train loss-0.8065, acc-0.7400, valid loss-0.7066, acc-0.8202, test loss-0.7088, acc-0.8216\n",
      "Iter-23290, train loss-0.8722, acc-0.7200, valid loss-0.7064, acc-0.8204, test loss-0.7086, acc-0.8217\n",
      "Iter-23300, train loss-0.5844, acc-0.8600, valid loss-0.7062, acc-0.8204, test loss-0.7084, acc-0.8220\n",
      "Iter-23310, train loss-0.7898, acc-0.8200, valid loss-0.7060, acc-0.8204, test loss-0.7082, acc-0.8222\n",
      "Iter-23320, train loss-0.7671, acc-0.8200, valid loss-0.7058, acc-0.8204, test loss-0.7080, acc-0.8223\n",
      "Iter-23330, train loss-0.9908, acc-0.7600, valid loss-0.7056, acc-0.8204, test loss-0.7078, acc-0.8225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-23340, train loss-0.7626, acc-0.7400, valid loss-0.7054, acc-0.8204, test loss-0.7076, acc-0.8225\n",
      "Iter-23350, train loss-0.6978, acc-0.8200, valid loss-0.7052, acc-0.8206, test loss-0.7073, acc-0.8225\n",
      "Iter-23360, train loss-0.7463, acc-0.7800, valid loss-0.7049, acc-0.8206, test loss-0.7071, acc-0.8225\n",
      "Iter-23370, train loss-0.7666, acc-0.7800, valid loss-0.7047, acc-0.8208, test loss-0.7069, acc-0.8226\n",
      "Iter-23380, train loss-0.7069, acc-0.7600, valid loss-0.7045, acc-0.8208, test loss-0.7067, acc-0.8227\n",
      "Iter-23390, train loss-0.6092, acc-0.8800, valid loss-0.7043, acc-0.8208, test loss-0.7065, acc-0.8227\n",
      "Iter-23400, train loss-0.7043, acc-0.8000, valid loss-0.7042, acc-0.8208, test loss-0.7063, acc-0.8229\n",
      "Iter-23410, train loss-0.7891, acc-0.7800, valid loss-0.7039, acc-0.8208, test loss-0.7061, acc-0.8229\n",
      "Iter-23420, train loss-0.6870, acc-0.8200, valid loss-0.7038, acc-0.8208, test loss-0.7059, acc-0.8228\n",
      "Iter-23430, train loss-0.8217, acc-0.7600, valid loss-0.7036, acc-0.8206, test loss-0.7057, acc-0.8230\n",
      "Iter-23440, train loss-0.8419, acc-0.7200, valid loss-0.7034, acc-0.8208, test loss-0.7055, acc-0.8232\n",
      "Iter-23450, train loss-0.9324, acc-0.7600, valid loss-0.7032, acc-0.8208, test loss-0.7053, acc-0.8232\n",
      "Iter-23460, train loss-0.7326, acc-0.8000, valid loss-0.7030, acc-0.8208, test loss-0.7052, acc-0.8233\n",
      "Iter-23470, train loss-0.6584, acc-0.8200, valid loss-0.7028, acc-0.8210, test loss-0.7049, acc-0.8232\n",
      "Iter-23480, train loss-0.7043, acc-0.8000, valid loss-0.7026, acc-0.8210, test loss-0.7048, acc-0.8234\n",
      "Iter-23490, train loss-0.7539, acc-0.7800, valid loss-0.7024, acc-0.8212, test loss-0.7046, acc-0.8234\n",
      "Iter-23500, train loss-0.6856, acc-0.8000, valid loss-0.7022, acc-0.8212, test loss-0.7044, acc-0.8235\n",
      "Iter-23510, train loss-0.6657, acc-0.8600, valid loss-0.7020, acc-0.8212, test loss-0.7042, acc-0.8235\n",
      "Iter-23520, train loss-0.8320, acc-0.7400, valid loss-0.7018, acc-0.8212, test loss-0.7040, acc-0.8236\n",
      "Iter-23530, train loss-0.7705, acc-0.8000, valid loss-0.7016, acc-0.8216, test loss-0.7038, acc-0.8235\n",
      "Iter-23540, train loss-0.6087, acc-0.8400, valid loss-0.7014, acc-0.8218, test loss-0.7036, acc-0.8236\n",
      "Iter-23550, train loss-0.6153, acc-0.9000, valid loss-0.7012, acc-0.8218, test loss-0.7034, acc-0.8235\n",
      "Iter-23560, train loss-0.7537, acc-0.7400, valid loss-0.7010, acc-0.8218, test loss-0.7032, acc-0.8236\n",
      "Iter-23570, train loss-0.8819, acc-0.8000, valid loss-0.7008, acc-0.8218, test loss-0.7030, acc-0.8237\n",
      "Iter-23580, train loss-0.7702, acc-0.8200, valid loss-0.7006, acc-0.8216, test loss-0.7028, acc-0.8238\n",
      "Iter-23590, train loss-0.7185, acc-0.8000, valid loss-0.7004, acc-0.8216, test loss-0.7026, acc-0.8236\n",
      "Iter-23600, train loss-0.6148, acc-0.9000, valid loss-0.7002, acc-0.8216, test loss-0.7024, acc-0.8238\n",
      "Iter-23610, train loss-0.7492, acc-0.7800, valid loss-0.7000, acc-0.8218, test loss-0.7022, acc-0.8238\n",
      "Iter-23620, train loss-0.7887, acc-0.7200, valid loss-0.6998, acc-0.8216, test loss-0.7020, acc-0.8239\n",
      "Iter-23630, train loss-0.6804, acc-0.8800, valid loss-0.6996, acc-0.8216, test loss-0.7018, acc-0.8240\n",
      "Iter-23640, train loss-0.8208, acc-0.7200, valid loss-0.6994, acc-0.8218, test loss-0.7016, acc-0.8240\n",
      "Iter-23650, train loss-0.5194, acc-0.9600, valid loss-0.6992, acc-0.8216, test loss-0.7014, acc-0.8240\n",
      "Iter-23660, train loss-0.6155, acc-0.8600, valid loss-0.6990, acc-0.8218, test loss-0.7012, acc-0.8241\n",
      "Iter-23670, train loss-0.6800, acc-0.8000, valid loss-0.6988, acc-0.8218, test loss-0.7010, acc-0.8240\n",
      "Iter-23680, train loss-0.5221, acc-0.9600, valid loss-0.6986, acc-0.8218, test loss-0.7008, acc-0.8241\n",
      "Iter-23690, train loss-0.7948, acc-0.7600, valid loss-0.6985, acc-0.8218, test loss-0.7007, acc-0.8240\n",
      "Iter-23700, train loss-0.6069, acc-0.8800, valid loss-0.6982, acc-0.8218, test loss-0.7004, acc-0.8240\n",
      "Iter-23710, train loss-0.7250, acc-0.8000, valid loss-0.6980, acc-0.8218, test loss-0.7002, acc-0.8240\n",
      "Iter-23720, train loss-0.7054, acc-0.8400, valid loss-0.6978, acc-0.8222, test loss-0.7000, acc-0.8240\n",
      "Iter-23730, train loss-0.6233, acc-0.8200, valid loss-0.6977, acc-0.8220, test loss-0.6999, acc-0.8241\n",
      "Iter-23740, train loss-0.7835, acc-0.8400, valid loss-0.6975, acc-0.8220, test loss-0.6997, acc-0.8240\n",
      "Iter-23750, train loss-0.9092, acc-0.7200, valid loss-0.6973, acc-0.8222, test loss-0.6995, acc-0.8240\n",
      "Iter-23760, train loss-0.7963, acc-0.8000, valid loss-0.6971, acc-0.8222, test loss-0.6993, acc-0.8241\n",
      "Iter-23770, train loss-0.5907, acc-0.8400, valid loss-0.6969, acc-0.8222, test loss-0.6991, acc-0.8242\n",
      "Iter-23780, train loss-0.4719, acc-0.9200, valid loss-0.6967, acc-0.8222, test loss-0.6989, acc-0.8243\n",
      "Iter-23790, train loss-0.6905, acc-0.8600, valid loss-0.6965, acc-0.8222, test loss-0.6987, acc-0.8243\n",
      "Iter-23800, train loss-0.7805, acc-0.8000, valid loss-0.6963, acc-0.8224, test loss-0.6985, acc-0.8243\n",
      "Iter-23810, train loss-0.7149, acc-0.8800, valid loss-0.6961, acc-0.8226, test loss-0.6983, acc-0.8244\n",
      "Iter-23820, train loss-0.6451, acc-0.8400, valid loss-0.6959, acc-0.8224, test loss-0.6981, acc-0.8243\n",
      "Iter-23830, train loss-0.6698, acc-0.8600, valid loss-0.6957, acc-0.8224, test loss-0.6979, acc-0.8243\n",
      "Iter-23840, train loss-0.6047, acc-0.8600, valid loss-0.6955, acc-0.8226, test loss-0.6977, acc-0.8244\n",
      "Iter-23850, train loss-0.9542, acc-0.6600, valid loss-0.6953, acc-0.8228, test loss-0.6975, acc-0.8245\n",
      "Iter-23860, train loss-0.6530, acc-0.8800, valid loss-0.6951, acc-0.8228, test loss-0.6973, acc-0.8244\n",
      "Iter-23870, train loss-0.6294, acc-0.8600, valid loss-0.6949, acc-0.8228, test loss-0.6971, acc-0.8245\n",
      "Iter-23880, train loss-0.6106, acc-0.8400, valid loss-0.6947, acc-0.8228, test loss-0.6969, acc-0.8247\n",
      "Iter-23890, train loss-0.5265, acc-0.9600, valid loss-0.6945, acc-0.8226, test loss-0.6967, acc-0.8247\n",
      "Iter-23900, train loss-0.8454, acc-0.7000, valid loss-0.6943, acc-0.8228, test loss-0.6965, acc-0.8249\n",
      "Iter-23910, train loss-0.9070, acc-0.6800, valid loss-0.6941, acc-0.8228, test loss-0.6963, acc-0.8249\n",
      "Iter-23920, train loss-0.6584, acc-0.8600, valid loss-0.6939, acc-0.8228, test loss-0.6961, acc-0.8248\n",
      "Iter-23930, train loss-0.8446, acc-0.7600, valid loss-0.6937, acc-0.8228, test loss-0.6960, acc-0.8248\n",
      "Iter-23940, train loss-0.6346, acc-0.9000, valid loss-0.6935, acc-0.8228, test loss-0.6958, acc-0.8250\n",
      "Iter-23950, train loss-0.7981, acc-0.7600, valid loss-0.6933, acc-0.8228, test loss-0.6956, acc-0.8252\n",
      "Iter-23960, train loss-0.6051, acc-0.8400, valid loss-0.6931, acc-0.8230, test loss-0.6954, acc-0.8251\n",
      "Iter-23970, train loss-0.6576, acc-0.8200, valid loss-0.6929, acc-0.8230, test loss-0.6952, acc-0.8251\n",
      "Iter-23980, train loss-0.7554, acc-0.7400, valid loss-0.6927, acc-0.8228, test loss-0.6950, acc-0.8251\n",
      "Iter-23990, train loss-0.7028, acc-0.8400, valid loss-0.6925, acc-0.8228, test loss-0.6948, acc-0.8253\n",
      "Iter-24000, train loss-0.5274, acc-0.8800, valid loss-0.6923, acc-0.8228, test loss-0.6946, acc-0.8253\n",
      "Iter-24010, train loss-0.8031, acc-0.8200, valid loss-0.6922, acc-0.8228, test loss-0.6944, acc-0.8253\n",
      "Iter-24020, train loss-0.7750, acc-0.8000, valid loss-0.6920, acc-0.8228, test loss-0.6942, acc-0.8252\n",
      "Iter-24030, train loss-0.8042, acc-0.8000, valid loss-0.6918, acc-0.8228, test loss-0.6940, acc-0.8253\n",
      "Iter-24040, train loss-0.7472, acc-0.7800, valid loss-0.6916, acc-0.8230, test loss-0.6938, acc-0.8253\n",
      "Iter-24050, train loss-0.5318, acc-0.9000, valid loss-0.6914, acc-0.8230, test loss-0.6936, acc-0.8256\n",
      "Iter-24060, train loss-0.5420, acc-0.8600, valid loss-0.6912, acc-0.8232, test loss-0.6935, acc-0.8256\n",
      "Iter-24070, train loss-0.7776, acc-0.8200, valid loss-0.6910, acc-0.8232, test loss-0.6933, acc-0.8257\n",
      "Iter-24080, train loss-0.8013, acc-0.7400, valid loss-0.6908, acc-0.8232, test loss-0.6931, acc-0.8258\n",
      "Iter-24090, train loss-0.6984, acc-0.8600, valid loss-0.6907, acc-0.8232, test loss-0.6929, acc-0.8257\n",
      "Iter-24100, train loss-0.8951, acc-0.7800, valid loss-0.6905, acc-0.8232, test loss-0.6927, acc-0.8258\n",
      "Iter-24110, train loss-0.6522, acc-0.8200, valid loss-0.6903, acc-0.8232, test loss-0.6926, acc-0.8258\n",
      "Iter-24120, train loss-0.7465, acc-0.8000, valid loss-0.6901, acc-0.8230, test loss-0.6924, acc-0.8257\n",
      "Iter-24130, train loss-0.7117, acc-0.8000, valid loss-0.6899, acc-0.8230, test loss-0.6922, acc-0.8257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-24140, train loss-0.6624, acc-0.8000, valid loss-0.6898, acc-0.8230, test loss-0.6920, acc-0.8257\n",
      "Iter-24150, train loss-0.6828, acc-0.8400, valid loss-0.6896, acc-0.8230, test loss-0.6919, acc-0.8259\n",
      "Iter-24160, train loss-0.8224, acc-0.7000, valid loss-0.6894, acc-0.8230, test loss-0.6917, acc-0.8258\n",
      "Iter-24170, train loss-0.8080, acc-0.7400, valid loss-0.6892, acc-0.8230, test loss-0.6915, acc-0.8257\n",
      "Iter-24180, train loss-0.5237, acc-0.9400, valid loss-0.6890, acc-0.8228, test loss-0.6913, acc-0.8258\n",
      "Iter-24190, train loss-0.8206, acc-0.7600, valid loss-0.6888, acc-0.8230, test loss-0.6911, acc-0.8256\n",
      "Iter-24200, train loss-0.5341, acc-0.8800, valid loss-0.6886, acc-0.8230, test loss-0.6909, acc-0.8258\n",
      "Iter-24210, train loss-0.5958, acc-0.8200, valid loss-0.6884, acc-0.8230, test loss-0.6907, acc-0.8259\n",
      "Iter-24220, train loss-0.5936, acc-0.9000, valid loss-0.6882, acc-0.8230, test loss-0.6905, acc-0.8259\n",
      "Iter-24230, train loss-0.7949, acc-0.7800, valid loss-0.6880, acc-0.8230, test loss-0.6903, acc-0.8261\n",
      "Iter-24240, train loss-0.7205, acc-0.8400, valid loss-0.6878, acc-0.8228, test loss-0.6902, acc-0.8258\n",
      "Iter-24250, train loss-0.8050, acc-0.7200, valid loss-0.6876, acc-0.8230, test loss-0.6900, acc-0.8260\n",
      "Iter-24260, train loss-0.7048, acc-0.8200, valid loss-0.6875, acc-0.8230, test loss-0.6898, acc-0.8257\n",
      "Iter-24270, train loss-0.7213, acc-0.8000, valid loss-0.6873, acc-0.8230, test loss-0.6896, acc-0.8261\n",
      "Iter-24280, train loss-0.6745, acc-0.8400, valid loss-0.6871, acc-0.8230, test loss-0.6894, acc-0.8260\n",
      "Iter-24290, train loss-0.8429, acc-0.6800, valid loss-0.6869, acc-0.8230, test loss-0.6892, acc-0.8263\n",
      "Iter-24300, train loss-0.6636, acc-0.7800, valid loss-0.6867, acc-0.8228, test loss-0.6891, acc-0.8262\n",
      "Iter-24310, train loss-0.9329, acc-0.6600, valid loss-0.6865, acc-0.8232, test loss-0.6889, acc-0.8261\n",
      "Iter-24320, train loss-0.9242, acc-0.7800, valid loss-0.6863, acc-0.8230, test loss-0.6887, acc-0.8261\n",
      "Iter-24330, train loss-0.7265, acc-0.7600, valid loss-0.6861, acc-0.8226, test loss-0.6885, acc-0.8261\n",
      "Iter-24340, train loss-0.7529, acc-0.7800, valid loss-0.6859, acc-0.8228, test loss-0.6883, acc-0.8262\n",
      "Iter-24350, train loss-0.6223, acc-0.9000, valid loss-0.6858, acc-0.8228, test loss-0.6882, acc-0.8261\n",
      "Iter-24360, train loss-0.7524, acc-0.8000, valid loss-0.6856, acc-0.8228, test loss-0.6880, acc-0.8263\n",
      "Iter-24370, train loss-0.6975, acc-0.8000, valid loss-0.6854, acc-0.8226, test loss-0.6878, acc-0.8262\n",
      "Iter-24380, train loss-0.5882, acc-0.8800, valid loss-0.6852, acc-0.8226, test loss-0.6876, acc-0.8264\n",
      "Iter-24390, train loss-0.8221, acc-0.8400, valid loss-0.6850, acc-0.8226, test loss-0.6874, acc-0.8264\n",
      "Iter-24400, train loss-0.6878, acc-0.8400, valid loss-0.6848, acc-0.8226, test loss-0.6872, acc-0.8264\n",
      "Iter-24410, train loss-0.7464, acc-0.8000, valid loss-0.6847, acc-0.8228, test loss-0.6871, acc-0.8262\n",
      "Iter-24420, train loss-0.7976, acc-0.7600, valid loss-0.6845, acc-0.8226, test loss-0.6869, acc-0.8263\n",
      "Iter-24430, train loss-0.8212, acc-0.7600, valid loss-0.6843, acc-0.8230, test loss-0.6867, acc-0.8264\n",
      "Iter-24440, train loss-0.5585, acc-0.8600, valid loss-0.6841, acc-0.8230, test loss-0.6865, acc-0.8265\n",
      "Iter-24450, train loss-0.9140, acc-0.7400, valid loss-0.6839, acc-0.8232, test loss-0.6863, acc-0.8265\n",
      "Iter-24460, train loss-0.6626, acc-0.8600, valid loss-0.6837, acc-0.8234, test loss-0.6862, acc-0.8267\n",
      "Iter-24470, train loss-0.6490, acc-0.7800, valid loss-0.6835, acc-0.8234, test loss-0.6860, acc-0.8267\n",
      "Iter-24480, train loss-0.7038, acc-0.8000, valid loss-0.6834, acc-0.8236, test loss-0.6858, acc-0.8267\n",
      "Iter-24490, train loss-0.6962, acc-0.7800, valid loss-0.6832, acc-0.8236, test loss-0.6856, acc-0.8267\n",
      "Iter-24500, train loss-0.5925, acc-0.8000, valid loss-0.6830, acc-0.8238, test loss-0.6854, acc-0.8270\n",
      "Iter-24510, train loss-0.6701, acc-0.8000, valid loss-0.6828, acc-0.8236, test loss-0.6852, acc-0.8270\n",
      "Iter-24520, train loss-0.7799, acc-0.8200, valid loss-0.6826, acc-0.8236, test loss-0.6851, acc-0.8270\n",
      "Iter-24530, train loss-0.6914, acc-0.8400, valid loss-0.6825, acc-0.8236, test loss-0.6849, acc-0.8267\n",
      "Iter-24540, train loss-0.9198, acc-0.8000, valid loss-0.6823, acc-0.8236, test loss-0.6847, acc-0.8268\n",
      "Iter-24550, train loss-0.4863, acc-0.9400, valid loss-0.6822, acc-0.8236, test loss-0.6845, acc-0.8270\n",
      "Iter-24560, train loss-0.6346, acc-0.8200, valid loss-0.6820, acc-0.8240, test loss-0.6844, acc-0.8270\n",
      "Iter-24570, train loss-0.7163, acc-0.8000, valid loss-0.6818, acc-0.8238, test loss-0.6842, acc-0.8271\n",
      "Iter-24580, train loss-0.7089, acc-0.8400, valid loss-0.6816, acc-0.8240, test loss-0.6840, acc-0.8269\n",
      "Iter-24590, train loss-0.6930, acc-0.8800, valid loss-0.6815, acc-0.8238, test loss-0.6838, acc-0.8270\n",
      "Iter-24600, train loss-0.6989, acc-0.7800, valid loss-0.6813, acc-0.8240, test loss-0.6837, acc-0.8270\n",
      "Iter-24610, train loss-0.6983, acc-0.8000, valid loss-0.6811, acc-0.8240, test loss-0.6835, acc-0.8269\n",
      "Iter-24620, train loss-0.8914, acc-0.8000, valid loss-0.6810, acc-0.8238, test loss-0.6833, acc-0.8269\n",
      "Iter-24630, train loss-0.7508, acc-0.7800, valid loss-0.6808, acc-0.8236, test loss-0.6831, acc-0.8269\n",
      "Iter-24640, train loss-0.7672, acc-0.8000, valid loss-0.6806, acc-0.8240, test loss-0.6829, acc-0.8272\n",
      "Iter-24650, train loss-0.7076, acc-0.8600, valid loss-0.6804, acc-0.8240, test loss-0.6827, acc-0.8273\n",
      "Iter-24660, train loss-0.7537, acc-0.7800, valid loss-0.6802, acc-0.8240, test loss-0.6826, acc-0.8273\n",
      "Iter-24670, train loss-0.7023, acc-0.8600, valid loss-0.6800, acc-0.8238, test loss-0.6824, acc-0.8274\n",
      "Iter-24680, train loss-0.4992, acc-0.8800, valid loss-0.6799, acc-0.8240, test loss-0.6822, acc-0.8275\n",
      "Iter-24690, train loss-0.9815, acc-0.7200, valid loss-0.6797, acc-0.8238, test loss-0.6820, acc-0.8274\n",
      "Iter-24700, train loss-0.7360, acc-0.8200, valid loss-0.6795, acc-0.8242, test loss-0.6819, acc-0.8273\n",
      "Iter-24710, train loss-0.4762, acc-0.9400, valid loss-0.6793, acc-0.8242, test loss-0.6817, acc-0.8277\n",
      "Iter-24720, train loss-0.7178, acc-0.8200, valid loss-0.6791, acc-0.8244, test loss-0.6815, acc-0.8278\n",
      "Iter-24730, train loss-0.7733, acc-0.7800, valid loss-0.6790, acc-0.8244, test loss-0.6813, acc-0.8279\n",
      "Iter-24740, train loss-0.5794, acc-0.8800, valid loss-0.6788, acc-0.8244, test loss-0.6812, acc-0.8278\n",
      "Iter-24750, train loss-0.6542, acc-0.8400, valid loss-0.6786, acc-0.8242, test loss-0.6810, acc-0.8280\n",
      "Iter-24760, train loss-0.7129, acc-0.7800, valid loss-0.6784, acc-0.8242, test loss-0.6808, acc-0.8278\n",
      "Iter-24770, train loss-0.6522, acc-0.8200, valid loss-0.6783, acc-0.8242, test loss-0.6807, acc-0.8280\n",
      "Iter-24780, train loss-0.6322, acc-0.8200, valid loss-0.6781, acc-0.8246, test loss-0.6805, acc-0.8280\n",
      "Iter-24790, train loss-0.5243, acc-0.9400, valid loss-0.6779, acc-0.8244, test loss-0.6803, acc-0.8283\n",
      "Iter-24800, train loss-0.9377, acc-0.6800, valid loss-0.6777, acc-0.8248, test loss-0.6801, acc-0.8281\n",
      "Iter-24810, train loss-0.6567, acc-0.8000, valid loss-0.6776, acc-0.8248, test loss-0.6799, acc-0.8281\n",
      "Iter-24820, train loss-0.6808, acc-0.8800, valid loss-0.6774, acc-0.8248, test loss-0.6797, acc-0.8283\n",
      "Iter-24830, train loss-0.7575, acc-0.7600, valid loss-0.6772, acc-0.8248, test loss-0.6796, acc-0.8282\n",
      "Iter-24840, train loss-0.6603, acc-0.8400, valid loss-0.6771, acc-0.8248, test loss-0.6794, acc-0.8280\n",
      "Iter-24850, train loss-0.7516, acc-0.8000, valid loss-0.6769, acc-0.8248, test loss-0.6792, acc-0.8281\n",
      "Iter-24860, train loss-0.6278, acc-0.8600, valid loss-0.6767, acc-0.8250, test loss-0.6791, acc-0.8281\n",
      "Iter-24870, train loss-0.6454, acc-0.8200, valid loss-0.6765, acc-0.8248, test loss-0.6789, acc-0.8283\n",
      "Iter-24880, train loss-0.8415, acc-0.7600, valid loss-0.6764, acc-0.8248, test loss-0.6787, acc-0.8283\n",
      "Iter-24890, train loss-0.7574, acc-0.7400, valid loss-0.6762, acc-0.8248, test loss-0.6785, acc-0.8282\n",
      "Iter-24900, train loss-0.5794, acc-0.8400, valid loss-0.6760, acc-0.8252, test loss-0.6784, acc-0.8285\n",
      "Iter-24910, train loss-0.7719, acc-0.7800, valid loss-0.6758, acc-0.8250, test loss-0.6782, acc-0.8286\n",
      "Iter-24920, train loss-0.5577, acc-0.8800, valid loss-0.6757, acc-0.8250, test loss-0.6780, acc-0.8286\n",
      "Iter-24930, train loss-0.5928, acc-0.8600, valid loss-0.6755, acc-0.8252, test loss-0.6778, acc-0.8287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-24940, train loss-0.6213, acc-0.8200, valid loss-0.6753, acc-0.8252, test loss-0.6776, acc-0.8287\n",
      "Iter-24950, train loss-0.6184, acc-0.8600, valid loss-0.6751, acc-0.8252, test loss-0.6775, acc-0.8288\n",
      "Iter-24960, train loss-0.7733, acc-0.8200, valid loss-0.6750, acc-0.8254, test loss-0.6773, acc-0.8292\n",
      "Iter-24970, train loss-0.6406, acc-0.8800, valid loss-0.6748, acc-0.8256, test loss-0.6771, acc-0.8292\n",
      "Iter-24980, train loss-0.5468, acc-0.9200, valid loss-0.6746, acc-0.8258, test loss-0.6770, acc-0.8294\n",
      "Iter-24990, train loss-0.7086, acc-0.8600, valid loss-0.6744, acc-0.8260, test loss-0.6768, acc-0.8294\n",
      "Iter-25000, train loss-0.6656, acc-0.8200, valid loss-0.6743, acc-0.8260, test loss-0.6766, acc-0.8296\n",
      "Iter-25010, train loss-0.5277, acc-0.9000, valid loss-0.6741, acc-0.8258, test loss-0.6765, acc-0.8295\n",
      "Iter-25020, train loss-0.7656, acc-0.7400, valid loss-0.6739, acc-0.8262, test loss-0.6763, acc-0.8295\n",
      "Iter-25030, train loss-0.7891, acc-0.7800, valid loss-0.6738, acc-0.8258, test loss-0.6761, acc-0.8295\n",
      "Iter-25040, train loss-0.6888, acc-0.8000, valid loss-0.6736, acc-0.8260, test loss-0.6759, acc-0.8294\n",
      "Iter-25050, train loss-0.8160, acc-0.8000, valid loss-0.6734, acc-0.8260, test loss-0.6758, acc-0.8294\n",
      "Iter-25060, train loss-0.8372, acc-0.7600, valid loss-0.6732, acc-0.8258, test loss-0.6756, acc-0.8295\n",
      "Iter-25070, train loss-0.7996, acc-0.8000, valid loss-0.6731, acc-0.8258, test loss-0.6755, acc-0.8295\n",
      "Iter-25080, train loss-0.7053, acc-0.8200, valid loss-0.6729, acc-0.8262, test loss-0.6753, acc-0.8294\n",
      "Iter-25090, train loss-0.6169, acc-0.8200, valid loss-0.6727, acc-0.8262, test loss-0.6751, acc-0.8295\n",
      "Iter-25100, train loss-0.6022, acc-0.8400, valid loss-0.6726, acc-0.8262, test loss-0.6749, acc-0.8295\n",
      "Iter-25110, train loss-0.7498, acc-0.7800, valid loss-0.6724, acc-0.8262, test loss-0.6748, acc-0.8293\n",
      "Iter-25120, train loss-0.6140, acc-0.8600, valid loss-0.6722, acc-0.8262, test loss-0.6746, acc-0.8293\n",
      "Iter-25130, train loss-0.7745, acc-0.7400, valid loss-0.6720, acc-0.8262, test loss-0.6744, acc-0.8294\n",
      "Iter-25140, train loss-0.6398, acc-0.8600, valid loss-0.6718, acc-0.8260, test loss-0.6742, acc-0.8296\n",
      "Iter-25150, train loss-1.0277, acc-0.7000, valid loss-0.6717, acc-0.8260, test loss-0.6741, acc-0.8296\n",
      "Iter-25160, train loss-0.5631, acc-0.8800, valid loss-0.6715, acc-0.8262, test loss-0.6739, acc-0.8296\n",
      "Iter-25170, train loss-0.6875, acc-0.8000, valid loss-0.6713, acc-0.8262, test loss-0.6737, acc-0.8299\n",
      "Iter-25180, train loss-0.7153, acc-0.7800, valid loss-0.6711, acc-0.8264, test loss-0.6735, acc-0.8299\n",
      "Iter-25190, train loss-0.5034, acc-0.9000, valid loss-0.6710, acc-0.8262, test loss-0.6734, acc-0.8298\n",
      "Iter-25200, train loss-0.6108, acc-0.8800, valid loss-0.6708, acc-0.8264, test loss-0.6732, acc-0.8298\n",
      "Iter-25210, train loss-0.5070, acc-0.8600, valid loss-0.6706, acc-0.8264, test loss-0.6730, acc-0.8299\n",
      "Iter-25220, train loss-0.7591, acc-0.8000, valid loss-0.6704, acc-0.8264, test loss-0.6728, acc-0.8299\n",
      "Iter-25230, train loss-0.5538, acc-0.8400, valid loss-0.6702, acc-0.8262, test loss-0.6727, acc-0.8300\n",
      "Iter-25240, train loss-0.7075, acc-0.8000, valid loss-0.6701, acc-0.8264, test loss-0.6725, acc-0.8300\n",
      "Iter-25250, train loss-0.6225, acc-0.8400, valid loss-0.6699, acc-0.8266, test loss-0.6723, acc-0.8299\n",
      "Iter-25260, train loss-0.6571, acc-0.8800, valid loss-0.6697, acc-0.8266, test loss-0.6722, acc-0.8301\n",
      "Iter-25270, train loss-0.6935, acc-0.8200, valid loss-0.6696, acc-0.8266, test loss-0.6720, acc-0.8299\n",
      "Iter-25280, train loss-0.6810, acc-0.7600, valid loss-0.6694, acc-0.8262, test loss-0.6718, acc-0.8298\n",
      "Iter-25290, train loss-0.6839, acc-0.8200, valid loss-0.6692, acc-0.8262, test loss-0.6717, acc-0.8301\n",
      "Iter-25300, train loss-0.4928, acc-0.9200, valid loss-0.6691, acc-0.8262, test loss-0.6715, acc-0.8301\n",
      "Iter-25310, train loss-0.8069, acc-0.7200, valid loss-0.6689, acc-0.8262, test loss-0.6713, acc-0.8300\n",
      "Iter-25320, train loss-0.6448, acc-0.8400, valid loss-0.6688, acc-0.8266, test loss-0.6712, acc-0.8300\n",
      "Iter-25330, train loss-0.7153, acc-0.8000, valid loss-0.6686, acc-0.8264, test loss-0.6710, acc-0.8300\n",
      "Iter-25340, train loss-0.6477, acc-0.8200, valid loss-0.6684, acc-0.8264, test loss-0.6708, acc-0.8302\n",
      "Iter-25350, train loss-0.6361, acc-0.9000, valid loss-0.6683, acc-0.8264, test loss-0.6707, acc-0.8302\n",
      "Iter-25360, train loss-0.7126, acc-0.7800, valid loss-0.6681, acc-0.8262, test loss-0.6705, acc-0.8304\n",
      "Iter-25370, train loss-0.6156, acc-0.8600, valid loss-0.6679, acc-0.8262, test loss-0.6703, acc-0.8304\n",
      "Iter-25380, train loss-0.6577, acc-0.8200, valid loss-0.6678, acc-0.8264, test loss-0.6702, acc-0.8303\n",
      "Iter-25390, train loss-0.7469, acc-0.8000, valid loss-0.6676, acc-0.8266, test loss-0.6700, acc-0.8303\n",
      "Iter-25400, train loss-0.6630, acc-0.8200, valid loss-0.6674, acc-0.8266, test loss-0.6698, acc-0.8305\n",
      "Iter-25410, train loss-0.6561, acc-0.8000, valid loss-0.6673, acc-0.8266, test loss-0.6696, acc-0.8305\n",
      "Iter-25420, train loss-0.6904, acc-0.8400, valid loss-0.6671, acc-0.8268, test loss-0.6695, acc-0.8305\n",
      "Iter-25430, train loss-0.6939, acc-0.8000, valid loss-0.6669, acc-0.8276, test loss-0.6693, acc-0.8304\n",
      "Iter-25440, train loss-0.5260, acc-0.8800, valid loss-0.6668, acc-0.8276, test loss-0.6692, acc-0.8305\n",
      "Iter-25450, train loss-0.5741, acc-0.8600, valid loss-0.6666, acc-0.8274, test loss-0.6690, acc-0.8306\n",
      "Iter-25460, train loss-0.8057, acc-0.7400, valid loss-0.6664, acc-0.8278, test loss-0.6688, acc-0.8304\n",
      "Iter-25470, train loss-0.6995, acc-0.8200, valid loss-0.6662, acc-0.8278, test loss-0.6687, acc-0.8305\n",
      "Iter-25480, train loss-0.5109, acc-0.8800, valid loss-0.6661, acc-0.8276, test loss-0.6685, acc-0.8305\n",
      "Iter-25490, train loss-0.6560, acc-0.8600, valid loss-0.6659, acc-0.8278, test loss-0.6683, acc-0.8305\n",
      "Iter-25500, train loss-0.6550, acc-0.7800, valid loss-0.6658, acc-0.8276, test loss-0.6681, acc-0.8304\n",
      "Iter-25510, train loss-0.7309, acc-0.8000, valid loss-0.6656, acc-0.8276, test loss-0.6680, acc-0.8305\n",
      "Iter-25520, train loss-0.6570, acc-0.8400, valid loss-0.6654, acc-0.8276, test loss-0.6678, acc-0.8306\n",
      "Iter-25530, train loss-0.9250, acc-0.7400, valid loss-0.6652, acc-0.8276, test loss-0.6676, acc-0.8305\n",
      "Iter-25540, train loss-0.5822, acc-0.8600, valid loss-0.6651, acc-0.8276, test loss-0.6675, acc-0.8305\n",
      "Iter-25550, train loss-0.5705, acc-0.9000, valid loss-0.6649, acc-0.8276, test loss-0.6673, acc-0.8306\n",
      "Iter-25560, train loss-0.6633, acc-0.7600, valid loss-0.6647, acc-0.8276, test loss-0.6671, acc-0.8307\n",
      "Iter-25570, train loss-0.7837, acc-0.8000, valid loss-0.6645, acc-0.8276, test loss-0.6669, acc-0.8307\n",
      "Iter-25580, train loss-0.4831, acc-0.9400, valid loss-0.6644, acc-0.8276, test loss-0.6668, acc-0.8309\n",
      "Iter-25590, train loss-0.5708, acc-0.8600, valid loss-0.6642, acc-0.8278, test loss-0.6666, acc-0.8308\n",
      "Iter-25600, train loss-0.7449, acc-0.7800, valid loss-0.6641, acc-0.8276, test loss-0.6665, acc-0.8307\n",
      "Iter-25610, train loss-0.7465, acc-0.8000, valid loss-0.6639, acc-0.8278, test loss-0.6663, acc-0.8307\n",
      "Iter-25620, train loss-0.8097, acc-0.8400, valid loss-0.6637, acc-0.8278, test loss-0.6661, acc-0.8307\n",
      "Iter-25630, train loss-0.5783, acc-0.8400, valid loss-0.6636, acc-0.8278, test loss-0.6660, acc-0.8308\n",
      "Iter-25640, train loss-0.5495, acc-0.8400, valid loss-0.6634, acc-0.8284, test loss-0.6658, acc-0.8308\n",
      "Iter-25650, train loss-0.7291, acc-0.7800, valid loss-0.6632, acc-0.8286, test loss-0.6657, acc-0.8310\n",
      "Iter-25660, train loss-0.7538, acc-0.7600, valid loss-0.6631, acc-0.8292, test loss-0.6655, acc-0.8312\n",
      "Iter-25670, train loss-0.7189, acc-0.8000, valid loss-0.6629, acc-0.8292, test loss-0.6654, acc-0.8310\n",
      "Iter-25680, train loss-0.6278, acc-0.8400, valid loss-0.6627, acc-0.8292, test loss-0.6652, acc-0.8308\n",
      "Iter-25690, train loss-0.6463, acc-0.8200, valid loss-0.6626, acc-0.8290, test loss-0.6650, acc-0.8309\n",
      "Iter-25700, train loss-0.7839, acc-0.7800, valid loss-0.6624, acc-0.8294, test loss-0.6648, acc-0.8310\n",
      "Iter-25710, train loss-0.6926, acc-0.8000, valid loss-0.6622, acc-0.8296, test loss-0.6647, acc-0.8309\n",
      "Iter-25720, train loss-0.8420, acc-0.7600, valid loss-0.6621, acc-0.8296, test loss-0.6645, acc-0.8310\n",
      "Iter-25730, train loss-0.7109, acc-0.8400, valid loss-0.6619, acc-0.8294, test loss-0.6643, acc-0.8310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-25740, train loss-0.6109, acc-0.8400, valid loss-0.6617, acc-0.8296, test loss-0.6642, acc-0.8307\n",
      "Iter-25750, train loss-0.7306, acc-0.8200, valid loss-0.6616, acc-0.8294, test loss-0.6641, acc-0.8310\n",
      "Iter-25760, train loss-0.6840, acc-0.8000, valid loss-0.6614, acc-0.8296, test loss-0.6639, acc-0.8309\n",
      "Iter-25770, train loss-0.7803, acc-0.7600, valid loss-0.6612, acc-0.8296, test loss-0.6637, acc-0.8309\n",
      "Iter-25780, train loss-0.4920, acc-0.8600, valid loss-0.6611, acc-0.8298, test loss-0.6636, acc-0.8309\n",
      "Iter-25790, train loss-0.7467, acc-0.8400, valid loss-0.6609, acc-0.8298, test loss-0.6634, acc-0.8309\n",
      "Iter-25800, train loss-0.7694, acc-0.7800, valid loss-0.6607, acc-0.8296, test loss-0.6632, acc-0.8309\n",
      "Iter-25810, train loss-0.7159, acc-0.7600, valid loss-0.6606, acc-0.8296, test loss-0.6631, acc-0.8309\n",
      "Iter-25820, train loss-0.6117, acc-0.9400, valid loss-0.6604, acc-0.8298, test loss-0.6629, acc-0.8310\n",
      "Iter-25830, train loss-0.6651, acc-0.8000, valid loss-0.6602, acc-0.8298, test loss-0.6628, acc-0.8310\n",
      "Iter-25840, train loss-0.7026, acc-0.8400, valid loss-0.6601, acc-0.8296, test loss-0.6626, acc-0.8309\n",
      "Iter-25850, train loss-0.8618, acc-0.7400, valid loss-0.6600, acc-0.8300, test loss-0.6625, acc-0.8311\n",
      "Iter-25860, train loss-0.6582, acc-0.7600, valid loss-0.6598, acc-0.8302, test loss-0.6623, acc-0.8312\n",
      "Iter-25870, train loss-0.6375, acc-0.8600, valid loss-0.6596, acc-0.8300, test loss-0.6622, acc-0.8312\n",
      "Iter-25880, train loss-0.5072, acc-0.9000, valid loss-0.6595, acc-0.8300, test loss-0.6620, acc-0.8311\n",
      "Iter-25890, train loss-0.4534, acc-0.9600, valid loss-0.6593, acc-0.8298, test loss-0.6619, acc-0.8311\n",
      "Iter-25900, train loss-0.7959, acc-0.8200, valid loss-0.6592, acc-0.8300, test loss-0.6617, acc-0.8309\n",
      "Iter-25910, train loss-0.5829, acc-0.8400, valid loss-0.6590, acc-0.8296, test loss-0.6616, acc-0.8311\n",
      "Iter-25920, train loss-0.8179, acc-0.6800, valid loss-0.6589, acc-0.8296, test loss-0.6614, acc-0.8312\n",
      "Iter-25930, train loss-0.6296, acc-0.8400, valid loss-0.6587, acc-0.8294, test loss-0.6612, acc-0.8312\n",
      "Iter-25940, train loss-0.9053, acc-0.7400, valid loss-0.6585, acc-0.8300, test loss-0.6611, acc-0.8310\n",
      "Iter-25950, train loss-0.8814, acc-0.7400, valid loss-0.6584, acc-0.8298, test loss-0.6609, acc-0.8313\n",
      "Iter-25960, train loss-0.7493, acc-0.8000, valid loss-0.6582, acc-0.8298, test loss-0.6608, acc-0.8313\n",
      "Iter-25970, train loss-1.0326, acc-0.7200, valid loss-0.6581, acc-0.8294, test loss-0.6606, acc-0.8313\n",
      "Iter-25980, train loss-0.4814, acc-0.8800, valid loss-0.6579, acc-0.8296, test loss-0.6604, acc-0.8313\n",
      "Iter-25990, train loss-0.6416, acc-0.7600, valid loss-0.6577, acc-0.8298, test loss-0.6603, acc-0.8313\n",
      "Iter-26000, train loss-0.8981, acc-0.7000, valid loss-0.6576, acc-0.8294, test loss-0.6601, acc-0.8313\n",
      "Iter-26010, train loss-0.5747, acc-0.8800, valid loss-0.6574, acc-0.8296, test loss-0.6600, acc-0.8313\n",
      "Iter-26020, train loss-0.7449, acc-0.7600, valid loss-0.6572, acc-0.8294, test loss-0.6598, acc-0.8313\n",
      "Iter-26030, train loss-0.7111, acc-0.8800, valid loss-0.6571, acc-0.8300, test loss-0.6597, acc-0.8312\n",
      "Iter-26040, train loss-0.7174, acc-0.8400, valid loss-0.6569, acc-0.8302, test loss-0.6595, acc-0.8314\n",
      "Iter-26050, train loss-0.4929, acc-0.9000, valid loss-0.6567, acc-0.8304, test loss-0.6593, acc-0.8316\n",
      "Iter-26060, train loss-0.6812, acc-0.8400, valid loss-0.6566, acc-0.8310, test loss-0.6592, acc-0.8316\n",
      "Iter-26070, train loss-0.7025, acc-0.8000, valid loss-0.6564, acc-0.8306, test loss-0.6590, acc-0.8314\n",
      "Iter-26080, train loss-0.6217, acc-0.8600, valid loss-0.6563, acc-0.8306, test loss-0.6589, acc-0.8315\n",
      "Iter-26090, train loss-0.5582, acc-0.8400, valid loss-0.6561, acc-0.8308, test loss-0.6587, acc-0.8315\n",
      "Iter-26100, train loss-0.8443, acc-0.7400, valid loss-0.6560, acc-0.8312, test loss-0.6586, acc-0.8317\n",
      "Iter-26110, train loss-0.7737, acc-0.7400, valid loss-0.6558, acc-0.8306, test loss-0.6584, acc-0.8317\n",
      "Iter-26120, train loss-0.7435, acc-0.7800, valid loss-0.6557, acc-0.8306, test loss-0.6583, acc-0.8317\n",
      "Iter-26130, train loss-0.5826, acc-0.8000, valid loss-0.6555, acc-0.8310, test loss-0.6581, acc-0.8317\n",
      "Iter-26140, train loss-0.5520, acc-0.9000, valid loss-0.6553, acc-0.8310, test loss-0.6579, acc-0.8315\n",
      "Iter-26150, train loss-0.6277, acc-0.8200, valid loss-0.6552, acc-0.8310, test loss-0.6578, acc-0.8314\n",
      "Iter-26160, train loss-0.7908, acc-0.8000, valid loss-0.6550, acc-0.8312, test loss-0.6576, acc-0.8316\n",
      "Iter-26170, train loss-0.7373, acc-0.7400, valid loss-0.6549, acc-0.8312, test loss-0.6575, acc-0.8317\n",
      "Iter-26180, train loss-0.7770, acc-0.8200, valid loss-0.6547, acc-0.8312, test loss-0.6573, acc-0.8317\n",
      "Iter-26190, train loss-0.5747, acc-0.8800, valid loss-0.6545, acc-0.8314, test loss-0.6572, acc-0.8316\n",
      "Iter-26200, train loss-0.4027, acc-0.9400, valid loss-0.6544, acc-0.8312, test loss-0.6570, acc-0.8315\n",
      "Iter-26210, train loss-0.7772, acc-0.7200, valid loss-0.6542, acc-0.8312, test loss-0.6569, acc-0.8317\n",
      "Iter-26220, train loss-0.6172, acc-0.8200, valid loss-0.6541, acc-0.8314, test loss-0.6567, acc-0.8317\n",
      "Iter-26230, train loss-0.8817, acc-0.7200, valid loss-0.6539, acc-0.8314, test loss-0.6565, acc-0.8319\n",
      "Iter-26240, train loss-0.6093, acc-0.8600, valid loss-0.6537, acc-0.8318, test loss-0.6564, acc-0.8319\n",
      "Iter-26250, train loss-0.5344, acc-0.9000, valid loss-0.6536, acc-0.8318, test loss-0.6562, acc-0.8319\n",
      "Iter-26260, train loss-0.4490, acc-0.9000, valid loss-0.6535, acc-0.8318, test loss-0.6561, acc-0.8319\n",
      "Iter-26270, train loss-0.5378, acc-0.8600, valid loss-0.6533, acc-0.8318, test loss-0.6559, acc-0.8318\n",
      "Iter-26280, train loss-0.6620, acc-0.8800, valid loss-0.6531, acc-0.8318, test loss-0.6557, acc-0.8321\n",
      "Iter-26290, train loss-0.6472, acc-0.8200, valid loss-0.6530, acc-0.8318, test loss-0.6556, acc-0.8319\n",
      "Iter-26300, train loss-0.7229, acc-0.8000, valid loss-0.6528, acc-0.8318, test loss-0.6554, acc-0.8321\n",
      "Iter-26310, train loss-0.7313, acc-0.7600, valid loss-0.6526, acc-0.8320, test loss-0.6553, acc-0.8321\n",
      "Iter-26320, train loss-0.4933, acc-0.8600, valid loss-0.6525, acc-0.8318, test loss-0.6551, acc-0.8320\n",
      "Iter-26330, train loss-0.6739, acc-0.8000, valid loss-0.6523, acc-0.8318, test loss-0.6550, acc-0.8319\n",
      "Iter-26340, train loss-0.7329, acc-0.7800, valid loss-0.6522, acc-0.8318, test loss-0.6548, acc-0.8320\n",
      "Iter-26350, train loss-0.5926, acc-0.8600, valid loss-0.6520, acc-0.8318, test loss-0.6547, acc-0.8319\n",
      "Iter-26360, train loss-0.6612, acc-0.8000, valid loss-0.6518, acc-0.8318, test loss-0.6545, acc-0.8319\n",
      "Iter-26370, train loss-0.6416, acc-0.8200, valid loss-0.6517, acc-0.8316, test loss-0.6543, acc-0.8318\n",
      "Iter-26380, train loss-0.6365, acc-0.8000, valid loss-0.6515, acc-0.8316, test loss-0.6542, acc-0.8322\n",
      "Iter-26390, train loss-0.6510, acc-0.8200, valid loss-0.6514, acc-0.8318, test loss-0.6540, acc-0.8322\n",
      "Iter-26400, train loss-0.7022, acc-0.8000, valid loss-0.6512, acc-0.8316, test loss-0.6539, acc-0.8322\n",
      "Iter-26410, train loss-0.6634, acc-0.8200, valid loss-0.6511, acc-0.8316, test loss-0.6537, acc-0.8323\n",
      "Iter-26420, train loss-0.8353, acc-0.8200, valid loss-0.6509, acc-0.8316, test loss-0.6536, acc-0.8323\n",
      "Iter-26430, train loss-0.5895, acc-0.8800, valid loss-0.6508, acc-0.8318, test loss-0.6534, acc-0.8323\n",
      "Iter-26440, train loss-0.6130, acc-0.8200, valid loss-0.6506, acc-0.8316, test loss-0.6533, acc-0.8322\n",
      "Iter-26450, train loss-0.5885, acc-0.8800, valid loss-0.6505, acc-0.8314, test loss-0.6531, acc-0.8323\n",
      "Iter-26460, train loss-0.4454, acc-0.9200, valid loss-0.6503, acc-0.8314, test loss-0.6530, acc-0.8323\n",
      "Iter-26470, train loss-0.7981, acc-0.7600, valid loss-0.6502, acc-0.8314, test loss-0.6528, acc-0.8325\n",
      "Iter-26480, train loss-0.5755, acc-0.8400, valid loss-0.6500, acc-0.8316, test loss-0.6527, acc-0.8324\n",
      "Iter-26490, train loss-0.6907, acc-0.7800, valid loss-0.6498, acc-0.8318, test loss-0.6525, acc-0.8324\n",
      "Iter-26500, train loss-0.7326, acc-0.7800, valid loss-0.6497, acc-0.8318, test loss-0.6524, acc-0.8324\n",
      "Iter-26510, train loss-0.5636, acc-0.8800, valid loss-0.6495, acc-0.8320, test loss-0.6522, acc-0.8327\n",
      "Iter-26520, train loss-0.6053, acc-0.8400, valid loss-0.6493, acc-0.8320, test loss-0.6520, acc-0.8326\n",
      "Iter-26530, train loss-0.8029, acc-0.8200, valid loss-0.6492, acc-0.8322, test loss-0.6519, acc-0.8326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-26540, train loss-0.6856, acc-0.8200, valid loss-0.6490, acc-0.8324, test loss-0.6518, acc-0.8326\n",
      "Iter-26550, train loss-0.5734, acc-0.8600, valid loss-0.6489, acc-0.8328, test loss-0.6516, acc-0.8327\n",
      "Iter-26560, train loss-0.7058, acc-0.8600, valid loss-0.6487, acc-0.8328, test loss-0.6514, acc-0.8328\n",
      "Iter-26570, train loss-0.7576, acc-0.7600, valid loss-0.6486, acc-0.8326, test loss-0.6513, acc-0.8327\n",
      "Iter-26580, train loss-0.6515, acc-0.7200, valid loss-0.6484, acc-0.8326, test loss-0.6511, acc-0.8328\n",
      "Iter-26590, train loss-0.6361, acc-0.9200, valid loss-0.6483, acc-0.8324, test loss-0.6510, acc-0.8329\n",
      "Iter-26600, train loss-0.6825, acc-0.7800, valid loss-0.6481, acc-0.8324, test loss-0.6508, acc-0.8328\n",
      "Iter-26610, train loss-0.7214, acc-0.8400, valid loss-0.6480, acc-0.8324, test loss-0.6507, acc-0.8328\n",
      "Iter-26620, train loss-0.8133, acc-0.7600, valid loss-0.6478, acc-0.8324, test loss-0.6505, acc-0.8328\n",
      "Iter-26630, train loss-0.6609, acc-0.8200, valid loss-0.6476, acc-0.8326, test loss-0.6504, acc-0.8329\n",
      "Iter-26640, train loss-0.6688, acc-0.8200, valid loss-0.6475, acc-0.8324, test loss-0.6502, acc-0.8328\n",
      "Iter-26650, train loss-0.5560, acc-0.8400, valid loss-0.6473, acc-0.8324, test loss-0.6500, acc-0.8328\n",
      "Iter-26660, train loss-0.6445, acc-0.8200, valid loss-0.6472, acc-0.8326, test loss-0.6499, acc-0.8330\n",
      "Iter-26670, train loss-0.6516, acc-0.8400, valid loss-0.6470, acc-0.8328, test loss-0.6497, acc-0.8330\n",
      "Iter-26680, train loss-0.8535, acc-0.7400, valid loss-0.6469, acc-0.8328, test loss-0.6496, acc-0.8330\n",
      "Iter-26690, train loss-0.5980, acc-0.8800, valid loss-0.6467, acc-0.8326, test loss-0.6495, acc-0.8330\n",
      "Iter-26700, train loss-0.5878, acc-0.8200, valid loss-0.6465, acc-0.8326, test loss-0.6493, acc-0.8330\n",
      "Iter-26710, train loss-0.6286, acc-0.9000, valid loss-0.6464, acc-0.8328, test loss-0.6491, acc-0.8330\n",
      "Iter-26720, train loss-0.6914, acc-0.7800, valid loss-0.6462, acc-0.8328, test loss-0.6490, acc-0.8332\n",
      "Iter-26730, train loss-0.6399, acc-0.8400, valid loss-0.6461, acc-0.8328, test loss-0.6489, acc-0.8331\n",
      "Iter-26740, train loss-0.6568, acc-0.8400, valid loss-0.6459, acc-0.8326, test loss-0.6487, acc-0.8331\n",
      "Iter-26750, train loss-0.7063, acc-0.7400, valid loss-0.6458, acc-0.8326, test loss-0.6486, acc-0.8330\n",
      "Iter-26760, train loss-0.7472, acc-0.7200, valid loss-0.6456, acc-0.8326, test loss-0.6484, acc-0.8329\n",
      "Iter-26770, train loss-0.4959, acc-0.8400, valid loss-0.6455, acc-0.8326, test loss-0.6483, acc-0.8329\n",
      "Iter-26780, train loss-0.5669, acc-0.8400, valid loss-0.6453, acc-0.8326, test loss-0.6481, acc-0.8331\n",
      "Iter-26790, train loss-0.5188, acc-0.9000, valid loss-0.6451, acc-0.8328, test loss-0.6480, acc-0.8332\n",
      "Iter-26800, train loss-0.5171, acc-0.9200, valid loss-0.6450, acc-0.8328, test loss-0.6478, acc-0.8331\n",
      "Iter-26810, train loss-0.8042, acc-0.8000, valid loss-0.6448, acc-0.8330, test loss-0.6477, acc-0.8330\n",
      "Iter-26820, train loss-0.7926, acc-0.7600, valid loss-0.6447, acc-0.8330, test loss-0.6475, acc-0.8332\n",
      "Iter-26830, train loss-0.6873, acc-0.7800, valid loss-0.6445, acc-0.8332, test loss-0.6473, acc-0.8332\n",
      "Iter-26840, train loss-0.5443, acc-0.8600, valid loss-0.6443, acc-0.8330, test loss-0.6472, acc-0.8333\n",
      "Iter-26850, train loss-0.8187, acc-0.7000, valid loss-0.6442, acc-0.8332, test loss-0.6470, acc-0.8333\n",
      "Iter-26860, train loss-0.4876, acc-0.8400, valid loss-0.6441, acc-0.8330, test loss-0.6469, acc-0.8335\n",
      "Iter-26870, train loss-0.7554, acc-0.7800, valid loss-0.6439, acc-0.8330, test loss-0.6467, acc-0.8334\n",
      "Iter-26880, train loss-0.5725, acc-0.8400, valid loss-0.6438, acc-0.8330, test loss-0.6466, acc-0.8338\n",
      "Iter-26890, train loss-0.5554, acc-0.8600, valid loss-0.6436, acc-0.8330, test loss-0.6464, acc-0.8338\n",
      "Iter-26900, train loss-0.5340, acc-0.9400, valid loss-0.6434, acc-0.8330, test loss-0.6463, acc-0.8339\n",
      "Iter-26910, train loss-0.7588, acc-0.7800, valid loss-0.6433, acc-0.8330, test loss-0.6461, acc-0.8338\n",
      "Iter-26920, train loss-0.5782, acc-0.8600, valid loss-0.6431, acc-0.8332, test loss-0.6460, acc-0.8337\n",
      "Iter-26930, train loss-0.6997, acc-0.8000, valid loss-0.6430, acc-0.8334, test loss-0.6459, acc-0.8340\n",
      "Iter-26940, train loss-0.7889, acc-0.7400, valid loss-0.6429, acc-0.8334, test loss-0.6457, acc-0.8340\n",
      "Iter-26950, train loss-0.6881, acc-0.8000, valid loss-0.6427, acc-0.8334, test loss-0.6456, acc-0.8339\n",
      "Iter-26960, train loss-0.6591, acc-0.8200, valid loss-0.6426, acc-0.8334, test loss-0.6454, acc-0.8340\n",
      "Iter-26970, train loss-0.8649, acc-0.7200, valid loss-0.6424, acc-0.8334, test loss-0.6453, acc-0.8338\n",
      "Iter-26980, train loss-0.5756, acc-0.8800, valid loss-0.6423, acc-0.8336, test loss-0.6451, acc-0.8339\n",
      "Iter-26990, train loss-0.7684, acc-0.7800, valid loss-0.6421, acc-0.8336, test loss-0.6450, acc-0.8343\n",
      "Iter-27000, train loss-0.6079, acc-0.8800, valid loss-0.6420, acc-0.8338, test loss-0.6448, acc-0.8344\n",
      "Iter-27010, train loss-0.7554, acc-0.7600, valid loss-0.6418, acc-0.8336, test loss-0.6447, acc-0.8344\n",
      "Iter-27020, train loss-0.8293, acc-0.7400, valid loss-0.6417, acc-0.8338, test loss-0.6445, acc-0.8345\n",
      "Iter-27030, train loss-0.6559, acc-0.8400, valid loss-0.6415, acc-0.8338, test loss-0.6444, acc-0.8346\n",
      "Iter-27040, train loss-0.6008, acc-0.8400, valid loss-0.6414, acc-0.8338, test loss-0.6443, acc-0.8346\n",
      "Iter-27050, train loss-0.7001, acc-0.7800, valid loss-0.6412, acc-0.8338, test loss-0.6441, acc-0.8345\n",
      "Iter-27060, train loss-0.5164, acc-0.9000, valid loss-0.6411, acc-0.8338, test loss-0.6440, acc-0.8348\n",
      "Iter-27070, train loss-0.5619, acc-0.8600, valid loss-0.6409, acc-0.8338, test loss-0.6438, acc-0.8345\n",
      "Iter-27080, train loss-0.5732, acc-0.9000, valid loss-0.6408, acc-0.8336, test loss-0.6437, acc-0.8345\n",
      "Iter-27090, train loss-0.6536, acc-0.8400, valid loss-0.6406, acc-0.8338, test loss-0.6435, acc-0.8345\n",
      "Iter-27100, train loss-1.0052, acc-0.7200, valid loss-0.6405, acc-0.8338, test loss-0.6434, acc-0.8346\n",
      "Iter-27110, train loss-0.6107, acc-0.8600, valid loss-0.6403, acc-0.8338, test loss-0.6432, acc-0.8346\n",
      "Iter-27120, train loss-0.6900, acc-0.8400, valid loss-0.6402, acc-0.8338, test loss-0.6431, acc-0.8345\n",
      "Iter-27130, train loss-0.7101, acc-0.8200, valid loss-0.6400, acc-0.8338, test loss-0.6429, acc-0.8345\n",
      "Iter-27140, train loss-0.5371, acc-0.8800, valid loss-0.6399, acc-0.8338, test loss-0.6428, acc-0.8347\n",
      "Iter-27150, train loss-0.9068, acc-0.7400, valid loss-0.6398, acc-0.8338, test loss-0.6427, acc-0.8347\n",
      "Iter-27160, train loss-0.4448, acc-0.9400, valid loss-0.6396, acc-0.8338, test loss-0.6425, acc-0.8347\n",
      "Iter-27170, train loss-0.5984, acc-0.8400, valid loss-0.6395, acc-0.8338, test loss-0.6424, acc-0.8349\n",
      "Iter-27180, train loss-0.7796, acc-0.7400, valid loss-0.6393, acc-0.8338, test loss-0.6422, acc-0.8349\n",
      "Iter-27190, train loss-0.6907, acc-0.7800, valid loss-0.6392, acc-0.8338, test loss-0.6421, acc-0.8350\n",
      "Iter-27200, train loss-0.5004, acc-0.8600, valid loss-0.6390, acc-0.8338, test loss-0.6419, acc-0.8350\n",
      "Iter-27210, train loss-0.6454, acc-0.8000, valid loss-0.6389, acc-0.8338, test loss-0.6418, acc-0.8350\n",
      "Iter-27220, train loss-0.7184, acc-0.7600, valid loss-0.6388, acc-0.8340, test loss-0.6416, acc-0.8349\n",
      "Iter-27230, train loss-0.7385, acc-0.8400, valid loss-0.6386, acc-0.8338, test loss-0.6415, acc-0.8351\n",
      "Iter-27240, train loss-0.6942, acc-0.7600, valid loss-0.6385, acc-0.8338, test loss-0.6413, acc-0.8351\n",
      "Iter-27250, train loss-0.6428, acc-0.8600, valid loss-0.6383, acc-0.8340, test loss-0.6412, acc-0.8351\n",
      "Iter-27260, train loss-0.6620, acc-0.8600, valid loss-0.6382, acc-0.8344, test loss-0.6410, acc-0.8350\n",
      "Iter-27270, train loss-0.6072, acc-0.8800, valid loss-0.6380, acc-0.8342, test loss-0.6409, acc-0.8350\n",
      "Iter-27280, train loss-0.5589, acc-0.8000, valid loss-0.6379, acc-0.8342, test loss-0.6408, acc-0.8350\n",
      "Iter-27290, train loss-0.6977, acc-0.8200, valid loss-0.6377, acc-0.8342, test loss-0.6406, acc-0.8350\n",
      "Iter-27300, train loss-0.7155, acc-0.8000, valid loss-0.6376, acc-0.8344, test loss-0.6405, acc-0.8350\n",
      "Iter-27310, train loss-0.6092, acc-0.8200, valid loss-0.6374, acc-0.8342, test loss-0.6403, acc-0.8350\n",
      "Iter-27320, train loss-0.7558, acc-0.8000, valid loss-0.6373, acc-0.8344, test loss-0.6402, acc-0.8350\n",
      "Iter-27330, train loss-0.6611, acc-0.8200, valid loss-0.6372, acc-0.8342, test loss-0.6401, acc-0.8352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-27340, train loss-0.7764, acc-0.8200, valid loss-0.6370, acc-0.8346, test loss-0.6399, acc-0.8352\n",
      "Iter-27350, train loss-0.5610, acc-0.8200, valid loss-0.6369, acc-0.8344, test loss-0.6398, acc-0.8353\n",
      "Iter-27360, train loss-0.5117, acc-0.8600, valid loss-0.6367, acc-0.8344, test loss-0.6396, acc-0.8353\n",
      "Iter-27370, train loss-0.5164, acc-0.9600, valid loss-0.6366, acc-0.8344, test loss-0.6395, acc-0.8352\n",
      "Iter-27380, train loss-0.7009, acc-0.8200, valid loss-0.6364, acc-0.8344, test loss-0.6394, acc-0.8354\n",
      "Iter-27390, train loss-0.6985, acc-0.7800, valid loss-0.6363, acc-0.8344, test loss-0.6392, acc-0.8352\n",
      "Iter-27400, train loss-0.6744, acc-0.8400, valid loss-0.6361, acc-0.8344, test loss-0.6391, acc-0.8352\n",
      "Iter-27410, train loss-0.6536, acc-0.8400, valid loss-0.6360, acc-0.8344, test loss-0.6390, acc-0.8353\n",
      "Iter-27420, train loss-0.4927, acc-0.9400, valid loss-0.6358, acc-0.8344, test loss-0.6388, acc-0.8353\n",
      "Iter-27430, train loss-0.6729, acc-0.8600, valid loss-0.6357, acc-0.8344, test loss-0.6387, acc-0.8351\n",
      "Iter-27440, train loss-0.6071, acc-0.8000, valid loss-0.6355, acc-0.8344, test loss-0.6385, acc-0.8353\n",
      "Iter-27450, train loss-0.5783, acc-0.8200, valid loss-0.6354, acc-0.8344, test loss-0.6384, acc-0.8354\n",
      "Iter-27460, train loss-0.6859, acc-0.8200, valid loss-0.6353, acc-0.8346, test loss-0.6383, acc-0.8353\n",
      "Iter-27470, train loss-0.8477, acc-0.7800, valid loss-0.6351, acc-0.8346, test loss-0.6381, acc-0.8352\n",
      "Iter-27480, train loss-0.5923, acc-0.7800, valid loss-0.6350, acc-0.8348, test loss-0.6380, acc-0.8355\n",
      "Iter-27490, train loss-0.5095, acc-0.8800, valid loss-0.6349, acc-0.8350, test loss-0.6378, acc-0.8356\n",
      "Iter-27500, train loss-0.7017, acc-0.8600, valid loss-0.6347, acc-0.8350, test loss-0.6377, acc-0.8354\n",
      "Iter-27510, train loss-0.6698, acc-0.7600, valid loss-0.6346, acc-0.8352, test loss-0.6376, acc-0.8355\n",
      "Iter-27520, train loss-0.7867, acc-0.7800, valid loss-0.6344, acc-0.8350, test loss-0.6374, acc-0.8354\n",
      "Iter-27530, train loss-0.6611, acc-0.8200, valid loss-0.6343, acc-0.8352, test loss-0.6373, acc-0.8355\n",
      "Iter-27540, train loss-0.6716, acc-0.8600, valid loss-0.6342, acc-0.8352, test loss-0.6371, acc-0.8355\n",
      "Iter-27550, train loss-0.4787, acc-0.9400, valid loss-0.6340, acc-0.8352, test loss-0.6370, acc-0.8354\n",
      "Iter-27560, train loss-0.5818, acc-0.8800, valid loss-0.6339, acc-0.8352, test loss-0.6368, acc-0.8356\n",
      "Iter-27570, train loss-0.7365, acc-0.8200, valid loss-0.6337, acc-0.8356, test loss-0.6367, acc-0.8355\n",
      "Iter-27580, train loss-0.5503, acc-0.8800, valid loss-0.6336, acc-0.8356, test loss-0.6366, acc-0.8356\n",
      "Iter-27590, train loss-0.7467, acc-0.8000, valid loss-0.6335, acc-0.8358, test loss-0.6365, acc-0.8356\n",
      "Iter-27600, train loss-0.6339, acc-0.8200, valid loss-0.6334, acc-0.8354, test loss-0.6363, acc-0.8356\n",
      "Iter-27610, train loss-0.6636, acc-0.7600, valid loss-0.6332, acc-0.8356, test loss-0.6362, acc-0.8356\n",
      "Iter-27620, train loss-0.6613, acc-0.8200, valid loss-0.6330, acc-0.8356, test loss-0.6360, acc-0.8356\n",
      "Iter-27630, train loss-0.6993, acc-0.7400, valid loss-0.6329, acc-0.8358, test loss-0.6359, acc-0.8356\n",
      "Iter-27640, train loss-0.4261, acc-0.9000, valid loss-0.6328, acc-0.8358, test loss-0.6358, acc-0.8356\n",
      "Iter-27650, train loss-0.6990, acc-0.7600, valid loss-0.6326, acc-0.8356, test loss-0.6356, acc-0.8355\n",
      "Iter-27660, train loss-0.5899, acc-0.8600, valid loss-0.6325, acc-0.8356, test loss-0.6355, acc-0.8354\n",
      "Iter-27670, train loss-0.7312, acc-0.7800, valid loss-0.6324, acc-0.8356, test loss-0.6353, acc-0.8354\n",
      "Iter-27680, train loss-0.5593, acc-0.8800, valid loss-0.6322, acc-0.8356, test loss-0.6352, acc-0.8357\n",
      "Iter-27690, train loss-0.5504, acc-0.8800, valid loss-0.6321, acc-0.8360, test loss-0.6351, acc-0.8354\n",
      "Iter-27700, train loss-0.4418, acc-0.9000, valid loss-0.6320, acc-0.8358, test loss-0.6349, acc-0.8353\n",
      "Iter-27710, train loss-0.4928, acc-0.8800, valid loss-0.6319, acc-0.8358, test loss-0.6348, acc-0.8354\n",
      "Iter-27720, train loss-0.5257, acc-0.8600, valid loss-0.6317, acc-0.8358, test loss-0.6347, acc-0.8354\n",
      "Iter-27730, train loss-0.4457, acc-0.9000, valid loss-0.6316, acc-0.8360, test loss-0.6345, acc-0.8355\n",
      "Iter-27740, train loss-0.5905, acc-0.8800, valid loss-0.6315, acc-0.8360, test loss-0.6344, acc-0.8353\n",
      "Iter-27750, train loss-0.4344, acc-0.9200, valid loss-0.6313, acc-0.8360, test loss-0.6343, acc-0.8354\n",
      "Iter-27760, train loss-0.7242, acc-0.8400, valid loss-0.6312, acc-0.8364, test loss-0.6341, acc-0.8354\n",
      "Iter-27770, train loss-0.5318, acc-0.8600, valid loss-0.6311, acc-0.8362, test loss-0.6340, acc-0.8356\n",
      "Iter-27780, train loss-0.9176, acc-0.7200, valid loss-0.6309, acc-0.8366, test loss-0.6339, acc-0.8357\n",
      "Iter-27790, train loss-0.6281, acc-0.8000, valid loss-0.6308, acc-0.8366, test loss-0.6337, acc-0.8358\n",
      "Iter-27800, train loss-0.5728, acc-0.8600, valid loss-0.6306, acc-0.8368, test loss-0.6336, acc-0.8357\n",
      "Iter-27810, train loss-0.6467, acc-0.8000, valid loss-0.6305, acc-0.8368, test loss-0.6334, acc-0.8358\n",
      "Iter-27820, train loss-0.6452, acc-0.8000, valid loss-0.6303, acc-0.8368, test loss-0.6333, acc-0.8360\n",
      "Iter-27830, train loss-0.6157, acc-0.8800, valid loss-0.6302, acc-0.8366, test loss-0.6332, acc-0.8360\n",
      "Iter-27840, train loss-0.7873, acc-0.7800, valid loss-0.6301, acc-0.8368, test loss-0.6331, acc-0.8360\n",
      "Iter-27850, train loss-0.6659, acc-0.8400, valid loss-0.6299, acc-0.8370, test loss-0.6329, acc-0.8359\n",
      "Iter-27860, train loss-0.5386, acc-0.8800, valid loss-0.6298, acc-0.8368, test loss-0.6328, acc-0.8359\n",
      "Iter-27870, train loss-0.6188, acc-0.8600, valid loss-0.6297, acc-0.8368, test loss-0.6327, acc-0.8359\n",
      "Iter-27880, train loss-0.7212, acc-0.8400, valid loss-0.6295, acc-0.8370, test loss-0.6325, acc-0.8358\n",
      "Iter-27890, train loss-0.7635, acc-0.7600, valid loss-0.6294, acc-0.8370, test loss-0.6324, acc-0.8358\n",
      "Iter-27900, train loss-0.8497, acc-0.7600, valid loss-0.6293, acc-0.8372, test loss-0.6323, acc-0.8358\n",
      "Iter-27910, train loss-0.5791, acc-0.8600, valid loss-0.6291, acc-0.8372, test loss-0.6321, acc-0.8358\n",
      "Iter-27920, train loss-0.5779, acc-0.8600, valid loss-0.6290, acc-0.8372, test loss-0.6320, acc-0.8360\n",
      "Iter-27930, train loss-0.7526, acc-0.8000, valid loss-0.6289, acc-0.8372, test loss-0.6319, acc-0.8360\n",
      "Iter-27940, train loss-0.6656, acc-0.8400, valid loss-0.6287, acc-0.8372, test loss-0.6317, acc-0.8361\n",
      "Iter-27950, train loss-0.7522, acc-0.7800, valid loss-0.6286, acc-0.8370, test loss-0.6316, acc-0.8360\n",
      "Iter-27960, train loss-0.6186, acc-0.8600, valid loss-0.6285, acc-0.8372, test loss-0.6315, acc-0.8359\n",
      "Iter-27970, train loss-0.6674, acc-0.7800, valid loss-0.6283, acc-0.8374, test loss-0.6313, acc-0.8361\n",
      "Iter-27980, train loss-0.6380, acc-0.8000, valid loss-0.6282, acc-0.8368, test loss-0.6312, acc-0.8361\n",
      "Iter-27990, train loss-0.7373, acc-0.8600, valid loss-0.6281, acc-0.8374, test loss-0.6311, acc-0.8361\n",
      "Iter-28000, train loss-0.9604, acc-0.8000, valid loss-0.6279, acc-0.8374, test loss-0.6309, acc-0.8361\n",
      "Iter-28010, train loss-0.5410, acc-0.8600, valid loss-0.6278, acc-0.8372, test loss-0.6308, acc-0.8361\n",
      "Iter-28020, train loss-0.6917, acc-0.7800, valid loss-0.6277, acc-0.8372, test loss-0.6306, acc-0.8359\n",
      "Iter-28030, train loss-0.7277, acc-0.8200, valid loss-0.6275, acc-0.8376, test loss-0.6305, acc-0.8360\n",
      "Iter-28040, train loss-0.6348, acc-0.8200, valid loss-0.6274, acc-0.8374, test loss-0.6304, acc-0.8360\n",
      "Iter-28050, train loss-0.5730, acc-0.9200, valid loss-0.6273, acc-0.8378, test loss-0.6303, acc-0.8360\n",
      "Iter-28060, train loss-0.6504, acc-0.8200, valid loss-0.6271, acc-0.8376, test loss-0.6302, acc-0.8360\n",
      "Iter-28070, train loss-0.6463, acc-0.8400, valid loss-0.6270, acc-0.8378, test loss-0.6300, acc-0.8360\n",
      "Iter-28080, train loss-0.7889, acc-0.7600, valid loss-0.6269, acc-0.8376, test loss-0.6299, acc-0.8359\n",
      "Iter-28090, train loss-0.6655, acc-0.8000, valid loss-0.6267, acc-0.8378, test loss-0.6298, acc-0.8359\n",
      "Iter-28100, train loss-0.6223, acc-0.8400, valid loss-0.6266, acc-0.8380, test loss-0.6296, acc-0.8359\n",
      "Iter-28110, train loss-0.6190, acc-0.8400, valid loss-0.6265, acc-0.8376, test loss-0.6295, acc-0.8359\n",
      "Iter-28120, train loss-0.6666, acc-0.8800, valid loss-0.6263, acc-0.8378, test loss-0.6293, acc-0.8358\n",
      "Iter-28130, train loss-0.7713, acc-0.7400, valid loss-0.6262, acc-0.8376, test loss-0.6292, acc-0.8358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-28140, train loss-0.4796, acc-0.9000, valid loss-0.6261, acc-0.8376, test loss-0.6291, acc-0.8356\n",
      "Iter-28150, train loss-0.6213, acc-0.8400, valid loss-0.6259, acc-0.8376, test loss-0.6289, acc-0.8357\n",
      "Iter-28160, train loss-0.6534, acc-0.8600, valid loss-0.6258, acc-0.8384, test loss-0.6288, acc-0.8356\n",
      "Iter-28170, train loss-0.6505, acc-0.8200, valid loss-0.6257, acc-0.8376, test loss-0.6287, acc-0.8356\n",
      "Iter-28180, train loss-0.6059, acc-0.7800, valid loss-0.6255, acc-0.8374, test loss-0.6285, acc-0.8357\n",
      "Iter-28190, train loss-0.5240, acc-0.8400, valid loss-0.6254, acc-0.8376, test loss-0.6284, acc-0.8358\n",
      "Iter-28200, train loss-0.6341, acc-0.8600, valid loss-0.6252, acc-0.8374, test loss-0.6282, acc-0.8359\n",
      "Iter-28210, train loss-0.5884, acc-0.8600, valid loss-0.6251, acc-0.8378, test loss-0.6281, acc-0.8358\n",
      "Iter-28220, train loss-0.5201, acc-0.8400, valid loss-0.6250, acc-0.8378, test loss-0.6280, acc-0.8357\n",
      "Iter-28230, train loss-0.5185, acc-0.8600, valid loss-0.6249, acc-0.8378, test loss-0.6279, acc-0.8357\n",
      "Iter-28240, train loss-0.5123, acc-0.9200, valid loss-0.6247, acc-0.8378, test loss-0.6277, acc-0.8358\n",
      "Iter-28250, train loss-0.5193, acc-0.8600, valid loss-0.6246, acc-0.8378, test loss-0.6276, acc-0.8358\n",
      "Iter-28260, train loss-0.6753, acc-0.7800, valid loss-0.6245, acc-0.8378, test loss-0.6274, acc-0.8357\n",
      "Iter-28270, train loss-0.8036, acc-0.7600, valid loss-0.6243, acc-0.8378, test loss-0.6273, acc-0.8358\n",
      "Iter-28280, train loss-0.6834, acc-0.8600, valid loss-0.6242, acc-0.8378, test loss-0.6272, acc-0.8356\n",
      "Iter-28290, train loss-0.6526, acc-0.8400, valid loss-0.6240, acc-0.8380, test loss-0.6270, acc-0.8356\n",
      "Iter-28300, train loss-0.6851, acc-0.8400, valid loss-0.6239, acc-0.8382, test loss-0.6269, acc-0.8357\n",
      "Iter-28310, train loss-0.6749, acc-0.8400, valid loss-0.6238, acc-0.8384, test loss-0.6268, acc-0.8356\n",
      "Iter-28320, train loss-0.7278, acc-0.8200, valid loss-0.6237, acc-0.8386, test loss-0.6266, acc-0.8356\n",
      "Iter-28330, train loss-0.6232, acc-0.8200, valid loss-0.6235, acc-0.8386, test loss-0.6265, acc-0.8357\n",
      "Iter-28340, train loss-0.7404, acc-0.7600, valid loss-0.6234, acc-0.8386, test loss-0.6264, acc-0.8358\n",
      "Iter-28350, train loss-0.5425, acc-0.9200, valid loss-0.6232, acc-0.8386, test loss-0.6262, acc-0.8358\n",
      "Iter-28360, train loss-0.6610, acc-0.8400, valid loss-0.6231, acc-0.8386, test loss-0.6261, acc-0.8358\n",
      "Iter-28370, train loss-0.7519, acc-0.7800, valid loss-0.6230, acc-0.8386, test loss-0.6260, acc-0.8361\n",
      "Iter-28380, train loss-0.4913, acc-0.9200, valid loss-0.6228, acc-0.8390, test loss-0.6259, acc-0.8359\n",
      "Iter-28390, train loss-0.7009, acc-0.8000, valid loss-0.6227, acc-0.8388, test loss-0.6257, acc-0.8359\n",
      "Iter-28400, train loss-0.7492, acc-0.9000, valid loss-0.6226, acc-0.8388, test loss-0.6256, acc-0.8358\n",
      "Iter-28410, train loss-0.5174, acc-0.8600, valid loss-0.6225, acc-0.8390, test loss-0.6255, acc-0.8359\n",
      "Iter-28420, train loss-0.6784, acc-0.7800, valid loss-0.6223, acc-0.8388, test loss-0.6253, acc-0.8361\n",
      "Iter-28430, train loss-0.6668, acc-0.8200, valid loss-0.6222, acc-0.8388, test loss-0.6252, acc-0.8363\n",
      "Iter-28440, train loss-0.5445, acc-0.8200, valid loss-0.6220, acc-0.8388, test loss-0.6251, acc-0.8363\n",
      "Iter-28450, train loss-0.5895, acc-0.8400, valid loss-0.6219, acc-0.8386, test loss-0.6249, acc-0.8362\n",
      "Iter-28460, train loss-0.5317, acc-0.8400, valid loss-0.6218, acc-0.8386, test loss-0.6248, acc-0.8362\n",
      "Iter-28470, train loss-0.8137, acc-0.7400, valid loss-0.6217, acc-0.8386, test loss-0.6247, acc-0.8364\n",
      "Iter-28480, train loss-0.8268, acc-0.7800, valid loss-0.6215, acc-0.8388, test loss-0.6245, acc-0.8365\n",
      "Iter-28490, train loss-0.5625, acc-0.8400, valid loss-0.6214, acc-0.8388, test loss-0.6244, acc-0.8364\n",
      "Iter-28500, train loss-0.6150, acc-0.8600, valid loss-0.6213, acc-0.8390, test loss-0.6243, acc-0.8365\n",
      "Iter-28510, train loss-0.6029, acc-0.8400, valid loss-0.6211, acc-0.8386, test loss-0.6241, acc-0.8364\n",
      "Iter-28520, train loss-0.5782, acc-0.9000, valid loss-0.6210, acc-0.8388, test loss-0.6240, acc-0.8365\n",
      "Iter-28530, train loss-0.6911, acc-0.8400, valid loss-0.6208, acc-0.8388, test loss-0.6238, acc-0.8366\n",
      "Iter-28540, train loss-0.6079, acc-0.8600, valid loss-0.6207, acc-0.8390, test loss-0.6237, acc-0.8364\n",
      "Iter-28550, train loss-0.5362, acc-0.8800, valid loss-0.6206, acc-0.8390, test loss-0.6236, acc-0.8366\n",
      "Iter-28560, train loss-0.6487, acc-0.8800, valid loss-0.6204, acc-0.8388, test loss-0.6235, acc-0.8366\n",
      "Iter-28570, train loss-0.5011, acc-0.9000, valid loss-0.6203, acc-0.8390, test loss-0.6233, acc-0.8365\n",
      "Iter-28580, train loss-0.7500, acc-0.7800, valid loss-0.6202, acc-0.8392, test loss-0.6232, acc-0.8364\n",
      "Iter-28590, train loss-0.7266, acc-0.8400, valid loss-0.6200, acc-0.8392, test loss-0.6231, acc-0.8365\n",
      "Iter-28600, train loss-0.5908, acc-0.8400, valid loss-0.6199, acc-0.8394, test loss-0.6229, acc-0.8365\n",
      "Iter-28610, train loss-0.7801, acc-0.7000, valid loss-0.6198, acc-0.8392, test loss-0.6228, acc-0.8365\n",
      "Iter-28620, train loss-0.5377, acc-0.8200, valid loss-0.6196, acc-0.8392, test loss-0.6227, acc-0.8365\n",
      "Iter-28630, train loss-0.7892, acc-0.6800, valid loss-0.6195, acc-0.8392, test loss-0.6225, acc-0.8365\n",
      "Iter-28640, train loss-0.6254, acc-0.8200, valid loss-0.6194, acc-0.8392, test loss-0.6224, acc-0.8365\n",
      "Iter-28650, train loss-0.4466, acc-0.9000, valid loss-0.6193, acc-0.8392, test loss-0.6223, acc-0.8365\n",
      "Iter-28660, train loss-0.5206, acc-0.8800, valid loss-0.6191, acc-0.8392, test loss-0.6222, acc-0.8366\n",
      "Iter-28670, train loss-0.5566, acc-0.8800, valid loss-0.6190, acc-0.8392, test loss-0.6220, acc-0.8365\n",
      "Iter-28680, train loss-0.5477, acc-0.9000, valid loss-0.6189, acc-0.8392, test loss-0.6219, acc-0.8367\n",
      "Iter-28690, train loss-0.5370, acc-0.8600, valid loss-0.6188, acc-0.8392, test loss-0.6218, acc-0.8367\n",
      "Iter-28700, train loss-0.6020, acc-0.8400, valid loss-0.6187, acc-0.8392, test loss-0.6217, acc-0.8367\n",
      "Iter-28710, train loss-0.5087, acc-0.8600, valid loss-0.6185, acc-0.8392, test loss-0.6215, acc-0.8367\n",
      "Iter-28720, train loss-0.6730, acc-0.8600, valid loss-0.6184, acc-0.8390, test loss-0.6214, acc-0.8369\n",
      "Iter-28730, train loss-0.6434, acc-0.8200, valid loss-0.6183, acc-0.8390, test loss-0.6213, acc-0.8370\n",
      "Iter-28740, train loss-0.5327, acc-0.8800, valid loss-0.6181, acc-0.8390, test loss-0.6212, acc-0.8373\n",
      "Iter-28750, train loss-0.5078, acc-0.8800, valid loss-0.6180, acc-0.8390, test loss-0.6210, acc-0.8370\n",
      "Iter-28760, train loss-0.4284, acc-0.9400, valid loss-0.6179, acc-0.8392, test loss-0.6209, acc-0.8371\n",
      "Iter-28770, train loss-0.4495, acc-0.9000, valid loss-0.6177, acc-0.8392, test loss-0.6208, acc-0.8371\n",
      "Iter-28780, train loss-0.6731, acc-0.7800, valid loss-0.6176, acc-0.8396, test loss-0.6206, acc-0.8371\n",
      "Iter-28790, train loss-0.6404, acc-0.8000, valid loss-0.6175, acc-0.8392, test loss-0.6205, acc-0.8373\n",
      "Iter-28800, train loss-0.5667, acc-0.8800, valid loss-0.6174, acc-0.8394, test loss-0.6204, acc-0.8373\n",
      "Iter-28810, train loss-0.8094, acc-0.8000, valid loss-0.6173, acc-0.8392, test loss-0.6203, acc-0.8373\n",
      "Iter-28820, train loss-0.4004, acc-0.9600, valid loss-0.6172, acc-0.8392, test loss-0.6202, acc-0.8374\n",
      "Iter-28830, train loss-0.5605, acc-0.8800, valid loss-0.6170, acc-0.8392, test loss-0.6200, acc-0.8373\n",
      "Iter-28840, train loss-0.5725, acc-0.8600, valid loss-0.6169, acc-0.8394, test loss-0.6199, acc-0.8373\n",
      "Iter-28850, train loss-0.4984, acc-0.8800, valid loss-0.6168, acc-0.8394, test loss-0.6198, acc-0.8375\n",
      "Iter-28860, train loss-0.6516, acc-0.8400, valid loss-0.6167, acc-0.8392, test loss-0.6197, acc-0.8376\n",
      "Iter-28870, train loss-0.5894, acc-0.8400, valid loss-0.6166, acc-0.8396, test loss-0.6196, acc-0.8375\n",
      "Iter-28880, train loss-0.6283, acc-0.8400, valid loss-0.6164, acc-0.8396, test loss-0.6195, acc-0.8377\n",
      "Iter-28890, train loss-0.6116, acc-0.8200, valid loss-0.6163, acc-0.8394, test loss-0.6193, acc-0.8377\n",
      "Iter-28900, train loss-0.4044, acc-0.9400, valid loss-0.6162, acc-0.8396, test loss-0.6192, acc-0.8376\n",
      "Iter-28910, train loss-0.5772, acc-0.8600, valid loss-0.6160, acc-0.8396, test loss-0.6191, acc-0.8377\n",
      "Iter-28920, train loss-0.5504, acc-0.8800, valid loss-0.6159, acc-0.8396, test loss-0.6190, acc-0.8379\n",
      "Iter-28930, train loss-0.5571, acc-0.8800, valid loss-0.6158, acc-0.8396, test loss-0.6189, acc-0.8378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-28940, train loss-0.5896, acc-0.7800, valid loss-0.6156, acc-0.8396, test loss-0.6187, acc-0.8379\n",
      "Iter-28950, train loss-0.5748, acc-0.8400, valid loss-0.6155, acc-0.8400, test loss-0.6186, acc-0.8379\n",
      "Iter-28960, train loss-0.5250, acc-0.8800, valid loss-0.6154, acc-0.8398, test loss-0.6185, acc-0.8379\n",
      "Iter-28970, train loss-0.5734, acc-0.8600, valid loss-0.6152, acc-0.8398, test loss-0.6183, acc-0.8379\n",
      "Iter-28980, train loss-0.7198, acc-0.8000, valid loss-0.6151, acc-0.8398, test loss-0.6182, acc-0.8380\n",
      "Iter-28990, train loss-0.6508, acc-0.8200, valid loss-0.6150, acc-0.8400, test loss-0.6181, acc-0.8380\n",
      "Iter-29000, train loss-0.6203, acc-0.8200, valid loss-0.6148, acc-0.8400, test loss-0.6179, acc-0.8380\n",
      "Iter-29010, train loss-0.6789, acc-0.8000, valid loss-0.6147, acc-0.8400, test loss-0.6178, acc-0.8380\n",
      "Iter-29020, train loss-0.9467, acc-0.7600, valid loss-0.6146, acc-0.8404, test loss-0.6177, acc-0.8380\n",
      "Iter-29030, train loss-0.5501, acc-0.8800, valid loss-0.6145, acc-0.8406, test loss-0.6176, acc-0.8380\n",
      "Iter-29040, train loss-0.6702, acc-0.8200, valid loss-0.6143, acc-0.8406, test loss-0.6174, acc-0.8380\n",
      "Iter-29050, train loss-0.6926, acc-0.8400, valid loss-0.6142, acc-0.8406, test loss-0.6173, acc-0.8381\n",
      "Iter-29060, train loss-0.7154, acc-0.8200, valid loss-0.6141, acc-0.8404, test loss-0.6172, acc-0.8382\n",
      "Iter-29070, train loss-0.5753, acc-0.8800, valid loss-0.6140, acc-0.8402, test loss-0.6171, acc-0.8381\n",
      "Iter-29080, train loss-0.6910, acc-0.8000, valid loss-0.6138, acc-0.8402, test loss-0.6169, acc-0.8382\n",
      "Iter-29090, train loss-0.7277, acc-0.8000, valid loss-0.6137, acc-0.8398, test loss-0.6168, acc-0.8382\n",
      "Iter-29100, train loss-0.5187, acc-0.8800, valid loss-0.6136, acc-0.8398, test loss-0.6167, acc-0.8384\n",
      "Iter-29110, train loss-0.4958, acc-0.8800, valid loss-0.6135, acc-0.8400, test loss-0.6166, acc-0.8383\n",
      "Iter-29120, train loss-0.6683, acc-0.7600, valid loss-0.6133, acc-0.8400, test loss-0.6164, acc-0.8384\n",
      "Iter-29130, train loss-0.6730, acc-0.8000, valid loss-0.6132, acc-0.8402, test loss-0.6163, acc-0.8383\n",
      "Iter-29140, train loss-0.8066, acc-0.8000, valid loss-0.6131, acc-0.8402, test loss-0.6162, acc-0.8382\n",
      "Iter-29150, train loss-0.5867, acc-0.8000, valid loss-0.6130, acc-0.8402, test loss-0.6161, acc-0.8383\n",
      "Iter-29160, train loss-0.6277, acc-0.8600, valid loss-0.6129, acc-0.8404, test loss-0.6160, acc-0.8383\n",
      "Iter-29170, train loss-0.5593, acc-0.9000, valid loss-0.6127, acc-0.8404, test loss-0.6158, acc-0.8382\n",
      "Iter-29180, train loss-0.4847, acc-0.8600, valid loss-0.6126, acc-0.8404, test loss-0.6157, acc-0.8382\n",
      "Iter-29190, train loss-0.7285, acc-0.8200, valid loss-0.6125, acc-0.8404, test loss-0.6156, acc-0.8384\n",
      "Iter-29200, train loss-0.4238, acc-0.8800, valid loss-0.6124, acc-0.8404, test loss-0.6155, acc-0.8384\n",
      "Iter-29210, train loss-0.6919, acc-0.7600, valid loss-0.6122, acc-0.8404, test loss-0.6153, acc-0.8384\n",
      "Iter-29220, train loss-0.7242, acc-0.8000, valid loss-0.6121, acc-0.8406, test loss-0.6152, acc-0.8385\n",
      "Iter-29230, train loss-0.5229, acc-0.9000, valid loss-0.6120, acc-0.8404, test loss-0.6151, acc-0.8386\n",
      "Iter-29240, train loss-0.7568, acc-0.8000, valid loss-0.6119, acc-0.8406, test loss-0.6150, acc-0.8387\n",
      "Iter-29250, train loss-0.7913, acc-0.8000, valid loss-0.6118, acc-0.8406, test loss-0.6148, acc-0.8385\n",
      "Iter-29260, train loss-0.8826, acc-0.8000, valid loss-0.6116, acc-0.8406, test loss-0.6147, acc-0.8385\n",
      "Iter-29270, train loss-0.3947, acc-0.9600, valid loss-0.6115, acc-0.8406, test loss-0.6146, acc-0.8385\n",
      "Iter-29280, train loss-0.7591, acc-0.7600, valid loss-0.6114, acc-0.8408, test loss-0.6145, acc-0.8387\n",
      "Iter-29290, train loss-0.6362, acc-0.8400, valid loss-0.6112, acc-0.8408, test loss-0.6143, acc-0.8386\n",
      "Iter-29300, train loss-0.6117, acc-0.8200, valid loss-0.6111, acc-0.8408, test loss-0.6142, acc-0.8387\n",
      "Iter-29310, train loss-0.6117, acc-0.7600, valid loss-0.6110, acc-0.8406, test loss-0.6141, acc-0.8388\n",
      "Iter-29320, train loss-0.7426, acc-0.8400, valid loss-0.6109, acc-0.8408, test loss-0.6140, acc-0.8389\n",
      "Iter-29330, train loss-0.4671, acc-0.8800, valid loss-0.6108, acc-0.8406, test loss-0.6138, acc-0.8389\n",
      "Iter-29340, train loss-0.7158, acc-0.8000, valid loss-0.6106, acc-0.8408, test loss-0.6137, acc-0.8389\n",
      "Iter-29350, train loss-0.5875, acc-0.8600, valid loss-0.6105, acc-0.8408, test loss-0.6136, acc-0.8389\n",
      "Iter-29360, train loss-0.8022, acc-0.8200, valid loss-0.6104, acc-0.8406, test loss-0.6135, acc-0.8389\n",
      "Iter-29370, train loss-0.6874, acc-0.7800, valid loss-0.6103, acc-0.8408, test loss-0.6134, acc-0.8389\n",
      "Iter-29380, train loss-0.5313, acc-0.8800, valid loss-0.6101, acc-0.8406, test loss-0.6132, acc-0.8390\n",
      "Iter-29390, train loss-0.6189, acc-0.8400, valid loss-0.6100, acc-0.8404, test loss-0.6131, acc-0.8389\n",
      "Iter-29400, train loss-0.4711, acc-0.8800, valid loss-0.6099, acc-0.8404, test loss-0.6130, acc-0.8389\n",
      "Iter-29410, train loss-0.6558, acc-0.8000, valid loss-0.6098, acc-0.8404, test loss-0.6128, acc-0.8389\n",
      "Iter-29420, train loss-0.6475, acc-0.8800, valid loss-0.6097, acc-0.8404, test loss-0.6127, acc-0.8389\n",
      "Iter-29430, train loss-0.5578, acc-0.8400, valid loss-0.6096, acc-0.8404, test loss-0.6126, acc-0.8389\n",
      "Iter-29440, train loss-0.6427, acc-0.8400, valid loss-0.6094, acc-0.8406, test loss-0.6125, acc-0.8389\n",
      "Iter-29450, train loss-0.6160, acc-0.8800, valid loss-0.6093, acc-0.8408, test loss-0.6124, acc-0.8389\n",
      "Iter-29460, train loss-0.5182, acc-0.9200, valid loss-0.6092, acc-0.8408, test loss-0.6123, acc-0.8389\n",
      "Iter-29470, train loss-0.4723, acc-0.9400, valid loss-0.6091, acc-0.8408, test loss-0.6121, acc-0.8389\n",
      "Iter-29480, train loss-0.6085, acc-0.8400, valid loss-0.6090, acc-0.8410, test loss-0.6120, acc-0.8388\n",
      "Iter-29490, train loss-0.5553, acc-0.8000, valid loss-0.6088, acc-0.8412, test loss-0.6119, acc-0.8388\n",
      "Iter-29500, train loss-0.6395, acc-0.8400, valid loss-0.6087, acc-0.8412, test loss-0.6118, acc-0.8388\n",
      "Iter-29510, train loss-0.4627, acc-0.9200, valid loss-0.6086, acc-0.8412, test loss-0.6117, acc-0.8388\n",
      "Iter-29520, train loss-0.5528, acc-0.9000, valid loss-0.6085, acc-0.8416, test loss-0.6115, acc-0.8390\n",
      "Iter-29530, train loss-0.5978, acc-0.8200, valid loss-0.6083, acc-0.8414, test loss-0.6114, acc-0.8390\n",
      "Iter-29540, train loss-0.7611, acc-0.7600, valid loss-0.6082, acc-0.8416, test loss-0.6113, acc-0.8391\n",
      "Iter-29550, train loss-0.6045, acc-0.8800, valid loss-0.6081, acc-0.8416, test loss-0.6112, acc-0.8392\n",
      "Iter-29560, train loss-0.6499, acc-0.8200, valid loss-0.6080, acc-0.8416, test loss-0.6110, acc-0.8390\n",
      "Iter-29570, train loss-0.5373, acc-0.8400, valid loss-0.6078, acc-0.8416, test loss-0.6109, acc-0.8392\n",
      "Iter-29580, train loss-0.6373, acc-0.8000, valid loss-0.6077, acc-0.8414, test loss-0.6108, acc-0.8392\n",
      "Iter-29590, train loss-0.6070, acc-0.8200, valid loss-0.6076, acc-0.8412, test loss-0.6106, acc-0.8392\n",
      "Iter-29600, train loss-0.6915, acc-0.7800, valid loss-0.6075, acc-0.8414, test loss-0.6105, acc-0.8392\n",
      "Iter-29610, train loss-0.6809, acc-0.7400, valid loss-0.6074, acc-0.8416, test loss-0.6104, acc-0.8394\n",
      "Iter-29620, train loss-0.4925, acc-0.9000, valid loss-0.6072, acc-0.8412, test loss-0.6103, acc-0.8393\n",
      "Iter-29630, train loss-0.7701, acc-0.8000, valid loss-0.6071, acc-0.8412, test loss-0.6101, acc-0.8394\n",
      "Iter-29640, train loss-0.5897, acc-0.8600, valid loss-0.6070, acc-0.8414, test loss-0.6100, acc-0.8393\n",
      "Iter-29650, train loss-0.6376, acc-0.7800, valid loss-0.6069, acc-0.8414, test loss-0.6099, acc-0.8395\n",
      "Iter-29660, train loss-0.7623, acc-0.7800, valid loss-0.6068, acc-0.8412, test loss-0.6098, acc-0.8395\n",
      "Iter-29670, train loss-0.5966, acc-0.8200, valid loss-0.6067, acc-0.8412, test loss-0.6097, acc-0.8394\n",
      "Iter-29680, train loss-0.5470, acc-0.8200, valid loss-0.6065, acc-0.8412, test loss-0.6096, acc-0.8396\n",
      "Iter-29690, train loss-0.7146, acc-0.7800, valid loss-0.6065, acc-0.8410, test loss-0.6094, acc-0.8397\n",
      "Iter-29700, train loss-0.4772, acc-0.9000, valid loss-0.6063, acc-0.8410, test loss-0.6093, acc-0.8397\n",
      "Iter-29710, train loss-0.5834, acc-0.8400, valid loss-0.6062, acc-0.8410, test loss-0.6092, acc-0.8397\n",
      "Iter-29720, train loss-0.6011, acc-0.8400, valid loss-0.6061, acc-0.8410, test loss-0.6090, acc-0.8396\n",
      "Iter-29730, train loss-0.5364, acc-0.8200, valid loss-0.6059, acc-0.8410, test loss-0.6089, acc-0.8397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-29740, train loss-0.6182, acc-0.8600, valid loss-0.6058, acc-0.8410, test loss-0.6088, acc-0.8397\n",
      "Iter-29750, train loss-0.5779, acc-0.8400, valid loss-0.6057, acc-0.8412, test loss-0.6087, acc-0.8397\n",
      "Iter-29760, train loss-0.6330, acc-0.8400, valid loss-0.6055, acc-0.8412, test loss-0.6086, acc-0.8397\n",
      "Iter-29770, train loss-0.7105, acc-0.7600, valid loss-0.6054, acc-0.8414, test loss-0.6084, acc-0.8398\n",
      "Iter-29780, train loss-0.6688, acc-0.8000, valid loss-0.6053, acc-0.8412, test loss-0.6083, acc-0.8399\n",
      "Iter-29790, train loss-0.6832, acc-0.8400, valid loss-0.6052, acc-0.8412, test loss-0.6082, acc-0.8399\n",
      "Iter-29800, train loss-0.8303, acc-0.7400, valid loss-0.6051, acc-0.8412, test loss-0.6081, acc-0.8399\n",
      "Iter-29810, train loss-0.6073, acc-0.7800, valid loss-0.6050, acc-0.8414, test loss-0.6080, acc-0.8398\n",
      "Iter-29820, train loss-0.5417, acc-0.7800, valid loss-0.6048, acc-0.8414, test loss-0.6079, acc-0.8399\n",
      "Iter-29830, train loss-0.4040, acc-0.9800, valid loss-0.6047, acc-0.8414, test loss-0.6078, acc-0.8400\n",
      "Iter-29840, train loss-0.6851, acc-0.8400, valid loss-0.6046, acc-0.8412, test loss-0.6076, acc-0.8400\n",
      "Iter-29850, train loss-0.5490, acc-0.8400, valid loss-0.6045, acc-0.8414, test loss-0.6075, acc-0.8401\n",
      "Iter-29860, train loss-0.5755, acc-0.8600, valid loss-0.6044, acc-0.8414, test loss-0.6074, acc-0.8401\n",
      "Iter-29870, train loss-0.5580, acc-0.8600, valid loss-0.6043, acc-0.8414, test loss-0.6073, acc-0.8400\n",
      "Iter-29880, train loss-0.6624, acc-0.8200, valid loss-0.6042, acc-0.8416, test loss-0.6072, acc-0.8400\n",
      "Iter-29890, train loss-0.6254, acc-0.8400, valid loss-0.6041, acc-0.8416, test loss-0.6071, acc-0.8400\n",
      "Iter-29900, train loss-0.5894, acc-0.8200, valid loss-0.6040, acc-0.8418, test loss-0.6070, acc-0.8400\n",
      "Iter-29910, train loss-0.5771, acc-0.8600, valid loss-0.6039, acc-0.8418, test loss-0.6069, acc-0.8401\n",
      "Iter-29920, train loss-0.6766, acc-0.8200, valid loss-0.6037, acc-0.8418, test loss-0.6068, acc-0.8402\n",
      "Iter-29930, train loss-0.5546, acc-0.8800, valid loss-0.6036, acc-0.8418, test loss-0.6066, acc-0.8402\n",
      "Iter-29940, train loss-0.7017, acc-0.8600, valid loss-0.6035, acc-0.8418, test loss-0.6065, acc-0.8401\n",
      "Iter-29950, train loss-0.6967, acc-0.7600, valid loss-0.6034, acc-0.8416, test loss-0.6064, acc-0.8401\n",
      "Iter-29960, train loss-0.6806, acc-0.7600, valid loss-0.6033, acc-0.8414, test loss-0.6062, acc-0.8402\n",
      "Iter-29970, train loss-0.7445, acc-0.7400, valid loss-0.6031, acc-0.8416, test loss-0.6061, acc-0.8401\n",
      "Iter-29980, train loss-0.5913, acc-0.8200, valid loss-0.6030, acc-0.8414, test loss-0.6060, acc-0.8401\n",
      "Iter-29990, train loss-0.3528, acc-0.9400, valid loss-0.6029, acc-0.8414, test loss-0.6059, acc-0.8400\n",
      "Iter-30000, train loss-0.5102, acc-0.8800, valid loss-0.6028, acc-0.8414, test loss-0.6058, acc-0.8402\n",
      "Iter-30010, train loss-0.7983, acc-0.7400, valid loss-0.6027, acc-0.8414, test loss-0.6056, acc-0.8401\n",
      "Iter-30020, train loss-0.6607, acc-0.8000, valid loss-0.6025, acc-0.8416, test loss-0.6055, acc-0.8402\n",
      "Iter-30030, train loss-0.5612, acc-0.8800, valid loss-0.6024, acc-0.8416, test loss-0.6054, acc-0.8403\n",
      "Iter-30040, train loss-0.7794, acc-0.7800, valid loss-0.6023, acc-0.8414, test loss-0.6053, acc-0.8403\n",
      "Iter-30050, train loss-0.7443, acc-0.8200, valid loss-0.6022, acc-0.8412, test loss-0.6052, acc-0.8402\n",
      "Iter-30060, train loss-0.5881, acc-0.8600, valid loss-0.6021, acc-0.8416, test loss-0.6050, acc-0.8402\n",
      "Iter-30070, train loss-0.6524, acc-0.8200, valid loss-0.6019, acc-0.8414, test loss-0.6049, acc-0.8401\n",
      "Iter-30080, train loss-0.5382, acc-0.7800, valid loss-0.6018, acc-0.8416, test loss-0.6048, acc-0.8402\n",
      "Iter-30090, train loss-0.7401, acc-0.8000, valid loss-0.6017, acc-0.8416, test loss-0.6047, acc-0.8403\n",
      "Iter-30100, train loss-0.5806, acc-0.8400, valid loss-0.6016, acc-0.8416, test loss-0.6045, acc-0.8403\n",
      "Iter-30110, train loss-0.5007, acc-0.8600, valid loss-0.6015, acc-0.8416, test loss-0.6044, acc-0.8403\n",
      "Iter-30120, train loss-0.5973, acc-0.8600, valid loss-0.6013, acc-0.8420, test loss-0.6043, acc-0.8403\n",
      "Iter-30130, train loss-0.8304, acc-0.7200, valid loss-0.6012, acc-0.8420, test loss-0.6041, acc-0.8404\n",
      "Iter-30140, train loss-0.6891, acc-0.8400, valid loss-0.6011, acc-0.8420, test loss-0.6040, acc-0.8405\n",
      "Iter-30150, train loss-0.6578, acc-0.7800, valid loss-0.6010, acc-0.8420, test loss-0.6039, acc-0.8404\n",
      "Iter-30160, train loss-0.7022, acc-0.7800, valid loss-0.6009, acc-0.8420, test loss-0.6038, acc-0.8405\n",
      "Iter-30170, train loss-0.5681, acc-0.8600, valid loss-0.6007, acc-0.8420, test loss-0.6037, acc-0.8405\n",
      "Iter-30180, train loss-0.6532, acc-0.8000, valid loss-0.6006, acc-0.8418, test loss-0.6035, acc-0.8405\n",
      "Iter-30190, train loss-0.5389, acc-0.8800, valid loss-0.6005, acc-0.8422, test loss-0.6034, acc-0.8405\n",
      "Iter-30200, train loss-0.7182, acc-0.8600, valid loss-0.6004, acc-0.8422, test loss-0.6033, acc-0.8405\n",
      "Iter-30210, train loss-0.5987, acc-0.8400, valid loss-0.6003, acc-0.8422, test loss-0.6032, acc-0.8405\n",
      "Iter-30220, train loss-0.5674, acc-0.8000, valid loss-0.6001, acc-0.8420, test loss-0.6031, acc-0.8405\n",
      "Iter-30230, train loss-0.6852, acc-0.8000, valid loss-0.6000, acc-0.8420, test loss-0.6030, acc-0.8406\n",
      "Iter-30240, train loss-0.7472, acc-0.8200, valid loss-0.5999, acc-0.8422, test loss-0.6028, acc-0.8405\n",
      "Iter-30250, train loss-0.6057, acc-0.8400, valid loss-0.5998, acc-0.8424, test loss-0.6028, acc-0.8406\n",
      "Iter-30260, train loss-0.5545, acc-0.8200, valid loss-0.5997, acc-0.8426, test loss-0.6026, acc-0.8406\n",
      "Iter-30270, train loss-0.5708, acc-0.8400, valid loss-0.5996, acc-0.8426, test loss-0.6025, acc-0.8406\n",
      "Iter-30280, train loss-0.5096, acc-0.8600, valid loss-0.5995, acc-0.8428, test loss-0.6024, acc-0.8407\n",
      "Iter-30290, train loss-0.6677, acc-0.8200, valid loss-0.5994, acc-0.8428, test loss-0.6023, acc-0.8406\n",
      "Iter-30300, train loss-0.5083, acc-0.9000, valid loss-0.5992, acc-0.8426, test loss-0.6022, acc-0.8409\n",
      "Iter-30310, train loss-0.9423, acc-0.7000, valid loss-0.5991, acc-0.8424, test loss-0.6020, acc-0.8407\n",
      "Iter-30320, train loss-0.8170, acc-0.7600, valid loss-0.5990, acc-0.8424, test loss-0.6019, acc-0.8409\n",
      "Iter-30330, train loss-0.5598, acc-0.8800, valid loss-0.5989, acc-0.8424, test loss-0.6018, acc-0.8409\n",
      "Iter-30340, train loss-0.7477, acc-0.7800, valid loss-0.5988, acc-0.8426, test loss-0.6017, acc-0.8409\n",
      "Iter-30350, train loss-0.7378, acc-0.7600, valid loss-0.5987, acc-0.8426, test loss-0.6016, acc-0.8409\n",
      "Iter-30360, train loss-0.5445, acc-0.9000, valid loss-0.5985, acc-0.8428, test loss-0.6015, acc-0.8409\n",
      "Iter-30370, train loss-0.5529, acc-0.8400, valid loss-0.5984, acc-0.8426, test loss-0.6013, acc-0.8407\n",
      "Iter-30380, train loss-0.6161, acc-0.8400, valid loss-0.5983, acc-0.8430, test loss-0.6012, acc-0.8408\n",
      "Iter-30390, train loss-0.5701, acc-0.8800, valid loss-0.5982, acc-0.8428, test loss-0.6011, acc-0.8407\n",
      "Iter-30400, train loss-0.7095, acc-0.7000, valid loss-0.5981, acc-0.8428, test loss-0.6010, acc-0.8406\n",
      "Iter-30410, train loss-0.6522, acc-0.8000, valid loss-0.5979, acc-0.8430, test loss-0.6009, acc-0.8409\n",
      "Iter-30420, train loss-0.7340, acc-0.8600, valid loss-0.5978, acc-0.8432, test loss-0.6008, acc-0.8408\n",
      "Iter-30430, train loss-0.5278, acc-0.9400, valid loss-0.5977, acc-0.8430, test loss-0.6006, acc-0.8409\n",
      "Iter-30440, train loss-0.5891, acc-0.8600, valid loss-0.5976, acc-0.8432, test loss-0.6005, acc-0.8407\n",
      "Iter-30450, train loss-0.6403, acc-0.8400, valid loss-0.5975, acc-0.8432, test loss-0.6004, acc-0.8408\n",
      "Iter-30460, train loss-0.7136, acc-0.8200, valid loss-0.5973, acc-0.8432, test loss-0.6003, acc-0.8408\n",
      "Iter-30470, train loss-0.8654, acc-0.7800, valid loss-0.5972, acc-0.8432, test loss-0.6001, acc-0.8408\n",
      "Iter-30480, train loss-0.6765, acc-0.7800, valid loss-0.5971, acc-0.8432, test loss-0.6000, acc-0.8408\n",
      "Iter-30490, train loss-0.9210, acc-0.7800, valid loss-0.5970, acc-0.8430, test loss-0.5999, acc-0.8407\n",
      "Iter-30500, train loss-0.4792, acc-0.9400, valid loss-0.5969, acc-0.8430, test loss-0.5998, acc-0.8410\n",
      "Iter-30510, train loss-0.6566, acc-0.8400, valid loss-0.5968, acc-0.8436, test loss-0.5997, acc-0.8410\n",
      "Iter-30520, train loss-0.7262, acc-0.7800, valid loss-0.5967, acc-0.8432, test loss-0.5996, acc-0.8413\n",
      "Iter-30530, train loss-0.5018, acc-0.8800, valid loss-0.5965, acc-0.8430, test loss-0.5994, acc-0.8413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-30540, train loss-0.5604, acc-0.8600, valid loss-0.5964, acc-0.8434, test loss-0.5994, acc-0.8414\n",
      "Iter-30550, train loss-0.5052, acc-0.8600, valid loss-0.5963, acc-0.8434, test loss-0.5992, acc-0.8417\n",
      "Iter-30560, train loss-0.6054, acc-0.8600, valid loss-0.5962, acc-0.8434, test loss-0.5991, acc-0.8415\n",
      "Iter-30570, train loss-0.8094, acc-0.8000, valid loss-0.5961, acc-0.8434, test loss-0.5990, acc-0.8416\n",
      "Iter-30580, train loss-0.6881, acc-0.7800, valid loss-0.5960, acc-0.8434, test loss-0.5989, acc-0.8418\n",
      "Iter-30590, train loss-0.3766, acc-0.9400, valid loss-0.5959, acc-0.8434, test loss-0.5988, acc-0.8418\n",
      "Iter-30600, train loss-0.5092, acc-0.8200, valid loss-0.5958, acc-0.8438, test loss-0.5987, acc-0.8418\n",
      "Iter-30610, train loss-0.6977, acc-0.7600, valid loss-0.5957, acc-0.8436, test loss-0.5986, acc-0.8421\n",
      "Iter-30620, train loss-0.5935, acc-0.8000, valid loss-0.5956, acc-0.8436, test loss-0.5985, acc-0.8419\n",
      "Iter-30630, train loss-0.4923, acc-0.9000, valid loss-0.5955, acc-0.8436, test loss-0.5984, acc-0.8421\n",
      "Iter-30640, train loss-0.5660, acc-0.8400, valid loss-0.5953, acc-0.8438, test loss-0.5983, acc-0.8421\n",
      "Iter-30650, train loss-0.6340, acc-0.8000, valid loss-0.5952, acc-0.8440, test loss-0.5981, acc-0.8420\n",
      "Iter-30660, train loss-0.6901, acc-0.8200, valid loss-0.5951, acc-0.8440, test loss-0.5980, acc-0.8421\n",
      "Iter-30670, train loss-0.7850, acc-0.7800, valid loss-0.5950, acc-0.8438, test loss-0.5979, acc-0.8421\n",
      "Iter-30680, train loss-0.5569, acc-0.9000, valid loss-0.5949, acc-0.8440, test loss-0.5978, acc-0.8420\n",
      "Iter-30690, train loss-0.7043, acc-0.7600, valid loss-0.5948, acc-0.8438, test loss-0.5977, acc-0.8420\n",
      "Iter-30700, train loss-0.7566, acc-0.7000, valid loss-0.5947, acc-0.8438, test loss-0.5976, acc-0.8421\n",
      "Iter-30710, train loss-0.7300, acc-0.7800, valid loss-0.5946, acc-0.8442, test loss-0.5975, acc-0.8422\n",
      "Iter-30720, train loss-0.6300, acc-0.8800, valid loss-0.5944, acc-0.8440, test loss-0.5974, acc-0.8422\n",
      "Iter-30730, train loss-0.7889, acc-0.7000, valid loss-0.5943, acc-0.8438, test loss-0.5973, acc-0.8422\n",
      "Iter-30740, train loss-0.4131, acc-0.9000, valid loss-0.5942, acc-0.8438, test loss-0.5971, acc-0.8423\n",
      "Iter-30750, train loss-0.6839, acc-0.8200, valid loss-0.5941, acc-0.8442, test loss-0.5970, acc-0.8422\n",
      "Iter-30760, train loss-0.5904, acc-0.8800, valid loss-0.5939, acc-0.8440, test loss-0.5969, acc-0.8424\n",
      "Iter-30770, train loss-0.8002, acc-0.7600, valid loss-0.5938, acc-0.8438, test loss-0.5968, acc-0.8425\n",
      "Iter-30780, train loss-0.5841, acc-0.8200, valid loss-0.5937, acc-0.8442, test loss-0.5967, acc-0.8425\n",
      "Iter-30790, train loss-0.5291, acc-0.8800, valid loss-0.5936, acc-0.8442, test loss-0.5966, acc-0.8427\n",
      "Iter-30800, train loss-0.7469, acc-0.7400, valid loss-0.5935, acc-0.8444, test loss-0.5965, acc-0.8426\n",
      "Iter-30810, train loss-0.4903, acc-0.8800, valid loss-0.5934, acc-0.8446, test loss-0.5964, acc-0.8426\n",
      "Iter-30820, train loss-0.7499, acc-0.8600, valid loss-0.5933, acc-0.8444, test loss-0.5962, acc-0.8424\n",
      "Iter-30830, train loss-0.7171, acc-0.8600, valid loss-0.5932, acc-0.8444, test loss-0.5961, acc-0.8424\n",
      "Iter-30840, train loss-0.6614, acc-0.8400, valid loss-0.5931, acc-0.8444, test loss-0.5960, acc-0.8421\n",
      "Iter-30850, train loss-0.6068, acc-0.8600, valid loss-0.5929, acc-0.8444, test loss-0.5959, acc-0.8424\n",
      "Iter-30860, train loss-0.5824, acc-0.8400, valid loss-0.5928, acc-0.8444, test loss-0.5958, acc-0.8426\n",
      "Iter-30870, train loss-0.5527, acc-0.8600, valid loss-0.5927, acc-0.8446, test loss-0.5957, acc-0.8425\n",
      "Iter-30880, train loss-0.9596, acc-0.7400, valid loss-0.5926, acc-0.8446, test loss-0.5956, acc-0.8423\n",
      "Iter-30890, train loss-0.7545, acc-0.8000, valid loss-0.5925, acc-0.8446, test loss-0.5955, acc-0.8424\n",
      "Iter-30900, train loss-0.5091, acc-0.9200, valid loss-0.5924, acc-0.8446, test loss-0.5954, acc-0.8425\n",
      "Iter-30910, train loss-0.5075, acc-0.8600, valid loss-0.5923, acc-0.8448, test loss-0.5952, acc-0.8426\n",
      "Iter-30920, train loss-0.6756, acc-0.8000, valid loss-0.5922, acc-0.8448, test loss-0.5951, acc-0.8425\n",
      "Iter-30930, train loss-0.6899, acc-0.8200, valid loss-0.5921, acc-0.8448, test loss-0.5950, acc-0.8425\n",
      "Iter-30940, train loss-0.5125, acc-0.8800, valid loss-0.5920, acc-0.8448, test loss-0.5949, acc-0.8426\n",
      "Iter-30950, train loss-0.5866, acc-0.8600, valid loss-0.5919, acc-0.8450, test loss-0.5948, acc-0.8425\n",
      "Iter-30960, train loss-0.4008, acc-0.9000, valid loss-0.5918, acc-0.8448, test loss-0.5947, acc-0.8425\n",
      "Iter-30970, train loss-0.5357, acc-0.8800, valid loss-0.5917, acc-0.8448, test loss-0.5946, acc-0.8426\n",
      "Iter-30980, train loss-0.6661, acc-0.7800, valid loss-0.5916, acc-0.8446, test loss-0.5945, acc-0.8426\n",
      "Iter-30990, train loss-0.6497, acc-0.8000, valid loss-0.5915, acc-0.8444, test loss-0.5944, acc-0.8426\n",
      "Iter-31000, train loss-0.5946, acc-0.9000, valid loss-0.5914, acc-0.8448, test loss-0.5943, acc-0.8427\n",
      "Iter-31010, train loss-0.7554, acc-0.7800, valid loss-0.5912, acc-0.8450, test loss-0.5941, acc-0.8427\n",
      "Iter-31020, train loss-0.9663, acc-0.7400, valid loss-0.5911, acc-0.8448, test loss-0.5940, acc-0.8427\n",
      "Iter-31030, train loss-0.6270, acc-0.8200, valid loss-0.5910, acc-0.8446, test loss-0.5939, acc-0.8427\n",
      "Iter-31040, train loss-0.6296, acc-0.8000, valid loss-0.5909, acc-0.8446, test loss-0.5938, acc-0.8428\n",
      "Iter-31050, train loss-0.7742, acc-0.7000, valid loss-0.5908, acc-0.8446, test loss-0.5937, acc-0.8429\n",
      "Iter-31060, train loss-0.6339, acc-0.8200, valid loss-0.5907, acc-0.8450, test loss-0.5936, acc-0.8430\n",
      "Iter-31070, train loss-0.9084, acc-0.6800, valid loss-0.5906, acc-0.8450, test loss-0.5935, acc-0.8430\n",
      "Iter-31080, train loss-0.6050, acc-0.8200, valid loss-0.5905, acc-0.8450, test loss-0.5934, acc-0.8430\n",
      "Iter-31090, train loss-0.5721, acc-0.9000, valid loss-0.5903, acc-0.8452, test loss-0.5933, acc-0.8430\n",
      "Iter-31100, train loss-0.5550, acc-0.9000, valid loss-0.5902, acc-0.8450, test loss-0.5932, acc-0.8430\n",
      "Iter-31110, train loss-0.5792, acc-0.8600, valid loss-0.5901, acc-0.8450, test loss-0.5931, acc-0.8431\n",
      "Iter-31120, train loss-0.4999, acc-0.8400, valid loss-0.5900, acc-0.8450, test loss-0.5930, acc-0.8431\n",
      "Iter-31130, train loss-0.6351, acc-0.8800, valid loss-0.5899, acc-0.8452, test loss-0.5929, acc-0.8432\n",
      "Iter-31140, train loss-0.7266, acc-0.8000, valid loss-0.5898, acc-0.8452, test loss-0.5928, acc-0.8431\n",
      "Iter-31150, train loss-0.8233, acc-0.8000, valid loss-0.5897, acc-0.8452, test loss-0.5926, acc-0.8432\n",
      "Iter-31160, train loss-0.7692, acc-0.7600, valid loss-0.5896, acc-0.8452, test loss-0.5925, acc-0.8431\n",
      "Iter-31170, train loss-0.6333, acc-0.8400, valid loss-0.5895, acc-0.8454, test loss-0.5924, acc-0.8432\n",
      "Iter-31180, train loss-0.4952, acc-0.9200, valid loss-0.5894, acc-0.8454, test loss-0.5923, acc-0.8432\n",
      "Iter-31190, train loss-0.6942, acc-0.8200, valid loss-0.5893, acc-0.8454, test loss-0.5922, acc-0.8433\n",
      "Iter-31200, train loss-0.5387, acc-0.9200, valid loss-0.5892, acc-0.8454, test loss-0.5921, acc-0.8433\n",
      "Iter-31210, train loss-0.5714, acc-0.8200, valid loss-0.5891, acc-0.8452, test loss-0.5920, acc-0.8434\n",
      "Iter-31220, train loss-0.4790, acc-0.9000, valid loss-0.5889, acc-0.8452, test loss-0.5919, acc-0.8434\n",
      "Iter-31230, train loss-0.7566, acc-0.7800, valid loss-0.5888, acc-0.8452, test loss-0.5918, acc-0.8434\n",
      "Iter-31240, train loss-0.5100, acc-0.8200, valid loss-0.5887, acc-0.8454, test loss-0.5917, acc-0.8434\n",
      "Iter-31250, train loss-0.5170, acc-0.8800, valid loss-0.5886, acc-0.8454, test loss-0.5916, acc-0.8435\n",
      "Iter-31260, train loss-0.7257, acc-0.7600, valid loss-0.5885, acc-0.8458, test loss-0.5915, acc-0.8436\n",
      "Iter-31270, train loss-0.6817, acc-0.7800, valid loss-0.5884, acc-0.8456, test loss-0.5913, acc-0.8435\n",
      "Iter-31280, train loss-0.7014, acc-0.8000, valid loss-0.5883, acc-0.8456, test loss-0.5912, acc-0.8434\n",
      "Iter-31290, train loss-0.5009, acc-0.8600, valid loss-0.5882, acc-0.8456, test loss-0.5911, acc-0.8435\n",
      "Iter-31300, train loss-0.4314, acc-0.8800, valid loss-0.5881, acc-0.8454, test loss-0.5910, acc-0.8436\n",
      "Iter-31310, train loss-0.5266, acc-0.9000, valid loss-0.5880, acc-0.8456, test loss-0.5909, acc-0.8434\n",
      "Iter-31320, train loss-0.6142, acc-0.8400, valid loss-0.5878, acc-0.8456, test loss-0.5908, acc-0.8432\n",
      "Iter-31330, train loss-0.7293, acc-0.8400, valid loss-0.5877, acc-0.8458, test loss-0.5907, acc-0.8434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-31340, train loss-0.7048, acc-0.8000, valid loss-0.5876, acc-0.8458, test loss-0.5905, acc-0.8434\n",
      "Iter-31350, train loss-0.9613, acc-0.6800, valid loss-0.5875, acc-0.8456, test loss-0.5904, acc-0.8434\n",
      "Iter-31360, train loss-0.6612, acc-0.8200, valid loss-0.5874, acc-0.8456, test loss-0.5903, acc-0.8434\n",
      "Iter-31370, train loss-0.6243, acc-0.8600, valid loss-0.5873, acc-0.8458, test loss-0.5902, acc-0.8434\n",
      "Iter-31380, train loss-0.4638, acc-0.9400, valid loss-0.5872, acc-0.8458, test loss-0.5901, acc-0.8436\n",
      "Iter-31390, train loss-0.6560, acc-0.9000, valid loss-0.5870, acc-0.8456, test loss-0.5900, acc-0.8437\n",
      "Iter-31400, train loss-0.4056, acc-0.9200, valid loss-0.5869, acc-0.8456, test loss-0.5899, acc-0.8437\n",
      "Iter-31410, train loss-0.6755, acc-0.7400, valid loss-0.5868, acc-0.8458, test loss-0.5898, acc-0.8438\n",
      "Iter-31420, train loss-0.5505, acc-0.8400, valid loss-0.5867, acc-0.8458, test loss-0.5896, acc-0.8438\n",
      "Iter-31430, train loss-0.5202, acc-0.8800, valid loss-0.5866, acc-0.8460, test loss-0.5895, acc-0.8439\n",
      "Iter-31440, train loss-0.4679, acc-0.9200, valid loss-0.5865, acc-0.8460, test loss-0.5894, acc-0.8438\n",
      "Iter-31450, train loss-0.5273, acc-0.8600, valid loss-0.5864, acc-0.8460, test loss-0.5893, acc-0.8438\n",
      "Iter-31460, train loss-0.7243, acc-0.7800, valid loss-0.5863, acc-0.8460, test loss-0.5892, acc-0.8438\n",
      "Iter-31470, train loss-0.7487, acc-0.8000, valid loss-0.5862, acc-0.8460, test loss-0.5891, acc-0.8438\n",
      "Iter-31480, train loss-0.5216, acc-0.8400, valid loss-0.5861, acc-0.8460, test loss-0.5890, acc-0.8437\n",
      "Iter-31490, train loss-0.5595, acc-0.8200, valid loss-0.5860, acc-0.8460, test loss-0.5889, acc-0.8437\n",
      "Iter-31500, train loss-0.5617, acc-0.8600, valid loss-0.5858, acc-0.8460, test loss-0.5888, acc-0.8437\n",
      "Iter-31510, train loss-0.6765, acc-0.8200, valid loss-0.5857, acc-0.8460, test loss-0.5887, acc-0.8437\n",
      "Iter-31520, train loss-0.5361, acc-0.8600, valid loss-0.5856, acc-0.8460, test loss-0.5886, acc-0.8437\n",
      "Iter-31530, train loss-0.5458, acc-0.9000, valid loss-0.5855, acc-0.8460, test loss-0.5885, acc-0.8437\n",
      "Iter-31540, train loss-0.7964, acc-0.8200, valid loss-0.5854, acc-0.8460, test loss-0.5884, acc-0.8439\n",
      "Iter-31550, train loss-0.5730, acc-0.8200, valid loss-0.5853, acc-0.8460, test loss-0.5883, acc-0.8439\n",
      "Iter-31560, train loss-0.6080, acc-0.8200, valid loss-0.5852, acc-0.8460, test loss-0.5882, acc-0.8442\n",
      "Iter-31570, train loss-0.4574, acc-0.8800, valid loss-0.5851, acc-0.8460, test loss-0.5880, acc-0.8440\n",
      "Iter-31580, train loss-0.4158, acc-0.9400, valid loss-0.5850, acc-0.8460, test loss-0.5879, acc-0.8439\n",
      "Iter-31590, train loss-0.5280, acc-0.8200, valid loss-0.5849, acc-0.8460, test loss-0.5878, acc-0.8439\n",
      "Iter-31600, train loss-0.6301, acc-0.8000, valid loss-0.5848, acc-0.8460, test loss-0.5877, acc-0.8441\n",
      "Iter-31610, train loss-0.5977, acc-0.8800, valid loss-0.5847, acc-0.8460, test loss-0.5876, acc-0.8443\n",
      "Iter-31620, train loss-0.5467, acc-0.8800, valid loss-0.5846, acc-0.8460, test loss-0.5875, acc-0.8443\n",
      "Iter-31630, train loss-0.7171, acc-0.8200, valid loss-0.5845, acc-0.8460, test loss-0.5874, acc-0.8443\n",
      "Iter-31640, train loss-0.5946, acc-0.9000, valid loss-0.5844, acc-0.8460, test loss-0.5873, acc-0.8441\n",
      "Iter-31650, train loss-0.4591, acc-0.9000, valid loss-0.5842, acc-0.8460, test loss-0.5872, acc-0.8441\n",
      "Iter-31660, train loss-0.6315, acc-0.8600, valid loss-0.5841, acc-0.8462, test loss-0.5871, acc-0.8443\n",
      "Iter-31670, train loss-0.6654, acc-0.8000, valid loss-0.5840, acc-0.8462, test loss-0.5870, acc-0.8445\n",
      "Iter-31680, train loss-0.7227, acc-0.8000, valid loss-0.5839, acc-0.8464, test loss-0.5869, acc-0.8444\n",
      "Iter-31690, train loss-0.6271, acc-0.8600, valid loss-0.5838, acc-0.8462, test loss-0.5868, acc-0.8443\n",
      "Iter-31700, train loss-0.7175, acc-0.7800, valid loss-0.5837, acc-0.8464, test loss-0.5867, acc-0.8442\n",
      "Iter-31710, train loss-0.6985, acc-0.8000, valid loss-0.5836, acc-0.8464, test loss-0.5866, acc-0.8444\n",
      "Iter-31720, train loss-0.5163, acc-0.8800, valid loss-0.5835, acc-0.8464, test loss-0.5865, acc-0.8445\n",
      "Iter-31730, train loss-0.4896, acc-0.9000, valid loss-0.5834, acc-0.8464, test loss-0.5864, acc-0.8443\n",
      "Iter-31740, train loss-0.4927, acc-0.8600, valid loss-0.5833, acc-0.8464, test loss-0.5863, acc-0.8446\n",
      "Iter-31750, train loss-0.5996, acc-0.8600, valid loss-0.5832, acc-0.8464, test loss-0.5862, acc-0.8446\n",
      "Iter-31760, train loss-0.7053, acc-0.8200, valid loss-0.5831, acc-0.8464, test loss-0.5861, acc-0.8447\n",
      "Iter-31770, train loss-0.6960, acc-0.8200, valid loss-0.5829, acc-0.8464, test loss-0.5859, acc-0.8446\n",
      "Iter-31780, train loss-0.8094, acc-0.7800, valid loss-0.5828, acc-0.8464, test loss-0.5858, acc-0.8447\n",
      "Iter-31790, train loss-0.7926, acc-0.7000, valid loss-0.5827, acc-0.8466, test loss-0.5857, acc-0.8446\n",
      "Iter-31800, train loss-0.5895, acc-0.8400, valid loss-0.5826, acc-0.8468, test loss-0.5856, acc-0.8446\n",
      "Iter-31810, train loss-0.4973, acc-0.8800, valid loss-0.5825, acc-0.8468, test loss-0.5855, acc-0.8447\n",
      "Iter-31820, train loss-0.5319, acc-0.8600, valid loss-0.5824, acc-0.8468, test loss-0.5854, acc-0.8447\n",
      "Iter-31830, train loss-0.4447, acc-0.9400, valid loss-0.5823, acc-0.8468, test loss-0.5853, acc-0.8449\n",
      "Iter-31840, train loss-0.5850, acc-0.8200, valid loss-0.5822, acc-0.8468, test loss-0.5852, acc-0.8448\n",
      "Iter-31850, train loss-0.4840, acc-0.9000, valid loss-0.5821, acc-0.8466, test loss-0.5851, acc-0.8448\n",
      "Iter-31860, train loss-0.4241, acc-0.9000, valid loss-0.5820, acc-0.8466, test loss-0.5850, acc-0.8447\n",
      "Iter-31870, train loss-0.6158, acc-0.8600, valid loss-0.5818, acc-0.8468, test loss-0.5849, acc-0.8448\n",
      "Iter-31880, train loss-0.7297, acc-0.7600, valid loss-0.5817, acc-0.8468, test loss-0.5847, acc-0.8449\n",
      "Iter-31890, train loss-0.7770, acc-0.8000, valid loss-0.5816, acc-0.8468, test loss-0.5847, acc-0.8449\n",
      "Iter-31900, train loss-0.6064, acc-0.8400, valid loss-0.5815, acc-0.8470, test loss-0.5846, acc-0.8447\n",
      "Iter-31910, train loss-0.6305, acc-0.8400, valid loss-0.5814, acc-0.8472, test loss-0.5844, acc-0.8444\n",
      "Iter-31920, train loss-0.7119, acc-0.7600, valid loss-0.5814, acc-0.8474, test loss-0.5843, acc-0.8446\n",
      "Iter-31930, train loss-0.8195, acc-0.8400, valid loss-0.5812, acc-0.8476, test loss-0.5842, acc-0.8445\n",
      "Iter-31940, train loss-0.4677, acc-0.9200, valid loss-0.5811, acc-0.8476, test loss-0.5841, acc-0.8446\n",
      "Iter-31950, train loss-0.6807, acc-0.7800, valid loss-0.5810, acc-0.8476, test loss-0.5840, acc-0.8447\n",
      "Iter-31960, train loss-0.6884, acc-0.7600, valid loss-0.5809, acc-0.8476, test loss-0.5839, acc-0.8447\n",
      "Iter-31970, train loss-0.7038, acc-0.8600, valid loss-0.5808, acc-0.8476, test loss-0.5838, acc-0.8445\n",
      "Iter-31980, train loss-0.6007, acc-0.8800, valid loss-0.5807, acc-0.8476, test loss-0.5837, acc-0.8448\n",
      "Iter-31990, train loss-0.4948, acc-0.8400, valid loss-0.5806, acc-0.8476, test loss-0.5837, acc-0.8449\n",
      "Iter-32000, train loss-0.4393, acc-0.8800, valid loss-0.5805, acc-0.8476, test loss-0.5836, acc-0.8449\n",
      "Iter-32010, train loss-0.5058, acc-0.8800, valid loss-0.5804, acc-0.8476, test loss-0.5835, acc-0.8449\n",
      "Iter-32020, train loss-0.4279, acc-0.9000, valid loss-0.5803, acc-0.8478, test loss-0.5833, acc-0.8449\n",
      "Iter-32030, train loss-0.6224, acc-0.8400, valid loss-0.5802, acc-0.8478, test loss-0.5832, acc-0.8449\n",
      "Iter-32040, train loss-0.7120, acc-0.8200, valid loss-0.5801, acc-0.8478, test loss-0.5831, acc-0.8449\n",
      "Iter-32050, train loss-0.5835, acc-0.8200, valid loss-0.5800, acc-0.8478, test loss-0.5830, acc-0.8449\n",
      "Iter-32060, train loss-0.5884, acc-0.8200, valid loss-0.5799, acc-0.8478, test loss-0.5829, acc-0.8449\n",
      "Iter-32070, train loss-0.5343, acc-0.9000, valid loss-0.5798, acc-0.8480, test loss-0.5828, acc-0.8449\n",
      "Iter-32080, train loss-0.6287, acc-0.8400, valid loss-0.5796, acc-0.8480, test loss-0.5827, acc-0.8452\n",
      "Iter-32090, train loss-0.5998, acc-0.8200, valid loss-0.5795, acc-0.8482, test loss-0.5826, acc-0.8452\n",
      "Iter-32100, train loss-0.5531, acc-0.8600, valid loss-0.5794, acc-0.8482, test loss-0.5825, acc-0.8452\n",
      "Iter-32110, train loss-0.7266, acc-0.7400, valid loss-0.5793, acc-0.8484, test loss-0.5824, acc-0.8451\n",
      "Iter-32120, train loss-0.6767, acc-0.7800, valid loss-0.5792, acc-0.8482, test loss-0.5823, acc-0.8454\n",
      "Iter-32130, train loss-0.7709, acc-0.7600, valid loss-0.5791, acc-0.8482, test loss-0.5822, acc-0.8452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-32140, train loss-0.8505, acc-0.7800, valid loss-0.5790, acc-0.8480, test loss-0.5821, acc-0.8453\n",
      "Iter-32150, train loss-0.6128, acc-0.8400, valid loss-0.5789, acc-0.8484, test loss-0.5820, acc-0.8454\n",
      "Iter-32160, train loss-0.5473, acc-0.8400, valid loss-0.5789, acc-0.8484, test loss-0.5819, acc-0.8452\n",
      "Iter-32170, train loss-0.5301, acc-0.8800, valid loss-0.5788, acc-0.8482, test loss-0.5818, acc-0.8454\n",
      "Iter-32180, train loss-0.5698, acc-0.8200, valid loss-0.5786, acc-0.8482, test loss-0.5817, acc-0.8456\n",
      "Iter-32190, train loss-0.5344, acc-0.8800, valid loss-0.5785, acc-0.8486, test loss-0.5816, acc-0.8455\n",
      "Iter-32200, train loss-0.6222, acc-0.8000, valid loss-0.5784, acc-0.8486, test loss-0.5815, acc-0.8456\n",
      "Iter-32210, train loss-0.6747, acc-0.8000, valid loss-0.5783, acc-0.8486, test loss-0.5814, acc-0.8457\n",
      "Iter-32220, train loss-0.5790, acc-0.8400, valid loss-0.5782, acc-0.8484, test loss-0.5813, acc-0.8457\n",
      "Iter-32230, train loss-0.5583, acc-0.8400, valid loss-0.5781, acc-0.8486, test loss-0.5812, acc-0.8455\n",
      "Iter-32240, train loss-0.5224, acc-0.8600, valid loss-0.5780, acc-0.8486, test loss-0.5811, acc-0.8458\n",
      "Iter-32250, train loss-0.6313, acc-0.8800, valid loss-0.5779, acc-0.8488, test loss-0.5810, acc-0.8457\n",
      "Iter-32260, train loss-0.6693, acc-0.8200, valid loss-0.5778, acc-0.8488, test loss-0.5809, acc-0.8457\n",
      "Iter-32270, train loss-0.6650, acc-0.7800, valid loss-0.5777, acc-0.8486, test loss-0.5808, acc-0.8457\n",
      "Iter-32280, train loss-0.4046, acc-0.9200, valid loss-0.5776, acc-0.8486, test loss-0.5806, acc-0.8458\n",
      "Iter-32290, train loss-0.5393, acc-0.8200, valid loss-0.5775, acc-0.8486, test loss-0.5806, acc-0.8460\n",
      "Iter-32300, train loss-0.8335, acc-0.7400, valid loss-0.5774, acc-0.8486, test loss-0.5805, acc-0.8460\n",
      "Iter-32310, train loss-0.5498, acc-0.8200, valid loss-0.5773, acc-0.8486, test loss-0.5804, acc-0.8460\n",
      "Iter-32320, train loss-0.7813, acc-0.7800, valid loss-0.5772, acc-0.8488, test loss-0.5803, acc-0.8460\n",
      "Iter-32330, train loss-0.5572, acc-0.8400, valid loss-0.5771, acc-0.8492, test loss-0.5802, acc-0.8461\n",
      "Iter-32340, train loss-0.6883, acc-0.7400, valid loss-0.5769, acc-0.8492, test loss-0.5801, acc-0.8460\n",
      "Iter-32350, train loss-0.6878, acc-0.8400, valid loss-0.5768, acc-0.8492, test loss-0.5800, acc-0.8461\n",
      "Iter-32360, train loss-0.3999, acc-0.9000, valid loss-0.5767, acc-0.8494, test loss-0.5798, acc-0.8460\n",
      "Iter-32370, train loss-0.6574, acc-0.7600, valid loss-0.5767, acc-0.8494, test loss-0.5798, acc-0.8461\n",
      "Iter-32380, train loss-0.6481, acc-0.7800, valid loss-0.5765, acc-0.8494, test loss-0.5797, acc-0.8459\n",
      "Iter-32390, train loss-0.6041, acc-0.8400, valid loss-0.5764, acc-0.8494, test loss-0.5796, acc-0.8459\n",
      "Iter-32400, train loss-0.5742, acc-0.8800, valid loss-0.5763, acc-0.8494, test loss-0.5795, acc-0.8459\n",
      "Iter-32410, train loss-0.4343, acc-0.8600, valid loss-0.5762, acc-0.8494, test loss-0.5793, acc-0.8458\n",
      "Iter-32420, train loss-0.5294, acc-0.9200, valid loss-0.5761, acc-0.8494, test loss-0.5792, acc-0.8460\n",
      "Iter-32430, train loss-0.5673, acc-0.8200, valid loss-0.5760, acc-0.8496, test loss-0.5791, acc-0.8459\n",
      "Iter-32440, train loss-0.6191, acc-0.8400, valid loss-0.5760, acc-0.8494, test loss-0.5791, acc-0.8460\n",
      "Iter-32450, train loss-0.5855, acc-0.8400, valid loss-0.5759, acc-0.8492, test loss-0.5790, acc-0.8462\n",
      "Iter-32460, train loss-0.5755, acc-0.8200, valid loss-0.5758, acc-0.8492, test loss-0.5789, acc-0.8464\n",
      "Iter-32470, train loss-0.5089, acc-0.8800, valid loss-0.5756, acc-0.8492, test loss-0.5788, acc-0.8461\n",
      "Iter-32480, train loss-0.5241, acc-0.9200, valid loss-0.5755, acc-0.8492, test loss-0.5787, acc-0.8462\n",
      "Iter-32490, train loss-0.6229, acc-0.8200, valid loss-0.5754, acc-0.8494, test loss-0.5786, acc-0.8463\n",
      "Iter-32500, train loss-0.4414, acc-0.9400, valid loss-0.5753, acc-0.8494, test loss-0.5784, acc-0.8463\n",
      "Iter-32510, train loss-0.5329, acc-0.8400, valid loss-0.5753, acc-0.8494, test loss-0.5784, acc-0.8464\n",
      "Iter-32520, train loss-0.4101, acc-0.8800, valid loss-0.5752, acc-0.8496, test loss-0.5783, acc-0.8464\n",
      "Iter-32530, train loss-0.6858, acc-0.8400, valid loss-0.5751, acc-0.8492, test loss-0.5782, acc-0.8465\n",
      "Iter-32540, train loss-0.7169, acc-0.7800, valid loss-0.5750, acc-0.8494, test loss-0.5781, acc-0.8465\n",
      "Iter-32550, train loss-0.5785, acc-0.8200, valid loss-0.5749, acc-0.8496, test loss-0.5780, acc-0.8464\n",
      "Iter-32560, train loss-0.4437, acc-0.9600, valid loss-0.5747, acc-0.8496, test loss-0.5779, acc-0.8465\n",
      "Iter-32570, train loss-0.6078, acc-0.8200, valid loss-0.5746, acc-0.8498, test loss-0.5778, acc-0.8464\n",
      "Iter-32580, train loss-0.7005, acc-0.7600, valid loss-0.5746, acc-0.8496, test loss-0.5777, acc-0.8464\n",
      "Iter-32590, train loss-0.7623, acc-0.7600, valid loss-0.5745, acc-0.8496, test loss-0.5776, acc-0.8465\n",
      "Iter-32600, train loss-0.9679, acc-0.7200, valid loss-0.5744, acc-0.8498, test loss-0.5775, acc-0.8466\n",
      "Iter-32610, train loss-0.6593, acc-0.8000, valid loss-0.5743, acc-0.8498, test loss-0.5774, acc-0.8466\n",
      "Iter-32620, train loss-0.6176, acc-0.8200, valid loss-0.5742, acc-0.8498, test loss-0.5773, acc-0.8466\n",
      "Iter-32630, train loss-0.6174, acc-0.8200, valid loss-0.5741, acc-0.8498, test loss-0.5772, acc-0.8464\n",
      "Iter-32640, train loss-0.4405, acc-0.9000, valid loss-0.5740, acc-0.8498, test loss-0.5771, acc-0.8466\n",
      "Iter-32650, train loss-0.4584, acc-0.8800, valid loss-0.5739, acc-0.8498, test loss-0.5770, acc-0.8466\n",
      "Iter-32660, train loss-0.6126, acc-0.8200, valid loss-0.5738, acc-0.8496, test loss-0.5769, acc-0.8467\n",
      "Iter-32670, train loss-0.7083, acc-0.8400, valid loss-0.5737, acc-0.8496, test loss-0.5768, acc-0.8467\n",
      "Iter-32680, train loss-0.5166, acc-0.8200, valid loss-0.5736, acc-0.8496, test loss-0.5767, acc-0.8468\n",
      "Iter-32690, train loss-0.5907, acc-0.8600, valid loss-0.5735, acc-0.8496, test loss-0.5766, acc-0.8467\n",
      "Iter-32700, train loss-0.5655, acc-0.8400, valid loss-0.5734, acc-0.8494, test loss-0.5765, acc-0.8467\n",
      "Iter-32710, train loss-0.6149, acc-0.8200, valid loss-0.5733, acc-0.8496, test loss-0.5764, acc-0.8467\n",
      "Iter-32720, train loss-0.7536, acc-0.8400, valid loss-0.5732, acc-0.8494, test loss-0.5764, acc-0.8466\n",
      "Iter-32730, train loss-0.7100, acc-0.8200, valid loss-0.5731, acc-0.8494, test loss-0.5763, acc-0.8467\n",
      "Iter-32740, train loss-0.6232, acc-0.8400, valid loss-0.5730, acc-0.8494, test loss-0.5762, acc-0.8469\n",
      "Iter-32750, train loss-0.4644, acc-0.9200, valid loss-0.5729, acc-0.8494, test loss-0.5761, acc-0.8467\n",
      "Iter-32760, train loss-0.5764, acc-0.8800, valid loss-0.5728, acc-0.8492, test loss-0.5760, acc-0.8469\n",
      "Iter-32770, train loss-0.5572, acc-0.7800, valid loss-0.5727, acc-0.8492, test loss-0.5759, acc-0.8470\n",
      "Iter-32780, train loss-0.4118, acc-0.9200, valid loss-0.5726, acc-0.8494, test loss-0.5758, acc-0.8468\n",
      "Iter-32790, train loss-0.5142, acc-0.8200, valid loss-0.5725, acc-0.8494, test loss-0.5757, acc-0.8467\n",
      "Iter-32800, train loss-0.5948, acc-0.8600, valid loss-0.5724, acc-0.8494, test loss-0.5756, acc-0.8469\n",
      "Iter-32810, train loss-0.6336, acc-0.8600, valid loss-0.5723, acc-0.8494, test loss-0.5755, acc-0.8470\n",
      "Iter-32820, train loss-0.7452, acc-0.8400, valid loss-0.5722, acc-0.8496, test loss-0.5754, acc-0.8471\n",
      "Iter-32830, train loss-0.5094, acc-0.8600, valid loss-0.5721, acc-0.8496, test loss-0.5753, acc-0.8472\n",
      "Iter-32840, train loss-0.8237, acc-0.7400, valid loss-0.5720, acc-0.8496, test loss-0.5752, acc-0.8473\n",
      "Iter-32850, train loss-0.7643, acc-0.8400, valid loss-0.5719, acc-0.8498, test loss-0.5751, acc-0.8472\n",
      "Iter-32860, train loss-0.7355, acc-0.7600, valid loss-0.5718, acc-0.8496, test loss-0.5750, acc-0.8471\n",
      "Iter-32870, train loss-0.6379, acc-0.8200, valid loss-0.5717, acc-0.8496, test loss-0.5749, acc-0.8471\n",
      "Iter-32880, train loss-0.5023, acc-0.8600, valid loss-0.5716, acc-0.8496, test loss-0.5748, acc-0.8470\n",
      "Iter-32890, train loss-0.7329, acc-0.7600, valid loss-0.5715, acc-0.8498, test loss-0.5747, acc-0.8470\n",
      "Iter-32900, train loss-0.4940, acc-0.9000, valid loss-0.5714, acc-0.8500, test loss-0.5746, acc-0.8470\n",
      "Iter-32910, train loss-0.5596, acc-0.9000, valid loss-0.5713, acc-0.8500, test loss-0.5745, acc-0.8470\n",
      "Iter-32920, train loss-0.5238, acc-0.8200, valid loss-0.5712, acc-0.8500, test loss-0.5744, acc-0.8470\n",
      "Iter-32930, train loss-0.6081, acc-0.8600, valid loss-0.5712, acc-0.8498, test loss-0.5743, acc-0.8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-32940, train loss-0.5782, acc-0.8200, valid loss-0.5711, acc-0.8500, test loss-0.5742, acc-0.8470\n",
      "Iter-32950, train loss-0.5868, acc-0.7800, valid loss-0.5710, acc-0.8500, test loss-0.5741, acc-0.8471\n",
      "Iter-32960, train loss-0.4474, acc-0.8800, valid loss-0.5709, acc-0.8500, test loss-0.5740, acc-0.8471\n",
      "Iter-32970, train loss-0.7019, acc-0.7600, valid loss-0.5708, acc-0.8500, test loss-0.5739, acc-0.8470\n",
      "Iter-32980, train loss-0.4754, acc-0.8800, valid loss-0.5707, acc-0.8500, test loss-0.5738, acc-0.8472\n",
      "Iter-32990, train loss-0.5877, acc-0.8200, valid loss-0.5706, acc-0.8500, test loss-0.5737, acc-0.8473\n",
      "Iter-33000, train loss-0.7974, acc-0.7600, valid loss-0.5705, acc-0.8500, test loss-0.5737, acc-0.8473\n",
      "Iter-33010, train loss-0.5235, acc-0.8400, valid loss-0.5704, acc-0.8500, test loss-0.5735, acc-0.8473\n",
      "Iter-33020, train loss-0.3787, acc-0.9200, valid loss-0.5703, acc-0.8502, test loss-0.5735, acc-0.8474\n",
      "Iter-33030, train loss-0.6145, acc-0.8600, valid loss-0.5702, acc-0.8502, test loss-0.5734, acc-0.8476\n",
      "Iter-33040, train loss-0.4878, acc-0.8600, valid loss-0.5701, acc-0.8502, test loss-0.5733, acc-0.8476\n",
      "Iter-33050, train loss-0.7760, acc-0.6800, valid loss-0.5700, acc-0.8504, test loss-0.5732, acc-0.8475\n",
      "Iter-33060, train loss-0.6659, acc-0.7800, valid loss-0.5699, acc-0.8504, test loss-0.5731, acc-0.8476\n",
      "Iter-33070, train loss-0.4577, acc-0.8800, valid loss-0.5698, acc-0.8502, test loss-0.5730, acc-0.8476\n",
      "Iter-33080, train loss-0.6397, acc-0.8400, valid loss-0.5697, acc-0.8502, test loss-0.5729, acc-0.8476\n",
      "Iter-33090, train loss-0.5202, acc-0.8800, valid loss-0.5697, acc-0.8504, test loss-0.5728, acc-0.8476\n",
      "Iter-33100, train loss-0.5470, acc-0.8600, valid loss-0.5696, acc-0.8506, test loss-0.5727, acc-0.8476\n",
      "Iter-33110, train loss-0.6361, acc-0.8400, valid loss-0.5695, acc-0.8504, test loss-0.5726, acc-0.8476\n",
      "Iter-33120, train loss-0.7543, acc-0.8000, valid loss-0.5694, acc-0.8506, test loss-0.5726, acc-0.8477\n",
      "Iter-33130, train loss-0.5761, acc-0.8800, valid loss-0.5693, acc-0.8506, test loss-0.5725, acc-0.8477\n",
      "Iter-33140, train loss-0.6327, acc-0.8200, valid loss-0.5692, acc-0.8506, test loss-0.5724, acc-0.8477\n",
      "Iter-33150, train loss-0.6475, acc-0.8400, valid loss-0.5691, acc-0.8506, test loss-0.5723, acc-0.8476\n",
      "Iter-33160, train loss-0.5421, acc-0.8600, valid loss-0.5690, acc-0.8508, test loss-0.5722, acc-0.8475\n",
      "Iter-33170, train loss-0.6622, acc-0.7600, valid loss-0.5689, acc-0.8506, test loss-0.5721, acc-0.8476\n",
      "Iter-33180, train loss-0.6964, acc-0.8200, valid loss-0.5688, acc-0.8506, test loss-0.5720, acc-0.8476\n",
      "Iter-33190, train loss-0.6897, acc-0.8000, valid loss-0.5687, acc-0.8506, test loss-0.5719, acc-0.8476\n",
      "Iter-33200, train loss-0.4598, acc-0.8200, valid loss-0.5686, acc-0.8506, test loss-0.5718, acc-0.8474\n",
      "Iter-33210, train loss-0.7082, acc-0.8000, valid loss-0.5685, acc-0.8506, test loss-0.5717, acc-0.8474\n",
      "Iter-33220, train loss-0.5856, acc-0.8200, valid loss-0.5684, acc-0.8506, test loss-0.5716, acc-0.8476\n",
      "Iter-33230, train loss-0.6620, acc-0.8400, valid loss-0.5683, acc-0.8504, test loss-0.5715, acc-0.8475\n",
      "Iter-33240, train loss-0.6712, acc-0.8200, valid loss-0.5682, acc-0.8508, test loss-0.5714, acc-0.8476\n",
      "Iter-33250, train loss-0.4939, acc-0.8800, valid loss-0.5681, acc-0.8508, test loss-0.5713, acc-0.8477\n",
      "Iter-33260, train loss-0.5668, acc-0.8400, valid loss-0.5680, acc-0.8510, test loss-0.5712, acc-0.8477\n",
      "Iter-33270, train loss-0.6777, acc-0.8400, valid loss-0.5680, acc-0.8510, test loss-0.5711, acc-0.8477\n",
      "Iter-33280, train loss-0.5224, acc-0.8600, valid loss-0.5679, acc-0.8508, test loss-0.5710, acc-0.8477\n",
      "Iter-33290, train loss-0.5496, acc-0.8800, valid loss-0.5678, acc-0.8510, test loss-0.5709, acc-0.8477\n",
      "Iter-33300, train loss-0.4998, acc-0.8600, valid loss-0.5677, acc-0.8506, test loss-0.5708, acc-0.8476\n",
      "Iter-33310, train loss-0.4413, acc-0.8600, valid loss-0.5676, acc-0.8508, test loss-0.5707, acc-0.8474\n",
      "Iter-33320, train loss-0.5402, acc-0.8200, valid loss-0.5675, acc-0.8510, test loss-0.5706, acc-0.8475\n",
      "Iter-33330, train loss-0.6021, acc-0.7600, valid loss-0.5674, acc-0.8510, test loss-0.5705, acc-0.8475\n",
      "Iter-33340, train loss-0.7859, acc-0.7600, valid loss-0.5673, acc-0.8510, test loss-0.5704, acc-0.8476\n",
      "Iter-33350, train loss-0.5350, acc-0.8400, valid loss-0.5672, acc-0.8508, test loss-0.5703, acc-0.8476\n",
      "Iter-33360, train loss-0.5292, acc-0.8400, valid loss-0.5671, acc-0.8506, test loss-0.5702, acc-0.8477\n",
      "Iter-33370, train loss-0.6575, acc-0.8200, valid loss-0.5670, acc-0.8508, test loss-0.5701, acc-0.8477\n",
      "Iter-33380, train loss-0.5854, acc-0.8600, valid loss-0.5669, acc-0.8508, test loss-0.5700, acc-0.8478\n",
      "Iter-33390, train loss-0.5376, acc-0.8800, valid loss-0.5668, acc-0.8510, test loss-0.5699, acc-0.8479\n",
      "Iter-33400, train loss-0.4950, acc-0.9200, valid loss-0.5667, acc-0.8510, test loss-0.5698, acc-0.8479\n",
      "Iter-33410, train loss-0.4661, acc-0.8800, valid loss-0.5666, acc-0.8510, test loss-0.5697, acc-0.8478\n",
      "Iter-33420, train loss-0.4080, acc-0.9000, valid loss-0.5665, acc-0.8510, test loss-0.5696, acc-0.8477\n",
      "Iter-33430, train loss-0.4109, acc-0.8800, valid loss-0.5664, acc-0.8510, test loss-0.5695, acc-0.8477\n",
      "Iter-33440, train loss-0.6575, acc-0.8000, valid loss-0.5664, acc-0.8510, test loss-0.5694, acc-0.8478\n",
      "Iter-33450, train loss-0.5409, acc-0.8600, valid loss-0.5663, acc-0.8508, test loss-0.5693, acc-0.8477\n",
      "Iter-33460, train loss-0.6129, acc-0.8200, valid loss-0.5661, acc-0.8508, test loss-0.5692, acc-0.8477\n",
      "Iter-33470, train loss-0.5375, acc-0.9000, valid loss-0.5660, acc-0.8508, test loss-0.5691, acc-0.8478\n",
      "Iter-33480, train loss-0.6446, acc-0.8400, valid loss-0.5659, acc-0.8510, test loss-0.5690, acc-0.8477\n",
      "Iter-33490, train loss-0.5028, acc-0.8400, valid loss-0.5658, acc-0.8510, test loss-0.5689, acc-0.8479\n",
      "Iter-33500, train loss-0.4865, acc-0.8600, valid loss-0.5658, acc-0.8510, test loss-0.5688, acc-0.8479\n",
      "Iter-33510, train loss-0.5976, acc-0.8800, valid loss-0.5656, acc-0.8514, test loss-0.5687, acc-0.8480\n",
      "Iter-33520, train loss-0.3681, acc-0.9600, valid loss-0.5656, acc-0.8514, test loss-0.5686, acc-0.8481\n",
      "Iter-33530, train loss-0.6547, acc-0.8000, valid loss-0.5655, acc-0.8516, test loss-0.5685, acc-0.8481\n",
      "Iter-33540, train loss-0.6727, acc-0.8200, valid loss-0.5654, acc-0.8516, test loss-0.5685, acc-0.8481\n",
      "Iter-33550, train loss-0.4107, acc-0.8800, valid loss-0.5653, acc-0.8516, test loss-0.5684, acc-0.8481\n",
      "Iter-33560, train loss-0.5677, acc-0.8400, valid loss-0.5652, acc-0.8514, test loss-0.5683, acc-0.8481\n",
      "Iter-33570, train loss-0.7049, acc-0.7800, valid loss-0.5651, acc-0.8516, test loss-0.5682, acc-0.8481\n",
      "Iter-33580, train loss-0.4550, acc-0.8600, valid loss-0.5650, acc-0.8516, test loss-0.5681, acc-0.8482\n",
      "Iter-33590, train loss-0.5174, acc-0.9000, valid loss-0.5649, acc-0.8516, test loss-0.5680, acc-0.8482\n",
      "Iter-33600, train loss-0.4281, acc-0.9200, valid loss-0.5648, acc-0.8518, test loss-0.5679, acc-0.8481\n",
      "Iter-33610, train loss-0.5181, acc-0.8400, valid loss-0.5647, acc-0.8516, test loss-0.5678, acc-0.8481\n",
      "Iter-33620, train loss-0.6350, acc-0.8400, valid loss-0.5646, acc-0.8518, test loss-0.5677, acc-0.8483\n",
      "Iter-33630, train loss-0.4854, acc-0.8400, valid loss-0.5645, acc-0.8518, test loss-0.5677, acc-0.8482\n",
      "Iter-33640, train loss-0.4812, acc-0.8600, valid loss-0.5644, acc-0.8518, test loss-0.5676, acc-0.8482\n",
      "Iter-33650, train loss-0.6265, acc-0.8400, valid loss-0.5644, acc-0.8516, test loss-0.5675, acc-0.8483\n",
      "Iter-33660, train loss-0.4524, acc-0.9400, valid loss-0.5643, acc-0.8516, test loss-0.5674, acc-0.8484\n",
      "Iter-33670, train loss-0.5136, acc-0.8800, valid loss-0.5642, acc-0.8516, test loss-0.5673, acc-0.8484\n",
      "Iter-33680, train loss-0.4837, acc-0.8600, valid loss-0.5641, acc-0.8514, test loss-0.5672, acc-0.8486\n",
      "Iter-33690, train loss-0.4536, acc-0.8600, valid loss-0.5640, acc-0.8516, test loss-0.5671, acc-0.8485\n",
      "Iter-33700, train loss-0.6009, acc-0.7800, valid loss-0.5639, acc-0.8516, test loss-0.5670, acc-0.8486\n",
      "Iter-33710, train loss-0.8155, acc-0.7600, valid loss-0.5638, acc-0.8516, test loss-0.5669, acc-0.8486\n",
      "Iter-33720, train loss-0.4749, acc-0.9200, valid loss-0.5637, acc-0.8516, test loss-0.5668, acc-0.8486\n",
      "Iter-33730, train loss-0.5092, acc-0.8800, valid loss-0.5636, acc-0.8516, test loss-0.5667, acc-0.8486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-33740, train loss-0.4381, acc-0.9000, valid loss-0.5635, acc-0.8516, test loss-0.5666, acc-0.8486\n",
      "Iter-33750, train loss-0.7670, acc-0.8400, valid loss-0.5635, acc-0.8516, test loss-0.5665, acc-0.8486\n",
      "Iter-33760, train loss-0.6147, acc-0.8600, valid loss-0.5634, acc-0.8516, test loss-0.5664, acc-0.8486\n",
      "Iter-33770, train loss-0.6321, acc-0.7800, valid loss-0.5633, acc-0.8516, test loss-0.5663, acc-0.8486\n",
      "Iter-33780, train loss-0.5017, acc-0.8800, valid loss-0.5632, acc-0.8516, test loss-0.5663, acc-0.8486\n",
      "Iter-33790, train loss-0.4382, acc-0.8800, valid loss-0.5631, acc-0.8516, test loss-0.5662, acc-0.8487\n",
      "Iter-33800, train loss-0.6364, acc-0.8600, valid loss-0.5630, acc-0.8516, test loss-0.5661, acc-0.8486\n",
      "Iter-33810, train loss-0.4701, acc-0.8600, valid loss-0.5629, acc-0.8516, test loss-0.5660, acc-0.8487\n",
      "Iter-33820, train loss-0.6600, acc-0.8200, valid loss-0.5628, acc-0.8516, test loss-0.5659, acc-0.8485\n",
      "Iter-33830, train loss-0.5404, acc-0.8400, valid loss-0.5628, acc-0.8516, test loss-0.5658, acc-0.8485\n",
      "Iter-33840, train loss-0.4975, acc-0.8400, valid loss-0.5627, acc-0.8516, test loss-0.5657, acc-0.8486\n",
      "Iter-33850, train loss-0.6098, acc-0.8200, valid loss-0.5626, acc-0.8516, test loss-0.5656, acc-0.8486\n",
      "Iter-33860, train loss-0.6008, acc-0.8200, valid loss-0.5625, acc-0.8516, test loss-0.5655, acc-0.8487\n",
      "Iter-33870, train loss-0.5286, acc-0.9000, valid loss-0.5624, acc-0.8516, test loss-0.5654, acc-0.8487\n",
      "Iter-33880, train loss-0.5602, acc-0.8400, valid loss-0.5623, acc-0.8516, test loss-0.5653, acc-0.8488\n",
      "Iter-33890, train loss-0.5996, acc-0.8400, valid loss-0.5622, acc-0.8516, test loss-0.5652, acc-0.8486\n",
      "Iter-33900, train loss-0.8307, acc-0.7600, valid loss-0.5621, acc-0.8516, test loss-0.5652, acc-0.8486\n",
      "Iter-33910, train loss-0.6236, acc-0.8200, valid loss-0.5620, acc-0.8516, test loss-0.5651, acc-0.8489\n",
      "Iter-33920, train loss-0.3954, acc-0.9200, valid loss-0.5619, acc-0.8516, test loss-0.5650, acc-0.8490\n",
      "Iter-33930, train loss-0.6873, acc-0.8000, valid loss-0.5618, acc-0.8516, test loss-0.5649, acc-0.8489\n",
      "Iter-33940, train loss-0.4709, acc-0.8800, valid loss-0.5617, acc-0.8516, test loss-0.5648, acc-0.8488\n",
      "Iter-33950, train loss-0.4799, acc-0.8600, valid loss-0.5616, acc-0.8516, test loss-0.5647, acc-0.8490\n",
      "Iter-33960, train loss-0.7636, acc-0.8000, valid loss-0.5615, acc-0.8516, test loss-0.5646, acc-0.8490\n",
      "Iter-33970, train loss-0.5140, acc-0.8600, valid loss-0.5614, acc-0.8516, test loss-0.5645, acc-0.8489\n",
      "Iter-33980, train loss-0.5464, acc-0.8400, valid loss-0.5613, acc-0.8516, test loss-0.5644, acc-0.8490\n",
      "Iter-33990, train loss-0.4554, acc-0.9000, valid loss-0.5613, acc-0.8518, test loss-0.5643, acc-0.8490\n",
      "Iter-34000, train loss-0.6192, acc-0.8200, valid loss-0.5612, acc-0.8518, test loss-0.5642, acc-0.8490\n",
      "Iter-34010, train loss-0.3913, acc-0.8600, valid loss-0.5610, acc-0.8518, test loss-0.5641, acc-0.8490\n",
      "Iter-34020, train loss-0.4576, acc-0.9000, valid loss-0.5609, acc-0.8518, test loss-0.5641, acc-0.8490\n",
      "Iter-34030, train loss-0.8761, acc-0.7600, valid loss-0.5609, acc-0.8518, test loss-0.5640, acc-0.8490\n",
      "Iter-34040, train loss-0.6516, acc-0.8200, valid loss-0.5608, acc-0.8520, test loss-0.5639, acc-0.8490\n",
      "Iter-34050, train loss-0.6210, acc-0.7800, valid loss-0.5607, acc-0.8522, test loss-0.5638, acc-0.8490\n",
      "Iter-34060, train loss-0.4977, acc-0.8400, valid loss-0.5606, acc-0.8522, test loss-0.5637, acc-0.8490\n",
      "Iter-34070, train loss-0.6080, acc-0.8200, valid loss-0.5605, acc-0.8522, test loss-0.5636, acc-0.8492\n",
      "Iter-34080, train loss-0.6321, acc-0.8200, valid loss-0.5604, acc-0.8520, test loss-0.5635, acc-0.8492\n",
      "Iter-34090, train loss-0.6407, acc-0.8200, valid loss-0.5603, acc-0.8524, test loss-0.5634, acc-0.8492\n",
      "Iter-34100, train loss-0.3543, acc-0.9600, valid loss-0.5602, acc-0.8524, test loss-0.5633, acc-0.8492\n",
      "Iter-34110, train loss-0.7399, acc-0.7400, valid loss-0.5601, acc-0.8524, test loss-0.5632, acc-0.8492\n",
      "Iter-34120, train loss-0.5336, acc-0.8800, valid loss-0.5600, acc-0.8524, test loss-0.5631, acc-0.8491\n",
      "Iter-34130, train loss-0.7746, acc-0.7400, valid loss-0.5599, acc-0.8524, test loss-0.5630, acc-0.8491\n",
      "Iter-34140, train loss-0.4922, acc-0.8800, valid loss-0.5598, acc-0.8524, test loss-0.5629, acc-0.8492\n",
      "Iter-34150, train loss-0.5159, acc-0.8600, valid loss-0.5598, acc-0.8526, test loss-0.5628, acc-0.8493\n",
      "Iter-34160, train loss-0.7383, acc-0.7600, valid loss-0.5597, acc-0.8526, test loss-0.5628, acc-0.8492\n",
      "Iter-34170, train loss-0.6353, acc-0.7800, valid loss-0.5596, acc-0.8526, test loss-0.5627, acc-0.8492\n",
      "Iter-34180, train loss-0.8980, acc-0.6600, valid loss-0.5595, acc-0.8526, test loss-0.5626, acc-0.8492\n",
      "Iter-34190, train loss-0.8283, acc-0.7800, valid loss-0.5594, acc-0.8524, test loss-0.5625, acc-0.8492\n",
      "Iter-34200, train loss-0.5362, acc-0.8600, valid loss-0.5593, acc-0.8526, test loss-0.5624, acc-0.8493\n",
      "Iter-34210, train loss-0.3371, acc-0.9200, valid loss-0.5592, acc-0.8526, test loss-0.5623, acc-0.8494\n",
      "Iter-34220, train loss-0.6750, acc-0.7600, valid loss-0.5591, acc-0.8526, test loss-0.5622, acc-0.8494\n",
      "Iter-34230, train loss-0.6896, acc-0.7800, valid loss-0.5590, acc-0.8526, test loss-0.5621, acc-0.8495\n",
      "Iter-34240, train loss-0.6781, acc-0.8400, valid loss-0.5589, acc-0.8526, test loss-0.5621, acc-0.8495\n",
      "Iter-34250, train loss-0.6603, acc-0.8400, valid loss-0.5588, acc-0.8524, test loss-0.5620, acc-0.8495\n",
      "Iter-34260, train loss-0.5367, acc-0.8000, valid loss-0.5587, acc-0.8526, test loss-0.5619, acc-0.8496\n",
      "Iter-34270, train loss-0.8721, acc-0.7600, valid loss-0.5586, acc-0.8524, test loss-0.5618, acc-0.8496\n",
      "Iter-34280, train loss-0.5684, acc-0.8400, valid loss-0.5585, acc-0.8524, test loss-0.5617, acc-0.8497\n",
      "Iter-34290, train loss-0.4838, acc-0.8400, valid loss-0.5584, acc-0.8524, test loss-0.5616, acc-0.8497\n",
      "Iter-34300, train loss-0.5346, acc-0.8800, valid loss-0.5584, acc-0.8524, test loss-0.5615, acc-0.8497\n",
      "Iter-34310, train loss-0.6119, acc-0.7800, valid loss-0.5583, acc-0.8524, test loss-0.5614, acc-0.8496\n",
      "Iter-34320, train loss-0.7062, acc-0.8200, valid loss-0.5582, acc-0.8522, test loss-0.5613, acc-0.8496\n",
      "Iter-34330, train loss-0.4249, acc-0.9000, valid loss-0.5581, acc-0.8526, test loss-0.5612, acc-0.8496\n",
      "Iter-34340, train loss-0.6442, acc-0.8200, valid loss-0.5580, acc-0.8526, test loss-0.5611, acc-0.8496\n",
      "Iter-34350, train loss-0.4701, acc-0.8800, valid loss-0.5579, acc-0.8524, test loss-0.5610, acc-0.8495\n",
      "Iter-34360, train loss-0.7281, acc-0.7600, valid loss-0.5578, acc-0.8524, test loss-0.5610, acc-0.8497\n",
      "Iter-34370, train loss-0.6302, acc-0.8000, valid loss-0.5577, acc-0.8524, test loss-0.5608, acc-0.8499\n",
      "Iter-34380, train loss-0.4757, acc-0.9200, valid loss-0.5576, acc-0.8524, test loss-0.5607, acc-0.8499\n",
      "Iter-34390, train loss-0.5606, acc-0.8800, valid loss-0.5575, acc-0.8524, test loss-0.5607, acc-0.8499\n",
      "Iter-34400, train loss-0.6978, acc-0.7800, valid loss-0.5575, acc-0.8524, test loss-0.5606, acc-0.8498\n",
      "Iter-34410, train loss-0.6759, acc-0.8200, valid loss-0.5574, acc-0.8524, test loss-0.5605, acc-0.8498\n",
      "Iter-34420, train loss-0.4890, acc-0.8400, valid loss-0.5573, acc-0.8524, test loss-0.5604, acc-0.8498\n",
      "Iter-34430, train loss-0.6191, acc-0.7600, valid loss-0.5572, acc-0.8524, test loss-0.5604, acc-0.8499\n",
      "Iter-34440, train loss-0.7468, acc-0.7400, valid loss-0.5572, acc-0.8524, test loss-0.5603, acc-0.8498\n",
      "Iter-34450, train loss-0.5780, acc-0.8200, valid loss-0.5571, acc-0.8522, test loss-0.5602, acc-0.8498\n",
      "Iter-34460, train loss-0.5237, acc-0.8200, valid loss-0.5570, acc-0.8522, test loss-0.5601, acc-0.8499\n",
      "Iter-34470, train loss-0.6022, acc-0.8400, valid loss-0.5569, acc-0.8522, test loss-0.5600, acc-0.8500\n",
      "Iter-34480, train loss-0.4748, acc-0.8600, valid loss-0.5568, acc-0.8522, test loss-0.5599, acc-0.8498\n",
      "Iter-34490, train loss-0.4459, acc-0.8600, valid loss-0.5567, acc-0.8522, test loss-0.5598, acc-0.8499\n",
      "Iter-34500, train loss-0.3711, acc-0.9400, valid loss-0.5566, acc-0.8520, test loss-0.5597, acc-0.8501\n",
      "Iter-34510, train loss-0.6130, acc-0.8200, valid loss-0.5566, acc-0.8520, test loss-0.5596, acc-0.8501\n",
      "Iter-34520, train loss-0.5939, acc-0.7600, valid loss-0.5565, acc-0.8520, test loss-0.5596, acc-0.8502\n",
      "Iter-34530, train loss-0.5801, acc-0.8400, valid loss-0.5564, acc-0.8520, test loss-0.5595, acc-0.8502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34540, train loss-0.6113, acc-0.7600, valid loss-0.5563, acc-0.8522, test loss-0.5594, acc-0.8503\n",
      "Iter-34550, train loss-0.6839, acc-0.8200, valid loss-0.5562, acc-0.8522, test loss-0.5593, acc-0.8502\n",
      "Iter-34560, train loss-0.6024, acc-0.8600, valid loss-0.5561, acc-0.8524, test loss-0.5592, acc-0.8503\n",
      "Iter-34570, train loss-0.7617, acc-0.7000, valid loss-0.5560, acc-0.8524, test loss-0.5591, acc-0.8504\n",
      "Iter-34580, train loss-0.8138, acc-0.7400, valid loss-0.5559, acc-0.8526, test loss-0.5590, acc-0.8504\n",
      "Iter-34590, train loss-0.6990, acc-0.7600, valid loss-0.5558, acc-0.8522, test loss-0.5589, acc-0.8504\n",
      "Iter-34600, train loss-0.5418, acc-0.8400, valid loss-0.5557, acc-0.8524, test loss-0.5588, acc-0.8503\n",
      "Iter-34610, train loss-0.5764, acc-0.8200, valid loss-0.5556, acc-0.8524, test loss-0.5587, acc-0.8503\n",
      "Iter-34620, train loss-0.3812, acc-0.9400, valid loss-0.5555, acc-0.8524, test loss-0.5587, acc-0.8500\n",
      "Iter-34630, train loss-0.9024, acc-0.7600, valid loss-0.5554, acc-0.8524, test loss-0.5586, acc-0.8500\n",
      "Iter-34640, train loss-0.3803, acc-0.9200, valid loss-0.5553, acc-0.8524, test loss-0.5585, acc-0.8500\n",
      "Iter-34650, train loss-0.8548, acc-0.7200, valid loss-0.5553, acc-0.8526, test loss-0.5584, acc-0.8500\n",
      "Iter-34660, train loss-0.4358, acc-0.9200, valid loss-0.5552, acc-0.8524, test loss-0.5583, acc-0.8500\n",
      "Iter-34670, train loss-0.6786, acc-0.7400, valid loss-0.5551, acc-0.8524, test loss-0.5582, acc-0.8500\n",
      "Iter-34680, train loss-0.6657, acc-0.7600, valid loss-0.5550, acc-0.8526, test loss-0.5581, acc-0.8502\n",
      "Iter-34690, train loss-0.8551, acc-0.7400, valid loss-0.5549, acc-0.8524, test loss-0.5581, acc-0.8502\n",
      "Iter-34700, train loss-0.6417, acc-0.8800, valid loss-0.5548, acc-0.8524, test loss-0.5580, acc-0.8502\n",
      "Iter-34710, train loss-0.4693, acc-0.8600, valid loss-0.5547, acc-0.8526, test loss-0.5579, acc-0.8502\n",
      "Iter-34720, train loss-0.4690, acc-0.8600, valid loss-0.5547, acc-0.8524, test loss-0.5578, acc-0.8503\n",
      "Iter-34730, train loss-0.7326, acc-0.8200, valid loss-0.5545, acc-0.8524, test loss-0.5577, acc-0.8505\n",
      "Iter-34740, train loss-0.6737, acc-0.8000, valid loss-0.5545, acc-0.8524, test loss-0.5576, acc-0.8503\n",
      "Iter-34750, train loss-0.6994, acc-0.7800, valid loss-0.5544, acc-0.8526, test loss-0.5575, acc-0.8504\n",
      "Iter-34760, train loss-0.5182, acc-0.9000, valid loss-0.5543, acc-0.8526, test loss-0.5574, acc-0.8503\n",
      "Iter-34770, train loss-0.6499, acc-0.8200, valid loss-0.5542, acc-0.8526, test loss-0.5573, acc-0.8504\n",
      "Iter-34780, train loss-0.6167, acc-0.8600, valid loss-0.5541, acc-0.8528, test loss-0.5572, acc-0.8504\n",
      "Iter-34790, train loss-0.5606, acc-0.8600, valid loss-0.5540, acc-0.8528, test loss-0.5572, acc-0.8505\n",
      "Iter-34800, train loss-0.4352, acc-0.8800, valid loss-0.5539, acc-0.8528, test loss-0.5571, acc-0.8505\n",
      "Iter-34810, train loss-0.6393, acc-0.8400, valid loss-0.5538, acc-0.8528, test loss-0.5570, acc-0.8506\n",
      "Iter-34820, train loss-0.4937, acc-0.8200, valid loss-0.5537, acc-0.8528, test loss-0.5569, acc-0.8506\n",
      "Iter-34830, train loss-0.6320, acc-0.8200, valid loss-0.5537, acc-0.8528, test loss-0.5568, acc-0.8506\n",
      "Iter-34840, train loss-0.5126, acc-0.8800, valid loss-0.5536, acc-0.8526, test loss-0.5568, acc-0.8505\n",
      "Iter-34850, train loss-0.5566, acc-0.9000, valid loss-0.5535, acc-0.8526, test loss-0.5567, acc-0.8504\n",
      "Iter-34860, train loss-0.8127, acc-0.7200, valid loss-0.5534, acc-0.8528, test loss-0.5566, acc-0.8505\n",
      "Iter-34870, train loss-0.5030, acc-0.8600, valid loss-0.5533, acc-0.8528, test loss-0.5565, acc-0.8505\n",
      "Iter-34880, train loss-0.5659, acc-0.8200, valid loss-0.5532, acc-0.8528, test loss-0.5564, acc-0.8506\n",
      "Iter-34890, train loss-0.7985, acc-0.7800, valid loss-0.5532, acc-0.8526, test loss-0.5563, acc-0.8505\n",
      "Iter-34900, train loss-0.4360, acc-0.9000, valid loss-0.5531, acc-0.8528, test loss-0.5562, acc-0.8506\n",
      "Iter-34910, train loss-0.4700, acc-0.9000, valid loss-0.5530, acc-0.8528, test loss-0.5561, acc-0.8506\n",
      "Iter-34920, train loss-0.5181, acc-0.8600, valid loss-0.5529, acc-0.8530, test loss-0.5561, acc-0.8506\n",
      "Iter-34930, train loss-0.5995, acc-0.7800, valid loss-0.5528, acc-0.8530, test loss-0.5560, acc-0.8506\n",
      "Iter-34940, train loss-0.4242, acc-0.8800, valid loss-0.5527, acc-0.8530, test loss-0.5559, acc-0.8505\n",
      "Iter-34950, train loss-0.4859, acc-0.8400, valid loss-0.5526, acc-0.8526, test loss-0.5558, acc-0.8505\n",
      "Iter-34960, train loss-0.4107, acc-0.9200, valid loss-0.5525, acc-0.8528, test loss-0.5557, acc-0.8505\n",
      "Iter-34970, train loss-0.4722, acc-0.8800, valid loss-0.5525, acc-0.8528, test loss-0.5556, acc-0.8505\n",
      "Iter-34980, train loss-0.5566, acc-0.8000, valid loss-0.5524, acc-0.8528, test loss-0.5556, acc-0.8505\n",
      "Iter-34990, train loss-0.5189, acc-0.8200, valid loss-0.5523, acc-0.8528, test loss-0.5555, acc-0.8504\n",
      "Iter-35000, train loss-0.5679, acc-0.8400, valid loss-0.5522, acc-0.8528, test loss-0.5554, acc-0.8504\n",
      "Iter-35010, train loss-0.4815, acc-0.9400, valid loss-0.5521, acc-0.8528, test loss-0.5553, acc-0.8504\n",
      "Iter-35020, train loss-0.6396, acc-0.8400, valid loss-0.5520, acc-0.8528, test loss-0.5552, acc-0.8504\n",
      "Iter-35030, train loss-0.4788, acc-0.9400, valid loss-0.5519, acc-0.8526, test loss-0.5551, acc-0.8505\n",
      "Iter-35040, train loss-0.3707, acc-0.9400, valid loss-0.5519, acc-0.8528, test loss-0.5550, acc-0.8505\n",
      "Iter-35050, train loss-0.5305, acc-0.8200, valid loss-0.5518, acc-0.8528, test loss-0.5550, acc-0.8505\n",
      "Iter-35060, train loss-0.4202, acc-0.9200, valid loss-0.5517, acc-0.8526, test loss-0.5549, acc-0.8505\n",
      "Iter-35070, train loss-0.4877, acc-0.7800, valid loss-0.5516, acc-0.8528, test loss-0.5548, acc-0.8505\n",
      "Iter-35080, train loss-0.5366, acc-0.8200, valid loss-0.5515, acc-0.8528, test loss-0.5547, acc-0.8505\n",
      "Iter-35090, train loss-0.5384, acc-0.8600, valid loss-0.5514, acc-0.8528, test loss-0.5546, acc-0.8506\n",
      "Iter-35100, train loss-0.6319, acc-0.8000, valid loss-0.5513, acc-0.8530, test loss-0.5545, acc-0.8506\n",
      "Iter-35110, train loss-0.5303, acc-0.8600, valid loss-0.5513, acc-0.8530, test loss-0.5544, acc-0.8507\n",
      "Iter-35120, train loss-0.3662, acc-0.9400, valid loss-0.5512, acc-0.8532, test loss-0.5543, acc-0.8508\n",
      "Iter-35130, train loss-0.5870, acc-0.8400, valid loss-0.5511, acc-0.8530, test loss-0.5542, acc-0.8510\n",
      "Iter-35140, train loss-0.8368, acc-0.8200, valid loss-0.5510, acc-0.8530, test loss-0.5542, acc-0.8510\n",
      "Iter-35150, train loss-0.6156, acc-0.8400, valid loss-0.5509, acc-0.8530, test loss-0.5541, acc-0.8510\n",
      "Iter-35160, train loss-0.3897, acc-0.9000, valid loss-0.5508, acc-0.8528, test loss-0.5540, acc-0.8509\n",
      "Iter-35170, train loss-0.2407, acc-0.9600, valid loss-0.5508, acc-0.8528, test loss-0.5539, acc-0.8509\n",
      "Iter-35180, train loss-0.5355, acc-0.9000, valid loss-0.5507, acc-0.8528, test loss-0.5539, acc-0.8509\n",
      "Iter-35190, train loss-0.5174, acc-0.8800, valid loss-0.5506, acc-0.8528, test loss-0.5538, acc-0.8509\n",
      "Iter-35200, train loss-0.4779, acc-0.8600, valid loss-0.5505, acc-0.8528, test loss-0.5537, acc-0.8510\n",
      "Iter-35210, train loss-0.4316, acc-0.8800, valid loss-0.5504, acc-0.8528, test loss-0.5536, acc-0.8511\n",
      "Iter-35220, train loss-0.4908, acc-0.8800, valid loss-0.5503, acc-0.8528, test loss-0.5535, acc-0.8510\n",
      "Iter-35230, train loss-0.5346, acc-0.8600, valid loss-0.5502, acc-0.8528, test loss-0.5534, acc-0.8510\n",
      "Iter-35240, train loss-0.4882, acc-0.9200, valid loss-0.5501, acc-0.8528, test loss-0.5534, acc-0.8507\n",
      "Iter-35250, train loss-0.6209, acc-0.8200, valid loss-0.5501, acc-0.8534, test loss-0.5533, acc-0.8509\n",
      "Iter-35260, train loss-0.6273, acc-0.8000, valid loss-0.5500, acc-0.8532, test loss-0.5532, acc-0.8509\n",
      "Iter-35270, train loss-0.5008, acc-0.8800, valid loss-0.5499, acc-0.8532, test loss-0.5531, acc-0.8511\n",
      "Iter-35280, train loss-0.5099, acc-0.8600, valid loss-0.5498, acc-0.8534, test loss-0.5530, acc-0.8511\n",
      "Iter-35290, train loss-0.6147, acc-0.8400, valid loss-0.5497, acc-0.8532, test loss-0.5529, acc-0.8513\n",
      "Iter-35300, train loss-0.7386, acc-0.7800, valid loss-0.5496, acc-0.8534, test loss-0.5529, acc-0.8512\n",
      "Iter-35310, train loss-0.5067, acc-0.9000, valid loss-0.5495, acc-0.8532, test loss-0.5528, acc-0.8513\n",
      "Iter-35320, train loss-0.5265, acc-0.8800, valid loss-0.5494, acc-0.8530, test loss-0.5527, acc-0.8513\n",
      "Iter-35330, train loss-0.5139, acc-0.8400, valid loss-0.5493, acc-0.8532, test loss-0.5526, acc-0.8514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-35340, train loss-0.6155, acc-0.8400, valid loss-0.5492, acc-0.8528, test loss-0.5525, acc-0.8513\n",
      "Iter-35350, train loss-0.5649, acc-0.8800, valid loss-0.5492, acc-0.8528, test loss-0.5524, acc-0.8513\n",
      "Iter-35360, train loss-0.5584, acc-0.8600, valid loss-0.5491, acc-0.8528, test loss-0.5523, acc-0.8514\n",
      "Iter-35370, train loss-0.7508, acc-0.8000, valid loss-0.5490, acc-0.8530, test loss-0.5523, acc-0.8512\n",
      "Iter-35380, train loss-0.4530, acc-0.9200, valid loss-0.5489, acc-0.8530, test loss-0.5522, acc-0.8514\n",
      "Iter-35390, train loss-0.6712, acc-0.7800, valid loss-0.5488, acc-0.8528, test loss-0.5521, acc-0.8515\n",
      "Iter-35400, train loss-0.5854, acc-0.8600, valid loss-0.5487, acc-0.8532, test loss-0.5520, acc-0.8514\n",
      "Iter-35410, train loss-0.5620, acc-0.8800, valid loss-0.5487, acc-0.8532, test loss-0.5519, acc-0.8515\n",
      "Iter-35420, train loss-0.6490, acc-0.8000, valid loss-0.5486, acc-0.8530, test loss-0.5518, acc-0.8516\n",
      "Iter-35430, train loss-0.6340, acc-0.7800, valid loss-0.5485, acc-0.8534, test loss-0.5518, acc-0.8515\n",
      "Iter-35440, train loss-0.4539, acc-0.8600, valid loss-0.5484, acc-0.8532, test loss-0.5517, acc-0.8515\n",
      "Iter-35450, train loss-0.6580, acc-0.7800, valid loss-0.5483, acc-0.8534, test loss-0.5516, acc-0.8515\n",
      "Iter-35460, train loss-0.4163, acc-0.8800, valid loss-0.5483, acc-0.8534, test loss-0.5515, acc-0.8517\n",
      "Iter-35470, train loss-0.4681, acc-0.9000, valid loss-0.5482, acc-0.8534, test loss-0.5514, acc-0.8518\n",
      "Iter-35480, train loss-0.6331, acc-0.8600, valid loss-0.5481, acc-0.8532, test loss-0.5514, acc-0.8518\n",
      "Iter-35490, train loss-0.6913, acc-0.8000, valid loss-0.5480, acc-0.8532, test loss-0.5513, acc-0.8518\n",
      "Iter-35500, train loss-0.6331, acc-0.8000, valid loss-0.5479, acc-0.8532, test loss-0.5512, acc-0.8519\n",
      "Iter-35510, train loss-0.4828, acc-0.8600, valid loss-0.5478, acc-0.8532, test loss-0.5511, acc-0.8519\n",
      "Iter-35520, train loss-0.3928, acc-0.9400, valid loss-0.5477, acc-0.8534, test loss-0.5511, acc-0.8519\n",
      "Iter-35530, train loss-0.4802, acc-0.8800, valid loss-0.5477, acc-0.8534, test loss-0.5510, acc-0.8518\n",
      "Iter-35540, train loss-0.3921, acc-0.9200, valid loss-0.5476, acc-0.8534, test loss-0.5509, acc-0.8520\n",
      "Iter-35550, train loss-0.5773, acc-0.8200, valid loss-0.5475, acc-0.8534, test loss-0.5508, acc-0.8519\n",
      "Iter-35560, train loss-0.6252, acc-0.7800, valid loss-0.5474, acc-0.8532, test loss-0.5507, acc-0.8518\n",
      "Iter-35570, train loss-0.4161, acc-0.9000, valid loss-0.5473, acc-0.8532, test loss-0.5507, acc-0.8519\n",
      "Iter-35580, train loss-0.5005, acc-0.9000, valid loss-0.5473, acc-0.8532, test loss-0.5506, acc-0.8520\n",
      "Iter-35590, train loss-0.6899, acc-0.7400, valid loss-0.5472, acc-0.8536, test loss-0.5505, acc-0.8520\n",
      "Iter-35600, train loss-0.3783, acc-0.9400, valid loss-0.5471, acc-0.8534, test loss-0.5504, acc-0.8521\n",
      "Iter-35610, train loss-0.6571, acc-0.8800, valid loss-0.5470, acc-0.8532, test loss-0.5503, acc-0.8522\n",
      "Iter-35620, train loss-0.4886, acc-0.8800, valid loss-0.5469, acc-0.8534, test loss-0.5502, acc-0.8523\n",
      "Iter-35630, train loss-0.5328, acc-0.8000, valid loss-0.5469, acc-0.8536, test loss-0.5501, acc-0.8523\n",
      "Iter-35640, train loss-0.6466, acc-0.8400, valid loss-0.5468, acc-0.8532, test loss-0.5501, acc-0.8522\n",
      "Iter-35650, train loss-0.5171, acc-0.8600, valid loss-0.5467, acc-0.8532, test loss-0.5500, acc-0.8523\n",
      "Iter-35660, train loss-0.7814, acc-0.7800, valid loss-0.5466, acc-0.8536, test loss-0.5499, acc-0.8522\n",
      "Iter-35670, train loss-0.4417, acc-0.9200, valid loss-0.5465, acc-0.8536, test loss-0.5498, acc-0.8523\n",
      "Iter-35680, train loss-0.5929, acc-0.8000, valid loss-0.5464, acc-0.8536, test loss-0.5497, acc-0.8522\n",
      "Iter-35690, train loss-0.4727, acc-0.8800, valid loss-0.5464, acc-0.8536, test loss-0.5496, acc-0.8523\n",
      "Iter-35700, train loss-0.5736, acc-0.8400, valid loss-0.5463, acc-0.8536, test loss-0.5496, acc-0.8522\n",
      "Iter-35710, train loss-0.5154, acc-0.8600, valid loss-0.5462, acc-0.8536, test loss-0.5495, acc-0.8521\n",
      "Iter-35720, train loss-0.6729, acc-0.8000, valid loss-0.5461, acc-0.8536, test loss-0.5494, acc-0.8520\n",
      "Iter-35730, train loss-0.6025, acc-0.8000, valid loss-0.5460, acc-0.8536, test loss-0.5493, acc-0.8521\n",
      "Iter-35740, train loss-0.5946, acc-0.8000, valid loss-0.5459, acc-0.8540, test loss-0.5492, acc-0.8522\n",
      "Iter-35750, train loss-0.4715, acc-0.8800, valid loss-0.5459, acc-0.8536, test loss-0.5492, acc-0.8521\n",
      "Iter-35760, train loss-0.5276, acc-0.8400, valid loss-0.5458, acc-0.8538, test loss-0.5491, acc-0.8522\n",
      "Iter-35770, train loss-0.5177, acc-0.8800, valid loss-0.5457, acc-0.8538, test loss-0.5490, acc-0.8521\n",
      "Iter-35780, train loss-0.5958, acc-0.7800, valid loss-0.5456, acc-0.8540, test loss-0.5489, acc-0.8521\n",
      "Iter-35790, train loss-0.4656, acc-0.8800, valid loss-0.5456, acc-0.8542, test loss-0.5488, acc-0.8522\n",
      "Iter-35800, train loss-0.3490, acc-0.9200, valid loss-0.5455, acc-0.8544, test loss-0.5488, acc-0.8521\n",
      "Iter-35810, train loss-0.5138, acc-0.8600, valid loss-0.5454, acc-0.8544, test loss-0.5487, acc-0.8522\n",
      "Iter-35820, train loss-0.4842, acc-0.9000, valid loss-0.5453, acc-0.8546, test loss-0.5486, acc-0.8523\n",
      "Iter-35830, train loss-0.8617, acc-0.7200, valid loss-0.5452, acc-0.8546, test loss-0.5485, acc-0.8522\n",
      "Iter-35840, train loss-0.5090, acc-0.8600, valid loss-0.5452, acc-0.8544, test loss-0.5484, acc-0.8522\n",
      "Iter-35850, train loss-0.5830, acc-0.8400, valid loss-0.5451, acc-0.8546, test loss-0.5484, acc-0.8523\n",
      "Iter-35860, train loss-0.6411, acc-0.8200, valid loss-0.5450, acc-0.8544, test loss-0.5483, acc-0.8522\n",
      "Iter-35870, train loss-0.5413, acc-0.8400, valid loss-0.5449, acc-0.8546, test loss-0.5482, acc-0.8522\n",
      "Iter-35880, train loss-0.5235, acc-0.8200, valid loss-0.5448, acc-0.8550, test loss-0.5481, acc-0.8524\n",
      "Iter-35890, train loss-0.7204, acc-0.7800, valid loss-0.5447, acc-0.8550, test loss-0.5481, acc-0.8524\n",
      "Iter-35900, train loss-0.7135, acc-0.7800, valid loss-0.5446, acc-0.8550, test loss-0.5480, acc-0.8525\n",
      "Iter-35910, train loss-0.7857, acc-0.7600, valid loss-0.5446, acc-0.8550, test loss-0.5479, acc-0.8524\n",
      "Iter-35920, train loss-0.5800, acc-0.8200, valid loss-0.5445, acc-0.8552, test loss-0.5478, acc-0.8525\n",
      "Iter-35930, train loss-0.7297, acc-0.8000, valid loss-0.5444, acc-0.8552, test loss-0.5477, acc-0.8525\n",
      "Iter-35940, train loss-0.8887, acc-0.7400, valid loss-0.5443, acc-0.8552, test loss-0.5476, acc-0.8526\n",
      "Iter-35950, train loss-0.5038, acc-0.9000, valid loss-0.5442, acc-0.8552, test loss-0.5475, acc-0.8526\n",
      "Iter-35960, train loss-0.5866, acc-0.8800, valid loss-0.5441, acc-0.8552, test loss-0.5474, acc-0.8526\n",
      "Iter-35970, train loss-0.5374, acc-0.8000, valid loss-0.5441, acc-0.8552, test loss-0.5474, acc-0.8526\n",
      "Iter-35980, train loss-0.5395, acc-0.8800, valid loss-0.5440, acc-0.8552, test loss-0.5473, acc-0.8526\n",
      "Iter-35990, train loss-0.4881, acc-0.8800, valid loss-0.5439, acc-0.8552, test loss-0.5472, acc-0.8526\n",
      "Iter-36000, train loss-0.6678, acc-0.8000, valid loss-0.5439, acc-0.8552, test loss-0.5472, acc-0.8527\n",
      "Iter-36010, train loss-0.5716, acc-0.8200, valid loss-0.5438, acc-0.8552, test loss-0.5471, acc-0.8527\n",
      "Iter-36020, train loss-0.5998, acc-0.7800, valid loss-0.5437, acc-0.8552, test loss-0.5470, acc-0.8528\n",
      "Iter-36030, train loss-0.5978, acc-0.8200, valid loss-0.5436, acc-0.8554, test loss-0.5469, acc-0.8528\n",
      "Iter-36040, train loss-0.5962, acc-0.8600, valid loss-0.5435, acc-0.8554, test loss-0.5468, acc-0.8526\n",
      "Iter-36050, train loss-0.6517, acc-0.8200, valid loss-0.5435, acc-0.8554, test loss-0.5467, acc-0.8527\n",
      "Iter-36060, train loss-0.5819, acc-0.8400, valid loss-0.5434, acc-0.8554, test loss-0.5466, acc-0.8527\n",
      "Iter-36070, train loss-0.4198, acc-0.9000, valid loss-0.5433, acc-0.8554, test loss-0.5466, acc-0.8526\n",
      "Iter-36080, train loss-0.4257, acc-0.8800, valid loss-0.5432, acc-0.8554, test loss-0.5465, acc-0.8527\n",
      "Iter-36090, train loss-0.6215, acc-0.8200, valid loss-0.5431, acc-0.8554, test loss-0.5464, acc-0.8528\n",
      "Iter-36100, train loss-0.6531, acc-0.8000, valid loss-0.5431, acc-0.8554, test loss-0.5463, acc-0.8527\n",
      "Iter-36110, train loss-0.4430, acc-0.9400, valid loss-0.5430, acc-0.8554, test loss-0.5462, acc-0.8526\n",
      "Iter-36120, train loss-0.4433, acc-0.9200, valid loss-0.5429, acc-0.8554, test loss-0.5462, acc-0.8527\n",
      "Iter-36130, train loss-0.4181, acc-0.9200, valid loss-0.5428, acc-0.8554, test loss-0.5461, acc-0.8526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-36140, train loss-0.6141, acc-0.7600, valid loss-0.5427, acc-0.8554, test loss-0.5460, acc-0.8525\n",
      "Iter-36150, train loss-0.6217, acc-0.7800, valid loss-0.5426, acc-0.8554, test loss-0.5459, acc-0.8526\n",
      "Iter-36160, train loss-0.8417, acc-0.8000, valid loss-0.5425, acc-0.8554, test loss-0.5458, acc-0.8526\n",
      "Iter-36170, train loss-0.6548, acc-0.8000, valid loss-0.5425, acc-0.8554, test loss-0.5457, acc-0.8527\n",
      "Iter-36180, train loss-0.6178, acc-0.8400, valid loss-0.5424, acc-0.8554, test loss-0.5457, acc-0.8527\n",
      "Iter-36190, train loss-0.4876, acc-0.9200, valid loss-0.5423, acc-0.8554, test loss-0.5456, acc-0.8527\n",
      "Iter-36200, train loss-0.5884, acc-0.8600, valid loss-0.5422, acc-0.8554, test loss-0.5455, acc-0.8527\n",
      "Iter-36210, train loss-0.4602, acc-0.8600, valid loss-0.5421, acc-0.8552, test loss-0.5454, acc-0.8527\n",
      "Iter-36220, train loss-0.6307, acc-0.8200, valid loss-0.5420, acc-0.8552, test loss-0.5453, acc-0.8525\n",
      "Iter-36230, train loss-0.5493, acc-0.8200, valid loss-0.5420, acc-0.8550, test loss-0.5453, acc-0.8528\n",
      "Iter-36240, train loss-0.7353, acc-0.7000, valid loss-0.5419, acc-0.8548, test loss-0.5452, acc-0.8526\n",
      "Iter-36250, train loss-0.5294, acc-0.8400, valid loss-0.5418, acc-0.8548, test loss-0.5451, acc-0.8527\n",
      "Iter-36260, train loss-0.6567, acc-0.8600, valid loss-0.5417, acc-0.8550, test loss-0.5450, acc-0.8527\n",
      "Iter-36270, train loss-0.3985, acc-0.8800, valid loss-0.5416, acc-0.8550, test loss-0.5449, acc-0.8527\n",
      "Iter-36280, train loss-0.3593, acc-0.9000, valid loss-0.5416, acc-0.8550, test loss-0.5448, acc-0.8527\n",
      "Iter-36290, train loss-0.5590, acc-0.8800, valid loss-0.5415, acc-0.8550, test loss-0.5448, acc-0.8526\n",
      "Iter-36300, train loss-0.4494, acc-0.9200, valid loss-0.5414, acc-0.8550, test loss-0.5447, acc-0.8526\n",
      "Iter-36310, train loss-0.5843, acc-0.8800, valid loss-0.5413, acc-0.8550, test loss-0.5446, acc-0.8526\n",
      "Iter-36320, train loss-0.3949, acc-0.9000, valid loss-0.5412, acc-0.8550, test loss-0.5445, acc-0.8527\n",
      "Iter-36330, train loss-0.6625, acc-0.8000, valid loss-0.5411, acc-0.8552, test loss-0.5444, acc-0.8526\n",
      "Iter-36340, train loss-0.4717, acc-0.8800, valid loss-0.5411, acc-0.8554, test loss-0.5443, acc-0.8527\n",
      "Iter-36350, train loss-0.5280, acc-0.8800, valid loss-0.5410, acc-0.8556, test loss-0.5442, acc-0.8528\n",
      "Iter-36360, train loss-0.4878, acc-0.9000, valid loss-0.5409, acc-0.8554, test loss-0.5441, acc-0.8528\n",
      "Iter-36370, train loss-0.4922, acc-0.9200, valid loss-0.5408, acc-0.8556, test loss-0.5441, acc-0.8529\n",
      "Iter-36380, train loss-0.5665, acc-0.8800, valid loss-0.5408, acc-0.8556, test loss-0.5440, acc-0.8529\n",
      "Iter-36390, train loss-0.5510, acc-0.8200, valid loss-0.5407, acc-0.8556, test loss-0.5439, acc-0.8529\n",
      "Iter-36400, train loss-0.4639, acc-0.8800, valid loss-0.5406, acc-0.8556, test loss-0.5439, acc-0.8530\n",
      "Iter-36410, train loss-0.5242, acc-0.8200, valid loss-0.5405, acc-0.8556, test loss-0.5438, acc-0.8530\n",
      "Iter-36420, train loss-0.5938, acc-0.7600, valid loss-0.5405, acc-0.8556, test loss-0.5437, acc-0.8529\n",
      "Iter-36430, train loss-0.6361, acc-0.8000, valid loss-0.5404, acc-0.8556, test loss-0.5436, acc-0.8529\n",
      "Iter-36440, train loss-0.5595, acc-0.8600, valid loss-0.5403, acc-0.8556, test loss-0.5436, acc-0.8529\n",
      "Iter-36450, train loss-0.5350, acc-0.8800, valid loss-0.5402, acc-0.8558, test loss-0.5435, acc-0.8528\n",
      "Iter-36460, train loss-0.4358, acc-0.8800, valid loss-0.5401, acc-0.8558, test loss-0.5434, acc-0.8528\n",
      "Iter-36470, train loss-0.3717, acc-0.9200, valid loss-0.5401, acc-0.8558, test loss-0.5433, acc-0.8528\n",
      "Iter-36480, train loss-0.5934, acc-0.8600, valid loss-0.5400, acc-0.8558, test loss-0.5432, acc-0.8528\n",
      "Iter-36490, train loss-0.4602, acc-0.8600, valid loss-0.5399, acc-0.8558, test loss-0.5431, acc-0.8528\n",
      "Iter-36500, train loss-0.5385, acc-0.8400, valid loss-0.5398, acc-0.8556, test loss-0.5431, acc-0.8528\n",
      "Iter-36510, train loss-0.6881, acc-0.8400, valid loss-0.5397, acc-0.8556, test loss-0.5430, acc-0.8530\n",
      "Iter-36520, train loss-0.7330, acc-0.8200, valid loss-0.5396, acc-0.8556, test loss-0.5429, acc-0.8530\n",
      "Iter-36530, train loss-0.5683, acc-0.7800, valid loss-0.5395, acc-0.8556, test loss-0.5428, acc-0.8530\n",
      "Iter-36540, train loss-0.5916, acc-0.8400, valid loss-0.5395, acc-0.8556, test loss-0.5427, acc-0.8531\n",
      "Iter-36550, train loss-0.6086, acc-0.8400, valid loss-0.5394, acc-0.8558, test loss-0.5427, acc-0.8532\n",
      "Iter-36560, train loss-0.5813, acc-0.8800, valid loss-0.5393, acc-0.8560, test loss-0.5426, acc-0.8532\n",
      "Iter-36570, train loss-0.4076, acc-0.8800, valid loss-0.5392, acc-0.8558, test loss-0.5425, acc-0.8533\n",
      "Iter-36580, train loss-0.4658, acc-0.9200, valid loss-0.5391, acc-0.8560, test loss-0.5424, acc-0.8533\n",
      "Iter-36590, train loss-0.6604, acc-0.8400, valid loss-0.5391, acc-0.8560, test loss-0.5423, acc-0.8531\n",
      "Iter-36600, train loss-0.5518, acc-0.8600, valid loss-0.5390, acc-0.8560, test loss-0.5423, acc-0.8532\n",
      "Iter-36610, train loss-0.4519, acc-0.9200, valid loss-0.5389, acc-0.8560, test loss-0.5422, acc-0.8531\n",
      "Iter-36620, train loss-0.7563, acc-0.8000, valid loss-0.5389, acc-0.8560, test loss-0.5421, acc-0.8532\n",
      "Iter-36630, train loss-0.5072, acc-0.8400, valid loss-0.5388, acc-0.8560, test loss-0.5421, acc-0.8531\n",
      "Iter-36640, train loss-0.5709, acc-0.8400, valid loss-0.5387, acc-0.8560, test loss-0.5420, acc-0.8531\n",
      "Iter-36650, train loss-0.5002, acc-0.8600, valid loss-0.5386, acc-0.8560, test loss-0.5419, acc-0.8533\n",
      "Iter-36660, train loss-0.7021, acc-0.7800, valid loss-0.5385, acc-0.8558, test loss-0.5418, acc-0.8534\n",
      "Iter-36670, train loss-0.6696, acc-0.8000, valid loss-0.5385, acc-0.8558, test loss-0.5417, acc-0.8536\n",
      "Iter-36680, train loss-0.6190, acc-0.8600, valid loss-0.5384, acc-0.8558, test loss-0.5417, acc-0.8536\n",
      "Iter-36690, train loss-0.3797, acc-0.9400, valid loss-0.5383, acc-0.8560, test loss-0.5416, acc-0.8536\n",
      "Iter-36700, train loss-0.5412, acc-0.8600, valid loss-0.5382, acc-0.8562, test loss-0.5415, acc-0.8536\n",
      "Iter-36710, train loss-0.4670, acc-0.8800, valid loss-0.5382, acc-0.8562, test loss-0.5414, acc-0.8536\n",
      "Iter-36720, train loss-0.4898, acc-0.8800, valid loss-0.5381, acc-0.8560, test loss-0.5414, acc-0.8534\n",
      "Iter-36730, train loss-0.5782, acc-0.8200, valid loss-0.5380, acc-0.8562, test loss-0.5413, acc-0.8535\n",
      "Iter-36740, train loss-0.4122, acc-0.8600, valid loss-0.5379, acc-0.8560, test loss-0.5412, acc-0.8535\n",
      "Iter-36750, train loss-0.7292, acc-0.8000, valid loss-0.5379, acc-0.8560, test loss-0.5411, acc-0.8536\n",
      "Iter-36760, train loss-0.6551, acc-0.8800, valid loss-0.5378, acc-0.8562, test loss-0.5411, acc-0.8535\n",
      "Iter-36770, train loss-0.3997, acc-0.8800, valid loss-0.5377, acc-0.8562, test loss-0.5410, acc-0.8535\n",
      "Iter-36780, train loss-0.3940, acc-0.9200, valid loss-0.5376, acc-0.8562, test loss-0.5409, acc-0.8536\n",
      "Iter-36790, train loss-0.8366, acc-0.8000, valid loss-0.5375, acc-0.8562, test loss-0.5408, acc-0.8537\n",
      "Iter-36800, train loss-0.5011, acc-0.8800, valid loss-0.5375, acc-0.8564, test loss-0.5407, acc-0.8534\n",
      "Iter-36810, train loss-0.4888, acc-0.8600, valid loss-0.5374, acc-0.8564, test loss-0.5407, acc-0.8536\n",
      "Iter-36820, train loss-0.5677, acc-0.8000, valid loss-0.5373, acc-0.8562, test loss-0.5406, acc-0.8535\n",
      "Iter-36830, train loss-0.6965, acc-0.8400, valid loss-0.5372, acc-0.8560, test loss-0.5405, acc-0.8535\n",
      "Iter-36840, train loss-0.6213, acc-0.7600, valid loss-0.5371, acc-0.8560, test loss-0.5404, acc-0.8535\n",
      "Iter-36850, train loss-0.4146, acc-0.9200, valid loss-0.5371, acc-0.8560, test loss-0.5403, acc-0.8535\n",
      "Iter-36860, train loss-0.6197, acc-0.8200, valid loss-0.5370, acc-0.8562, test loss-0.5402, acc-0.8535\n",
      "Iter-36870, train loss-0.4368, acc-0.9000, valid loss-0.5369, acc-0.8562, test loss-0.5402, acc-0.8538\n",
      "Iter-36880, train loss-0.6217, acc-0.8000, valid loss-0.5368, acc-0.8560, test loss-0.5401, acc-0.8538\n",
      "Iter-36890, train loss-0.6109, acc-0.8200, valid loss-0.5367, acc-0.8560, test loss-0.5400, acc-0.8538\n",
      "Iter-36900, train loss-0.5645, acc-0.8400, valid loss-0.5367, acc-0.8560, test loss-0.5399, acc-0.8541\n",
      "Iter-36910, train loss-0.6539, acc-0.8400, valid loss-0.5366, acc-0.8560, test loss-0.5399, acc-0.8540\n",
      "Iter-36920, train loss-0.3484, acc-0.9200, valid loss-0.5365, acc-0.8560, test loss-0.5398, acc-0.8540\n",
      "Iter-36930, train loss-0.4718, acc-0.8600, valid loss-0.5364, acc-0.8560, test loss-0.5397, acc-0.8540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-36940, train loss-0.5211, acc-0.8000, valid loss-0.5363, acc-0.8560, test loss-0.5396, acc-0.8539\n",
      "Iter-36950, train loss-0.4803, acc-0.9000, valid loss-0.5363, acc-0.8560, test loss-0.5396, acc-0.8540\n",
      "Iter-36960, train loss-0.9109, acc-0.8000, valid loss-0.5362, acc-0.8558, test loss-0.5395, acc-0.8540\n",
      "Iter-36970, train loss-0.6656, acc-0.8400, valid loss-0.5361, acc-0.8560, test loss-0.5394, acc-0.8542\n",
      "Iter-36980, train loss-0.4951, acc-0.8800, valid loss-0.5361, acc-0.8562, test loss-0.5393, acc-0.8541\n",
      "Iter-36990, train loss-0.4956, acc-0.8600, valid loss-0.5360, acc-0.8560, test loss-0.5392, acc-0.8542\n",
      "Iter-37000, train loss-0.5368, acc-0.8400, valid loss-0.5359, acc-0.8560, test loss-0.5392, acc-0.8542\n",
      "Iter-37010, train loss-0.4608, acc-0.9000, valid loss-0.5358, acc-0.8560, test loss-0.5391, acc-0.8542\n",
      "Iter-37020, train loss-0.7271, acc-0.8000, valid loss-0.5357, acc-0.8560, test loss-0.5390, acc-0.8542\n",
      "Iter-37030, train loss-0.5327, acc-0.8600, valid loss-0.5357, acc-0.8560, test loss-0.5389, acc-0.8542\n",
      "Iter-37040, train loss-0.6954, acc-0.8400, valid loss-0.5356, acc-0.8560, test loss-0.5388, acc-0.8542\n",
      "Iter-37050, train loss-0.6356, acc-0.8200, valid loss-0.5355, acc-0.8560, test loss-0.5388, acc-0.8542\n",
      "Iter-37060, train loss-0.4862, acc-0.8200, valid loss-0.5354, acc-0.8560, test loss-0.5387, acc-0.8542\n",
      "Iter-37070, train loss-0.4637, acc-0.8400, valid loss-0.5353, acc-0.8560, test loss-0.5386, acc-0.8542\n",
      "Iter-37080, train loss-0.6185, acc-0.8200, valid loss-0.5352, acc-0.8560, test loss-0.5385, acc-0.8543\n",
      "Iter-37090, train loss-0.6000, acc-0.8800, valid loss-0.5352, acc-0.8560, test loss-0.5384, acc-0.8542\n",
      "Iter-37100, train loss-0.4585, acc-0.8600, valid loss-0.5351, acc-0.8560, test loss-0.5383, acc-0.8543\n",
      "Iter-37110, train loss-0.5234, acc-0.8600, valid loss-0.5350, acc-0.8560, test loss-0.5382, acc-0.8543\n",
      "Iter-37120, train loss-0.4369, acc-0.9000, valid loss-0.5349, acc-0.8560, test loss-0.5381, acc-0.8543\n",
      "Iter-37130, train loss-0.5590, acc-0.8200, valid loss-0.5348, acc-0.8560, test loss-0.5381, acc-0.8542\n",
      "Iter-37140, train loss-0.5303, acc-0.8600, valid loss-0.5347, acc-0.8560, test loss-0.5380, acc-0.8542\n",
      "Iter-37150, train loss-0.3666, acc-0.9000, valid loss-0.5347, acc-0.8560, test loss-0.5379, acc-0.8543\n",
      "Iter-37160, train loss-0.4690, acc-0.9400, valid loss-0.5346, acc-0.8560, test loss-0.5378, acc-0.8543\n",
      "Iter-37170, train loss-0.5604, acc-0.8400, valid loss-0.5345, acc-0.8560, test loss-0.5378, acc-0.8543\n",
      "Iter-37180, train loss-0.6617, acc-0.8000, valid loss-0.5344, acc-0.8562, test loss-0.5377, acc-0.8543\n",
      "Iter-37190, train loss-0.4503, acc-0.9200, valid loss-0.5343, acc-0.8560, test loss-0.5376, acc-0.8543\n",
      "Iter-37200, train loss-0.7205, acc-0.8000, valid loss-0.5343, acc-0.8560, test loss-0.5375, acc-0.8544\n",
      "Iter-37210, train loss-0.8623, acc-0.7200, valid loss-0.5342, acc-0.8562, test loss-0.5374, acc-0.8544\n",
      "Iter-37220, train loss-0.3453, acc-0.9400, valid loss-0.5341, acc-0.8562, test loss-0.5373, acc-0.8544\n",
      "Iter-37230, train loss-0.7822, acc-0.7400, valid loss-0.5340, acc-0.8562, test loss-0.5373, acc-0.8544\n",
      "Iter-37240, train loss-0.4304, acc-0.9000, valid loss-0.5339, acc-0.8562, test loss-0.5372, acc-0.8544\n",
      "Iter-37250, train loss-0.4393, acc-0.9200, valid loss-0.5339, acc-0.8562, test loss-0.5371, acc-0.8544\n",
      "Iter-37260, train loss-0.4622, acc-0.8400, valid loss-0.5338, acc-0.8562, test loss-0.5371, acc-0.8544\n",
      "Iter-37270, train loss-0.3977, acc-0.9200, valid loss-0.5338, acc-0.8562, test loss-0.5370, acc-0.8543\n",
      "Iter-37280, train loss-0.4440, acc-0.8800, valid loss-0.5337, acc-0.8562, test loss-0.5369, acc-0.8543\n",
      "Iter-37290, train loss-0.8001, acc-0.7600, valid loss-0.5336, acc-0.8562, test loss-0.5369, acc-0.8543\n",
      "Iter-37300, train loss-0.5672, acc-0.8200, valid loss-0.5335, acc-0.8562, test loss-0.5368, acc-0.8545\n",
      "Iter-37310, train loss-0.4408, acc-0.9200, valid loss-0.5334, acc-0.8562, test loss-0.5367, acc-0.8544\n",
      "Iter-37320, train loss-0.4223, acc-0.8800, valid loss-0.5334, acc-0.8560, test loss-0.5367, acc-0.8543\n",
      "Iter-37330, train loss-0.5113, acc-0.8400, valid loss-0.5333, acc-0.8560, test loss-0.5366, acc-0.8545\n",
      "Iter-37340, train loss-0.3651, acc-0.9400, valid loss-0.5332, acc-0.8560, test loss-0.5365, acc-0.8545\n",
      "Iter-37350, train loss-0.5209, acc-0.8400, valid loss-0.5331, acc-0.8560, test loss-0.5364, acc-0.8546\n",
      "Iter-37360, train loss-0.5502, acc-0.8800, valid loss-0.5330, acc-0.8560, test loss-0.5363, acc-0.8546\n",
      "Iter-37370, train loss-0.4378, acc-0.8800, valid loss-0.5329, acc-0.8560, test loss-0.5363, acc-0.8547\n",
      "Iter-37380, train loss-0.7151, acc-0.8000, valid loss-0.5329, acc-0.8560, test loss-0.5362, acc-0.8548\n",
      "Iter-37390, train loss-0.6049, acc-0.8600, valid loss-0.5328, acc-0.8560, test loss-0.5361, acc-0.8549\n",
      "Iter-37400, train loss-0.3375, acc-0.9000, valid loss-0.5327, acc-0.8560, test loss-0.5360, acc-0.8550\n",
      "Iter-37410, train loss-0.8254, acc-0.7200, valid loss-0.5326, acc-0.8562, test loss-0.5359, acc-0.8550\n",
      "Iter-37420, train loss-0.6004, acc-0.8400, valid loss-0.5326, acc-0.8562, test loss-0.5359, acc-0.8550\n",
      "Iter-37430, train loss-0.7370, acc-0.8200, valid loss-0.5325, acc-0.8560, test loss-0.5358, acc-0.8551\n",
      "Iter-37440, train loss-0.3077, acc-0.9200, valid loss-0.5324, acc-0.8560, test loss-0.5357, acc-0.8550\n",
      "Iter-37450, train loss-0.6115, acc-0.8200, valid loss-0.5323, acc-0.8564, test loss-0.5357, acc-0.8551\n",
      "Iter-37460, train loss-0.5936, acc-0.8800, valid loss-0.5323, acc-0.8566, test loss-0.5356, acc-0.8551\n",
      "Iter-37470, train loss-0.5847, acc-0.8400, valid loss-0.5322, acc-0.8564, test loss-0.5355, acc-0.8551\n",
      "Iter-37480, train loss-0.6587, acc-0.8200, valid loss-0.5321, acc-0.8564, test loss-0.5355, acc-0.8550\n",
      "Iter-37490, train loss-0.5692, acc-0.7800, valid loss-0.5320, acc-0.8564, test loss-0.5354, acc-0.8548\n",
      "Iter-37500, train loss-0.5615, acc-0.8600, valid loss-0.5319, acc-0.8564, test loss-0.5353, acc-0.8550\n",
      "Iter-37510, train loss-0.4004, acc-0.8600, valid loss-0.5319, acc-0.8564, test loss-0.5352, acc-0.8549\n",
      "Iter-37520, train loss-0.7821, acc-0.8000, valid loss-0.5318, acc-0.8564, test loss-0.5351, acc-0.8549\n",
      "Iter-37530, train loss-0.5032, acc-0.8400, valid loss-0.5317, acc-0.8564, test loss-0.5351, acc-0.8548\n",
      "Iter-37540, train loss-0.7168, acc-0.8200, valid loss-0.5316, acc-0.8564, test loss-0.5350, acc-0.8549\n",
      "Iter-37550, train loss-0.5173, acc-0.8600, valid loss-0.5315, acc-0.8564, test loss-0.5349, acc-0.8550\n",
      "Iter-37560, train loss-0.4357, acc-0.8800, valid loss-0.5315, acc-0.8564, test loss-0.5348, acc-0.8550\n",
      "Iter-37570, train loss-0.7473, acc-0.8600, valid loss-0.5314, acc-0.8564, test loss-0.5348, acc-0.8550\n",
      "Iter-37580, train loss-0.6260, acc-0.7800, valid loss-0.5313, acc-0.8564, test loss-0.5347, acc-0.8550\n",
      "Iter-37590, train loss-0.6098, acc-0.8600, valid loss-0.5312, acc-0.8564, test loss-0.5346, acc-0.8548\n",
      "Iter-37600, train loss-0.4921, acc-0.8800, valid loss-0.5312, acc-0.8564, test loss-0.5346, acc-0.8548\n",
      "Iter-37610, train loss-0.5237, acc-0.8600, valid loss-0.5311, acc-0.8564, test loss-0.5345, acc-0.8548\n",
      "Iter-37620, train loss-0.6394, acc-0.8000, valid loss-0.5310, acc-0.8564, test loss-0.5344, acc-0.8549\n",
      "Iter-37630, train loss-0.5925, acc-0.8200, valid loss-0.5309, acc-0.8564, test loss-0.5343, acc-0.8551\n",
      "Iter-37640, train loss-0.5455, acc-0.8600, valid loss-0.5309, acc-0.8564, test loss-0.5343, acc-0.8550\n",
      "Iter-37650, train loss-0.6501, acc-0.8600, valid loss-0.5308, acc-0.8564, test loss-0.5342, acc-0.8552\n",
      "Iter-37660, train loss-0.4076, acc-0.8800, valid loss-0.5307, acc-0.8564, test loss-0.5341, acc-0.8552\n",
      "Iter-37670, train loss-0.5452, acc-0.8200, valid loss-0.5306, acc-0.8564, test loss-0.5340, acc-0.8552\n",
      "Iter-37680, train loss-0.4068, acc-0.9000, valid loss-0.5306, acc-0.8564, test loss-0.5340, acc-0.8551\n",
      "Iter-37690, train loss-0.7680, acc-0.7800, valid loss-0.5305, acc-0.8564, test loss-0.5339, acc-0.8551\n",
      "Iter-37700, train loss-0.4442, acc-0.9400, valid loss-0.5304, acc-0.8566, test loss-0.5338, acc-0.8551\n",
      "Iter-37710, train loss-0.5114, acc-0.8800, valid loss-0.5304, acc-0.8566, test loss-0.5337, acc-0.8551\n",
      "Iter-37720, train loss-0.4796, acc-0.9000, valid loss-0.5303, acc-0.8566, test loss-0.5336, acc-0.8551\n",
      "Iter-37730, train loss-0.4854, acc-0.9000, valid loss-0.5302, acc-0.8566, test loss-0.5336, acc-0.8550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-37740, train loss-0.4674, acc-0.8600, valid loss-0.5302, acc-0.8566, test loss-0.5335, acc-0.8552\n",
      "Iter-37750, train loss-0.6082, acc-0.8200, valid loss-0.5301, acc-0.8566, test loss-0.5334, acc-0.8552\n",
      "Iter-37760, train loss-0.5686, acc-0.8200, valid loss-0.5300, acc-0.8566, test loss-0.5333, acc-0.8551\n",
      "Iter-37770, train loss-0.5740, acc-0.8400, valid loss-0.5299, acc-0.8568, test loss-0.5332, acc-0.8552\n",
      "Iter-37780, train loss-0.3651, acc-0.9200, valid loss-0.5298, acc-0.8568, test loss-0.5332, acc-0.8552\n",
      "Iter-37790, train loss-0.5986, acc-0.8000, valid loss-0.5298, acc-0.8568, test loss-0.5331, acc-0.8552\n",
      "Iter-37800, train loss-0.4717, acc-0.9200, valid loss-0.5297, acc-0.8568, test loss-0.5330, acc-0.8552\n",
      "Iter-37810, train loss-0.7175, acc-0.7600, valid loss-0.5296, acc-0.8570, test loss-0.5330, acc-0.8553\n",
      "Iter-37820, train loss-0.5366, acc-0.9000, valid loss-0.5296, acc-0.8570, test loss-0.5329, acc-0.8552\n",
      "Iter-37830, train loss-0.3822, acc-0.9400, valid loss-0.5295, acc-0.8570, test loss-0.5328, acc-0.8552\n",
      "Iter-37840, train loss-0.4651, acc-0.9000, valid loss-0.5294, acc-0.8568, test loss-0.5327, acc-0.8551\n",
      "Iter-37850, train loss-0.6042, acc-0.8400, valid loss-0.5293, acc-0.8568, test loss-0.5326, acc-0.8551\n",
      "Iter-37860, train loss-0.5010, acc-0.8800, valid loss-0.5293, acc-0.8568, test loss-0.5326, acc-0.8553\n",
      "Iter-37870, train loss-0.5719, acc-0.8600, valid loss-0.5292, acc-0.8568, test loss-0.5325, acc-0.8553\n",
      "Iter-37880, train loss-0.7682, acc-0.8200, valid loss-0.5291, acc-0.8570, test loss-0.5324, acc-0.8555\n",
      "Iter-37890, train loss-0.5961, acc-0.8400, valid loss-0.5290, acc-0.8570, test loss-0.5323, acc-0.8555\n",
      "Iter-37900, train loss-0.4787, acc-0.9000, valid loss-0.5290, acc-0.8572, test loss-0.5323, acc-0.8554\n",
      "Iter-37910, train loss-0.8092, acc-0.7600, valid loss-0.5289, acc-0.8572, test loss-0.5322, acc-0.8554\n",
      "Iter-37920, train loss-0.4739, acc-0.9200, valid loss-0.5289, acc-0.8572, test loss-0.5321, acc-0.8555\n",
      "Iter-37930, train loss-0.5897, acc-0.7600, valid loss-0.5288, acc-0.8572, test loss-0.5321, acc-0.8554\n",
      "Iter-37940, train loss-0.3922, acc-0.9200, valid loss-0.5287, acc-0.8572, test loss-0.5320, acc-0.8554\n",
      "Iter-37950, train loss-0.3979, acc-0.9200, valid loss-0.5286, acc-0.8572, test loss-0.5319, acc-0.8554\n",
      "Iter-37960, train loss-0.3724, acc-0.9400, valid loss-0.5285, acc-0.8572, test loss-0.5319, acc-0.8552\n",
      "Iter-37970, train loss-0.3879, acc-0.9200, valid loss-0.5285, acc-0.8570, test loss-0.5318, acc-0.8552\n",
      "Iter-37980, train loss-0.5893, acc-0.7800, valid loss-0.5284, acc-0.8570, test loss-0.5317, acc-0.8553\n",
      "Iter-37990, train loss-0.5230, acc-0.8400, valid loss-0.5283, acc-0.8570, test loss-0.5316, acc-0.8555\n",
      "Iter-38000, train loss-0.5811, acc-0.8200, valid loss-0.5282, acc-0.8572, test loss-0.5316, acc-0.8555\n",
      "Iter-38010, train loss-0.5415, acc-0.7600, valid loss-0.5282, acc-0.8574, test loss-0.5315, acc-0.8555\n",
      "Iter-38020, train loss-0.6900, acc-0.8200, valid loss-0.5281, acc-0.8574, test loss-0.5314, acc-0.8555\n",
      "Iter-38030, train loss-0.4948, acc-0.9200, valid loss-0.5280, acc-0.8574, test loss-0.5313, acc-0.8555\n",
      "Iter-38040, train loss-0.4909, acc-0.8600, valid loss-0.5279, acc-0.8574, test loss-0.5313, acc-0.8555\n",
      "Iter-38050, train loss-0.6843, acc-0.8400, valid loss-0.5278, acc-0.8574, test loss-0.5312, acc-0.8555\n",
      "Iter-38060, train loss-0.5616, acc-0.7800, valid loss-0.5278, acc-0.8574, test loss-0.5311, acc-0.8555\n",
      "Iter-38070, train loss-0.5526, acc-0.8600, valid loss-0.5277, acc-0.8574, test loss-0.5310, acc-0.8555\n",
      "Iter-38080, train loss-0.5140, acc-0.8800, valid loss-0.5276, acc-0.8574, test loss-0.5310, acc-0.8556\n",
      "Iter-38090, train loss-0.7050, acc-0.8400, valid loss-0.5275, acc-0.8574, test loss-0.5309, acc-0.8557\n",
      "Iter-38100, train loss-0.5744, acc-0.8400, valid loss-0.5274, acc-0.8574, test loss-0.5308, acc-0.8557\n",
      "Iter-38110, train loss-0.7607, acc-0.8200, valid loss-0.5273, acc-0.8574, test loss-0.5307, acc-0.8555\n",
      "Iter-38120, train loss-0.4922, acc-0.9000, valid loss-0.5273, acc-0.8576, test loss-0.5306, acc-0.8557\n",
      "Iter-38130, train loss-0.4535, acc-0.9200, valid loss-0.5272, acc-0.8578, test loss-0.5306, acc-0.8556\n",
      "Iter-38140, train loss-0.5635, acc-0.8400, valid loss-0.5271, acc-0.8578, test loss-0.5304, acc-0.8556\n",
      "Iter-38150, train loss-0.5145, acc-0.8400, valid loss-0.5270, acc-0.8578, test loss-0.5304, acc-0.8556\n",
      "Iter-38160, train loss-0.5762, acc-0.8800, valid loss-0.5270, acc-0.8578, test loss-0.5303, acc-0.8557\n",
      "Iter-38170, train loss-0.6435, acc-0.8400, valid loss-0.5269, acc-0.8578, test loss-0.5302, acc-0.8557\n",
      "Iter-38180, train loss-0.4008, acc-0.8800, valid loss-0.5268, acc-0.8580, test loss-0.5302, acc-0.8557\n",
      "Iter-38190, train loss-0.3787, acc-0.9200, valid loss-0.5267, acc-0.8580, test loss-0.5301, acc-0.8556\n",
      "Iter-38200, train loss-0.5241, acc-0.8400, valid loss-0.5266, acc-0.8582, test loss-0.5300, acc-0.8557\n",
      "Iter-38210, train loss-0.3776, acc-0.9200, valid loss-0.5266, acc-0.8582, test loss-0.5299, acc-0.8557\n",
      "Iter-38220, train loss-0.4753, acc-0.8800, valid loss-0.5265, acc-0.8582, test loss-0.5299, acc-0.8558\n",
      "Iter-38230, train loss-0.5264, acc-0.8600, valid loss-0.5264, acc-0.8582, test loss-0.5298, acc-0.8557\n",
      "Iter-38240, train loss-0.5383, acc-0.8600, valid loss-0.5263, acc-0.8582, test loss-0.5297, acc-0.8557\n",
      "Iter-38250, train loss-0.5143, acc-0.8400, valid loss-0.5263, acc-0.8584, test loss-0.5297, acc-0.8559\n",
      "Iter-38260, train loss-0.5577, acc-0.8000, valid loss-0.5262, acc-0.8582, test loss-0.5296, acc-0.8561\n",
      "Iter-38270, train loss-0.4052, acc-0.8800, valid loss-0.5261, acc-0.8582, test loss-0.5295, acc-0.8561\n",
      "Iter-38280, train loss-0.3935, acc-0.9200, valid loss-0.5260, acc-0.8582, test loss-0.5294, acc-0.8561\n",
      "Iter-38290, train loss-0.5320, acc-0.8400, valid loss-0.5259, acc-0.8582, test loss-0.5293, acc-0.8561\n",
      "Iter-38300, train loss-0.5525, acc-0.8200, valid loss-0.5259, acc-0.8582, test loss-0.5293, acc-0.8561\n",
      "Iter-38310, train loss-0.4580, acc-0.8400, valid loss-0.5258, acc-0.8582, test loss-0.5292, acc-0.8561\n",
      "Iter-38320, train loss-0.4678, acc-0.9200, valid loss-0.5257, acc-0.8582, test loss-0.5291, acc-0.8561\n",
      "Iter-38330, train loss-0.8075, acc-0.7800, valid loss-0.5257, acc-0.8582, test loss-0.5290, acc-0.8561\n",
      "Iter-38340, train loss-0.5293, acc-0.8800, valid loss-0.5256, acc-0.8582, test loss-0.5290, acc-0.8561\n",
      "Iter-38350, train loss-0.5743, acc-0.8400, valid loss-0.5255, acc-0.8582, test loss-0.5289, acc-0.8561\n",
      "Iter-38360, train loss-0.5113, acc-0.8600, valid loss-0.5254, acc-0.8582, test loss-0.5288, acc-0.8562\n",
      "Iter-38370, train loss-0.4353, acc-0.8800, valid loss-0.5254, acc-0.8582, test loss-0.5288, acc-0.8562\n",
      "Iter-38380, train loss-0.5078, acc-0.9000, valid loss-0.5253, acc-0.8582, test loss-0.5287, acc-0.8562\n",
      "Iter-38390, train loss-0.5596, acc-0.7600, valid loss-0.5252, acc-0.8582, test loss-0.5286, acc-0.8562\n",
      "Iter-38400, train loss-0.5791, acc-0.8400, valid loss-0.5251, acc-0.8582, test loss-0.5286, acc-0.8561\n",
      "Iter-38410, train loss-0.5662, acc-0.8400, valid loss-0.5251, acc-0.8584, test loss-0.5285, acc-0.8561\n",
      "Iter-38420, train loss-0.5540, acc-0.8400, valid loss-0.5250, acc-0.8586, test loss-0.5284, acc-0.8562\n",
      "Iter-38430, train loss-0.6515, acc-0.7600, valid loss-0.5249, acc-0.8586, test loss-0.5284, acc-0.8563\n",
      "Iter-38440, train loss-0.3843, acc-0.9600, valid loss-0.5249, acc-0.8586, test loss-0.5283, acc-0.8561\n",
      "Iter-38450, train loss-0.5155, acc-0.8400, valid loss-0.5248, acc-0.8586, test loss-0.5282, acc-0.8561\n",
      "Iter-38460, train loss-0.6033, acc-0.8200, valid loss-0.5247, acc-0.8586, test loss-0.5282, acc-0.8559\n",
      "Iter-38470, train loss-0.7372, acc-0.8000, valid loss-0.5247, acc-0.8586, test loss-0.5281, acc-0.8561\n",
      "Iter-38480, train loss-0.5612, acc-0.8600, valid loss-0.5246, acc-0.8586, test loss-0.5280, acc-0.8560\n",
      "Iter-38490, train loss-0.4360, acc-0.8400, valid loss-0.5245, acc-0.8586, test loss-0.5279, acc-0.8559\n",
      "Iter-38500, train loss-0.4522, acc-0.9000, valid loss-0.5244, acc-0.8586, test loss-0.5279, acc-0.8559\n",
      "Iter-38510, train loss-0.3738, acc-0.9000, valid loss-0.5243, acc-0.8586, test loss-0.5278, acc-0.8559\n",
      "Iter-38520, train loss-0.6071, acc-0.8200, valid loss-0.5243, acc-0.8586, test loss-0.5277, acc-0.8560\n",
      "Iter-38530, train loss-0.6640, acc-0.8000, valid loss-0.5242, acc-0.8586, test loss-0.5277, acc-0.8560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-38540, train loss-0.4897, acc-0.8400, valid loss-0.5241, acc-0.8586, test loss-0.5276, acc-0.8559\n",
      "Iter-38550, train loss-0.5616, acc-0.8600, valid loss-0.5241, acc-0.8586, test loss-0.5275, acc-0.8561\n",
      "Iter-38560, train loss-0.6654, acc-0.7600, valid loss-0.5240, acc-0.8586, test loss-0.5274, acc-0.8561\n",
      "Iter-38570, train loss-0.4933, acc-0.8400, valid loss-0.5239, acc-0.8586, test loss-0.5274, acc-0.8561\n",
      "Iter-38580, train loss-0.4615, acc-0.8800, valid loss-0.5239, acc-0.8586, test loss-0.5273, acc-0.8563\n",
      "Iter-38590, train loss-0.6427, acc-0.8200, valid loss-0.5238, acc-0.8586, test loss-0.5272, acc-0.8563\n",
      "Iter-38600, train loss-0.5446, acc-0.8200, valid loss-0.5238, acc-0.8586, test loss-0.5272, acc-0.8564\n",
      "Iter-38610, train loss-0.5636, acc-0.8200, valid loss-0.5237, acc-0.8588, test loss-0.5271, acc-0.8565\n",
      "Iter-38620, train loss-0.6140, acc-0.8600, valid loss-0.5236, acc-0.8586, test loss-0.5270, acc-0.8564\n",
      "Iter-38630, train loss-0.5729, acc-0.8600, valid loss-0.5236, acc-0.8586, test loss-0.5270, acc-0.8563\n",
      "Iter-38640, train loss-0.4111, acc-0.9000, valid loss-0.5235, acc-0.8586, test loss-0.5269, acc-0.8564\n",
      "Iter-38650, train loss-0.6997, acc-0.7800, valid loss-0.5234, acc-0.8588, test loss-0.5268, acc-0.8564\n",
      "Iter-38660, train loss-0.4846, acc-0.8600, valid loss-0.5233, acc-0.8590, test loss-0.5267, acc-0.8565\n",
      "Iter-38670, train loss-0.4723, acc-0.8400, valid loss-0.5232, acc-0.8590, test loss-0.5266, acc-0.8566\n",
      "Iter-38680, train loss-0.7548, acc-0.7800, valid loss-0.5232, acc-0.8590, test loss-0.5266, acc-0.8566\n",
      "Iter-38690, train loss-0.4428, acc-0.8600, valid loss-0.5231, acc-0.8590, test loss-0.5265, acc-0.8565\n",
      "Iter-38700, train loss-0.5880, acc-0.8000, valid loss-0.5230, acc-0.8590, test loss-0.5265, acc-0.8565\n",
      "Iter-38710, train loss-0.7031, acc-0.7800, valid loss-0.5230, acc-0.8590, test loss-0.5264, acc-0.8565\n",
      "Iter-38720, train loss-0.7385, acc-0.8400, valid loss-0.5229, acc-0.8590, test loss-0.5263, acc-0.8565\n",
      "Iter-38730, train loss-0.5988, acc-0.8400, valid loss-0.5228, acc-0.8590, test loss-0.5262, acc-0.8563\n",
      "Iter-38740, train loss-0.4273, acc-0.8600, valid loss-0.5227, acc-0.8590, test loss-0.5262, acc-0.8563\n",
      "Iter-38750, train loss-0.5387, acc-0.8400, valid loss-0.5226, acc-0.8590, test loss-0.5261, acc-0.8563\n",
      "Iter-38760, train loss-0.4970, acc-0.8600, valid loss-0.5225, acc-0.8592, test loss-0.5260, acc-0.8564\n",
      "Iter-38770, train loss-0.4797, acc-0.9000, valid loss-0.5225, acc-0.8590, test loss-0.5259, acc-0.8564\n",
      "Iter-38780, train loss-0.4932, acc-0.8400, valid loss-0.5224, acc-0.8592, test loss-0.5259, acc-0.8563\n",
      "Iter-38790, train loss-0.7036, acc-0.7600, valid loss-0.5224, acc-0.8592, test loss-0.5258, acc-0.8564\n",
      "Iter-38800, train loss-0.5285, acc-0.8600, valid loss-0.5223, acc-0.8592, test loss-0.5257, acc-0.8565\n",
      "Iter-38810, train loss-0.5869, acc-0.8800, valid loss-0.5222, acc-0.8590, test loss-0.5256, acc-0.8564\n",
      "Iter-38820, train loss-0.5843, acc-0.8200, valid loss-0.5222, acc-0.8592, test loss-0.5256, acc-0.8566\n",
      "Iter-38830, train loss-0.8814, acc-0.7400, valid loss-0.5221, acc-0.8592, test loss-0.5255, acc-0.8565\n",
      "Iter-38840, train loss-0.4489, acc-0.8400, valid loss-0.5220, acc-0.8592, test loss-0.5254, acc-0.8565\n",
      "Iter-38850, train loss-0.3514, acc-0.9600, valid loss-0.5220, acc-0.8596, test loss-0.5254, acc-0.8565\n",
      "Iter-38860, train loss-0.7700, acc-0.7800, valid loss-0.5219, acc-0.8594, test loss-0.5253, acc-0.8565\n",
      "Iter-38870, train loss-0.3432, acc-0.9200, valid loss-0.5219, acc-0.8594, test loss-0.5252, acc-0.8565\n",
      "Iter-38880, train loss-0.8002, acc-0.7200, valid loss-0.5218, acc-0.8594, test loss-0.5252, acc-0.8566\n",
      "Iter-38890, train loss-0.6608, acc-0.7800, valid loss-0.5217, acc-0.8592, test loss-0.5251, acc-0.8566\n",
      "Iter-38900, train loss-0.5554, acc-0.8400, valid loss-0.5216, acc-0.8592, test loss-0.5250, acc-0.8568\n",
      "Iter-38910, train loss-0.6124, acc-0.8400, valid loss-0.5216, acc-0.8592, test loss-0.5250, acc-0.8568\n",
      "Iter-38920, train loss-0.2689, acc-0.9600, valid loss-0.5215, acc-0.8596, test loss-0.5249, acc-0.8568\n",
      "Iter-38930, train loss-0.4508, acc-0.9000, valid loss-0.5215, acc-0.8594, test loss-0.5248, acc-0.8569\n",
      "Iter-38940, train loss-0.4997, acc-0.9000, valid loss-0.5214, acc-0.8594, test loss-0.5247, acc-0.8570\n",
      "Iter-38950, train loss-0.7108, acc-0.8200, valid loss-0.5213, acc-0.8594, test loss-0.5247, acc-0.8569\n",
      "Iter-38960, train loss-0.5230, acc-0.8200, valid loss-0.5212, acc-0.8592, test loss-0.5246, acc-0.8570\n",
      "Iter-38970, train loss-0.5036, acc-0.8400, valid loss-0.5212, acc-0.8594, test loss-0.5245, acc-0.8569\n",
      "Iter-38980, train loss-0.5409, acc-0.8600, valid loss-0.5211, acc-0.8594, test loss-0.5245, acc-0.8570\n",
      "Iter-38990, train loss-0.4396, acc-0.9000, valid loss-0.5210, acc-0.8598, test loss-0.5244, acc-0.8570\n",
      "Iter-39000, train loss-0.4529, acc-0.9200, valid loss-0.5209, acc-0.8598, test loss-0.5243, acc-0.8569\n",
      "Iter-39010, train loss-0.5918, acc-0.8600, valid loss-0.5209, acc-0.8592, test loss-0.5243, acc-0.8569\n",
      "Iter-39020, train loss-0.6136, acc-0.8400, valid loss-0.5208, acc-0.8596, test loss-0.5242, acc-0.8568\n",
      "Iter-39030, train loss-0.7795, acc-0.7600, valid loss-0.5207, acc-0.8596, test loss-0.5241, acc-0.8569\n",
      "Iter-39040, train loss-0.4961, acc-0.8800, valid loss-0.5207, acc-0.8594, test loss-0.5240, acc-0.8570\n",
      "Iter-39050, train loss-0.5732, acc-0.8600, valid loss-0.5206, acc-0.8596, test loss-0.5240, acc-0.8569\n",
      "Iter-39060, train loss-0.5915, acc-0.8000, valid loss-0.5205, acc-0.8596, test loss-0.5239, acc-0.8568\n",
      "Iter-39070, train loss-0.7367, acc-0.8000, valid loss-0.5204, acc-0.8598, test loss-0.5238, acc-0.8568\n",
      "Iter-39080, train loss-0.4224, acc-0.9000, valid loss-0.5203, acc-0.8598, test loss-0.5237, acc-0.8567\n",
      "Iter-39090, train loss-0.5506, acc-0.8200, valid loss-0.5203, acc-0.8604, test loss-0.5237, acc-0.8567\n",
      "Iter-39100, train loss-0.6672, acc-0.8200, valid loss-0.5202, acc-0.8602, test loss-0.5236, acc-0.8567\n",
      "Iter-39110, train loss-0.6413, acc-0.8400, valid loss-0.5201, acc-0.8602, test loss-0.5235, acc-0.8567\n",
      "Iter-39120, train loss-0.3209, acc-0.9400, valid loss-0.5201, acc-0.8602, test loss-0.5235, acc-0.8568\n",
      "Iter-39130, train loss-0.7430, acc-0.7400, valid loss-0.5200, acc-0.8600, test loss-0.5234, acc-0.8567\n",
      "Iter-39140, train loss-0.3814, acc-0.9200, valid loss-0.5200, acc-0.8600, test loss-0.5233, acc-0.8568\n",
      "Iter-39150, train loss-0.3169, acc-0.9200, valid loss-0.5199, acc-0.8602, test loss-0.5233, acc-0.8568\n",
      "Iter-39160, train loss-0.5536, acc-0.8200, valid loss-0.5198, acc-0.8602, test loss-0.5232, acc-0.8567\n",
      "Iter-39170, train loss-0.5616, acc-0.8400, valid loss-0.5197, acc-0.8602, test loss-0.5231, acc-0.8567\n",
      "Iter-39180, train loss-0.5629, acc-0.8800, valid loss-0.5197, acc-0.8602, test loss-0.5231, acc-0.8567\n",
      "Iter-39190, train loss-0.6067, acc-0.8200, valid loss-0.5196, acc-0.8602, test loss-0.5230, acc-0.8567\n",
      "Iter-39200, train loss-0.5139, acc-0.8200, valid loss-0.5195, acc-0.8604, test loss-0.5229, acc-0.8566\n",
      "Iter-39210, train loss-0.6635, acc-0.8200, valid loss-0.5195, acc-0.8602, test loss-0.5229, acc-0.8566\n",
      "Iter-39220, train loss-0.3884, acc-0.9000, valid loss-0.5194, acc-0.8604, test loss-0.5228, acc-0.8566\n",
      "Iter-39230, train loss-0.3847, acc-0.9200, valid loss-0.5193, acc-0.8602, test loss-0.5227, acc-0.8566\n",
      "Iter-39240, train loss-0.5867, acc-0.8200, valid loss-0.5192, acc-0.8602, test loss-0.5226, acc-0.8565\n",
      "Iter-39250, train loss-0.7104, acc-0.7800, valid loss-0.5191, acc-0.8602, test loss-0.5226, acc-0.8566\n",
      "Iter-39260, train loss-0.3397, acc-0.9000, valid loss-0.5191, acc-0.8604, test loss-0.5225, acc-0.8567\n",
      "Iter-39270, train loss-0.4694, acc-0.8800, valid loss-0.5190, acc-0.8604, test loss-0.5224, acc-0.8567\n",
      "Iter-39280, train loss-0.5911, acc-0.9000, valid loss-0.5189, acc-0.8602, test loss-0.5223, acc-0.8569\n",
      "Iter-39290, train loss-0.4629, acc-0.9000, valid loss-0.5188, acc-0.8602, test loss-0.5222, acc-0.8567\n",
      "Iter-39300, train loss-0.4298, acc-0.9200, valid loss-0.5187, acc-0.8600, test loss-0.5222, acc-0.8567\n",
      "Iter-39310, train loss-0.3266, acc-0.9200, valid loss-0.5187, acc-0.8602, test loss-0.5221, acc-0.8567\n",
      "Iter-39320, train loss-0.4822, acc-0.8400, valid loss-0.5186, acc-0.8602, test loss-0.5220, acc-0.8570\n",
      "Iter-39330, train loss-0.4312, acc-0.9000, valid loss-0.5185, acc-0.8602, test loss-0.5219, acc-0.8568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-39340, train loss-0.4441, acc-0.9200, valid loss-0.5185, acc-0.8602, test loss-0.5219, acc-0.8569\n",
      "Iter-39350, train loss-0.5735, acc-0.8400, valid loss-0.5184, acc-0.8604, test loss-0.5218, acc-0.8569\n",
      "Iter-39360, train loss-0.5036, acc-0.8400, valid loss-0.5183, acc-0.8604, test loss-0.5217, acc-0.8570\n",
      "Iter-39370, train loss-0.6576, acc-0.8000, valid loss-0.5182, acc-0.8602, test loss-0.5217, acc-0.8570\n",
      "Iter-39380, train loss-0.6680, acc-0.8400, valid loss-0.5182, acc-0.8602, test loss-0.5216, acc-0.8571\n",
      "Iter-39390, train loss-0.7990, acc-0.8000, valid loss-0.5181, acc-0.8600, test loss-0.5215, acc-0.8571\n",
      "Iter-39400, train loss-0.4575, acc-0.8600, valid loss-0.5181, acc-0.8602, test loss-0.5215, acc-0.8571\n",
      "Iter-39410, train loss-0.3358, acc-0.9400, valid loss-0.5180, acc-0.8600, test loss-0.5214, acc-0.8571\n",
      "Iter-39420, train loss-0.6139, acc-0.8600, valid loss-0.5179, acc-0.8602, test loss-0.5213, acc-0.8572\n",
      "Iter-39430, train loss-0.4202, acc-0.9000, valid loss-0.5179, acc-0.8600, test loss-0.5213, acc-0.8573\n",
      "Iter-39440, train loss-0.3193, acc-0.9600, valid loss-0.5178, acc-0.8600, test loss-0.5212, acc-0.8572\n",
      "Iter-39450, train loss-0.4830, acc-0.8600, valid loss-0.5177, acc-0.8600, test loss-0.5211, acc-0.8573\n",
      "Iter-39460, train loss-0.3839, acc-0.9000, valid loss-0.5177, acc-0.8602, test loss-0.5210, acc-0.8572\n",
      "Iter-39470, train loss-0.5966, acc-0.8600, valid loss-0.5176, acc-0.8602, test loss-0.5210, acc-0.8573\n",
      "Iter-39480, train loss-0.3656, acc-0.9200, valid loss-0.5175, acc-0.8604, test loss-0.5209, acc-0.8573\n",
      "Iter-39490, train loss-0.5220, acc-0.8800, valid loss-0.5175, acc-0.8602, test loss-0.5208, acc-0.8573\n",
      "Iter-39500, train loss-0.4372, acc-0.8800, valid loss-0.5174, acc-0.8602, test loss-0.5207, acc-0.8573\n",
      "Iter-39510, train loss-0.5348, acc-0.9000, valid loss-0.5173, acc-0.8602, test loss-0.5207, acc-0.8572\n",
      "Iter-39520, train loss-0.3781, acc-0.9400, valid loss-0.5173, acc-0.8600, test loss-0.5206, acc-0.8573\n",
      "Iter-39530, train loss-0.4667, acc-0.8800, valid loss-0.5172, acc-0.8602, test loss-0.5206, acc-0.8573\n",
      "Iter-39540, train loss-0.4592, acc-0.8600, valid loss-0.5171, acc-0.8602, test loss-0.5205, acc-0.8575\n",
      "Iter-39550, train loss-0.6256, acc-0.8200, valid loss-0.5170, acc-0.8604, test loss-0.5204, acc-0.8574\n",
      "Iter-39560, train loss-0.6552, acc-0.9000, valid loss-0.5170, acc-0.8604, test loss-0.5204, acc-0.8573\n",
      "Iter-39570, train loss-0.4564, acc-0.8600, valid loss-0.5169, acc-0.8604, test loss-0.5203, acc-0.8574\n",
      "Iter-39580, train loss-0.5806, acc-0.8400, valid loss-0.5168, acc-0.8602, test loss-0.5202, acc-0.8574\n",
      "Iter-39590, train loss-0.7041, acc-0.7800, valid loss-0.5168, acc-0.8602, test loss-0.5202, acc-0.8573\n",
      "Iter-39600, train loss-0.5797, acc-0.8200, valid loss-0.5167, acc-0.8604, test loss-0.5201, acc-0.8573\n",
      "Iter-39610, train loss-0.5140, acc-0.9000, valid loss-0.5166, acc-0.8608, test loss-0.5201, acc-0.8574\n",
      "Iter-39620, train loss-0.5594, acc-0.8800, valid loss-0.5166, acc-0.8608, test loss-0.5200, acc-0.8574\n",
      "Iter-39630, train loss-0.4761, acc-0.8600, valid loss-0.5165, acc-0.8608, test loss-0.5199, acc-0.8574\n",
      "Iter-39640, train loss-0.5885, acc-0.8200, valid loss-0.5164, acc-0.8606, test loss-0.5198, acc-0.8575\n",
      "Iter-39650, train loss-0.5684, acc-0.8400, valid loss-0.5164, acc-0.8604, test loss-0.5198, acc-0.8575\n",
      "Iter-39660, train loss-0.5298, acc-0.8400, valid loss-0.5163, acc-0.8604, test loss-0.5197, acc-0.8577\n",
      "Iter-39670, train loss-0.6959, acc-0.7800, valid loss-0.5162, acc-0.8604, test loss-0.5196, acc-0.8575\n",
      "Iter-39680, train loss-0.6098, acc-0.7400, valid loss-0.5162, acc-0.8602, test loss-0.5195, acc-0.8577\n",
      "Iter-39690, train loss-0.6338, acc-0.7600, valid loss-0.5161, acc-0.8606, test loss-0.5195, acc-0.8578\n",
      "Iter-39700, train loss-0.5242, acc-0.8600, valid loss-0.5161, acc-0.8606, test loss-0.5194, acc-0.8578\n",
      "Iter-39710, train loss-0.4702, acc-0.9000, valid loss-0.5160, acc-0.8604, test loss-0.5194, acc-0.8577\n",
      "Iter-39720, train loss-0.5687, acc-0.8400, valid loss-0.5159, acc-0.8602, test loss-0.5193, acc-0.8575\n",
      "Iter-39730, train loss-0.6301, acc-0.8000, valid loss-0.5159, acc-0.8604, test loss-0.5192, acc-0.8578\n",
      "Iter-39740, train loss-0.3366, acc-0.9200, valid loss-0.5158, acc-0.8606, test loss-0.5192, acc-0.8579\n",
      "Iter-39750, train loss-0.5389, acc-0.9000, valid loss-0.5157, acc-0.8606, test loss-0.5191, acc-0.8578\n",
      "Iter-39760, train loss-0.5743, acc-0.8200, valid loss-0.5157, acc-0.8606, test loss-0.5190, acc-0.8579\n",
      "Iter-39770, train loss-0.4886, acc-0.9000, valid loss-0.5156, acc-0.8606, test loss-0.5189, acc-0.8580\n",
      "Iter-39780, train loss-0.7944, acc-0.7800, valid loss-0.5155, acc-0.8606, test loss-0.5189, acc-0.8577\n",
      "Iter-39790, train loss-0.6070, acc-0.8400, valid loss-0.5155, acc-0.8606, test loss-0.5188, acc-0.8578\n",
      "Iter-39800, train loss-0.5404, acc-0.8000, valid loss-0.5154, acc-0.8606, test loss-0.5187, acc-0.8581\n",
      "Iter-39810, train loss-0.3169, acc-0.9600, valid loss-0.5153, acc-0.8608, test loss-0.5187, acc-0.8581\n",
      "Iter-39820, train loss-0.6075, acc-0.8200, valid loss-0.5153, acc-0.8608, test loss-0.5186, acc-0.8580\n",
      "Iter-39830, train loss-0.6911, acc-0.7400, valid loss-0.5152, acc-0.8608, test loss-0.5185, acc-0.8580\n",
      "Iter-39840, train loss-0.7148, acc-0.7600, valid loss-0.5151, acc-0.8608, test loss-0.5185, acc-0.8579\n",
      "Iter-39850, train loss-0.4764, acc-0.8800, valid loss-0.5151, acc-0.8608, test loss-0.5184, acc-0.8579\n",
      "Iter-39860, train loss-0.5461, acc-0.8600, valid loss-0.5150, acc-0.8610, test loss-0.5183, acc-0.8580\n",
      "Iter-39870, train loss-0.4962, acc-0.9000, valid loss-0.5149, acc-0.8610, test loss-0.5183, acc-0.8582\n",
      "Iter-39880, train loss-0.5773, acc-0.8400, valid loss-0.5149, acc-0.8610, test loss-0.5182, acc-0.8581\n",
      "Iter-39890, train loss-0.4518, acc-0.8600, valid loss-0.5148, acc-0.8612, test loss-0.5182, acc-0.8581\n",
      "Iter-39900, train loss-0.6184, acc-0.8400, valid loss-0.5147, acc-0.8612, test loss-0.5181, acc-0.8581\n",
      "Iter-39910, train loss-0.3998, acc-0.9400, valid loss-0.5147, acc-0.8610, test loss-0.5180, acc-0.8581\n",
      "Iter-39920, train loss-0.4439, acc-0.8600, valid loss-0.5146, acc-0.8612, test loss-0.5179, acc-0.8581\n",
      "Iter-39930, train loss-0.5738, acc-0.8400, valid loss-0.5145, acc-0.8612, test loss-0.5179, acc-0.8582\n",
      "Iter-39940, train loss-0.8647, acc-0.7600, valid loss-0.5144, acc-0.8610, test loss-0.5178, acc-0.8582\n",
      "Iter-39950, train loss-0.6346, acc-0.8400, valid loss-0.5144, acc-0.8610, test loss-0.5177, acc-0.8583\n",
      "Iter-39960, train loss-0.3950, acc-0.8400, valid loss-0.5143, acc-0.8610, test loss-0.5177, acc-0.8582\n",
      "Iter-39970, train loss-0.5970, acc-0.8200, valid loss-0.5143, acc-0.8610, test loss-0.5176, acc-0.8586\n",
      "Iter-39980, train loss-0.5510, acc-0.8800, valid loss-0.5142, acc-0.8610, test loss-0.5175, acc-0.8587\n",
      "Iter-39990, train loss-0.5444, acc-0.8800, valid loss-0.5141, acc-0.8610, test loss-0.5175, acc-0.8585\n",
      "Iter-40000, train loss-0.5752, acc-0.8600, valid loss-0.5140, acc-0.8610, test loss-0.5174, acc-0.8585\n",
      "Iter-40010, train loss-0.4645, acc-0.8200, valid loss-0.5140, acc-0.8610, test loss-0.5173, acc-0.8589\n",
      "Iter-40020, train loss-0.5884, acc-0.8000, valid loss-0.5139, acc-0.8610, test loss-0.5172, acc-0.8588\n",
      "Iter-40030, train loss-0.6134, acc-0.8400, valid loss-0.5139, acc-0.8610, test loss-0.5172, acc-0.8590\n",
      "Iter-40040, train loss-0.6023, acc-0.8200, valid loss-0.5138, acc-0.8610, test loss-0.5171, acc-0.8590\n",
      "Iter-40050, train loss-0.5084, acc-0.8200, valid loss-0.5137, acc-0.8610, test loss-0.5170, acc-0.8591\n",
      "Iter-40060, train loss-0.4986, acc-0.8400, valid loss-0.5137, acc-0.8610, test loss-0.5170, acc-0.8591\n",
      "Iter-40070, train loss-0.5845, acc-0.8000, valid loss-0.5136, acc-0.8610, test loss-0.5169, acc-0.8590\n",
      "Iter-40080, train loss-0.8260, acc-0.7800, valid loss-0.5135, acc-0.8610, test loss-0.5168, acc-0.8590\n",
      "Iter-40090, train loss-0.8767, acc-0.7600, valid loss-0.5135, acc-0.8610, test loss-0.5168, acc-0.8592\n",
      "Iter-40100, train loss-0.4818, acc-0.8600, valid loss-0.5134, acc-0.8610, test loss-0.5167, acc-0.8592\n",
      "Iter-40110, train loss-0.5378, acc-0.8800, valid loss-0.5133, acc-0.8610, test loss-0.5166, acc-0.8592\n",
      "Iter-40120, train loss-0.4867, acc-0.9000, valid loss-0.5133, acc-0.8610, test loss-0.5165, acc-0.8592\n",
      "Iter-40130, train loss-0.5226, acc-0.8400, valid loss-0.5132, acc-0.8610, test loss-0.5165, acc-0.8592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-40140, train loss-0.4207, acc-0.8800, valid loss-0.5131, acc-0.8608, test loss-0.5164, acc-0.8592\n",
      "Iter-40150, train loss-0.5558, acc-0.8200, valid loss-0.5131, acc-0.8610, test loss-0.5163, acc-0.8592\n",
      "Iter-40160, train loss-0.6488, acc-0.7400, valid loss-0.5130, acc-0.8610, test loss-0.5162, acc-0.8592\n",
      "Iter-40170, train loss-0.5562, acc-0.8400, valid loss-0.5129, acc-0.8610, test loss-0.5162, acc-0.8593\n",
      "Iter-40180, train loss-0.6126, acc-0.8800, valid loss-0.5129, acc-0.8610, test loss-0.5161, acc-0.8592\n",
      "Iter-40190, train loss-0.6976, acc-0.8400, valid loss-0.5128, acc-0.8610, test loss-0.5161, acc-0.8592\n",
      "Iter-40200, train loss-0.7311, acc-0.7600, valid loss-0.5128, acc-0.8610, test loss-0.5160, acc-0.8592\n",
      "Iter-40210, train loss-0.5108, acc-0.8600, valid loss-0.5127, acc-0.8610, test loss-0.5159, acc-0.8593\n",
      "Iter-40220, train loss-0.4498, acc-0.8600, valid loss-0.5126, acc-0.8610, test loss-0.5159, acc-0.8592\n",
      "Iter-40230, train loss-0.4098, acc-0.8800, valid loss-0.5125, acc-0.8610, test loss-0.5158, acc-0.8594\n",
      "Iter-40240, train loss-0.3959, acc-0.9000, valid loss-0.5125, acc-0.8610, test loss-0.5157, acc-0.8594\n",
      "Iter-40250, train loss-0.4907, acc-0.9000, valid loss-0.5124, acc-0.8608, test loss-0.5157, acc-0.8594\n",
      "Iter-40260, train loss-0.4813, acc-0.8400, valid loss-0.5123, acc-0.8610, test loss-0.5156, acc-0.8595\n",
      "Iter-40270, train loss-0.5521, acc-0.8600, valid loss-0.5122, acc-0.8610, test loss-0.5155, acc-0.8595\n",
      "Iter-40280, train loss-0.5346, acc-0.8000, valid loss-0.5122, acc-0.8610, test loss-0.5155, acc-0.8595\n",
      "Iter-40290, train loss-0.4124, acc-0.9200, valid loss-0.5121, acc-0.8610, test loss-0.5154, acc-0.8596\n",
      "Iter-40300, train loss-0.4275, acc-0.8800, valid loss-0.5120, acc-0.8610, test loss-0.5153, acc-0.8596\n",
      "Iter-40310, train loss-0.4419, acc-0.9000, valid loss-0.5120, acc-0.8610, test loss-0.5153, acc-0.8596\n",
      "Iter-40320, train loss-0.7279, acc-0.8200, valid loss-0.5119, acc-0.8610, test loss-0.5152, acc-0.8595\n",
      "Iter-40330, train loss-0.4921, acc-0.8400, valid loss-0.5118, acc-0.8610, test loss-0.5151, acc-0.8595\n",
      "Iter-40340, train loss-0.6466, acc-0.7200, valid loss-0.5117, acc-0.8610, test loss-0.5150, acc-0.8595\n",
      "Iter-40350, train loss-0.5963, acc-0.8400, valid loss-0.5117, acc-0.8610, test loss-0.5150, acc-0.8594\n",
      "Iter-40360, train loss-0.4137, acc-0.9000, valid loss-0.5116, acc-0.8608, test loss-0.5149, acc-0.8597\n",
      "Iter-40370, train loss-0.5825, acc-0.8000, valid loss-0.5115, acc-0.8608, test loss-0.5148, acc-0.8597\n",
      "Iter-40380, train loss-0.4261, acc-0.8800, valid loss-0.5115, acc-0.8608, test loss-0.5148, acc-0.8597\n",
      "Iter-40390, train loss-0.4648, acc-0.8800, valid loss-0.5114, acc-0.8608, test loss-0.5147, acc-0.8593\n",
      "Iter-40400, train loss-0.5452, acc-0.8600, valid loss-0.5113, acc-0.8608, test loss-0.5146, acc-0.8593\n",
      "Iter-40410, train loss-0.5695, acc-0.8800, valid loss-0.5113, acc-0.8610, test loss-0.5146, acc-0.8594\n",
      "Iter-40420, train loss-0.5551, acc-0.8000, valid loss-0.5112, acc-0.8612, test loss-0.5145, acc-0.8594\n",
      "Iter-40430, train loss-0.3948, acc-0.9200, valid loss-0.5112, acc-0.8612, test loss-0.5144, acc-0.8594\n",
      "Iter-40440, train loss-0.5353, acc-0.8400, valid loss-0.5111, acc-0.8612, test loss-0.5144, acc-0.8593\n",
      "Iter-40450, train loss-0.3904, acc-0.8800, valid loss-0.5110, acc-0.8612, test loss-0.5143, acc-0.8593\n",
      "Iter-40460, train loss-0.5129, acc-0.8400, valid loss-0.5110, acc-0.8612, test loss-0.5143, acc-0.8593\n",
      "Iter-40470, train loss-0.4230, acc-0.8800, valid loss-0.5109, acc-0.8612, test loss-0.5142, acc-0.8594\n",
      "Iter-40480, train loss-0.7122, acc-0.7600, valid loss-0.5108, acc-0.8612, test loss-0.5142, acc-0.8593\n",
      "Iter-40490, train loss-0.5853, acc-0.8800, valid loss-0.5108, acc-0.8612, test loss-0.5141, acc-0.8593\n",
      "Iter-40500, train loss-0.5523, acc-0.8400, valid loss-0.5107, acc-0.8612, test loss-0.5140, acc-0.8593\n",
      "Iter-40510, train loss-0.6272, acc-0.7800, valid loss-0.5106, acc-0.8612, test loss-0.5140, acc-0.8593\n",
      "Iter-40520, train loss-0.5542, acc-0.8200, valid loss-0.5106, acc-0.8612, test loss-0.5139, acc-0.8593\n",
      "Iter-40530, train loss-0.4903, acc-0.8600, valid loss-0.5105, acc-0.8612, test loss-0.5138, acc-0.8593\n",
      "Iter-40540, train loss-0.4437, acc-0.9000, valid loss-0.5105, acc-0.8612, test loss-0.5137, acc-0.8595\n",
      "Iter-40550, train loss-0.6559, acc-0.8000, valid loss-0.5104, acc-0.8612, test loss-0.5137, acc-0.8593\n",
      "Iter-40560, train loss-0.5336, acc-0.8400, valid loss-0.5103, acc-0.8612, test loss-0.5136, acc-0.8593\n",
      "Iter-40570, train loss-0.4481, acc-0.8800, valid loss-0.5102, acc-0.8612, test loss-0.5135, acc-0.8593\n",
      "Iter-40580, train loss-0.6429, acc-0.8000, valid loss-0.5102, acc-0.8612, test loss-0.5135, acc-0.8596\n",
      "Iter-40590, train loss-0.3968, acc-0.8800, valid loss-0.5101, acc-0.8612, test loss-0.5134, acc-0.8596\n",
      "Iter-40600, train loss-0.4769, acc-0.9200, valid loss-0.5100, acc-0.8612, test loss-0.5134, acc-0.8596\n",
      "Iter-40610, train loss-0.4102, acc-0.8600, valid loss-0.5100, acc-0.8612, test loss-0.5133, acc-0.8596\n",
      "Iter-40620, train loss-0.5732, acc-0.8200, valid loss-0.5099, acc-0.8612, test loss-0.5132, acc-0.8596\n",
      "Iter-40630, train loss-0.5498, acc-0.8600, valid loss-0.5098, acc-0.8614, test loss-0.5131, acc-0.8598\n",
      "Iter-40640, train loss-0.3636, acc-0.9400, valid loss-0.5098, acc-0.8612, test loss-0.5131, acc-0.8598\n",
      "Iter-40650, train loss-0.7026, acc-0.8600, valid loss-0.5097, acc-0.8614, test loss-0.5130, acc-0.8598\n",
      "Iter-40660, train loss-0.4927, acc-0.8800, valid loss-0.5096, acc-0.8614, test loss-0.5129, acc-0.8598\n",
      "Iter-40670, train loss-0.5330, acc-0.8200, valid loss-0.5095, acc-0.8612, test loss-0.5129, acc-0.8598\n",
      "Iter-40680, train loss-0.4073, acc-0.9200, valid loss-0.5095, acc-0.8612, test loss-0.5128, acc-0.8599\n",
      "Iter-40690, train loss-0.5934, acc-0.8400, valid loss-0.5094, acc-0.8612, test loss-0.5127, acc-0.8599\n",
      "Iter-40700, train loss-0.5624, acc-0.8800, valid loss-0.5093, acc-0.8612, test loss-0.5127, acc-0.8599\n",
      "Iter-40710, train loss-0.4572, acc-0.8800, valid loss-0.5093, acc-0.8612, test loss-0.5126, acc-0.8599\n",
      "Iter-40720, train loss-0.3870, acc-0.9400, valid loss-0.5092, acc-0.8612, test loss-0.5125, acc-0.8598\n",
      "Iter-40730, train loss-0.6138, acc-0.8200, valid loss-0.5091, acc-0.8612, test loss-0.5125, acc-0.8600\n",
      "Iter-40740, train loss-0.4825, acc-0.8600, valid loss-0.5091, acc-0.8612, test loss-0.5124, acc-0.8600\n",
      "Iter-40750, train loss-0.4887, acc-0.8400, valid loss-0.5090, acc-0.8614, test loss-0.5123, acc-0.8599\n",
      "Iter-40760, train loss-0.5264, acc-0.8800, valid loss-0.5090, acc-0.8614, test loss-0.5123, acc-0.8600\n",
      "Iter-40770, train loss-0.7723, acc-0.8000, valid loss-0.5089, acc-0.8614, test loss-0.5122, acc-0.8600\n",
      "Iter-40780, train loss-0.5060, acc-0.8400, valid loss-0.5088, acc-0.8614, test loss-0.5121, acc-0.8600\n",
      "Iter-40790, train loss-0.4904, acc-0.9000, valid loss-0.5088, acc-0.8614, test loss-0.5121, acc-0.8600\n",
      "Iter-40800, train loss-0.4686, acc-0.8600, valid loss-0.5087, acc-0.8614, test loss-0.5120, acc-0.8601\n",
      "Iter-40810, train loss-0.3960, acc-0.8800, valid loss-0.5087, acc-0.8614, test loss-0.5120, acc-0.8601\n",
      "Iter-40820, train loss-0.6457, acc-0.8600, valid loss-0.5086, acc-0.8614, test loss-0.5119, acc-0.8602\n",
      "Iter-40830, train loss-0.6301, acc-0.8400, valid loss-0.5085, acc-0.8614, test loss-0.5118, acc-0.8601\n",
      "Iter-40840, train loss-0.3912, acc-0.9000, valid loss-0.5084, acc-0.8614, test loss-0.5118, acc-0.8602\n",
      "Iter-40850, train loss-0.5493, acc-0.8200, valid loss-0.5084, acc-0.8614, test loss-0.5117, acc-0.8601\n",
      "Iter-40860, train loss-0.3058, acc-0.9400, valid loss-0.5083, acc-0.8614, test loss-0.5117, acc-0.8602\n",
      "Iter-40870, train loss-0.4825, acc-0.8200, valid loss-0.5083, acc-0.8614, test loss-0.5116, acc-0.8602\n",
      "Iter-40880, train loss-0.4553, acc-0.8800, valid loss-0.5082, acc-0.8614, test loss-0.5115, acc-0.8601\n",
      "Iter-40890, train loss-0.5121, acc-0.8600, valid loss-0.5081, acc-0.8614, test loss-0.5115, acc-0.8602\n",
      "Iter-40900, train loss-0.5910, acc-0.8200, valid loss-0.5081, acc-0.8614, test loss-0.5114, acc-0.8602\n",
      "Iter-40910, train loss-0.5923, acc-0.8000, valid loss-0.5080, acc-0.8616, test loss-0.5113, acc-0.8603\n",
      "Iter-40920, train loss-0.4047, acc-0.9200, valid loss-0.5079, acc-0.8616, test loss-0.5113, acc-0.8603\n",
      "Iter-40930, train loss-0.5747, acc-0.8400, valid loss-0.5079, acc-0.8616, test loss-0.5112, acc-0.8602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-40940, train loss-0.4035, acc-0.9000, valid loss-0.5078, acc-0.8614, test loss-0.5111, acc-0.8603\n",
      "Iter-40950, train loss-0.5257, acc-0.8600, valid loss-0.5077, acc-0.8616, test loss-0.5111, acc-0.8602\n",
      "Iter-40960, train loss-0.3953, acc-0.8800, valid loss-0.5077, acc-0.8616, test loss-0.5110, acc-0.8603\n",
      "Iter-40970, train loss-0.3197, acc-0.9200, valid loss-0.5076, acc-0.8616, test loss-0.5109, acc-0.8603\n",
      "Iter-40980, train loss-0.3349, acc-0.9200, valid loss-0.5075, acc-0.8616, test loss-0.5108, acc-0.8602\n",
      "Iter-40990, train loss-0.4866, acc-0.8400, valid loss-0.5074, acc-0.8616, test loss-0.5108, acc-0.8602\n",
      "Iter-41000, train loss-0.4720, acc-0.8400, valid loss-0.5073, acc-0.8616, test loss-0.5107, acc-0.8602\n",
      "Iter-41010, train loss-0.4659, acc-0.9200, valid loss-0.5073, acc-0.8616, test loss-0.5106, acc-0.8602\n",
      "Iter-41020, train loss-0.5031, acc-0.8200, valid loss-0.5072, acc-0.8616, test loss-0.5106, acc-0.8602\n",
      "Iter-41030, train loss-0.6101, acc-0.8200, valid loss-0.5072, acc-0.8616, test loss-0.5105, acc-0.8601\n",
      "Iter-41040, train loss-0.4843, acc-0.8800, valid loss-0.5071, acc-0.8616, test loss-0.5105, acc-0.8601\n",
      "Iter-41050, train loss-0.5911, acc-0.9200, valid loss-0.5070, acc-0.8616, test loss-0.5104, acc-0.8602\n",
      "Iter-41060, train loss-0.5095, acc-0.9000, valid loss-0.5070, acc-0.8616, test loss-0.5103, acc-0.8606\n",
      "Iter-41070, train loss-0.4013, acc-0.9000, valid loss-0.5069, acc-0.8616, test loss-0.5103, acc-0.8604\n",
      "Iter-41080, train loss-0.5169, acc-0.8800, valid loss-0.5068, acc-0.8616, test loss-0.5102, acc-0.8605\n",
      "Iter-41090, train loss-0.4477, acc-0.8600, valid loss-0.5068, acc-0.8618, test loss-0.5101, acc-0.8605\n",
      "Iter-41100, train loss-0.4974, acc-0.8400, valid loss-0.5067, acc-0.8618, test loss-0.5101, acc-0.8606\n",
      "Iter-41110, train loss-0.3819, acc-0.9200, valid loss-0.5066, acc-0.8618, test loss-0.5100, acc-0.8605\n",
      "Iter-41120, train loss-0.5299, acc-0.8200, valid loss-0.5066, acc-0.8616, test loss-0.5100, acc-0.8605\n",
      "Iter-41130, train loss-0.5434, acc-0.8000, valid loss-0.5065, acc-0.8618, test loss-0.5099, acc-0.8605\n",
      "Iter-41140, train loss-0.4749, acc-0.9000, valid loss-0.5064, acc-0.8618, test loss-0.5098, acc-0.8607\n",
      "Iter-41150, train loss-0.5390, acc-0.8400, valid loss-0.5064, acc-0.8618, test loss-0.5097, acc-0.8607\n",
      "Iter-41160, train loss-0.5608, acc-0.8200, valid loss-0.5063, acc-0.8618, test loss-0.5097, acc-0.8606\n",
      "Iter-41170, train loss-0.5118, acc-0.8000, valid loss-0.5062, acc-0.8618, test loss-0.5096, acc-0.8606\n",
      "Iter-41180, train loss-0.4992, acc-0.8800, valid loss-0.5062, acc-0.8618, test loss-0.5095, acc-0.8605\n",
      "Iter-41190, train loss-0.6727, acc-0.7800, valid loss-0.5061, acc-0.8618, test loss-0.5095, acc-0.8606\n",
      "Iter-41200, train loss-0.4912, acc-0.8600, valid loss-0.5061, acc-0.8618, test loss-0.5094, acc-0.8603\n",
      "Iter-41210, train loss-0.4867, acc-0.8600, valid loss-0.5060, acc-0.8618, test loss-0.5094, acc-0.8604\n",
      "Iter-41220, train loss-0.4223, acc-0.8800, valid loss-0.5059, acc-0.8618, test loss-0.5093, acc-0.8605\n",
      "Iter-41230, train loss-0.4603, acc-0.8600, valid loss-0.5059, acc-0.8618, test loss-0.5092, acc-0.8605\n",
      "Iter-41240, train loss-0.4841, acc-0.9400, valid loss-0.5058, acc-0.8620, test loss-0.5092, acc-0.8605\n",
      "Iter-41250, train loss-0.6511, acc-0.7800, valid loss-0.5057, acc-0.8620, test loss-0.5091, acc-0.8604\n",
      "Iter-41260, train loss-0.5125, acc-0.8200, valid loss-0.5057, acc-0.8620, test loss-0.5090, acc-0.8604\n",
      "Iter-41270, train loss-0.5078, acc-0.8800, valid loss-0.5056, acc-0.8620, test loss-0.5090, acc-0.8605\n",
      "Iter-41280, train loss-0.4760, acc-0.9000, valid loss-0.5055, acc-0.8620, test loss-0.5089, acc-0.8606\n",
      "Iter-41290, train loss-0.5989, acc-0.8400, valid loss-0.5055, acc-0.8620, test loss-0.5088, acc-0.8605\n",
      "Iter-41300, train loss-0.5931, acc-0.8400, valid loss-0.5054, acc-0.8620, test loss-0.5088, acc-0.8605\n",
      "Iter-41310, train loss-0.5030, acc-0.8200, valid loss-0.5053, acc-0.8620, test loss-0.5087, acc-0.8605\n",
      "Iter-41320, train loss-0.4112, acc-0.8800, valid loss-0.5053, acc-0.8620, test loss-0.5087, acc-0.8603\n",
      "Iter-41330, train loss-0.4491, acc-0.9000, valid loss-0.5052, acc-0.8620, test loss-0.5086, acc-0.8603\n",
      "Iter-41340, train loss-0.5486, acc-0.8400, valid loss-0.5052, acc-0.8620, test loss-0.5085, acc-0.8603\n",
      "Iter-41350, train loss-0.7187, acc-0.8400, valid loss-0.5051, acc-0.8620, test loss-0.5084, acc-0.8602\n",
      "Iter-41360, train loss-0.5046, acc-0.8600, valid loss-0.5050, acc-0.8622, test loss-0.5084, acc-0.8601\n",
      "Iter-41370, train loss-0.4725, acc-0.8800, valid loss-0.5050, acc-0.8624, test loss-0.5083, acc-0.8603\n",
      "Iter-41380, train loss-0.6145, acc-0.7600, valid loss-0.5049, acc-0.8624, test loss-0.5083, acc-0.8602\n",
      "Iter-41390, train loss-0.6223, acc-0.8200, valid loss-0.5049, acc-0.8622, test loss-0.5082, acc-0.8603\n",
      "Iter-41400, train loss-0.6336, acc-0.8200, valid loss-0.5048, acc-0.8622, test loss-0.5081, acc-0.8602\n",
      "Iter-41410, train loss-0.5123, acc-0.8800, valid loss-0.5048, acc-0.8622, test loss-0.5081, acc-0.8602\n",
      "Iter-41420, train loss-0.5102, acc-0.8600, valid loss-0.5047, acc-0.8624, test loss-0.5080, acc-0.8602\n",
      "Iter-41430, train loss-0.5709, acc-0.8200, valid loss-0.5046, acc-0.8624, test loss-0.5080, acc-0.8601\n",
      "Iter-41440, train loss-0.3755, acc-0.9200, valid loss-0.5046, acc-0.8624, test loss-0.5079, acc-0.8602\n",
      "Iter-41450, train loss-0.5355, acc-0.8800, valid loss-0.5045, acc-0.8624, test loss-0.5078, acc-0.8603\n",
      "Iter-41460, train loss-0.6165, acc-0.9000, valid loss-0.5044, acc-0.8624, test loss-0.5078, acc-0.8603\n",
      "Iter-41470, train loss-0.7418, acc-0.7000, valid loss-0.5044, acc-0.8624, test loss-0.5077, acc-0.8604\n",
      "Iter-41480, train loss-0.5139, acc-0.8600, valid loss-0.5043, acc-0.8622, test loss-0.5077, acc-0.8604\n",
      "Iter-41490, train loss-0.5354, acc-0.8400, valid loss-0.5043, acc-0.8624, test loss-0.5076, acc-0.8603\n",
      "Iter-41500, train loss-0.5150, acc-0.8800, valid loss-0.5042, acc-0.8626, test loss-0.5076, acc-0.8603\n",
      "Iter-41510, train loss-0.5108, acc-0.9000, valid loss-0.5041, acc-0.8626, test loss-0.5075, acc-0.8603\n",
      "Iter-41520, train loss-0.5864, acc-0.8200, valid loss-0.5041, acc-0.8626, test loss-0.5074, acc-0.8605\n",
      "Iter-41530, train loss-0.6321, acc-0.8200, valid loss-0.5040, acc-0.8626, test loss-0.5073, acc-0.8604\n",
      "Iter-41540, train loss-0.6485, acc-0.8000, valid loss-0.5039, acc-0.8626, test loss-0.5073, acc-0.8603\n",
      "Iter-41550, train loss-0.4214, acc-0.8800, valid loss-0.5039, acc-0.8626, test loss-0.5072, acc-0.8605\n",
      "Iter-41560, train loss-0.3637, acc-0.9600, valid loss-0.5038, acc-0.8626, test loss-0.5072, acc-0.8604\n",
      "Iter-41570, train loss-0.6093, acc-0.8200, valid loss-0.5037, acc-0.8626, test loss-0.5071, acc-0.8605\n",
      "Iter-41580, train loss-0.5676, acc-0.8000, valid loss-0.5037, acc-0.8628, test loss-0.5071, acc-0.8605\n",
      "Iter-41590, train loss-0.5161, acc-0.8200, valid loss-0.5036, acc-0.8628, test loss-0.5070, acc-0.8605\n",
      "Iter-41600, train loss-0.5496, acc-0.8400, valid loss-0.5036, acc-0.8628, test loss-0.5069, acc-0.8603\n",
      "Iter-41610, train loss-0.3847, acc-0.8800, valid loss-0.5035, acc-0.8628, test loss-0.5069, acc-0.8605\n",
      "Iter-41620, train loss-0.5434, acc-0.8800, valid loss-0.5034, acc-0.8628, test loss-0.5068, acc-0.8606\n",
      "Iter-41630, train loss-0.5285, acc-0.8800, valid loss-0.5034, acc-0.8628, test loss-0.5067, acc-0.8605\n",
      "Iter-41640, train loss-0.5570, acc-0.8400, valid loss-0.5033, acc-0.8628, test loss-0.5067, acc-0.8606\n",
      "Iter-41650, train loss-0.4671, acc-0.8600, valid loss-0.5033, acc-0.8628, test loss-0.5066, acc-0.8606\n",
      "Iter-41660, train loss-0.6671, acc-0.8000, valid loss-0.5032, acc-0.8628, test loss-0.5066, acc-0.8606\n",
      "Iter-41670, train loss-0.5706, acc-0.8400, valid loss-0.5031, acc-0.8628, test loss-0.5065, acc-0.8606\n",
      "Iter-41680, train loss-0.5325, acc-0.8800, valid loss-0.5031, acc-0.8628, test loss-0.5064, acc-0.8607\n",
      "Iter-41690, train loss-0.5384, acc-0.8400, valid loss-0.5030, acc-0.8628, test loss-0.5064, acc-0.8607\n",
      "Iter-41700, train loss-0.3097, acc-0.9600, valid loss-0.5029, acc-0.8628, test loss-0.5063, acc-0.8608\n",
      "Iter-41710, train loss-0.5966, acc-0.8000, valid loss-0.5029, acc-0.8628, test loss-0.5062, acc-0.8607\n",
      "Iter-41720, train loss-0.4620, acc-0.8800, valid loss-0.5028, acc-0.8628, test loss-0.5062, acc-0.8607\n",
      "Iter-41730, train loss-0.5441, acc-0.8800, valid loss-0.5028, acc-0.8628, test loss-0.5061, acc-0.8607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-41740, train loss-0.5540, acc-0.8600, valid loss-0.5027, acc-0.8628, test loss-0.5060, acc-0.8607\n",
      "Iter-41750, train loss-0.6084, acc-0.8600, valid loss-0.5026, acc-0.8628, test loss-0.5060, acc-0.8607\n",
      "Iter-41760, train loss-0.3632, acc-0.9200, valid loss-0.5025, acc-0.8628, test loss-0.5059, acc-0.8607\n",
      "Iter-41770, train loss-0.5223, acc-0.8400, valid loss-0.5025, acc-0.8628, test loss-0.5058, acc-0.8607\n",
      "Iter-41780, train loss-0.4412, acc-0.8800, valid loss-0.5024, acc-0.8628, test loss-0.5058, acc-0.8606\n",
      "Iter-41790, train loss-0.3865, acc-0.9200, valid loss-0.5024, acc-0.8628, test loss-0.5057, acc-0.8606\n",
      "Iter-41800, train loss-0.4685, acc-0.9000, valid loss-0.5023, acc-0.8628, test loss-0.5057, acc-0.8606\n",
      "Iter-41810, train loss-0.4734, acc-0.8800, valid loss-0.5023, acc-0.8628, test loss-0.5056, acc-0.8606\n",
      "Iter-41820, train loss-0.5414, acc-0.8200, valid loss-0.5022, acc-0.8630, test loss-0.5056, acc-0.8606\n",
      "Iter-41830, train loss-0.4757, acc-0.8600, valid loss-0.5022, acc-0.8630, test loss-0.5055, acc-0.8606\n",
      "Iter-41840, train loss-0.6922, acc-0.8200, valid loss-0.5021, acc-0.8630, test loss-0.5054, acc-0.8607\n",
      "Iter-41850, train loss-0.4107, acc-0.8400, valid loss-0.5020, acc-0.8630, test loss-0.5054, acc-0.8607\n",
      "Iter-41860, train loss-0.4667, acc-0.8800, valid loss-0.5020, acc-0.8628, test loss-0.5053, acc-0.8606\n",
      "Iter-41870, train loss-0.3836, acc-0.9000, valid loss-0.5019, acc-0.8630, test loss-0.5052, acc-0.8606\n",
      "Iter-41880, train loss-0.3513, acc-0.9800, valid loss-0.5018, acc-0.8630, test loss-0.5052, acc-0.8606\n",
      "Iter-41890, train loss-0.9269, acc-0.7600, valid loss-0.5018, acc-0.8628, test loss-0.5051, acc-0.8606\n",
      "Iter-41900, train loss-0.5388, acc-0.8600, valid loss-0.5017, acc-0.8632, test loss-0.5051, acc-0.8607\n",
      "Iter-41910, train loss-0.5164, acc-0.8800, valid loss-0.5017, acc-0.8632, test loss-0.5050, acc-0.8607\n",
      "Iter-41920, train loss-0.4595, acc-0.8600, valid loss-0.5016, acc-0.8630, test loss-0.5049, acc-0.8607\n",
      "Iter-41930, train loss-0.5898, acc-0.8400, valid loss-0.5015, acc-0.8630, test loss-0.5049, acc-0.8607\n",
      "Iter-41940, train loss-0.4058, acc-0.9000, valid loss-0.5015, acc-0.8630, test loss-0.5048, acc-0.8607\n",
      "Iter-41950, train loss-0.4576, acc-0.9200, valid loss-0.5014, acc-0.8632, test loss-0.5047, acc-0.8607\n",
      "Iter-41960, train loss-0.4234, acc-0.9200, valid loss-0.5013, acc-0.8628, test loss-0.5046, acc-0.8607\n",
      "Iter-41970, train loss-0.5044, acc-0.8400, valid loss-0.5013, acc-0.8628, test loss-0.5046, acc-0.8607\n",
      "Iter-41980, train loss-0.6316, acc-0.7800, valid loss-0.5012, acc-0.8628, test loss-0.5045, acc-0.8607\n",
      "Iter-41990, train loss-0.4161, acc-0.8800, valid loss-0.5012, acc-0.8628, test loss-0.5045, acc-0.8607\n",
      "Iter-42000, train loss-0.4882, acc-0.8600, valid loss-0.5011, acc-0.8628, test loss-0.5044, acc-0.8607\n",
      "Iter-42010, train loss-0.4769, acc-0.8600, valid loss-0.5010, acc-0.8628, test loss-0.5043, acc-0.8607\n",
      "Iter-42020, train loss-0.4790, acc-0.8800, valid loss-0.5009, acc-0.8630, test loss-0.5043, acc-0.8608\n",
      "Iter-42030, train loss-0.4243, acc-0.8800, valid loss-0.5009, acc-0.8630, test loss-0.5042, acc-0.8607\n",
      "Iter-42040, train loss-0.4184, acc-0.8800, valid loss-0.5008, acc-0.8630, test loss-0.5042, acc-0.8609\n",
      "Iter-42050, train loss-0.3092, acc-0.9600, valid loss-0.5008, acc-0.8630, test loss-0.5041, acc-0.8609\n",
      "Iter-42060, train loss-0.4553, acc-0.9200, valid loss-0.5007, acc-0.8630, test loss-0.5041, acc-0.8609\n",
      "Iter-42070, train loss-0.3214, acc-0.9400, valid loss-0.5006, acc-0.8630, test loss-0.5040, acc-0.8609\n",
      "Iter-42080, train loss-0.6829, acc-0.8000, valid loss-0.5006, acc-0.8630, test loss-0.5039, acc-0.8609\n",
      "Iter-42090, train loss-0.5479, acc-0.8400, valid loss-0.5005, acc-0.8630, test loss-0.5039, acc-0.8610\n",
      "Iter-42100, train loss-0.5110, acc-0.8800, valid loss-0.5005, acc-0.8630, test loss-0.5038, acc-0.8610\n",
      "Iter-42110, train loss-0.5884, acc-0.8000, valid loss-0.5004, acc-0.8630, test loss-0.5037, acc-0.8610\n",
      "Iter-42120, train loss-0.8770, acc-0.7000, valid loss-0.5003, acc-0.8630, test loss-0.5037, acc-0.8611\n",
      "Iter-42130, train loss-0.5696, acc-0.8400, valid loss-0.5003, acc-0.8632, test loss-0.5036, acc-0.8611\n",
      "Iter-42140, train loss-0.3720, acc-0.9200, valid loss-0.5002, acc-0.8632, test loss-0.5035, acc-0.8611\n",
      "Iter-42150, train loss-0.8372, acc-0.7200, valid loss-0.5001, acc-0.8630, test loss-0.5035, acc-0.8610\n",
      "Iter-42160, train loss-0.4252, acc-0.9200, valid loss-0.5001, acc-0.8628, test loss-0.5034, acc-0.8610\n",
      "Iter-42170, train loss-0.5623, acc-0.8400, valid loss-0.5000, acc-0.8630, test loss-0.5034, acc-0.8610\n",
      "Iter-42180, train loss-0.6384, acc-0.8400, valid loss-0.5000, acc-0.8632, test loss-0.5033, acc-0.8610\n",
      "Iter-42190, train loss-0.5273, acc-0.9000, valid loss-0.4999, acc-0.8632, test loss-0.5033, acc-0.8610\n",
      "Iter-42200, train loss-0.3100, acc-0.9600, valid loss-0.4999, acc-0.8632, test loss-0.5032, acc-0.8610\n",
      "Iter-42210, train loss-0.6729, acc-0.8200, valid loss-0.4998, acc-0.8632, test loss-0.5031, acc-0.8611\n",
      "Iter-42220, train loss-0.2979, acc-0.9200, valid loss-0.4998, acc-0.8634, test loss-0.5031, acc-0.8610\n",
      "Iter-42230, train loss-0.4398, acc-0.8800, valid loss-0.4997, acc-0.8632, test loss-0.5030, acc-0.8611\n",
      "Iter-42240, train loss-0.5869, acc-0.8200, valid loss-0.4996, acc-0.8632, test loss-0.5030, acc-0.8612\n",
      "Iter-42250, train loss-0.5181, acc-0.8600, valid loss-0.4996, acc-0.8634, test loss-0.5029, acc-0.8613\n",
      "Iter-42260, train loss-0.4283, acc-0.9000, valid loss-0.4995, acc-0.8634, test loss-0.5028, acc-0.8613\n",
      "Iter-42270, train loss-0.3957, acc-0.9000, valid loss-0.4994, acc-0.8632, test loss-0.5028, acc-0.8614\n",
      "Iter-42280, train loss-0.5356, acc-0.8400, valid loss-0.4994, acc-0.8632, test loss-0.5027, acc-0.8614\n",
      "Iter-42290, train loss-0.4430, acc-0.8200, valid loss-0.4993, acc-0.8634, test loss-0.5027, acc-0.8613\n",
      "Iter-42300, train loss-0.4410, acc-0.9000, valid loss-0.4992, acc-0.8634, test loss-0.5026, acc-0.8613\n",
      "Iter-42310, train loss-0.6243, acc-0.7600, valid loss-0.4992, acc-0.8634, test loss-0.5026, acc-0.8613\n",
      "Iter-42320, train loss-0.4637, acc-0.8400, valid loss-0.4991, acc-0.8634, test loss-0.5025, acc-0.8613\n",
      "Iter-42330, train loss-0.4076, acc-0.8200, valid loss-0.4991, acc-0.8634, test loss-0.5024, acc-0.8613\n",
      "Iter-42340, train loss-0.5225, acc-0.8600, valid loss-0.4990, acc-0.8634, test loss-0.5023, acc-0.8614\n",
      "Iter-42350, train loss-0.5783, acc-0.8200, valid loss-0.4989, acc-0.8634, test loss-0.5023, acc-0.8613\n",
      "Iter-42360, train loss-0.4157, acc-0.9000, valid loss-0.4989, acc-0.8634, test loss-0.5022, acc-0.8614\n",
      "Iter-42370, train loss-0.4456, acc-0.8800, valid loss-0.4988, acc-0.8634, test loss-0.5022, acc-0.8615\n",
      "Iter-42380, train loss-0.7168, acc-0.7800, valid loss-0.4988, acc-0.8634, test loss-0.5021, acc-0.8615\n",
      "Iter-42390, train loss-0.4822, acc-0.8400, valid loss-0.4987, acc-0.8632, test loss-0.5021, acc-0.8615\n",
      "Iter-42400, train loss-0.5872, acc-0.8200, valid loss-0.4986, acc-0.8632, test loss-0.5020, acc-0.8615\n",
      "Iter-42410, train loss-0.5194, acc-0.8800, valid loss-0.4986, acc-0.8632, test loss-0.5019, acc-0.8614\n",
      "Iter-42420, train loss-0.4477, acc-0.8200, valid loss-0.4985, acc-0.8632, test loss-0.5019, acc-0.8614\n",
      "Iter-42430, train loss-0.5386, acc-0.8400, valid loss-0.4984, acc-0.8634, test loss-0.5018, acc-0.8614\n",
      "Iter-42440, train loss-0.4562, acc-0.9200, valid loss-0.4984, acc-0.8634, test loss-0.5017, acc-0.8614\n",
      "Iter-42450, train loss-0.3995, acc-0.9000, valid loss-0.4983, acc-0.8634, test loss-0.5017, acc-0.8614\n",
      "Iter-42460, train loss-0.6474, acc-0.8000, valid loss-0.4983, acc-0.8634, test loss-0.5016, acc-0.8614\n",
      "Iter-42470, train loss-0.5893, acc-0.8800, valid loss-0.4982, acc-0.8634, test loss-0.5016, acc-0.8613\n",
      "Iter-42480, train loss-0.5243, acc-0.8200, valid loss-0.4981, acc-0.8634, test loss-0.5015, acc-0.8614\n",
      "Iter-42490, train loss-0.3842, acc-0.9200, valid loss-0.4981, acc-0.8634, test loss-0.5014, acc-0.8615\n",
      "Iter-42500, train loss-0.6412, acc-0.7400, valid loss-0.4980, acc-0.8634, test loss-0.5014, acc-0.8615\n",
      "Iter-42510, train loss-0.4039, acc-0.8800, valid loss-0.4979, acc-0.8634, test loss-0.5013, acc-0.8616\n",
      "Iter-42520, train loss-0.5868, acc-0.8400, valid loss-0.4979, acc-0.8632, test loss-0.5012, acc-0.8616\n",
      "Iter-42530, train loss-0.3513, acc-0.8800, valid loss-0.4978, acc-0.8634, test loss-0.5012, acc-0.8616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-42540, train loss-0.3279, acc-0.9000, valid loss-0.4978, acc-0.8634, test loss-0.5011, acc-0.8617\n",
      "Iter-42550, train loss-0.4089, acc-0.8800, valid loss-0.4977, acc-0.8634, test loss-0.5011, acc-0.8616\n",
      "Iter-42560, train loss-0.4125, acc-0.9200, valid loss-0.4976, acc-0.8632, test loss-0.5010, acc-0.8617\n",
      "Iter-42570, train loss-0.4498, acc-0.9200, valid loss-0.4976, acc-0.8632, test loss-0.5010, acc-0.8618\n",
      "Iter-42580, train loss-0.3456, acc-0.9400, valid loss-0.4976, acc-0.8634, test loss-0.5009, acc-0.8616\n",
      "Iter-42590, train loss-0.4280, acc-0.8600, valid loss-0.4975, acc-0.8636, test loss-0.5008, acc-0.8616\n",
      "Iter-42600, train loss-0.6021, acc-0.8800, valid loss-0.4974, acc-0.8634, test loss-0.5008, acc-0.8618\n",
      "Iter-42610, train loss-0.4767, acc-0.8800, valid loss-0.4974, acc-0.8634, test loss-0.5007, acc-0.8618\n",
      "Iter-42620, train loss-0.5880, acc-0.8000, valid loss-0.4973, acc-0.8634, test loss-0.5007, acc-0.8618\n",
      "Iter-42630, train loss-0.4736, acc-0.8800, valid loss-0.4973, acc-0.8634, test loss-0.5006, acc-0.8618\n",
      "Iter-42640, train loss-0.7803, acc-0.7600, valid loss-0.4972, acc-0.8642, test loss-0.5006, acc-0.8619\n",
      "Iter-42650, train loss-0.5207, acc-0.8000, valid loss-0.4971, acc-0.8640, test loss-0.5005, acc-0.8620\n",
      "Iter-42660, train loss-0.4676, acc-0.8800, valid loss-0.4971, acc-0.8640, test loss-0.5004, acc-0.8617\n",
      "Iter-42670, train loss-0.4174, acc-0.9200, valid loss-0.4970, acc-0.8638, test loss-0.5004, acc-0.8617\n",
      "Iter-42680, train loss-0.5433, acc-0.8400, valid loss-0.4970, acc-0.8640, test loss-0.5003, acc-0.8620\n",
      "Iter-42690, train loss-0.4962, acc-0.8600, valid loss-0.4970, acc-0.8640, test loss-0.5003, acc-0.8618\n",
      "Iter-42700, train loss-0.5135, acc-0.8600, valid loss-0.4969, acc-0.8638, test loss-0.5002, acc-0.8618\n",
      "Iter-42710, train loss-0.4016, acc-0.8800, valid loss-0.4968, acc-0.8638, test loss-0.5002, acc-0.8618\n",
      "Iter-42720, train loss-0.4426, acc-0.8600, valid loss-0.4967, acc-0.8638, test loss-0.5001, acc-0.8617\n",
      "Iter-42730, train loss-0.5708, acc-0.8800, valid loss-0.4967, acc-0.8638, test loss-0.5001, acc-0.8618\n",
      "Iter-42740, train loss-0.6591, acc-0.8600, valid loss-0.4966, acc-0.8636, test loss-0.5000, acc-0.8618\n",
      "Iter-42750, train loss-0.4707, acc-0.8000, valid loss-0.4966, acc-0.8638, test loss-0.4999, acc-0.8618\n",
      "Iter-42760, train loss-0.4396, acc-0.8000, valid loss-0.4965, acc-0.8638, test loss-0.4999, acc-0.8619\n",
      "Iter-42770, train loss-0.5317, acc-0.8600, valid loss-0.4965, acc-0.8638, test loss-0.4998, acc-0.8620\n",
      "Iter-42780, train loss-0.6372, acc-0.8200, valid loss-0.4964, acc-0.8638, test loss-0.4998, acc-0.8620\n",
      "Iter-42790, train loss-0.4516, acc-0.8800, valid loss-0.4964, acc-0.8638, test loss-0.4997, acc-0.8620\n",
      "Iter-42800, train loss-0.3535, acc-0.9000, valid loss-0.4963, acc-0.8638, test loss-0.4997, acc-0.8620\n",
      "Iter-42810, train loss-0.5685, acc-0.8600, valid loss-0.4963, acc-0.8638, test loss-0.4996, acc-0.8621\n",
      "Iter-42820, train loss-0.5477, acc-0.8000, valid loss-0.4962, acc-0.8636, test loss-0.4996, acc-0.8620\n",
      "Iter-42830, train loss-0.3939, acc-0.9000, valid loss-0.4962, acc-0.8636, test loss-0.4995, acc-0.8620\n",
      "Iter-42840, train loss-0.6033, acc-0.8400, valid loss-0.4961, acc-0.8640, test loss-0.4995, acc-0.8620\n",
      "Iter-42850, train loss-0.4445, acc-0.8800, valid loss-0.4960, acc-0.8638, test loss-0.4994, acc-0.8621\n",
      "Iter-42860, train loss-0.6757, acc-0.8200, valid loss-0.4960, acc-0.8642, test loss-0.4993, acc-0.8620\n",
      "Iter-42870, train loss-0.4485, acc-0.9000, valid loss-0.4959, acc-0.8640, test loss-0.4993, acc-0.8620\n",
      "Iter-42880, train loss-0.4877, acc-0.9000, valid loss-0.4958, acc-0.8642, test loss-0.4992, acc-0.8620\n",
      "Iter-42890, train loss-0.3993, acc-0.9000, valid loss-0.4958, acc-0.8640, test loss-0.4992, acc-0.8620\n",
      "Iter-42900, train loss-0.4788, acc-0.8400, valid loss-0.4957, acc-0.8642, test loss-0.4991, acc-0.8620\n",
      "Iter-42910, train loss-0.5382, acc-0.8400, valid loss-0.4956, acc-0.8642, test loss-0.4990, acc-0.8620\n",
      "Iter-42920, train loss-0.3877, acc-0.9200, valid loss-0.4956, acc-0.8642, test loss-0.4990, acc-0.8620\n",
      "Iter-42930, train loss-0.6712, acc-0.7400, valid loss-0.4955, acc-0.8644, test loss-0.4989, acc-0.8619\n",
      "Iter-42940, train loss-0.4205, acc-0.8600, valid loss-0.4955, acc-0.8642, test loss-0.4989, acc-0.8619\n",
      "Iter-42950, train loss-0.4370, acc-0.8800, valid loss-0.4954, acc-0.8642, test loss-0.4988, acc-0.8619\n",
      "Iter-42960, train loss-0.4184, acc-0.9200, valid loss-0.4953, acc-0.8642, test loss-0.4988, acc-0.8619\n",
      "Iter-42970, train loss-0.4030, acc-0.9000, valid loss-0.4953, acc-0.8642, test loss-0.4987, acc-0.8619\n",
      "Iter-42980, train loss-0.3713, acc-0.9000, valid loss-0.4953, acc-0.8642, test loss-0.4987, acc-0.8619\n",
      "Iter-42990, train loss-0.6915, acc-0.8200, valid loss-0.4952, acc-0.8644, test loss-0.4986, acc-0.8619\n",
      "Iter-43000, train loss-0.3839, acc-0.9400, valid loss-0.4951, acc-0.8644, test loss-0.4985, acc-0.8619\n",
      "Iter-43010, train loss-0.4606, acc-0.8600, valid loss-0.4951, acc-0.8644, test loss-0.4985, acc-0.8619\n",
      "Iter-43020, train loss-0.4639, acc-0.8600, valid loss-0.4951, acc-0.8644, test loss-0.4984, acc-0.8619\n",
      "Iter-43030, train loss-0.5000, acc-0.8400, valid loss-0.4950, acc-0.8644, test loss-0.4984, acc-0.8619\n",
      "Iter-43040, train loss-0.5565, acc-0.8400, valid loss-0.4949, acc-0.8644, test loss-0.4983, acc-0.8619\n",
      "Iter-43050, train loss-0.4827, acc-0.9000, valid loss-0.4949, acc-0.8644, test loss-0.4983, acc-0.8619\n",
      "Iter-43060, train loss-0.4359, acc-0.8800, valid loss-0.4948, acc-0.8644, test loss-0.4982, acc-0.8620\n",
      "Iter-43070, train loss-0.6206, acc-0.8000, valid loss-0.4948, acc-0.8646, test loss-0.4981, acc-0.8619\n",
      "Iter-43080, train loss-0.4048, acc-0.9400, valid loss-0.4947, acc-0.8644, test loss-0.4981, acc-0.8619\n",
      "Iter-43090, train loss-0.6827, acc-0.8400, valid loss-0.4947, acc-0.8642, test loss-0.4980, acc-0.8619\n",
      "Iter-43100, train loss-0.4102, acc-0.9000, valid loss-0.4946, acc-0.8640, test loss-0.4980, acc-0.8619\n",
      "Iter-43110, train loss-0.6371, acc-0.8400, valid loss-0.4946, acc-0.8642, test loss-0.4979, acc-0.8619\n",
      "Iter-43120, train loss-0.7975, acc-0.7600, valid loss-0.4945, acc-0.8644, test loss-0.4978, acc-0.8620\n",
      "Iter-43130, train loss-0.4952, acc-0.8400, valid loss-0.4945, acc-0.8644, test loss-0.4978, acc-0.8620\n",
      "Iter-43140, train loss-0.5449, acc-0.8200, valid loss-0.4944, acc-0.8644, test loss-0.4977, acc-0.8621\n",
      "Iter-43150, train loss-0.4501, acc-0.9000, valid loss-0.4943, acc-0.8644, test loss-0.4977, acc-0.8623\n",
      "Iter-43160, train loss-0.6419, acc-0.8600, valid loss-0.4942, acc-0.8644, test loss-0.4976, acc-0.8623\n",
      "Iter-43170, train loss-0.4073, acc-0.8800, valid loss-0.4942, acc-0.8644, test loss-0.4976, acc-0.8623\n",
      "Iter-43180, train loss-0.5274, acc-0.8600, valid loss-0.4941, acc-0.8644, test loss-0.4975, acc-0.8622\n",
      "Iter-43190, train loss-0.3456, acc-0.9400, valid loss-0.4941, acc-0.8644, test loss-0.4974, acc-0.8621\n",
      "Iter-43200, train loss-0.5110, acc-0.8400, valid loss-0.4940, acc-0.8644, test loss-0.4974, acc-0.8622\n",
      "Iter-43210, train loss-0.3343, acc-0.9200, valid loss-0.4939, acc-0.8646, test loss-0.4973, acc-0.8622\n",
      "Iter-43220, train loss-0.8159, acc-0.7600, valid loss-0.4939, acc-0.8646, test loss-0.4973, acc-0.8622\n",
      "Iter-43230, train loss-0.4524, acc-0.8800, valid loss-0.4938, acc-0.8646, test loss-0.4972, acc-0.8622\n",
      "Iter-43240, train loss-0.5352, acc-0.8200, valid loss-0.4938, acc-0.8646, test loss-0.4972, acc-0.8621\n",
      "Iter-43250, train loss-0.6977, acc-0.7600, valid loss-0.4937, acc-0.8646, test loss-0.4971, acc-0.8621\n",
      "Iter-43260, train loss-0.4848, acc-0.8600, valid loss-0.4937, acc-0.8648, test loss-0.4971, acc-0.8620\n",
      "Iter-43270, train loss-0.5550, acc-0.8400, valid loss-0.4936, acc-0.8648, test loss-0.4970, acc-0.8621\n",
      "Iter-43280, train loss-0.3727, acc-0.9200, valid loss-0.4935, acc-0.8648, test loss-0.4970, acc-0.8622\n",
      "Iter-43290, train loss-0.5007, acc-0.9000, valid loss-0.4935, acc-0.8650, test loss-0.4969, acc-0.8622\n",
      "Iter-43300, train loss-0.7288, acc-0.7400, valid loss-0.4934, acc-0.8650, test loss-0.4969, acc-0.8623\n",
      "Iter-43310, train loss-0.5322, acc-0.8400, valid loss-0.4934, acc-0.8648, test loss-0.4968, acc-0.8624\n",
      "Iter-43320, train loss-0.3371, acc-0.8800, valid loss-0.4933, acc-0.8650, test loss-0.4967, acc-0.8621\n",
      "Iter-43330, train loss-0.4147, acc-0.9000, valid loss-0.4932, acc-0.8650, test loss-0.4967, acc-0.8624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-43340, train loss-0.3491, acc-0.8600, valid loss-0.4932, acc-0.8650, test loss-0.4966, acc-0.8623\n",
      "Iter-43350, train loss-0.4433, acc-0.8800, valid loss-0.4931, acc-0.8650, test loss-0.4966, acc-0.8624\n",
      "Iter-43360, train loss-0.5137, acc-0.8000, valid loss-0.4930, acc-0.8650, test loss-0.4965, acc-0.8623\n",
      "Iter-43370, train loss-0.3746, acc-0.8800, valid loss-0.4930, acc-0.8650, test loss-0.4965, acc-0.8623\n",
      "Iter-43380, train loss-0.4776, acc-0.9000, valid loss-0.4929, acc-0.8650, test loss-0.4964, acc-0.8623\n",
      "Iter-43390, train loss-0.4617, acc-0.8600, valid loss-0.4929, acc-0.8650, test loss-0.4964, acc-0.8623\n",
      "Iter-43400, train loss-0.6673, acc-0.8000, valid loss-0.4928, acc-0.8650, test loss-0.4963, acc-0.8623\n",
      "Iter-43410, train loss-0.4680, acc-0.8600, valid loss-0.4927, acc-0.8650, test loss-0.4962, acc-0.8625\n",
      "Iter-43420, train loss-0.5282, acc-0.8600, valid loss-0.4927, acc-0.8650, test loss-0.4962, acc-0.8625\n",
      "Iter-43430, train loss-0.3139, acc-0.9400, valid loss-0.4926, acc-0.8650, test loss-0.4961, acc-0.8625\n",
      "Iter-43440, train loss-0.4181, acc-0.9000, valid loss-0.4926, acc-0.8650, test loss-0.4960, acc-0.8624\n",
      "Iter-43450, train loss-0.3466, acc-0.9000, valid loss-0.4925, acc-0.8652, test loss-0.4960, acc-0.8624\n",
      "Iter-43460, train loss-0.5185, acc-0.8200, valid loss-0.4925, acc-0.8652, test loss-0.4959, acc-0.8625\n",
      "Iter-43470, train loss-0.3936, acc-0.8800, valid loss-0.4924, acc-0.8652, test loss-0.4959, acc-0.8626\n",
      "Iter-43480, train loss-0.3051, acc-0.9400, valid loss-0.4923, acc-0.8652, test loss-0.4958, acc-0.8625\n",
      "Iter-43490, train loss-0.6106, acc-0.8200, valid loss-0.4923, acc-0.8652, test loss-0.4958, acc-0.8626\n",
      "Iter-43500, train loss-0.6282, acc-0.8000, valid loss-0.4922, acc-0.8654, test loss-0.4957, acc-0.8626\n",
      "Iter-43510, train loss-0.5147, acc-0.8200, valid loss-0.4922, acc-0.8654, test loss-0.4956, acc-0.8625\n",
      "Iter-43520, train loss-0.3895, acc-0.9600, valid loss-0.4921, acc-0.8654, test loss-0.4956, acc-0.8625\n",
      "Iter-43530, train loss-0.5352, acc-0.8800, valid loss-0.4920, acc-0.8654, test loss-0.4955, acc-0.8626\n",
      "Iter-43540, train loss-0.3608, acc-0.9000, valid loss-0.4920, acc-0.8654, test loss-0.4955, acc-0.8626\n",
      "Iter-43550, train loss-0.4512, acc-0.9200, valid loss-0.4919, acc-0.8654, test loss-0.4954, acc-0.8626\n",
      "Iter-43560, train loss-0.5630, acc-0.8000, valid loss-0.4919, acc-0.8654, test loss-0.4954, acc-0.8627\n",
      "Iter-43570, train loss-0.5267, acc-0.8400, valid loss-0.4918, acc-0.8652, test loss-0.4953, acc-0.8627\n",
      "Iter-43580, train loss-0.3091, acc-0.9400, valid loss-0.4917, acc-0.8654, test loss-0.4952, acc-0.8627\n",
      "Iter-43590, train loss-0.4991, acc-0.8800, valid loss-0.4917, acc-0.8654, test loss-0.4952, acc-0.8626\n",
      "Iter-43600, train loss-0.4747, acc-0.8800, valid loss-0.4916, acc-0.8654, test loss-0.4951, acc-0.8627\n",
      "Iter-43610, train loss-0.5692, acc-0.8400, valid loss-0.4916, acc-0.8654, test loss-0.4951, acc-0.8626\n",
      "Iter-43620, train loss-0.4131, acc-0.8800, valid loss-0.4915, acc-0.8654, test loss-0.4950, acc-0.8625\n",
      "Iter-43630, train loss-0.4258, acc-0.9000, valid loss-0.4915, acc-0.8654, test loss-0.4950, acc-0.8626\n",
      "Iter-43640, train loss-0.5101, acc-0.8200, valid loss-0.4914, acc-0.8654, test loss-0.4949, acc-0.8626\n",
      "Iter-43650, train loss-0.3949, acc-0.9000, valid loss-0.4913, acc-0.8654, test loss-0.4948, acc-0.8626\n",
      "Iter-43660, train loss-0.3512, acc-0.9000, valid loss-0.4913, acc-0.8656, test loss-0.4948, acc-0.8625\n",
      "Iter-43670, train loss-0.4561, acc-0.8600, valid loss-0.4912, acc-0.8656, test loss-0.4947, acc-0.8626\n",
      "Iter-43680, train loss-0.7363, acc-0.7400, valid loss-0.4911, acc-0.8656, test loss-0.4947, acc-0.8626\n",
      "Iter-43690, train loss-0.5654, acc-0.7800, valid loss-0.4911, acc-0.8656, test loss-0.4946, acc-0.8626\n",
      "Iter-43700, train loss-0.4315, acc-0.9200, valid loss-0.4911, acc-0.8656, test loss-0.4946, acc-0.8628\n",
      "Iter-43710, train loss-0.5843, acc-0.8200, valid loss-0.4910, acc-0.8656, test loss-0.4945, acc-0.8627\n",
      "Iter-43720, train loss-0.4640, acc-0.9000, valid loss-0.4909, acc-0.8656, test loss-0.4945, acc-0.8627\n",
      "Iter-43730, train loss-0.4194, acc-0.9000, valid loss-0.4909, acc-0.8656, test loss-0.4944, acc-0.8627\n",
      "Iter-43740, train loss-0.7290, acc-0.7800, valid loss-0.4908, acc-0.8656, test loss-0.4944, acc-0.8627\n",
      "Iter-43750, train loss-0.5834, acc-0.8400, valid loss-0.4908, acc-0.8656, test loss-0.4943, acc-0.8627\n",
      "Iter-43760, train loss-0.5354, acc-0.8400, valid loss-0.4907, acc-0.8656, test loss-0.4943, acc-0.8628\n",
      "Iter-43770, train loss-0.5904, acc-0.7800, valid loss-0.4907, acc-0.8656, test loss-0.4942, acc-0.8625\n",
      "Iter-43780, train loss-0.3376, acc-0.9400, valid loss-0.4906, acc-0.8656, test loss-0.4941, acc-0.8625\n",
      "Iter-43790, train loss-0.3764, acc-0.8800, valid loss-0.4905, acc-0.8658, test loss-0.4940, acc-0.8629\n",
      "Iter-43800, train loss-0.4068, acc-0.9200, valid loss-0.4905, acc-0.8658, test loss-0.4940, acc-0.8628\n",
      "Iter-43810, train loss-0.4929, acc-0.8600, valid loss-0.4904, acc-0.8658, test loss-0.4939, acc-0.8629\n",
      "Iter-43820, train loss-0.3989, acc-0.9200, valid loss-0.4904, acc-0.8658, test loss-0.4939, acc-0.8629\n",
      "Iter-43830, train loss-0.4162, acc-0.8600, valid loss-0.4903, acc-0.8656, test loss-0.4938, acc-0.8629\n",
      "Iter-43840, train loss-0.5255, acc-0.9200, valid loss-0.4902, acc-0.8656, test loss-0.4938, acc-0.8629\n",
      "Iter-43850, train loss-0.5959, acc-0.8600, valid loss-0.4902, acc-0.8658, test loss-0.4937, acc-0.8630\n",
      "Iter-43860, train loss-0.4807, acc-0.8800, valid loss-0.4901, acc-0.8656, test loss-0.4936, acc-0.8629\n",
      "Iter-43870, train loss-0.4475, acc-0.9000, valid loss-0.4901, acc-0.8658, test loss-0.4936, acc-0.8629\n",
      "Iter-43880, train loss-0.6766, acc-0.7800, valid loss-0.4900, acc-0.8656, test loss-0.4935, acc-0.8628\n",
      "Iter-43890, train loss-0.4946, acc-0.8800, valid loss-0.4900, acc-0.8656, test loss-0.4935, acc-0.8628\n",
      "Iter-43900, train loss-0.3927, acc-0.9200, valid loss-0.4899, acc-0.8656, test loss-0.4934, acc-0.8628\n",
      "Iter-43910, train loss-0.7444, acc-0.7800, valid loss-0.4899, acc-0.8654, test loss-0.4934, acc-0.8629\n",
      "Iter-43920, train loss-0.4018, acc-0.8800, valid loss-0.4898, acc-0.8656, test loss-0.4933, acc-0.8629\n",
      "Iter-43930, train loss-0.5254, acc-0.8600, valid loss-0.4898, acc-0.8656, test loss-0.4933, acc-0.8629\n",
      "Iter-43940, train loss-0.6058, acc-0.7800, valid loss-0.4897, acc-0.8656, test loss-0.4932, acc-0.8630\n",
      "Iter-43950, train loss-0.4119, acc-0.8400, valid loss-0.4897, acc-0.8656, test loss-0.4932, acc-0.8630\n",
      "Iter-43960, train loss-0.4013, acc-0.9200, valid loss-0.4896, acc-0.8654, test loss-0.4931, acc-0.8630\n",
      "Iter-43970, train loss-0.6009, acc-0.8200, valid loss-0.4895, acc-0.8658, test loss-0.4931, acc-0.8631\n",
      "Iter-43980, train loss-0.4630, acc-0.8600, valid loss-0.4895, acc-0.8654, test loss-0.4930, acc-0.8630\n",
      "Iter-43990, train loss-0.4379, acc-0.8800, valid loss-0.4894, acc-0.8658, test loss-0.4930, acc-0.8631\n",
      "Iter-44000, train loss-0.4559, acc-0.8400, valid loss-0.4894, acc-0.8656, test loss-0.4929, acc-0.8630\n",
      "Iter-44010, train loss-0.5024, acc-0.8400, valid loss-0.4893, acc-0.8654, test loss-0.4928, acc-0.8630\n",
      "Iter-44020, train loss-0.4747, acc-0.8800, valid loss-0.4892, acc-0.8656, test loss-0.4928, acc-0.8631\n",
      "Iter-44030, train loss-0.4682, acc-0.8800, valid loss-0.4892, acc-0.8656, test loss-0.4927, acc-0.8631\n",
      "Iter-44040, train loss-0.3898, acc-0.9400, valid loss-0.4891, acc-0.8656, test loss-0.4927, acc-0.8631\n",
      "Iter-44050, train loss-0.6328, acc-0.8400, valid loss-0.4891, acc-0.8656, test loss-0.4926, acc-0.8632\n",
      "Iter-44060, train loss-0.5544, acc-0.8200, valid loss-0.4890, acc-0.8658, test loss-0.4926, acc-0.8632\n",
      "Iter-44070, train loss-0.3544, acc-0.9000, valid loss-0.4890, acc-0.8656, test loss-0.4925, acc-0.8633\n",
      "Iter-44080, train loss-0.6712, acc-0.8800, valid loss-0.4890, acc-0.8660, test loss-0.4925, acc-0.8633\n",
      "Iter-44090, train loss-0.6567, acc-0.8000, valid loss-0.4889, acc-0.8660, test loss-0.4924, acc-0.8633\n",
      "Iter-44100, train loss-0.4417, acc-0.8800, valid loss-0.4889, acc-0.8660, test loss-0.4924, acc-0.8633\n",
      "Iter-44110, train loss-0.5028, acc-0.8400, valid loss-0.4888, acc-0.8658, test loss-0.4923, acc-0.8633\n",
      "Iter-44120, train loss-0.5382, acc-0.8600, valid loss-0.4887, acc-0.8660, test loss-0.4922, acc-0.8633\n",
      "Iter-44130, train loss-0.3066, acc-0.9200, valid loss-0.4887, acc-0.8660, test loss-0.4922, acc-0.8633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-44140, train loss-0.3644, acc-0.9200, valid loss-0.4886, acc-0.8662, test loss-0.4921, acc-0.8633\n",
      "Iter-44150, train loss-0.4413, acc-0.8800, valid loss-0.4886, acc-0.8662, test loss-0.4921, acc-0.8633\n",
      "Iter-44160, train loss-0.3857, acc-0.9200, valid loss-0.4885, acc-0.8664, test loss-0.4920, acc-0.8633\n",
      "Iter-44170, train loss-0.5326, acc-0.8400, valid loss-0.4885, acc-0.8664, test loss-0.4919, acc-0.8633\n",
      "Iter-44180, train loss-0.5589, acc-0.7800, valid loss-0.4884, acc-0.8668, test loss-0.4919, acc-0.8634\n",
      "Iter-44190, train loss-0.5458, acc-0.8800, valid loss-0.4883, acc-0.8666, test loss-0.4918, acc-0.8633\n",
      "Iter-44200, train loss-0.5619, acc-0.8600, valid loss-0.4883, acc-0.8668, test loss-0.4918, acc-0.8634\n",
      "Iter-44210, train loss-0.5522, acc-0.7600, valid loss-0.4882, acc-0.8668, test loss-0.4917, acc-0.8633\n",
      "Iter-44220, train loss-0.4978, acc-0.8600, valid loss-0.4882, acc-0.8664, test loss-0.4917, acc-0.8634\n",
      "Iter-44230, train loss-0.4316, acc-0.8800, valid loss-0.4881, acc-0.8664, test loss-0.4916, acc-0.8636\n",
      "Iter-44240, train loss-0.4771, acc-0.8800, valid loss-0.4880, acc-0.8664, test loss-0.4915, acc-0.8636\n",
      "Iter-44250, train loss-0.4121, acc-0.8600, valid loss-0.4880, acc-0.8664, test loss-0.4915, acc-0.8636\n",
      "Iter-44260, train loss-0.4820, acc-0.8400, valid loss-0.4879, acc-0.8664, test loss-0.4914, acc-0.8636\n",
      "Iter-44270, train loss-0.6984, acc-0.8200, valid loss-0.4879, acc-0.8664, test loss-0.4914, acc-0.8635\n",
      "Iter-44280, train loss-0.5510, acc-0.7800, valid loss-0.4878, acc-0.8666, test loss-0.4913, acc-0.8635\n",
      "Iter-44290, train loss-0.5551, acc-0.8800, valid loss-0.4878, acc-0.8666, test loss-0.4913, acc-0.8635\n",
      "Iter-44300, train loss-0.4106, acc-0.8400, valid loss-0.4877, acc-0.8666, test loss-0.4912, acc-0.8635\n",
      "Iter-44310, train loss-0.4145, acc-0.9000, valid loss-0.4877, acc-0.8664, test loss-0.4912, acc-0.8636\n",
      "Iter-44320, train loss-0.5730, acc-0.8600, valid loss-0.4876, acc-0.8666, test loss-0.4911, acc-0.8635\n",
      "Iter-44330, train loss-0.6683, acc-0.8600, valid loss-0.4875, acc-0.8666, test loss-0.4911, acc-0.8635\n",
      "Iter-44340, train loss-0.4827, acc-0.8400, valid loss-0.4875, acc-0.8666, test loss-0.4910, acc-0.8637\n",
      "Iter-44350, train loss-0.6439, acc-0.8200, valid loss-0.4874, acc-0.8664, test loss-0.4910, acc-0.8637\n",
      "Iter-44360, train loss-0.4519, acc-0.8600, valid loss-0.4874, acc-0.8666, test loss-0.4909, acc-0.8637\n",
      "Iter-44370, train loss-0.3617, acc-0.9000, valid loss-0.4873, acc-0.8664, test loss-0.4909, acc-0.8638\n",
      "Iter-44380, train loss-0.7021, acc-0.8400, valid loss-0.4872, acc-0.8664, test loss-0.4908, acc-0.8639\n",
      "Iter-44390, train loss-0.3866, acc-0.8600, valid loss-0.4872, acc-0.8662, test loss-0.4907, acc-0.8638\n",
      "Iter-44400, train loss-0.4355, acc-0.9000, valid loss-0.4871, acc-0.8666, test loss-0.4907, acc-0.8638\n",
      "Iter-44410, train loss-0.4776, acc-0.9000, valid loss-0.4871, acc-0.8666, test loss-0.4906, acc-0.8638\n",
      "Iter-44420, train loss-0.4752, acc-0.8800, valid loss-0.4870, acc-0.8666, test loss-0.4906, acc-0.8638\n",
      "Iter-44430, train loss-0.5113, acc-0.8600, valid loss-0.4870, acc-0.8666, test loss-0.4905, acc-0.8638\n",
      "Iter-44440, train loss-0.5467, acc-0.8600, valid loss-0.4869, acc-0.8666, test loss-0.4905, acc-0.8639\n",
      "Iter-44450, train loss-0.6605, acc-0.7800, valid loss-0.4869, acc-0.8668, test loss-0.4904, acc-0.8640\n",
      "Iter-44460, train loss-0.4032, acc-0.8400, valid loss-0.4868, acc-0.8664, test loss-0.4903, acc-0.8640\n",
      "Iter-44470, train loss-0.4783, acc-0.8800, valid loss-0.4868, acc-0.8668, test loss-0.4903, acc-0.8642\n",
      "Iter-44480, train loss-0.5513, acc-0.8400, valid loss-0.4867, acc-0.8664, test loss-0.4902, acc-0.8642\n",
      "Iter-44490, train loss-0.6329, acc-0.8200, valid loss-0.4866, acc-0.8668, test loss-0.4902, acc-0.8642\n",
      "Iter-44500, train loss-0.6335, acc-0.7600, valid loss-0.4866, acc-0.8670, test loss-0.4901, acc-0.8643\n",
      "Iter-44510, train loss-0.4252, acc-0.8400, valid loss-0.4865, acc-0.8672, test loss-0.4900, acc-0.8642\n",
      "Iter-44520, train loss-0.6217, acc-0.8000, valid loss-0.4865, acc-0.8672, test loss-0.4900, acc-0.8644\n",
      "Iter-44530, train loss-0.6230, acc-0.8400, valid loss-0.4865, acc-0.8672, test loss-0.4899, acc-0.8644\n",
      "Iter-44540, train loss-0.3770, acc-0.9200, valid loss-0.4864, acc-0.8672, test loss-0.4899, acc-0.8645\n",
      "Iter-44550, train loss-0.5504, acc-0.8400, valid loss-0.4864, acc-0.8672, test loss-0.4898, acc-0.8645\n",
      "Iter-44560, train loss-0.4966, acc-0.8600, valid loss-0.4863, acc-0.8674, test loss-0.4897, acc-0.8645\n",
      "Iter-44570, train loss-0.3326, acc-0.9000, valid loss-0.4862, acc-0.8672, test loss-0.4897, acc-0.8644\n",
      "Iter-44580, train loss-0.4862, acc-0.8800, valid loss-0.4862, acc-0.8672, test loss-0.4896, acc-0.8644\n",
      "Iter-44590, train loss-0.4817, acc-0.8800, valid loss-0.4861, acc-0.8674, test loss-0.4896, acc-0.8644\n",
      "Iter-44600, train loss-0.4699, acc-0.9200, valid loss-0.4861, acc-0.8674, test loss-0.4895, acc-0.8646\n",
      "Iter-44610, train loss-0.6032, acc-0.7600, valid loss-0.4860, acc-0.8674, test loss-0.4895, acc-0.8646\n",
      "Iter-44620, train loss-0.5848, acc-0.8200, valid loss-0.4860, acc-0.8674, test loss-0.4894, acc-0.8644\n",
      "Iter-44630, train loss-0.5770, acc-0.8600, valid loss-0.4859, acc-0.8672, test loss-0.4894, acc-0.8643\n",
      "Iter-44640, train loss-0.5186, acc-0.8800, valid loss-0.4859, acc-0.8670, test loss-0.4893, acc-0.8645\n",
      "Iter-44650, train loss-0.3860, acc-0.9400, valid loss-0.4858, acc-0.8674, test loss-0.4893, acc-0.8646\n",
      "Iter-44660, train loss-0.5358, acc-0.8400, valid loss-0.4858, acc-0.8672, test loss-0.4892, acc-0.8647\n",
      "Iter-44670, train loss-0.6209, acc-0.8400, valid loss-0.4857, acc-0.8674, test loss-0.4891, acc-0.8647\n",
      "Iter-44680, train loss-0.5911, acc-0.8400, valid loss-0.4856, acc-0.8674, test loss-0.4891, acc-0.8645\n",
      "Iter-44690, train loss-0.3674, acc-0.9000, valid loss-0.4856, acc-0.8674, test loss-0.4890, acc-0.8646\n",
      "Iter-44700, train loss-0.5114, acc-0.9400, valid loss-0.4855, acc-0.8674, test loss-0.4890, acc-0.8647\n",
      "Iter-44710, train loss-0.5400, acc-0.8400, valid loss-0.4855, acc-0.8674, test loss-0.4889, acc-0.8648\n",
      "Iter-44720, train loss-0.7808, acc-0.7800, valid loss-0.4854, acc-0.8674, test loss-0.4889, acc-0.8648\n",
      "Iter-44730, train loss-0.3341, acc-0.9200, valid loss-0.4854, acc-0.8674, test loss-0.4888, acc-0.8647\n",
      "Iter-44740, train loss-0.3772, acc-0.9000, valid loss-0.4853, acc-0.8674, test loss-0.4888, acc-0.8648\n",
      "Iter-44750, train loss-0.5539, acc-0.8200, valid loss-0.4853, acc-0.8674, test loss-0.4887, acc-0.8646\n",
      "Iter-44760, train loss-0.5007, acc-0.8800, valid loss-0.4852, acc-0.8674, test loss-0.4887, acc-0.8647\n",
      "Iter-44770, train loss-0.6477, acc-0.7800, valid loss-0.4851, acc-0.8674, test loss-0.4886, acc-0.8647\n",
      "Iter-44780, train loss-0.4578, acc-0.8800, valid loss-0.4851, acc-0.8674, test loss-0.4885, acc-0.8647\n",
      "Iter-44790, train loss-0.3812, acc-0.9200, valid loss-0.4850, acc-0.8674, test loss-0.4885, acc-0.8647\n",
      "Iter-44800, train loss-0.3337, acc-0.9400, valid loss-0.4850, acc-0.8672, test loss-0.4884, acc-0.8647\n",
      "Iter-44810, train loss-0.5952, acc-0.7800, valid loss-0.4849, acc-0.8674, test loss-0.4884, acc-0.8647\n",
      "Iter-44820, train loss-0.8059, acc-0.7400, valid loss-0.4849, acc-0.8674, test loss-0.4883, acc-0.8647\n",
      "Iter-44830, train loss-0.6515, acc-0.8400, valid loss-0.4848, acc-0.8674, test loss-0.4882, acc-0.8648\n",
      "Iter-44840, train loss-0.4268, acc-0.8400, valid loss-0.4848, acc-0.8674, test loss-0.4882, acc-0.8649\n",
      "Iter-44850, train loss-0.2694, acc-0.9400, valid loss-0.4847, acc-0.8674, test loss-0.4881, acc-0.8648\n",
      "Iter-44860, train loss-0.5629, acc-0.8800, valid loss-0.4847, acc-0.8674, test loss-0.4881, acc-0.8649\n",
      "Iter-44870, train loss-0.4728, acc-0.8800, valid loss-0.4846, acc-0.8674, test loss-0.4880, acc-0.8649\n",
      "Iter-44880, train loss-0.5335, acc-0.8600, valid loss-0.4845, acc-0.8674, test loss-0.4879, acc-0.8648\n",
      "Iter-44890, train loss-0.4553, acc-0.8600, valid loss-0.4845, acc-0.8674, test loss-0.4879, acc-0.8648\n",
      "Iter-44900, train loss-0.4031, acc-0.9000, valid loss-0.4844, acc-0.8674, test loss-0.4878, acc-0.8649\n",
      "Iter-44910, train loss-0.4622, acc-0.8800, valid loss-0.4844, acc-0.8674, test loss-0.4878, acc-0.8649\n",
      "Iter-44920, train loss-0.5984, acc-0.8200, valid loss-0.4843, acc-0.8674, test loss-0.4877, acc-0.8648\n",
      "Iter-44930, train loss-0.5296, acc-0.8200, valid loss-0.4843, acc-0.8672, test loss-0.4877, acc-0.8649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-44940, train loss-0.6674, acc-0.8200, valid loss-0.4842, acc-0.8672, test loss-0.4876, acc-0.8649\n",
      "Iter-44950, train loss-0.5647, acc-0.8400, valid loss-0.4842, acc-0.8672, test loss-0.4875, acc-0.8649\n",
      "Iter-44960, train loss-0.3681, acc-0.9200, valid loss-0.4841, acc-0.8674, test loss-0.4875, acc-0.8649\n",
      "Iter-44970, train loss-0.4310, acc-0.8800, valid loss-0.4841, acc-0.8674, test loss-0.4874, acc-0.8650\n",
      "Iter-44980, train loss-0.5047, acc-0.8400, valid loss-0.4840, acc-0.8674, test loss-0.4874, acc-0.8650\n",
      "Iter-44990, train loss-0.5722, acc-0.8400, valid loss-0.4840, acc-0.8674, test loss-0.4873, acc-0.8650\n",
      "Iter-45000, train loss-0.5147, acc-0.8800, valid loss-0.4839, acc-0.8672, test loss-0.4873, acc-0.8650\n",
      "Iter-45010, train loss-0.5681, acc-0.8000, valid loss-0.4839, acc-0.8672, test loss-0.4872, acc-0.8651\n",
      "Iter-45020, train loss-0.4784, acc-0.8400, valid loss-0.4838, acc-0.8674, test loss-0.4872, acc-0.8651\n",
      "Iter-45030, train loss-0.3682, acc-0.8800, valid loss-0.4837, acc-0.8676, test loss-0.4871, acc-0.8651\n",
      "Iter-45040, train loss-0.5193, acc-0.8800, valid loss-0.4837, acc-0.8676, test loss-0.4871, acc-0.8651\n",
      "Iter-45050, train loss-0.4150, acc-0.9000, valid loss-0.4836, acc-0.8676, test loss-0.4870, acc-0.8651\n",
      "Iter-45060, train loss-0.5672, acc-0.8400, valid loss-0.4836, acc-0.8676, test loss-0.4870, acc-0.8651\n",
      "Iter-45070, train loss-0.3821, acc-0.8800, valid loss-0.4835, acc-0.8676, test loss-0.4869, acc-0.8651\n",
      "Iter-45080, train loss-0.6091, acc-0.8200, valid loss-0.4835, acc-0.8676, test loss-0.4869, acc-0.8651\n",
      "Iter-45090, train loss-0.3471, acc-0.9000, valid loss-0.4834, acc-0.8676, test loss-0.4868, acc-0.8651\n",
      "Iter-45100, train loss-0.5124, acc-0.8000, valid loss-0.4834, acc-0.8676, test loss-0.4867, acc-0.8651\n",
      "Iter-45110, train loss-0.4709, acc-0.8600, valid loss-0.4833, acc-0.8676, test loss-0.4867, acc-0.8651\n",
      "Iter-45120, train loss-0.4906, acc-0.8600, valid loss-0.4832, acc-0.8678, test loss-0.4866, acc-0.8651\n",
      "Iter-45130, train loss-0.6007, acc-0.8600, valid loss-0.4832, acc-0.8676, test loss-0.4866, acc-0.8651\n",
      "Iter-45140, train loss-0.4583, acc-0.8600, valid loss-0.4831, acc-0.8676, test loss-0.4865, acc-0.8651\n",
      "Iter-45150, train loss-0.5525, acc-0.8600, valid loss-0.4831, acc-0.8678, test loss-0.4865, acc-0.8652\n",
      "Iter-45160, train loss-0.5030, acc-0.8400, valid loss-0.4830, acc-0.8676, test loss-0.4864, acc-0.8651\n",
      "Iter-45170, train loss-0.5409, acc-0.8400, valid loss-0.4830, acc-0.8676, test loss-0.4863, acc-0.8651\n",
      "Iter-45180, train loss-0.4458, acc-0.9000, valid loss-0.4829, acc-0.8676, test loss-0.4863, acc-0.8651\n",
      "Iter-45190, train loss-0.5007, acc-0.8800, valid loss-0.4829, acc-0.8676, test loss-0.4862, acc-0.8650\n",
      "Iter-45200, train loss-0.4369, acc-0.8800, valid loss-0.4828, acc-0.8676, test loss-0.4862, acc-0.8651\n",
      "Iter-45210, train loss-0.5685, acc-0.8000, valid loss-0.4828, acc-0.8676, test loss-0.4862, acc-0.8651\n",
      "Iter-45220, train loss-0.3862, acc-0.9200, valid loss-0.4827, acc-0.8676, test loss-0.4861, acc-0.8650\n",
      "Iter-45230, train loss-0.6042, acc-0.8800, valid loss-0.4826, acc-0.8674, test loss-0.4861, acc-0.8651\n",
      "Iter-45240, train loss-0.3361, acc-0.9200, valid loss-0.4826, acc-0.8676, test loss-0.4860, acc-0.8651\n",
      "Iter-45250, train loss-0.5335, acc-0.8400, valid loss-0.4825, acc-0.8676, test loss-0.4859, acc-0.8650\n",
      "Iter-45260, train loss-0.4773, acc-0.8600, valid loss-0.4825, acc-0.8676, test loss-0.4859, acc-0.8652\n",
      "Iter-45270, train loss-0.3475, acc-0.8400, valid loss-0.4824, acc-0.8676, test loss-0.4858, acc-0.8651\n",
      "Iter-45280, train loss-0.5328, acc-0.8200, valid loss-0.4824, acc-0.8676, test loss-0.4858, acc-0.8651\n",
      "Iter-45290, train loss-0.3602, acc-0.9200, valid loss-0.4823, acc-0.8676, test loss-0.4857, acc-0.8652\n",
      "Iter-45300, train loss-0.5049, acc-0.8800, valid loss-0.4823, acc-0.8676, test loss-0.4857, acc-0.8652\n",
      "Iter-45310, train loss-0.6801, acc-0.8400, valid loss-0.4822, acc-0.8676, test loss-0.4856, acc-0.8652\n",
      "Iter-45320, train loss-0.4182, acc-0.8800, valid loss-0.4822, acc-0.8678, test loss-0.4856, acc-0.8653\n",
      "Iter-45330, train loss-0.4861, acc-0.8800, valid loss-0.4821, acc-0.8678, test loss-0.4855, acc-0.8654\n",
      "Iter-45340, train loss-0.7125, acc-0.8000, valid loss-0.4821, acc-0.8678, test loss-0.4855, acc-0.8652\n",
      "Iter-45350, train loss-0.4826, acc-0.8600, valid loss-0.4820, acc-0.8680, test loss-0.4854, acc-0.8653\n",
      "Iter-45360, train loss-0.5728, acc-0.8400, valid loss-0.4819, acc-0.8680, test loss-0.4853, acc-0.8653\n",
      "Iter-45370, train loss-0.4523, acc-0.9000, valid loss-0.4819, acc-0.8680, test loss-0.4853, acc-0.8652\n",
      "Iter-45380, train loss-0.5348, acc-0.8800, valid loss-0.4818, acc-0.8682, test loss-0.4852, acc-0.8653\n",
      "Iter-45390, train loss-0.4458, acc-0.8400, valid loss-0.4818, acc-0.8682, test loss-0.4852, acc-0.8655\n",
      "Iter-45400, train loss-0.5993, acc-0.8000, valid loss-0.4817, acc-0.8680, test loss-0.4851, acc-0.8654\n",
      "Iter-45410, train loss-0.5018, acc-0.8800, valid loss-0.4817, acc-0.8680, test loss-0.4851, acc-0.8653\n",
      "Iter-45420, train loss-0.3764, acc-0.9000, valid loss-0.4816, acc-0.8678, test loss-0.4850, acc-0.8654\n",
      "Iter-45430, train loss-0.5986, acc-0.8200, valid loss-0.4816, acc-0.8680, test loss-0.4850, acc-0.8653\n",
      "Iter-45440, train loss-0.5017, acc-0.8400, valid loss-0.4815, acc-0.8680, test loss-0.4849, acc-0.8653\n",
      "Iter-45450, train loss-0.4268, acc-0.8400, valid loss-0.4815, acc-0.8680, test loss-0.4849, acc-0.8654\n",
      "Iter-45460, train loss-0.5979, acc-0.8200, valid loss-0.4814, acc-0.8680, test loss-0.4848, acc-0.8654\n",
      "Iter-45470, train loss-0.5026, acc-0.8000, valid loss-0.4814, acc-0.8680, test loss-0.4848, acc-0.8654\n",
      "Iter-45480, train loss-0.4499, acc-0.8600, valid loss-0.4813, acc-0.8680, test loss-0.4847, acc-0.8653\n",
      "Iter-45490, train loss-0.7052, acc-0.8000, valid loss-0.4813, acc-0.8682, test loss-0.4847, acc-0.8653\n",
      "Iter-45500, train loss-0.3910, acc-0.8800, valid loss-0.4812, acc-0.8680, test loss-0.4846, acc-0.8652\n",
      "Iter-45510, train loss-0.3184, acc-0.9400, valid loss-0.4812, acc-0.8678, test loss-0.4846, acc-0.8652\n",
      "Iter-45520, train loss-0.5904, acc-0.8800, valid loss-0.4811, acc-0.8678, test loss-0.4845, acc-0.8652\n",
      "Iter-45530, train loss-0.4785, acc-0.8600, valid loss-0.4811, acc-0.8680, test loss-0.4845, acc-0.8653\n",
      "Iter-45540, train loss-0.3926, acc-0.9600, valid loss-0.4810, acc-0.8680, test loss-0.4844, acc-0.8653\n",
      "Iter-45550, train loss-0.5375, acc-0.9000, valid loss-0.4809, acc-0.8680, test loss-0.4843, acc-0.8654\n",
      "Iter-45560, train loss-0.3232, acc-0.9200, valid loss-0.4809, acc-0.8678, test loss-0.4843, acc-0.8655\n",
      "Iter-45570, train loss-0.6462, acc-0.8200, valid loss-0.4808, acc-0.8680, test loss-0.4842, acc-0.8654\n",
      "Iter-45580, train loss-0.4040, acc-0.9200, valid loss-0.4808, acc-0.8684, test loss-0.4842, acc-0.8656\n",
      "Iter-45590, train loss-0.6723, acc-0.8200, valid loss-0.4807, acc-0.8686, test loss-0.4841, acc-0.8656\n",
      "Iter-45600, train loss-0.5723, acc-0.8200, valid loss-0.4807, acc-0.8684, test loss-0.4840, acc-0.8657\n",
      "Iter-45610, train loss-0.4603, acc-0.8400, valid loss-0.4806, acc-0.8686, test loss-0.4840, acc-0.8657\n",
      "Iter-45620, train loss-0.7418, acc-0.8200, valid loss-0.4805, acc-0.8684, test loss-0.4839, acc-0.8657\n",
      "Iter-45630, train loss-0.5968, acc-0.8600, valid loss-0.4805, acc-0.8686, test loss-0.4839, acc-0.8659\n",
      "Iter-45640, train loss-0.5905, acc-0.8200, valid loss-0.4804, acc-0.8686, test loss-0.4838, acc-0.8658\n",
      "Iter-45650, train loss-0.5467, acc-0.8400, valid loss-0.4804, acc-0.8686, test loss-0.4838, acc-0.8658\n",
      "Iter-45660, train loss-0.4973, acc-0.9200, valid loss-0.4803, acc-0.8686, test loss-0.4837, acc-0.8657\n",
      "Iter-45670, train loss-0.7022, acc-0.7800, valid loss-0.4803, acc-0.8688, test loss-0.4837, acc-0.8657\n",
      "Iter-45680, train loss-0.5428, acc-0.8400, valid loss-0.4802, acc-0.8690, test loss-0.4836, acc-0.8657\n",
      "Iter-45690, train loss-0.6182, acc-0.8200, valid loss-0.4802, acc-0.8688, test loss-0.4836, acc-0.8659\n",
      "Iter-45700, train loss-0.3800, acc-0.8800, valid loss-0.4801, acc-0.8690, test loss-0.4836, acc-0.8658\n",
      "Iter-45710, train loss-0.5211, acc-0.8400, valid loss-0.4801, acc-0.8688, test loss-0.4835, acc-0.8658\n",
      "Iter-45720, train loss-0.5977, acc-0.8600, valid loss-0.4800, acc-0.8688, test loss-0.4835, acc-0.8661\n",
      "Iter-45730, train loss-0.4882, acc-0.8600, valid loss-0.4799, acc-0.8690, test loss-0.4834, acc-0.8660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45740, train loss-0.4755, acc-0.8400, valid loss-0.4799, acc-0.8688, test loss-0.4834, acc-0.8660\n",
      "Iter-45750, train loss-0.4331, acc-0.8800, valid loss-0.4798, acc-0.8688, test loss-0.4833, acc-0.8660\n",
      "Iter-45760, train loss-0.5733, acc-0.8400, valid loss-0.4798, acc-0.8690, test loss-0.4832, acc-0.8660\n",
      "Iter-45770, train loss-0.4871, acc-0.8600, valid loss-0.4797, acc-0.8690, test loss-0.4832, acc-0.8659\n",
      "Iter-45780, train loss-0.4037, acc-0.8400, valid loss-0.4797, acc-0.8688, test loss-0.4831, acc-0.8660\n",
      "Iter-45790, train loss-0.3956, acc-0.8800, valid loss-0.4796, acc-0.8688, test loss-0.4831, acc-0.8660\n",
      "Iter-45800, train loss-0.6777, acc-0.8000, valid loss-0.4796, acc-0.8690, test loss-0.4830, acc-0.8659\n",
      "Iter-45810, train loss-0.4054, acc-0.8800, valid loss-0.4795, acc-0.8690, test loss-0.4830, acc-0.8661\n",
      "Iter-45820, train loss-0.5699, acc-0.8200, valid loss-0.4795, acc-0.8690, test loss-0.4829, acc-0.8659\n",
      "Iter-45830, train loss-0.6442, acc-0.8200, valid loss-0.4794, acc-0.8690, test loss-0.4829, acc-0.8660\n",
      "Iter-45840, train loss-0.5369, acc-0.8800, valid loss-0.4794, acc-0.8690, test loss-0.4828, acc-0.8660\n",
      "Iter-45850, train loss-0.5990, acc-0.7600, valid loss-0.4793, acc-0.8690, test loss-0.4828, acc-0.8660\n",
      "Iter-45860, train loss-0.7192, acc-0.8200, valid loss-0.4793, acc-0.8690, test loss-0.4827, acc-0.8660\n",
      "Iter-45870, train loss-0.3361, acc-0.9400, valid loss-0.4792, acc-0.8690, test loss-0.4827, acc-0.8661\n",
      "Iter-45880, train loss-0.5256, acc-0.8800, valid loss-0.4791, acc-0.8688, test loss-0.4826, acc-0.8661\n",
      "Iter-45890, train loss-0.4615, acc-0.8600, valid loss-0.4791, acc-0.8688, test loss-0.4825, acc-0.8662\n",
      "Iter-45900, train loss-0.5964, acc-0.8400, valid loss-0.4790, acc-0.8686, test loss-0.4825, acc-0.8662\n",
      "Iter-45910, train loss-0.5779, acc-0.8800, valid loss-0.4790, acc-0.8688, test loss-0.4824, acc-0.8662\n",
      "Iter-45920, train loss-0.4584, acc-0.9200, valid loss-0.4789, acc-0.8688, test loss-0.4824, acc-0.8661\n",
      "Iter-45930, train loss-0.7232, acc-0.8200, valid loss-0.4788, acc-0.8688, test loss-0.4823, acc-0.8662\n",
      "Iter-45940, train loss-0.5227, acc-0.8400, valid loss-0.4788, acc-0.8688, test loss-0.4823, acc-0.8660\n",
      "Iter-45950, train loss-0.6258, acc-0.8200, valid loss-0.4787, acc-0.8688, test loss-0.4822, acc-0.8661\n",
      "Iter-45960, train loss-0.4532, acc-0.9000, valid loss-0.4787, acc-0.8690, test loss-0.4822, acc-0.8660\n",
      "Iter-45970, train loss-0.4716, acc-0.9200, valid loss-0.4786, acc-0.8692, test loss-0.4821, acc-0.8661\n",
      "Iter-45980, train loss-0.4523, acc-0.8400, valid loss-0.4786, acc-0.8692, test loss-0.4821, acc-0.8661\n",
      "Iter-45990, train loss-0.5056, acc-0.8600, valid loss-0.4786, acc-0.8688, test loss-0.4820, acc-0.8661\n",
      "Iter-46000, train loss-0.5200, acc-0.8400, valid loss-0.4785, acc-0.8686, test loss-0.4819, acc-0.8661\n",
      "Iter-46010, train loss-0.4125, acc-0.9000, valid loss-0.4785, acc-0.8692, test loss-0.4819, acc-0.8661\n",
      "Iter-46020, train loss-0.5892, acc-0.8400, valid loss-0.4784, acc-0.8692, test loss-0.4819, acc-0.8661\n",
      "Iter-46030, train loss-0.7158, acc-0.7800, valid loss-0.4784, acc-0.8694, test loss-0.4818, acc-0.8662\n",
      "Iter-46040, train loss-0.5319, acc-0.9000, valid loss-0.4783, acc-0.8694, test loss-0.4818, acc-0.8662\n",
      "Iter-46050, train loss-0.6898, acc-0.8000, valid loss-0.4782, acc-0.8696, test loss-0.4817, acc-0.8662\n",
      "Iter-46060, train loss-0.3238, acc-0.9600, valid loss-0.4782, acc-0.8692, test loss-0.4817, acc-0.8662\n",
      "Iter-46070, train loss-0.2499, acc-0.9800, valid loss-0.4781, acc-0.8692, test loss-0.4816, acc-0.8662\n",
      "Iter-46080, train loss-0.5812, acc-0.8400, valid loss-0.4781, acc-0.8692, test loss-0.4815, acc-0.8662\n",
      "Iter-46090, train loss-0.4619, acc-0.9200, valid loss-0.4780, acc-0.8696, test loss-0.4815, acc-0.8662\n",
      "Iter-46100, train loss-0.7982, acc-0.7400, valid loss-0.4780, acc-0.8694, test loss-0.4815, acc-0.8663\n",
      "Iter-46110, train loss-0.3853, acc-0.9200, valid loss-0.4779, acc-0.8696, test loss-0.4814, acc-0.8662\n",
      "Iter-46120, train loss-0.4846, acc-0.8600, valid loss-0.4779, acc-0.8694, test loss-0.4814, acc-0.8661\n",
      "Iter-46130, train loss-0.3344, acc-0.9000, valid loss-0.4778, acc-0.8696, test loss-0.4813, acc-0.8660\n",
      "Iter-46140, train loss-0.5268, acc-0.7800, valid loss-0.4778, acc-0.8694, test loss-0.4813, acc-0.8663\n",
      "Iter-46150, train loss-0.4233, acc-0.9000, valid loss-0.4777, acc-0.8698, test loss-0.4812, acc-0.8661\n",
      "Iter-46160, train loss-0.3045, acc-0.9400, valid loss-0.4777, acc-0.8700, test loss-0.4812, acc-0.8661\n",
      "Iter-46170, train loss-0.5124, acc-0.8400, valid loss-0.4776, acc-0.8698, test loss-0.4811, acc-0.8662\n",
      "Iter-46180, train loss-0.3580, acc-0.9400, valid loss-0.4776, acc-0.8694, test loss-0.4811, acc-0.8662\n",
      "Iter-46190, train loss-0.5128, acc-0.8800, valid loss-0.4775, acc-0.8698, test loss-0.4810, acc-0.8662\n",
      "Iter-46200, train loss-0.4886, acc-0.8000, valid loss-0.4775, acc-0.8694, test loss-0.4810, acc-0.8662\n",
      "Iter-46210, train loss-0.5245, acc-0.8600, valid loss-0.4774, acc-0.8696, test loss-0.4809, acc-0.8663\n",
      "Iter-46220, train loss-0.5358, acc-0.8200, valid loss-0.4774, acc-0.8696, test loss-0.4808, acc-0.8663\n",
      "Iter-46230, train loss-0.5518, acc-0.8600, valid loss-0.4773, acc-0.8700, test loss-0.4808, acc-0.8662\n",
      "Iter-46240, train loss-0.5653, acc-0.8000, valid loss-0.4773, acc-0.8700, test loss-0.4807, acc-0.8663\n",
      "Iter-46250, train loss-0.5476, acc-0.8600, valid loss-0.4772, acc-0.8702, test loss-0.4807, acc-0.8663\n",
      "Iter-46260, train loss-0.4244, acc-0.8800, valid loss-0.4772, acc-0.8702, test loss-0.4806, acc-0.8663\n",
      "Iter-46270, train loss-0.5971, acc-0.8000, valid loss-0.4771, acc-0.8700, test loss-0.4806, acc-0.8664\n",
      "Iter-46280, train loss-0.5822, acc-0.8400, valid loss-0.4771, acc-0.8698, test loss-0.4805, acc-0.8665\n",
      "Iter-46290, train loss-0.4032, acc-0.8800, valid loss-0.4770, acc-0.8698, test loss-0.4805, acc-0.8663\n",
      "Iter-46300, train loss-0.4817, acc-0.8600, valid loss-0.4770, acc-0.8698, test loss-0.4804, acc-0.8664\n",
      "Iter-46310, train loss-0.6219, acc-0.8400, valid loss-0.4769, acc-0.8698, test loss-0.4804, acc-0.8666\n",
      "Iter-46320, train loss-0.3637, acc-0.9000, valid loss-0.4769, acc-0.8696, test loss-0.4804, acc-0.8664\n",
      "Iter-46330, train loss-0.6372, acc-0.8200, valid loss-0.4768, acc-0.8702, test loss-0.4803, acc-0.8663\n",
      "Iter-46340, train loss-0.5748, acc-0.8400, valid loss-0.4768, acc-0.8700, test loss-0.4802, acc-0.8663\n",
      "Iter-46350, train loss-0.5583, acc-0.8400, valid loss-0.4767, acc-0.8702, test loss-0.4802, acc-0.8663\n",
      "Iter-46360, train loss-0.4377, acc-0.8600, valid loss-0.4767, acc-0.8700, test loss-0.4801, acc-0.8663\n",
      "Iter-46370, train loss-0.3462, acc-0.9200, valid loss-0.4766, acc-0.8700, test loss-0.4801, acc-0.8664\n",
      "Iter-46380, train loss-0.4624, acc-0.8600, valid loss-0.4766, acc-0.8698, test loss-0.4800, acc-0.8663\n",
      "Iter-46390, train loss-0.4415, acc-0.9000, valid loss-0.4765, acc-0.8700, test loss-0.4799, acc-0.8665\n",
      "Iter-46400, train loss-0.5531, acc-0.8600, valid loss-0.4765, acc-0.8700, test loss-0.4799, acc-0.8666\n",
      "Iter-46410, train loss-0.5115, acc-0.8400, valid loss-0.4764, acc-0.8700, test loss-0.4798, acc-0.8666\n",
      "Iter-46420, train loss-0.5941, acc-0.8800, valid loss-0.4763, acc-0.8702, test loss-0.4798, acc-0.8666\n",
      "Iter-46430, train loss-0.4344, acc-0.8800, valid loss-0.4763, acc-0.8702, test loss-0.4797, acc-0.8665\n",
      "Iter-46440, train loss-0.3963, acc-0.9000, valid loss-0.4762, acc-0.8702, test loss-0.4797, acc-0.8665\n",
      "Iter-46450, train loss-0.5664, acc-0.7800, valid loss-0.4762, acc-0.8702, test loss-0.4796, acc-0.8665\n",
      "Iter-46460, train loss-0.3990, acc-0.9000, valid loss-0.4761, acc-0.8702, test loss-0.4796, acc-0.8665\n",
      "Iter-46470, train loss-0.3952, acc-0.9000, valid loss-0.4761, acc-0.8702, test loss-0.4795, acc-0.8667\n",
      "Iter-46480, train loss-0.5842, acc-0.8600, valid loss-0.4760, acc-0.8704, test loss-0.4795, acc-0.8665\n",
      "Iter-46490, train loss-0.5522, acc-0.8600, valid loss-0.4760, acc-0.8704, test loss-0.4794, acc-0.8666\n",
      "Iter-46500, train loss-0.5093, acc-0.9200, valid loss-0.4759, acc-0.8704, test loss-0.4794, acc-0.8666\n",
      "Iter-46510, train loss-0.4555, acc-0.8400, valid loss-0.4759, acc-0.8704, test loss-0.4793, acc-0.8666\n",
      "Iter-46520, train loss-0.3492, acc-0.8800, valid loss-0.4759, acc-0.8704, test loss-0.4793, acc-0.8667\n",
      "Iter-46530, train loss-0.3563, acc-0.9000, valid loss-0.4758, acc-0.8704, test loss-0.4793, acc-0.8665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-46540, train loss-0.4903, acc-0.9000, valid loss-0.4757, acc-0.8706, test loss-0.4792, acc-0.8665\n",
      "Iter-46550, train loss-0.4036, acc-0.8800, valid loss-0.4757, acc-0.8708, test loss-0.4791, acc-0.8664\n",
      "Iter-46560, train loss-0.3907, acc-0.9000, valid loss-0.4756, acc-0.8706, test loss-0.4791, acc-0.8668\n",
      "Iter-46570, train loss-0.4040, acc-0.9200, valid loss-0.4756, acc-0.8708, test loss-0.4790, acc-0.8667\n",
      "Iter-46580, train loss-0.6479, acc-0.7600, valid loss-0.4755, acc-0.8704, test loss-0.4790, acc-0.8667\n",
      "Iter-46590, train loss-0.5951, acc-0.8600, valid loss-0.4755, acc-0.8702, test loss-0.4789, acc-0.8670\n",
      "Iter-46600, train loss-0.3874, acc-0.8600, valid loss-0.4754, acc-0.8704, test loss-0.4789, acc-0.8669\n",
      "Iter-46610, train loss-0.4609, acc-0.9000, valid loss-0.4754, acc-0.8710, test loss-0.4788, acc-0.8671\n",
      "Iter-46620, train loss-0.5699, acc-0.8600, valid loss-0.4754, acc-0.8708, test loss-0.4788, acc-0.8671\n",
      "Iter-46630, train loss-0.4522, acc-0.8800, valid loss-0.4753, acc-0.8708, test loss-0.4787, acc-0.8670\n",
      "Iter-46640, train loss-0.3491, acc-0.8800, valid loss-0.4753, acc-0.8708, test loss-0.4787, acc-0.8669\n",
      "Iter-46650, train loss-0.7322, acc-0.7800, valid loss-0.4752, acc-0.8706, test loss-0.4786, acc-0.8669\n",
      "Iter-46660, train loss-0.4356, acc-0.8800, valid loss-0.4752, acc-0.8706, test loss-0.4786, acc-0.8668\n",
      "Iter-46670, train loss-0.6302, acc-0.7800, valid loss-0.4751, acc-0.8710, test loss-0.4785, acc-0.8666\n",
      "Iter-46680, train loss-0.5494, acc-0.8000, valid loss-0.4751, acc-0.8708, test loss-0.4785, acc-0.8667\n",
      "Iter-46690, train loss-0.4731, acc-0.8600, valid loss-0.4750, acc-0.8708, test loss-0.4784, acc-0.8666\n",
      "Iter-46700, train loss-0.3889, acc-0.9000, valid loss-0.4749, acc-0.8708, test loss-0.4784, acc-0.8666\n",
      "Iter-46710, train loss-0.4838, acc-0.8800, valid loss-0.4749, acc-0.8708, test loss-0.4783, acc-0.8667\n",
      "Iter-46720, train loss-0.5015, acc-0.8600, valid loss-0.4748, acc-0.8708, test loss-0.4783, acc-0.8667\n",
      "Iter-46730, train loss-0.4927, acc-0.8800, valid loss-0.4748, acc-0.8708, test loss-0.4782, acc-0.8667\n",
      "Iter-46740, train loss-0.5269, acc-0.8400, valid loss-0.4748, acc-0.8708, test loss-0.4782, acc-0.8667\n",
      "Iter-46750, train loss-0.6036, acc-0.8200, valid loss-0.4747, acc-0.8708, test loss-0.4781, acc-0.8667\n",
      "Iter-46760, train loss-0.6142, acc-0.8400, valid loss-0.4746, acc-0.8708, test loss-0.4781, acc-0.8667\n",
      "Iter-46770, train loss-0.5166, acc-0.8400, valid loss-0.4746, acc-0.8708, test loss-0.4780, acc-0.8667\n",
      "Iter-46780, train loss-0.7736, acc-0.6800, valid loss-0.4745, acc-0.8708, test loss-0.4780, acc-0.8668\n",
      "Iter-46790, train loss-0.4701, acc-0.8600, valid loss-0.4745, acc-0.8708, test loss-0.4779, acc-0.8667\n",
      "Iter-46800, train loss-0.5345, acc-0.8200, valid loss-0.4744, acc-0.8708, test loss-0.4779, acc-0.8669\n",
      "Iter-46810, train loss-0.6711, acc-0.8000, valid loss-0.4744, acc-0.8708, test loss-0.4778, acc-0.8670\n",
      "Iter-46820, train loss-0.4421, acc-0.8800, valid loss-0.4743, acc-0.8708, test loss-0.4778, acc-0.8672\n",
      "Iter-46830, train loss-0.3591, acc-0.8800, valid loss-0.4743, acc-0.8708, test loss-0.4777, acc-0.8671\n",
      "Iter-46840, train loss-0.3372, acc-0.8600, valid loss-0.4742, acc-0.8708, test loss-0.4777, acc-0.8671\n",
      "Iter-46850, train loss-0.5715, acc-0.8800, valid loss-0.4742, acc-0.8710, test loss-0.4776, acc-0.8671\n",
      "Iter-46860, train loss-0.7076, acc-0.7400, valid loss-0.4742, acc-0.8710, test loss-0.4776, acc-0.8671\n",
      "Iter-46870, train loss-0.6995, acc-0.7800, valid loss-0.4741, acc-0.8710, test loss-0.4775, acc-0.8672\n",
      "Iter-46880, train loss-0.4009, acc-0.8800, valid loss-0.4740, acc-0.8710, test loss-0.4775, acc-0.8672\n",
      "Iter-46890, train loss-0.4280, acc-0.9000, valid loss-0.4740, acc-0.8708, test loss-0.4774, acc-0.8671\n",
      "Iter-46900, train loss-0.5346, acc-0.8400, valid loss-0.4739, acc-0.8708, test loss-0.4774, acc-0.8673\n",
      "Iter-46910, train loss-0.4740, acc-0.8400, valid loss-0.4739, acc-0.8708, test loss-0.4774, acc-0.8674\n",
      "Iter-46920, train loss-0.5328, acc-0.8600, valid loss-0.4739, acc-0.8710, test loss-0.4773, acc-0.8673\n",
      "Iter-46930, train loss-0.5224, acc-0.8600, valid loss-0.4738, acc-0.8710, test loss-0.4773, acc-0.8673\n",
      "Iter-46940, train loss-0.3883, acc-0.9200, valid loss-0.4738, acc-0.8712, test loss-0.4772, acc-0.8675\n",
      "Iter-46950, train loss-0.5400, acc-0.8000, valid loss-0.4737, acc-0.8710, test loss-0.4772, acc-0.8674\n",
      "Iter-46960, train loss-0.5806, acc-0.9200, valid loss-0.4737, acc-0.8710, test loss-0.4771, acc-0.8673\n",
      "Iter-46970, train loss-0.4252, acc-0.8400, valid loss-0.4736, acc-0.8712, test loss-0.4771, acc-0.8673\n",
      "Iter-46980, train loss-0.3976, acc-0.8800, valid loss-0.4736, acc-0.8710, test loss-0.4770, acc-0.8673\n",
      "Iter-46990, train loss-0.3834, acc-0.9000, valid loss-0.4735, acc-0.8710, test loss-0.4770, acc-0.8674\n",
      "Iter-47000, train loss-0.4970, acc-0.8400, valid loss-0.4735, acc-0.8708, test loss-0.4769, acc-0.8675\n",
      "Iter-47010, train loss-0.5501, acc-0.8800, valid loss-0.4734, acc-0.8708, test loss-0.4769, acc-0.8675\n",
      "Iter-47020, train loss-0.5071, acc-0.8600, valid loss-0.4734, acc-0.8708, test loss-0.4768, acc-0.8675\n",
      "Iter-47030, train loss-0.4313, acc-0.8600, valid loss-0.4733, acc-0.8708, test loss-0.4768, acc-0.8675\n",
      "Iter-47040, train loss-0.3960, acc-0.8600, valid loss-0.4733, acc-0.8708, test loss-0.4767, acc-0.8676\n",
      "Iter-47050, train loss-0.5066, acc-0.8800, valid loss-0.4732, acc-0.8708, test loss-0.4767, acc-0.8676\n",
      "Iter-47060, train loss-0.5601, acc-0.8800, valid loss-0.4731, acc-0.8710, test loss-0.4767, acc-0.8676\n",
      "Iter-47070, train loss-0.3402, acc-0.9400, valid loss-0.4731, acc-0.8710, test loss-0.4766, acc-0.8677\n",
      "Iter-47080, train loss-0.4393, acc-0.8800, valid loss-0.4730, acc-0.8710, test loss-0.4766, acc-0.8677\n",
      "Iter-47090, train loss-0.3308, acc-0.9000, valid loss-0.4730, acc-0.8710, test loss-0.4765, acc-0.8677\n",
      "Iter-47100, train loss-0.4114, acc-0.9000, valid loss-0.4729, acc-0.8710, test loss-0.4765, acc-0.8677\n",
      "Iter-47110, train loss-0.4847, acc-0.8400, valid loss-0.4729, acc-0.8710, test loss-0.4764, acc-0.8677\n",
      "Iter-47120, train loss-0.4298, acc-0.8800, valid loss-0.4729, acc-0.8712, test loss-0.4764, acc-0.8676\n",
      "Iter-47130, train loss-0.5583, acc-0.8800, valid loss-0.4728, acc-0.8710, test loss-0.4763, acc-0.8675\n",
      "Iter-47140, train loss-0.6420, acc-0.8000, valid loss-0.4728, acc-0.8710, test loss-0.4763, acc-0.8675\n",
      "Iter-47150, train loss-0.4460, acc-0.8800, valid loss-0.4727, acc-0.8710, test loss-0.4762, acc-0.8674\n",
      "Iter-47160, train loss-0.3942, acc-0.9400, valid loss-0.4727, acc-0.8710, test loss-0.4762, acc-0.8675\n",
      "Iter-47170, train loss-0.6205, acc-0.8200, valid loss-0.4726, acc-0.8706, test loss-0.4761, acc-0.8677\n",
      "Iter-47180, train loss-0.5168, acc-0.8600, valid loss-0.4726, acc-0.8712, test loss-0.4761, acc-0.8676\n",
      "Iter-47190, train loss-0.6616, acc-0.8600, valid loss-0.4725, acc-0.8712, test loss-0.4760, acc-0.8676\n",
      "Iter-47200, train loss-0.3773, acc-0.9200, valid loss-0.4725, acc-0.8710, test loss-0.4760, acc-0.8676\n",
      "Iter-47210, train loss-0.3373, acc-0.9000, valid loss-0.4724, acc-0.8708, test loss-0.4759, acc-0.8677\n",
      "Iter-47220, train loss-0.5394, acc-0.8600, valid loss-0.4724, acc-0.8710, test loss-0.4759, acc-0.8678\n",
      "Iter-47230, train loss-0.5731, acc-0.8000, valid loss-0.4723, acc-0.8708, test loss-0.4759, acc-0.8677\n",
      "Iter-47240, train loss-0.3093, acc-0.9400, valid loss-0.4723, acc-0.8708, test loss-0.4758, acc-0.8677\n",
      "Iter-47250, train loss-0.4857, acc-0.8600, valid loss-0.4722, acc-0.8706, test loss-0.4758, acc-0.8676\n",
      "Iter-47260, train loss-0.5509, acc-0.8600, valid loss-0.4722, acc-0.8708, test loss-0.4757, acc-0.8676\n",
      "Iter-47270, train loss-0.4108, acc-0.8800, valid loss-0.4722, acc-0.8708, test loss-0.4757, acc-0.8676\n",
      "Iter-47280, train loss-0.6742, acc-0.8200, valid loss-0.4721, acc-0.8712, test loss-0.4757, acc-0.8676\n",
      "Iter-47290, train loss-0.5255, acc-0.8200, valid loss-0.4721, acc-0.8712, test loss-0.4756, acc-0.8676\n",
      "Iter-47300, train loss-0.5452, acc-0.7200, valid loss-0.4720, acc-0.8710, test loss-0.4755, acc-0.8676\n",
      "Iter-47310, train loss-0.3363, acc-0.9000, valid loss-0.4720, acc-0.8710, test loss-0.4755, acc-0.8676\n",
      "Iter-47320, train loss-0.6384, acc-0.8400, valid loss-0.4719, acc-0.8710, test loss-0.4755, acc-0.8677\n",
      "Iter-47330, train loss-0.5579, acc-0.8400, valid loss-0.4719, acc-0.8710, test loss-0.4754, acc-0.8678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-47340, train loss-0.4180, acc-0.8600, valid loss-0.4718, acc-0.8710, test loss-0.4754, acc-0.8677\n",
      "Iter-47350, train loss-0.4993, acc-0.9000, valid loss-0.4718, acc-0.8710, test loss-0.4753, acc-0.8676\n",
      "Iter-47360, train loss-0.4122, acc-0.8800, valid loss-0.4718, acc-0.8710, test loss-0.4753, acc-0.8678\n",
      "Iter-47370, train loss-0.4132, acc-0.8800, valid loss-0.4717, acc-0.8708, test loss-0.4753, acc-0.8677\n",
      "Iter-47380, train loss-0.4099, acc-0.8800, valid loss-0.4717, acc-0.8710, test loss-0.4752, acc-0.8677\n",
      "Iter-47390, train loss-0.5482, acc-0.8600, valid loss-0.4716, acc-0.8708, test loss-0.4752, acc-0.8679\n",
      "Iter-47400, train loss-0.3379, acc-0.9000, valid loss-0.4716, acc-0.8708, test loss-0.4751, acc-0.8678\n",
      "Iter-47410, train loss-0.4719, acc-0.9000, valid loss-0.4715, acc-0.8708, test loss-0.4751, acc-0.8678\n",
      "Iter-47420, train loss-0.3981, acc-0.8800, valid loss-0.4715, acc-0.8708, test loss-0.4750, acc-0.8678\n",
      "Iter-47430, train loss-0.4045, acc-0.9000, valid loss-0.4714, acc-0.8708, test loss-0.4750, acc-0.8678\n",
      "Iter-47440, train loss-0.4647, acc-0.8800, valid loss-0.4714, acc-0.8708, test loss-0.4749, acc-0.8678\n",
      "Iter-47450, train loss-0.3029, acc-0.9400, valid loss-0.4713, acc-0.8708, test loss-0.4749, acc-0.8679\n",
      "Iter-47460, train loss-0.3578, acc-0.9000, valid loss-0.4713, acc-0.8708, test loss-0.4748, acc-0.8679\n",
      "Iter-47470, train loss-0.4613, acc-0.9000, valid loss-0.4713, acc-0.8708, test loss-0.4748, acc-0.8679\n",
      "Iter-47480, train loss-0.3744, acc-0.9000, valid loss-0.4712, acc-0.8708, test loss-0.4747, acc-0.8679\n",
      "Iter-47490, train loss-0.5813, acc-0.8200, valid loss-0.4711, acc-0.8708, test loss-0.4747, acc-0.8679\n",
      "Iter-47500, train loss-0.4660, acc-0.8600, valid loss-0.4711, acc-0.8708, test loss-0.4746, acc-0.8679\n",
      "Iter-47510, train loss-0.4808, acc-0.8800, valid loss-0.4711, acc-0.8708, test loss-0.4746, acc-0.8678\n",
      "Iter-47520, train loss-0.2936, acc-0.9800, valid loss-0.4710, acc-0.8708, test loss-0.4746, acc-0.8678\n",
      "Iter-47530, train loss-0.4599, acc-0.9000, valid loss-0.4710, acc-0.8704, test loss-0.4745, acc-0.8678\n",
      "Iter-47540, train loss-0.3543, acc-0.9000, valid loss-0.4709, acc-0.8704, test loss-0.4745, acc-0.8678\n",
      "Iter-47550, train loss-0.5506, acc-0.8000, valid loss-0.4709, acc-0.8706, test loss-0.4744, acc-0.8678\n",
      "Iter-47560, train loss-0.3475, acc-0.9200, valid loss-0.4708, acc-0.8706, test loss-0.4744, acc-0.8678\n",
      "Iter-47570, train loss-0.4470, acc-0.8600, valid loss-0.4708, acc-0.8708, test loss-0.4743, acc-0.8678\n",
      "Iter-47580, train loss-0.5314, acc-0.8400, valid loss-0.4707, acc-0.8706, test loss-0.4743, acc-0.8678\n",
      "Iter-47590, train loss-0.3195, acc-0.8800, valid loss-0.4707, acc-0.8706, test loss-0.4742, acc-0.8679\n",
      "Iter-47600, train loss-0.3948, acc-0.9000, valid loss-0.4706, acc-0.8706, test loss-0.4741, acc-0.8679\n",
      "Iter-47610, train loss-0.5508, acc-0.7800, valid loss-0.4706, acc-0.8706, test loss-0.4741, acc-0.8679\n",
      "Iter-47620, train loss-0.5515, acc-0.8000, valid loss-0.4706, acc-0.8708, test loss-0.4741, acc-0.8679\n",
      "Iter-47630, train loss-0.3845, acc-0.9000, valid loss-0.4705, acc-0.8708, test loss-0.4740, acc-0.8678\n",
      "Iter-47640, train loss-0.5536, acc-0.8400, valid loss-0.4705, acc-0.8706, test loss-0.4740, acc-0.8678\n",
      "Iter-47650, train loss-0.3215, acc-0.9400, valid loss-0.4705, acc-0.8706, test loss-0.4739, acc-0.8679\n",
      "Iter-47660, train loss-0.4355, acc-0.9000, valid loss-0.4704, acc-0.8708, test loss-0.4739, acc-0.8679\n",
      "Iter-47670, train loss-0.4279, acc-0.9000, valid loss-0.4704, acc-0.8706, test loss-0.4738, acc-0.8679\n",
      "Iter-47680, train loss-0.4716, acc-0.8600, valid loss-0.4703, acc-0.8708, test loss-0.4738, acc-0.8679\n",
      "Iter-47690, train loss-0.3713, acc-0.9400, valid loss-0.4703, acc-0.8708, test loss-0.4737, acc-0.8680\n",
      "Iter-47700, train loss-0.6737, acc-0.7400, valid loss-0.4702, acc-0.8710, test loss-0.4737, acc-0.8680\n",
      "Iter-47710, train loss-0.5227, acc-0.8400, valid loss-0.4702, acc-0.8710, test loss-0.4737, acc-0.8681\n",
      "Iter-47720, train loss-0.6474, acc-0.8200, valid loss-0.4701, acc-0.8710, test loss-0.4736, acc-0.8681\n",
      "Iter-47730, train loss-0.3466, acc-0.9000, valid loss-0.4701, acc-0.8708, test loss-0.4736, acc-0.8680\n",
      "Iter-47740, train loss-0.6404, acc-0.7800, valid loss-0.4700, acc-0.8708, test loss-0.4735, acc-0.8681\n",
      "Iter-47750, train loss-0.4502, acc-0.8400, valid loss-0.4700, acc-0.8710, test loss-0.4735, acc-0.8682\n",
      "Iter-47760, train loss-0.4483, acc-0.8800, valid loss-0.4699, acc-0.8708, test loss-0.4734, acc-0.8681\n",
      "Iter-47770, train loss-0.3930, acc-0.9400, valid loss-0.4699, acc-0.8710, test loss-0.4733, acc-0.8681\n",
      "Iter-47780, train loss-0.5080, acc-0.8600, valid loss-0.4698, acc-0.8708, test loss-0.4733, acc-0.8683\n",
      "Iter-47790, train loss-0.5471, acc-0.8200, valid loss-0.4697, acc-0.8710, test loss-0.4732, acc-0.8683\n",
      "Iter-47800, train loss-0.6786, acc-0.8200, valid loss-0.4697, acc-0.8708, test loss-0.4732, acc-0.8683\n",
      "Iter-47810, train loss-0.5281, acc-0.8400, valid loss-0.4696, acc-0.8710, test loss-0.4731, acc-0.8684\n",
      "Iter-47820, train loss-0.4216, acc-0.9000, valid loss-0.4696, acc-0.8708, test loss-0.4731, acc-0.8684\n",
      "Iter-47830, train loss-0.4352, acc-0.9400, valid loss-0.4695, acc-0.8706, test loss-0.4730, acc-0.8684\n",
      "Iter-47840, train loss-0.4526, acc-0.8600, valid loss-0.4695, acc-0.8706, test loss-0.4730, acc-0.8684\n",
      "Iter-47850, train loss-0.4161, acc-0.9200, valid loss-0.4694, acc-0.8706, test loss-0.4729, acc-0.8683\n",
      "Iter-47860, train loss-0.4860, acc-0.8400, valid loss-0.4694, acc-0.8708, test loss-0.4729, acc-0.8683\n",
      "Iter-47870, train loss-0.4341, acc-0.8800, valid loss-0.4693, acc-0.8706, test loss-0.4728, acc-0.8684\n",
      "Iter-47880, train loss-0.5135, acc-0.8600, valid loss-0.4693, acc-0.8706, test loss-0.4728, acc-0.8683\n",
      "Iter-47890, train loss-0.5359, acc-0.8200, valid loss-0.4692, acc-0.8706, test loss-0.4727, acc-0.8683\n",
      "Iter-47900, train loss-0.5604, acc-0.8600, valid loss-0.4692, acc-0.8708, test loss-0.4727, acc-0.8684\n",
      "Iter-47910, train loss-0.3511, acc-0.9000, valid loss-0.4691, acc-0.8708, test loss-0.4727, acc-0.8683\n",
      "Iter-47920, train loss-0.3587, acc-0.9400, valid loss-0.4691, acc-0.8708, test loss-0.4726, acc-0.8683\n",
      "Iter-47930, train loss-0.4691, acc-0.8800, valid loss-0.4690, acc-0.8710, test loss-0.4726, acc-0.8683\n",
      "Iter-47940, train loss-0.4845, acc-0.9000, valid loss-0.4690, acc-0.8710, test loss-0.4725, acc-0.8684\n",
      "Iter-47950, train loss-0.5303, acc-0.8400, valid loss-0.4689, acc-0.8710, test loss-0.4725, acc-0.8685\n",
      "Iter-47960, train loss-0.4979, acc-0.8600, valid loss-0.4689, acc-0.8710, test loss-0.4724, acc-0.8685\n",
      "Iter-47970, train loss-0.6499, acc-0.8400, valid loss-0.4688, acc-0.8710, test loss-0.4724, acc-0.8685\n",
      "Iter-47980, train loss-0.3845, acc-0.9000, valid loss-0.4688, acc-0.8712, test loss-0.4723, acc-0.8685\n",
      "Iter-47990, train loss-0.7022, acc-0.8400, valid loss-0.4687, acc-0.8712, test loss-0.4723, acc-0.8685\n",
      "Iter-48000, train loss-0.6063, acc-0.8200, valid loss-0.4687, acc-0.8712, test loss-0.4722, acc-0.8685\n",
      "Iter-48010, train loss-0.3791, acc-0.9000, valid loss-0.4686, acc-0.8712, test loss-0.4722, acc-0.8685\n",
      "Iter-48020, train loss-0.4726, acc-0.8600, valid loss-0.4686, acc-0.8712, test loss-0.4721, acc-0.8685\n",
      "Iter-48030, train loss-0.4418, acc-0.9000, valid loss-0.4685, acc-0.8712, test loss-0.4721, acc-0.8685\n",
      "Iter-48040, train loss-0.5251, acc-0.8800, valid loss-0.4685, acc-0.8714, test loss-0.4720, acc-0.8685\n",
      "Iter-48050, train loss-0.5448, acc-0.8000, valid loss-0.4685, acc-0.8714, test loss-0.4720, acc-0.8686\n",
      "Iter-48060, train loss-0.4430, acc-0.8600, valid loss-0.4684, acc-0.8716, test loss-0.4720, acc-0.8688\n",
      "Iter-48070, train loss-0.1837, acc-0.9800, valid loss-0.4684, acc-0.8716, test loss-0.4719, acc-0.8689\n",
      "Iter-48080, train loss-0.3661, acc-0.9200, valid loss-0.4683, acc-0.8716, test loss-0.4719, acc-0.8688\n",
      "Iter-48090, train loss-0.4272, acc-0.8800, valid loss-0.4683, acc-0.8716, test loss-0.4718, acc-0.8687\n",
      "Iter-48100, train loss-0.4327, acc-0.8400, valid loss-0.4682, acc-0.8714, test loss-0.4718, acc-0.8688\n",
      "Iter-48110, train loss-0.3144, acc-0.9200, valid loss-0.4682, acc-0.8714, test loss-0.4717, acc-0.8688\n",
      "Iter-48120, train loss-0.5343, acc-0.8600, valid loss-0.4681, acc-0.8714, test loss-0.4717, acc-0.8687\n",
      "Iter-48130, train loss-0.3983, acc-0.9200, valid loss-0.4681, acc-0.8714, test loss-0.4717, acc-0.8688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-48140, train loss-0.6066, acc-0.7800, valid loss-0.4681, acc-0.8714, test loss-0.4716, acc-0.8687\n",
      "Iter-48150, train loss-0.5011, acc-0.8600, valid loss-0.4680, acc-0.8714, test loss-0.4716, acc-0.8687\n",
      "Iter-48160, train loss-0.6729, acc-0.8000, valid loss-0.4680, acc-0.8716, test loss-0.4715, acc-0.8687\n",
      "Iter-48170, train loss-0.3944, acc-0.9000, valid loss-0.4679, acc-0.8716, test loss-0.4715, acc-0.8687\n",
      "Iter-48180, train loss-0.5356, acc-0.9000, valid loss-0.4679, acc-0.8718, test loss-0.4715, acc-0.8687\n",
      "Iter-48190, train loss-0.4499, acc-0.9000, valid loss-0.4678, acc-0.8720, test loss-0.4714, acc-0.8687\n",
      "Iter-48200, train loss-0.4282, acc-0.9000, valid loss-0.4678, acc-0.8720, test loss-0.4714, acc-0.8687\n",
      "Iter-48210, train loss-0.4350, acc-0.8400, valid loss-0.4677, acc-0.8720, test loss-0.4713, acc-0.8687\n",
      "Iter-48220, train loss-0.3541, acc-0.9400, valid loss-0.4677, acc-0.8718, test loss-0.4713, acc-0.8687\n",
      "Iter-48230, train loss-0.3837, acc-0.9000, valid loss-0.4676, acc-0.8718, test loss-0.4712, acc-0.8687\n",
      "Iter-48240, train loss-0.5641, acc-0.8400, valid loss-0.4676, acc-0.8720, test loss-0.4712, acc-0.8687\n",
      "Iter-48250, train loss-0.5719, acc-0.8000, valid loss-0.4675, acc-0.8718, test loss-0.4711, acc-0.8687\n",
      "Iter-48260, train loss-0.3725, acc-0.9400, valid loss-0.4675, acc-0.8718, test loss-0.4711, acc-0.8688\n",
      "Iter-48270, train loss-0.4329, acc-0.9000, valid loss-0.4675, acc-0.8718, test loss-0.4710, acc-0.8686\n",
      "Iter-48280, train loss-0.7409, acc-0.7600, valid loss-0.4674, acc-0.8718, test loss-0.4710, acc-0.8686\n",
      "Iter-48290, train loss-0.4502, acc-0.9000, valid loss-0.4674, acc-0.8718, test loss-0.4709, acc-0.8686\n",
      "Iter-48300, train loss-0.3479, acc-0.9200, valid loss-0.4673, acc-0.8718, test loss-0.4709, acc-0.8686\n",
      "Iter-48310, train loss-0.5796, acc-0.8000, valid loss-0.4672, acc-0.8716, test loss-0.4708, acc-0.8687\n",
      "Iter-48320, train loss-0.4136, acc-0.9000, valid loss-0.4672, acc-0.8718, test loss-0.4707, acc-0.8688\n",
      "Iter-48330, train loss-0.3546, acc-0.9400, valid loss-0.4671, acc-0.8718, test loss-0.4707, acc-0.8688\n",
      "Iter-48340, train loss-0.6432, acc-0.7800, valid loss-0.4671, acc-0.8718, test loss-0.4706, acc-0.8687\n",
      "Iter-48350, train loss-0.4154, acc-0.8600, valid loss-0.4670, acc-0.8716, test loss-0.4706, acc-0.8687\n",
      "Iter-48360, train loss-0.5186, acc-0.8400, valid loss-0.4670, acc-0.8718, test loss-0.4705, acc-0.8687\n",
      "Iter-48370, train loss-0.5160, acc-0.8000, valid loss-0.4669, acc-0.8716, test loss-0.4705, acc-0.8688\n",
      "Iter-48380, train loss-0.4813, acc-0.9000, valid loss-0.4669, acc-0.8718, test loss-0.4704, acc-0.8687\n",
      "Iter-48390, train loss-0.4779, acc-0.8800, valid loss-0.4668, acc-0.8718, test loss-0.4704, acc-0.8687\n",
      "Iter-48400, train loss-0.5167, acc-0.8600, valid loss-0.4668, acc-0.8716, test loss-0.4703, acc-0.8687\n",
      "Iter-48410, train loss-0.4499, acc-0.8000, valid loss-0.4667, acc-0.8716, test loss-0.4703, acc-0.8687\n",
      "Iter-48420, train loss-0.4737, acc-0.8600, valid loss-0.4667, acc-0.8716, test loss-0.4702, acc-0.8688\n",
      "Iter-48430, train loss-0.4331, acc-0.8400, valid loss-0.4666, acc-0.8716, test loss-0.4702, acc-0.8689\n",
      "Iter-48440, train loss-0.6304, acc-0.8800, valid loss-0.4666, acc-0.8716, test loss-0.4702, acc-0.8689\n",
      "Iter-48450, train loss-0.4611, acc-0.9000, valid loss-0.4665, acc-0.8718, test loss-0.4701, acc-0.8688\n",
      "Iter-48460, train loss-0.4365, acc-0.9000, valid loss-0.4665, acc-0.8718, test loss-0.4701, acc-0.8689\n",
      "Iter-48470, train loss-0.3596, acc-0.8800, valid loss-0.4664, acc-0.8718, test loss-0.4700, acc-0.8689\n",
      "Iter-48480, train loss-0.4706, acc-0.8000, valid loss-0.4664, acc-0.8720, test loss-0.4700, acc-0.8689\n",
      "Iter-48490, train loss-0.5108, acc-0.8400, valid loss-0.4663, acc-0.8718, test loss-0.4699, acc-0.8690\n",
      "Iter-48500, train loss-0.5645, acc-0.7800, valid loss-0.4663, acc-0.8718, test loss-0.4699, acc-0.8690\n",
      "Iter-48510, train loss-0.6583, acc-0.8400, valid loss-0.4662, acc-0.8720, test loss-0.4698, acc-0.8690\n",
      "Iter-48520, train loss-0.3954, acc-0.8200, valid loss-0.4662, acc-0.8720, test loss-0.4698, acc-0.8691\n",
      "Iter-48530, train loss-0.4652, acc-0.8600, valid loss-0.4661, acc-0.8718, test loss-0.4697, acc-0.8690\n",
      "Iter-48540, train loss-0.5196, acc-0.8200, valid loss-0.4661, acc-0.8722, test loss-0.4697, acc-0.8690\n",
      "Iter-48550, train loss-0.3225, acc-0.9200, valid loss-0.4660, acc-0.8720, test loss-0.4697, acc-0.8691\n",
      "Iter-48560, train loss-0.4971, acc-0.8400, valid loss-0.4660, acc-0.8724, test loss-0.4696, acc-0.8690\n",
      "Iter-48570, train loss-0.3138, acc-0.9000, valid loss-0.4659, acc-0.8720, test loss-0.4696, acc-0.8691\n",
      "Iter-48580, train loss-0.5355, acc-0.8400, valid loss-0.4659, acc-0.8724, test loss-0.4695, acc-0.8691\n",
      "Iter-48590, train loss-0.3689, acc-0.9000, valid loss-0.4658, acc-0.8724, test loss-0.4695, acc-0.8691\n",
      "Iter-48600, train loss-0.5615, acc-0.8000, valid loss-0.4658, acc-0.8730, test loss-0.4694, acc-0.8691\n",
      "Iter-48610, train loss-0.4193, acc-0.9000, valid loss-0.4657, acc-0.8728, test loss-0.4694, acc-0.8691\n",
      "Iter-48620, train loss-0.6546, acc-0.8000, valid loss-0.4657, acc-0.8728, test loss-0.4693, acc-0.8691\n",
      "Iter-48630, train loss-0.3423, acc-0.9200, valid loss-0.4656, acc-0.8728, test loss-0.4693, acc-0.8690\n",
      "Iter-48640, train loss-0.4461, acc-0.8600, valid loss-0.4656, acc-0.8730, test loss-0.4692, acc-0.8690\n",
      "Iter-48650, train loss-0.5315, acc-0.8600, valid loss-0.4655, acc-0.8728, test loss-0.4692, acc-0.8690\n",
      "Iter-48660, train loss-0.5148, acc-0.8400, valid loss-0.4655, acc-0.8728, test loss-0.4691, acc-0.8690\n",
      "Iter-48670, train loss-0.6074, acc-0.8200, valid loss-0.4655, acc-0.8728, test loss-0.4691, acc-0.8690\n",
      "Iter-48680, train loss-0.3301, acc-0.9200, valid loss-0.4654, acc-0.8724, test loss-0.4690, acc-0.8691\n",
      "Iter-48690, train loss-0.3864, acc-0.8400, valid loss-0.4654, acc-0.8724, test loss-0.4690, acc-0.8691\n",
      "Iter-48700, train loss-0.3691, acc-0.9200, valid loss-0.4653, acc-0.8724, test loss-0.4689, acc-0.8690\n",
      "Iter-48710, train loss-0.4628, acc-0.8600, valid loss-0.4653, acc-0.8724, test loss-0.4689, acc-0.8693\n",
      "Iter-48720, train loss-0.5211, acc-0.8600, valid loss-0.4653, acc-0.8724, test loss-0.4689, acc-0.8692\n",
      "Iter-48730, train loss-0.4972, acc-0.8400, valid loss-0.4652, acc-0.8724, test loss-0.4688, acc-0.8693\n",
      "Iter-48740, train loss-0.2845, acc-0.9600, valid loss-0.4651, acc-0.8724, test loss-0.4688, acc-0.8693\n",
      "Iter-48750, train loss-0.4669, acc-0.8600, valid loss-0.4651, acc-0.8724, test loss-0.4687, acc-0.8693\n",
      "Iter-48760, train loss-0.4985, acc-0.9000, valid loss-0.4650, acc-0.8726, test loss-0.4686, acc-0.8691\n",
      "Iter-48770, train loss-0.5413, acc-0.8000, valid loss-0.4650, acc-0.8724, test loss-0.4686, acc-0.8692\n",
      "Iter-48780, train loss-0.5901, acc-0.8000, valid loss-0.4650, acc-0.8724, test loss-0.4686, acc-0.8692\n",
      "Iter-48790, train loss-0.3476, acc-0.9200, valid loss-0.4649, acc-0.8724, test loss-0.4685, acc-0.8692\n",
      "Iter-48800, train loss-0.5331, acc-0.8400, valid loss-0.4649, acc-0.8724, test loss-0.4685, acc-0.8692\n",
      "Iter-48810, train loss-0.7953, acc-0.8200, valid loss-0.4648, acc-0.8724, test loss-0.4684, acc-0.8692\n",
      "Iter-48820, train loss-0.5184, acc-0.8600, valid loss-0.4648, acc-0.8724, test loss-0.4684, acc-0.8692\n",
      "Iter-48830, train loss-0.6521, acc-0.7800, valid loss-0.4647, acc-0.8724, test loss-0.4683, acc-0.8692\n",
      "Iter-48840, train loss-0.4553, acc-0.8800, valid loss-0.4647, acc-0.8724, test loss-0.4683, acc-0.8692\n",
      "Iter-48850, train loss-0.4671, acc-0.8400, valid loss-0.4646, acc-0.8726, test loss-0.4682, acc-0.8692\n",
      "Iter-48860, train loss-0.4892, acc-0.8000, valid loss-0.4646, acc-0.8724, test loss-0.4682, acc-0.8692\n",
      "Iter-48870, train loss-0.4711, acc-0.8400, valid loss-0.4645, acc-0.8726, test loss-0.4682, acc-0.8693\n",
      "Iter-48880, train loss-0.3851, acc-0.9400, valid loss-0.4645, acc-0.8726, test loss-0.4681, acc-0.8693\n",
      "Iter-48890, train loss-0.3701, acc-0.9000, valid loss-0.4644, acc-0.8726, test loss-0.4681, acc-0.8692\n",
      "Iter-48900, train loss-0.2443, acc-0.9600, valid loss-0.4644, acc-0.8726, test loss-0.4680, acc-0.8692\n",
      "Iter-48910, train loss-0.3254, acc-0.8800, valid loss-0.4644, acc-0.8726, test loss-0.4680, acc-0.8692\n",
      "Iter-48920, train loss-0.7931, acc-0.8200, valid loss-0.4643, acc-0.8726, test loss-0.4680, acc-0.8693\n",
      "Iter-48930, train loss-0.4899, acc-0.8800, valid loss-0.4643, acc-0.8726, test loss-0.4679, acc-0.8693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-48940, train loss-0.3418, acc-0.9200, valid loss-0.4642, acc-0.8726, test loss-0.4679, acc-0.8694\n",
      "Iter-48950, train loss-0.6042, acc-0.8200, valid loss-0.4642, acc-0.8726, test loss-0.4678, acc-0.8694\n",
      "Iter-48960, train loss-0.4784, acc-0.8800, valid loss-0.4641, acc-0.8726, test loss-0.4678, acc-0.8694\n",
      "Iter-48970, train loss-0.4154, acc-0.8600, valid loss-0.4641, acc-0.8726, test loss-0.4677, acc-0.8694\n",
      "Iter-48980, train loss-0.3022, acc-0.9400, valid loss-0.4641, acc-0.8728, test loss-0.4677, acc-0.8694\n",
      "Iter-48990, train loss-0.3955, acc-0.8800, valid loss-0.4640, acc-0.8728, test loss-0.4677, acc-0.8694\n",
      "Iter-49000, train loss-0.4153, acc-0.9200, valid loss-0.4640, acc-0.8728, test loss-0.4676, acc-0.8695\n",
      "Iter-49010, train loss-0.3186, acc-0.9200, valid loss-0.4639, acc-0.8728, test loss-0.4676, acc-0.8695\n",
      "Iter-49020, train loss-0.6232, acc-0.8400, valid loss-0.4639, acc-0.8728, test loss-0.4675, acc-0.8694\n",
      "Iter-49030, train loss-0.5828, acc-0.8400, valid loss-0.4638, acc-0.8728, test loss-0.4675, acc-0.8694\n",
      "Iter-49040, train loss-0.4219, acc-0.9200, valid loss-0.4638, acc-0.8728, test loss-0.4674, acc-0.8695\n",
      "Iter-49050, train loss-0.5631, acc-0.8400, valid loss-0.4637, acc-0.8728, test loss-0.4674, acc-0.8694\n",
      "Iter-49060, train loss-0.5320, acc-0.8200, valid loss-0.4637, acc-0.8728, test loss-0.4673, acc-0.8693\n",
      "Iter-49070, train loss-0.3156, acc-0.8800, valid loss-0.4636, acc-0.8728, test loss-0.4673, acc-0.8693\n",
      "Iter-49080, train loss-0.5499, acc-0.8600, valid loss-0.4636, acc-0.8728, test loss-0.4672, acc-0.8693\n",
      "Iter-49090, train loss-0.3775, acc-0.9200, valid loss-0.4635, acc-0.8728, test loss-0.4672, acc-0.8693\n",
      "Iter-49100, train loss-0.5088, acc-0.8400, valid loss-0.4635, acc-0.8728, test loss-0.4671, acc-0.8693\n",
      "Iter-49110, train loss-0.4965, acc-0.8200, valid loss-0.4634, acc-0.8728, test loss-0.4671, acc-0.8695\n",
      "Iter-49120, train loss-0.3916, acc-0.9000, valid loss-0.4634, acc-0.8728, test loss-0.4670, acc-0.8694\n",
      "Iter-49130, train loss-0.6741, acc-0.8000, valid loss-0.4634, acc-0.8728, test loss-0.4670, acc-0.8694\n",
      "Iter-49140, train loss-0.4713, acc-0.8200, valid loss-0.4633, acc-0.8728, test loss-0.4670, acc-0.8696\n",
      "Iter-49150, train loss-0.4851, acc-0.8800, valid loss-0.4632, acc-0.8728, test loss-0.4669, acc-0.8696\n",
      "Iter-49160, train loss-0.6329, acc-0.8000, valid loss-0.4632, acc-0.8730, test loss-0.4669, acc-0.8696\n",
      "Iter-49170, train loss-0.6098, acc-0.8400, valid loss-0.4632, acc-0.8730, test loss-0.4668, acc-0.8696\n",
      "Iter-49180, train loss-0.3981, acc-0.9000, valid loss-0.4631, acc-0.8730, test loss-0.4668, acc-0.8696\n",
      "Iter-49190, train loss-0.4403, acc-0.8800, valid loss-0.4631, acc-0.8730, test loss-0.4667, acc-0.8696\n",
      "Iter-49200, train loss-0.5045, acc-0.8400, valid loss-0.4630, acc-0.8732, test loss-0.4667, acc-0.8696\n",
      "Iter-49210, train loss-0.5059, acc-0.8400, valid loss-0.4630, acc-0.8730, test loss-0.4666, acc-0.8696\n",
      "Iter-49220, train loss-0.6369, acc-0.8400, valid loss-0.4629, acc-0.8730, test loss-0.4666, acc-0.8696\n",
      "Iter-49230, train loss-0.4576, acc-0.8800, valid loss-0.4629, acc-0.8730, test loss-0.4665, acc-0.8696\n",
      "Iter-49240, train loss-0.5745, acc-0.8000, valid loss-0.4628, acc-0.8730, test loss-0.4665, acc-0.8697\n",
      "Iter-49250, train loss-0.8552, acc-0.7800, valid loss-0.4628, acc-0.8732, test loss-0.4665, acc-0.8695\n",
      "Iter-49260, train loss-0.5526, acc-0.8200, valid loss-0.4628, acc-0.8734, test loss-0.4664, acc-0.8697\n",
      "Iter-49270, train loss-0.5089, acc-0.7800, valid loss-0.4627, acc-0.8734, test loss-0.4664, acc-0.8694\n",
      "Iter-49280, train loss-0.7968, acc-0.8000, valid loss-0.4627, acc-0.8734, test loss-0.4663, acc-0.8695\n",
      "Iter-49290, train loss-0.5455, acc-0.8600, valid loss-0.4626, acc-0.8734, test loss-0.4663, acc-0.8695\n",
      "Iter-49300, train loss-0.6857, acc-0.7400, valid loss-0.4626, acc-0.8732, test loss-0.4662, acc-0.8695\n",
      "Iter-49310, train loss-0.6284, acc-0.8000, valid loss-0.4625, acc-0.8734, test loss-0.4662, acc-0.8696\n",
      "Iter-49320, train loss-0.4120, acc-0.9200, valid loss-0.4625, acc-0.8734, test loss-0.4662, acc-0.8696\n",
      "Iter-49330, train loss-0.3688, acc-0.8800, valid loss-0.4624, acc-0.8734, test loss-0.4661, acc-0.8696\n",
      "Iter-49340, train loss-0.4523, acc-0.9200, valid loss-0.4624, acc-0.8734, test loss-0.4661, acc-0.8696\n",
      "Iter-49350, train loss-0.4445, acc-0.8600, valid loss-0.4624, acc-0.8734, test loss-0.4660, acc-0.8696\n",
      "Iter-49360, train loss-0.5175, acc-0.8600, valid loss-0.4623, acc-0.8734, test loss-0.4660, acc-0.8697\n",
      "Iter-49370, train loss-0.5963, acc-0.7600, valid loss-0.4623, acc-0.8734, test loss-0.4659, acc-0.8697\n",
      "Iter-49380, train loss-0.4680, acc-0.8000, valid loss-0.4622, acc-0.8732, test loss-0.4659, acc-0.8697\n",
      "Iter-49390, train loss-0.6100, acc-0.7800, valid loss-0.4622, acc-0.8732, test loss-0.4658, acc-0.8696\n",
      "Iter-49400, train loss-0.5225, acc-0.8200, valid loss-0.4621, acc-0.8732, test loss-0.4658, acc-0.8696\n",
      "Iter-49410, train loss-0.3428, acc-0.9200, valid loss-0.4621, acc-0.8732, test loss-0.4657, acc-0.8698\n",
      "Iter-49420, train loss-0.2768, acc-0.9600, valid loss-0.4620, acc-0.8730, test loss-0.4657, acc-0.8698\n",
      "Iter-49430, train loss-0.6955, acc-0.8000, valid loss-0.4620, acc-0.8730, test loss-0.4656, acc-0.8698\n",
      "Iter-49440, train loss-0.5374, acc-0.8000, valid loss-0.4619, acc-0.8732, test loss-0.4656, acc-0.8699\n",
      "Iter-49450, train loss-0.6284, acc-0.8000, valid loss-0.4619, acc-0.8730, test loss-0.4655, acc-0.8699\n",
      "Iter-49460, train loss-0.4018, acc-0.8600, valid loss-0.4618, acc-0.8734, test loss-0.4655, acc-0.8699\n",
      "Iter-49470, train loss-0.4459, acc-0.8600, valid loss-0.4618, acc-0.8732, test loss-0.4655, acc-0.8700\n",
      "Iter-49480, train loss-0.4384, acc-0.8600, valid loss-0.4617, acc-0.8734, test loss-0.4654, acc-0.8700\n",
      "Iter-49490, train loss-0.3350, acc-0.9400, valid loss-0.4617, acc-0.8732, test loss-0.4654, acc-0.8700\n",
      "Iter-49500, train loss-0.3880, acc-0.8600, valid loss-0.4617, acc-0.8732, test loss-0.4653, acc-0.8700\n",
      "Iter-49510, train loss-0.6079, acc-0.8800, valid loss-0.4616, acc-0.8732, test loss-0.4653, acc-0.8701\n",
      "Iter-49520, train loss-0.5108, acc-0.8800, valid loss-0.4616, acc-0.8732, test loss-0.4652, acc-0.8701\n",
      "Iter-49530, train loss-0.3385, acc-0.9200, valid loss-0.4615, acc-0.8732, test loss-0.4652, acc-0.8701\n",
      "Iter-49540, train loss-0.9137, acc-0.7400, valid loss-0.4615, acc-0.8732, test loss-0.4651, acc-0.8701\n",
      "Iter-49550, train loss-0.4135, acc-0.8800, valid loss-0.4614, acc-0.8732, test loss-0.4651, acc-0.8702\n",
      "Iter-49560, train loss-0.3303, acc-0.9400, valid loss-0.4614, acc-0.8732, test loss-0.4650, acc-0.8702\n",
      "Iter-49570, train loss-0.5013, acc-0.8800, valid loss-0.4613, acc-0.8730, test loss-0.4650, acc-0.8702\n",
      "Iter-49580, train loss-0.4333, acc-0.9200, valid loss-0.4613, acc-0.8730, test loss-0.4650, acc-0.8703\n",
      "Iter-49590, train loss-0.3476, acc-0.9400, valid loss-0.4612, acc-0.8730, test loss-0.4649, acc-0.8703\n",
      "Iter-49600, train loss-0.5622, acc-0.8800, valid loss-0.4612, acc-0.8732, test loss-0.4649, acc-0.8704\n",
      "Iter-49610, train loss-0.5375, acc-0.8400, valid loss-0.4612, acc-0.8734, test loss-0.4648, acc-0.8703\n",
      "Iter-49620, train loss-0.4974, acc-0.8000, valid loss-0.4611, acc-0.8734, test loss-0.4648, acc-0.8704\n",
      "Iter-49630, train loss-0.6594, acc-0.7600, valid loss-0.4611, acc-0.8734, test loss-0.4647, acc-0.8703\n",
      "Iter-49640, train loss-0.5485, acc-0.8400, valid loss-0.4610, acc-0.8734, test loss-0.4647, acc-0.8703\n",
      "Iter-49650, train loss-0.4124, acc-0.8600, valid loss-0.4610, acc-0.8734, test loss-0.4647, acc-0.8703\n",
      "Iter-49660, train loss-0.4752, acc-0.8400, valid loss-0.4609, acc-0.8732, test loss-0.4646, acc-0.8703\n",
      "Iter-49670, train loss-0.4123, acc-0.9200, valid loss-0.4609, acc-0.8734, test loss-0.4646, acc-0.8703\n",
      "Iter-49680, train loss-0.4426, acc-0.8400, valid loss-0.4608, acc-0.8734, test loss-0.4646, acc-0.8703\n",
      "Iter-49690, train loss-0.5723, acc-0.8200, valid loss-0.4608, acc-0.8734, test loss-0.4645, acc-0.8703\n",
      "Iter-49700, train loss-0.4237, acc-0.8600, valid loss-0.4607, acc-0.8732, test loss-0.4645, acc-0.8702\n",
      "Iter-49710, train loss-0.5586, acc-0.8600, valid loss-0.4607, acc-0.8730, test loss-0.4644, acc-0.8702\n",
      "Iter-49720, train loss-0.3876, acc-0.9200, valid loss-0.4607, acc-0.8730, test loss-0.4644, acc-0.8703\n",
      "Iter-49730, train loss-0.5407, acc-0.8400, valid loss-0.4606, acc-0.8730, test loss-0.4643, acc-0.8703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-49740, train loss-0.4482, acc-0.8800, valid loss-0.4606, acc-0.8730, test loss-0.4643, acc-0.8703\n",
      "Iter-49750, train loss-0.5006, acc-0.9200, valid loss-0.4605, acc-0.8732, test loss-0.4642, acc-0.8703\n",
      "Iter-49760, train loss-0.3926, acc-0.8600, valid loss-0.4605, acc-0.8732, test loss-0.4642, acc-0.8703\n",
      "Iter-49770, train loss-0.4720, acc-0.8200, valid loss-0.4605, acc-0.8732, test loss-0.4641, acc-0.8703\n",
      "Iter-49780, train loss-0.3743, acc-0.9200, valid loss-0.4604, acc-0.8732, test loss-0.4641, acc-0.8703\n",
      "Iter-49790, train loss-0.5720, acc-0.8200, valid loss-0.4604, acc-0.8732, test loss-0.4641, acc-0.8703\n",
      "Iter-49800, train loss-0.4752, acc-0.9000, valid loss-0.4603, acc-0.8730, test loss-0.4640, acc-0.8703\n",
      "Iter-49810, train loss-0.6421, acc-0.8800, valid loss-0.4603, acc-0.8732, test loss-0.4640, acc-0.8703\n",
      "Iter-49820, train loss-0.4216, acc-0.8600, valid loss-0.4602, acc-0.8730, test loss-0.4639, acc-0.8703\n",
      "Iter-49830, train loss-0.4447, acc-0.8800, valid loss-0.4602, acc-0.8732, test loss-0.4639, acc-0.8703\n",
      "Iter-49840, train loss-0.5423, acc-0.8600, valid loss-0.4601, acc-0.8734, test loss-0.4638, acc-0.8703\n",
      "Iter-49850, train loss-0.6247, acc-0.8600, valid loss-0.4601, acc-0.8734, test loss-0.4638, acc-0.8704\n",
      "Iter-49860, train loss-0.4937, acc-0.8200, valid loss-0.4601, acc-0.8734, test loss-0.4637, acc-0.8704\n",
      "Iter-49870, train loss-0.3350, acc-0.9400, valid loss-0.4600, acc-0.8738, test loss-0.4637, acc-0.8704\n",
      "Iter-49880, train loss-0.7124, acc-0.8000, valid loss-0.4600, acc-0.8740, test loss-0.4636, acc-0.8704\n",
      "Iter-49890, train loss-0.5878, acc-0.8600, valid loss-0.4599, acc-0.8740, test loss-0.4636, acc-0.8705\n",
      "Iter-49900, train loss-0.3415, acc-0.9600, valid loss-0.4599, acc-0.8740, test loss-0.4635, acc-0.8705\n",
      "Iter-49910, train loss-0.6282, acc-0.8400, valid loss-0.4598, acc-0.8742, test loss-0.4635, acc-0.8705\n",
      "Iter-49920, train loss-0.4079, acc-0.8600, valid loss-0.4598, acc-0.8740, test loss-0.4635, acc-0.8705\n",
      "Iter-49930, train loss-0.4162, acc-0.8600, valid loss-0.4598, acc-0.8742, test loss-0.4634, acc-0.8706\n",
      "Iter-49940, train loss-0.4871, acc-0.8600, valid loss-0.4597, acc-0.8740, test loss-0.4634, acc-0.8706\n",
      "Iter-49950, train loss-0.5086, acc-0.8800, valid loss-0.4597, acc-0.8742, test loss-0.4634, acc-0.8705\n",
      "Iter-49960, train loss-0.3626, acc-0.9600, valid loss-0.4597, acc-0.8740, test loss-0.4633, acc-0.8705\n",
      "Iter-49970, train loss-0.4834, acc-0.8800, valid loss-0.4596, acc-0.8740, test loss-0.4633, acc-0.8704\n",
      "Iter-49980, train loss-0.5071, acc-0.8200, valid loss-0.4596, acc-0.8742, test loss-0.4632, acc-0.8704\n",
      "Iter-49990, train loss-0.3298, acc-0.9400, valid loss-0.4595, acc-0.8742, test loss-0.4632, acc-0.8706\n",
      "Iter-50000, train loss-0.6405, acc-0.8000, valid loss-0.4595, acc-0.8738, test loss-0.4631, acc-0.8704\n",
      "Iter-50010, train loss-0.7687, acc-0.7400, valid loss-0.4594, acc-0.8742, test loss-0.4631, acc-0.8705\n",
      "Iter-50020, train loss-0.4641, acc-0.8800, valid loss-0.4594, acc-0.8738, test loss-0.4631, acc-0.8706\n",
      "Iter-50030, train loss-0.4154, acc-0.8800, valid loss-0.4594, acc-0.8740, test loss-0.4630, acc-0.8707\n",
      "Iter-50040, train loss-0.4681, acc-0.9000, valid loss-0.4593, acc-0.8740, test loss-0.4630, acc-0.8707\n",
      "Iter-50050, train loss-0.5833, acc-0.7800, valid loss-0.4593, acc-0.8742, test loss-0.4629, acc-0.8708\n",
      "Iter-50060, train loss-0.3605, acc-0.9200, valid loss-0.4592, acc-0.8742, test loss-0.4629, acc-0.8708\n",
      "Iter-50070, train loss-0.6520, acc-0.8000, valid loss-0.4592, acc-0.8742, test loss-0.4628, acc-0.8708\n",
      "Iter-50080, train loss-0.7191, acc-0.8000, valid loss-0.4591, acc-0.8742, test loss-0.4628, acc-0.8708\n",
      "Iter-50090, train loss-0.6071, acc-0.7600, valid loss-0.4591, acc-0.8744, test loss-0.4627, acc-0.8708\n",
      "Iter-50100, train loss-0.3599, acc-0.8800, valid loss-0.4590, acc-0.8744, test loss-0.4627, acc-0.8708\n",
      "Iter-50110, train loss-0.4223, acc-0.8800, valid loss-0.4590, acc-0.8744, test loss-0.4627, acc-0.8709\n",
      "Iter-50120, train loss-0.4309, acc-0.8600, valid loss-0.4590, acc-0.8744, test loss-0.4626, acc-0.8708\n",
      "Iter-50130, train loss-0.2830, acc-0.9000, valid loss-0.4589, acc-0.8744, test loss-0.4625, acc-0.8709\n",
      "Iter-50140, train loss-0.7740, acc-0.7400, valid loss-0.4589, acc-0.8746, test loss-0.4625, acc-0.8709\n",
      "Iter-50150, train loss-0.5236, acc-0.8600, valid loss-0.4588, acc-0.8746, test loss-0.4625, acc-0.8709\n",
      "Iter-50160, train loss-0.3889, acc-0.8800, valid loss-0.4588, acc-0.8746, test loss-0.4624, acc-0.8709\n",
      "Iter-50170, train loss-0.4204, acc-0.9000, valid loss-0.4587, acc-0.8746, test loss-0.4624, acc-0.8709\n",
      "Iter-50180, train loss-0.5094, acc-0.8600, valid loss-0.4587, acc-0.8746, test loss-0.4623, acc-0.8709\n",
      "Iter-50190, train loss-0.5299, acc-0.8000, valid loss-0.4586, acc-0.8746, test loss-0.4623, acc-0.8709\n",
      "Iter-50200, train loss-0.6717, acc-0.8800, valid loss-0.4586, acc-0.8744, test loss-0.4623, acc-0.8709\n",
      "Iter-50210, train loss-0.3417, acc-0.9000, valid loss-0.4585, acc-0.8744, test loss-0.4622, acc-0.8709\n",
      "Iter-50220, train loss-0.6672, acc-0.7600, valid loss-0.4585, acc-0.8744, test loss-0.4622, acc-0.8709\n",
      "Iter-50230, train loss-0.3454, acc-0.9200, valid loss-0.4584, acc-0.8744, test loss-0.4621, acc-0.8709\n",
      "Iter-50240, train loss-0.3968, acc-0.9200, valid loss-0.4584, acc-0.8744, test loss-0.4621, acc-0.8710\n",
      "Iter-50250, train loss-0.4941, acc-0.8800, valid loss-0.4584, acc-0.8742, test loss-0.4621, acc-0.8709\n",
      "Iter-50260, train loss-0.4630, acc-0.9000, valid loss-0.4583, acc-0.8740, test loss-0.4620, acc-0.8709\n",
      "Iter-50270, train loss-0.4294, acc-0.9000, valid loss-0.4583, acc-0.8742, test loss-0.4620, acc-0.8709\n",
      "Iter-50280, train loss-0.4291, acc-0.8800, valid loss-0.4582, acc-0.8742, test loss-0.4619, acc-0.8709\n",
      "Iter-50290, train loss-0.4730, acc-0.9000, valid loss-0.4582, acc-0.8742, test loss-0.4619, acc-0.8709\n",
      "Iter-50300, train loss-0.3870, acc-0.8800, valid loss-0.4581, acc-0.8742, test loss-0.4619, acc-0.8709\n",
      "Iter-50310, train loss-0.4784, acc-0.8800, valid loss-0.4581, acc-0.8742, test loss-0.4618, acc-0.8709\n",
      "Iter-50320, train loss-0.4446, acc-0.9000, valid loss-0.4580, acc-0.8740, test loss-0.4618, acc-0.8709\n",
      "Iter-50330, train loss-0.4226, acc-0.8800, valid loss-0.4580, acc-0.8740, test loss-0.4617, acc-0.8708\n",
      "Iter-50340, train loss-0.3282, acc-0.9200, valid loss-0.4579, acc-0.8740, test loss-0.4617, acc-0.8707\n",
      "Iter-50350, train loss-0.5439, acc-0.8800, valid loss-0.4579, acc-0.8740, test loss-0.4616, acc-0.8708\n",
      "Iter-50360, train loss-0.4054, acc-0.8800, valid loss-0.4578, acc-0.8742, test loss-0.4616, acc-0.8711\n",
      "Iter-50370, train loss-0.3694, acc-0.9000, valid loss-0.4578, acc-0.8740, test loss-0.4616, acc-0.8711\n",
      "Iter-50380, train loss-0.5317, acc-0.9200, valid loss-0.4577, acc-0.8740, test loss-0.4615, acc-0.8712\n",
      "Iter-50390, train loss-0.4190, acc-0.8800, valid loss-0.4577, acc-0.8742, test loss-0.4615, acc-0.8713\n",
      "Iter-50400, train loss-0.6495, acc-0.7800, valid loss-0.4577, acc-0.8744, test loss-0.4614, acc-0.8713\n",
      "Iter-50410, train loss-0.6067, acc-0.8400, valid loss-0.4576, acc-0.8742, test loss-0.4614, acc-0.8713\n",
      "Iter-50420, train loss-0.6502, acc-0.8600, valid loss-0.4576, acc-0.8742, test loss-0.4613, acc-0.8713\n",
      "Iter-50430, train loss-0.4822, acc-0.9200, valid loss-0.4575, acc-0.8742, test loss-0.4613, acc-0.8712\n",
      "Iter-50440, train loss-0.3769, acc-0.9000, valid loss-0.4575, acc-0.8742, test loss-0.4613, acc-0.8713\n",
      "Iter-50450, train loss-0.3671, acc-0.8800, valid loss-0.4574, acc-0.8742, test loss-0.4612, acc-0.8712\n",
      "Iter-50460, train loss-0.4975, acc-0.8400, valid loss-0.4574, acc-0.8742, test loss-0.4612, acc-0.8713\n",
      "Iter-50470, train loss-0.4058, acc-0.8600, valid loss-0.4573, acc-0.8742, test loss-0.4611, acc-0.8713\n",
      "Iter-50480, train loss-0.6552, acc-0.8000, valid loss-0.4573, acc-0.8742, test loss-0.4611, acc-0.8713\n",
      "Iter-50490, train loss-0.4914, acc-0.9000, valid loss-0.4572, acc-0.8742, test loss-0.4610, acc-0.8712\n",
      "Iter-50500, train loss-0.7507, acc-0.7600, valid loss-0.4572, acc-0.8744, test loss-0.4610, acc-0.8712\n",
      "Iter-50510, train loss-0.4095, acc-0.9200, valid loss-0.4572, acc-0.8742, test loss-0.4609, acc-0.8713\n",
      "Iter-50520, train loss-0.6681, acc-0.7800, valid loss-0.4571, acc-0.8742, test loss-0.4609, acc-0.8713\n",
      "Iter-50530, train loss-0.6648, acc-0.8000, valid loss-0.4571, acc-0.8740, test loss-0.4609, acc-0.8713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-50540, train loss-0.3272, acc-0.9000, valid loss-0.4570, acc-0.8742, test loss-0.4608, acc-0.8711\n",
      "Iter-50550, train loss-0.3573, acc-0.8800, valid loss-0.4570, acc-0.8742, test loss-0.4608, acc-0.8711\n",
      "Iter-50560, train loss-0.5420, acc-0.9000, valid loss-0.4570, acc-0.8744, test loss-0.4607, acc-0.8712\n",
      "Iter-50570, train loss-0.7246, acc-0.8200, valid loss-0.4569, acc-0.8744, test loss-0.4607, acc-0.8712\n",
      "Iter-50580, train loss-0.3367, acc-0.9200, valid loss-0.4569, acc-0.8744, test loss-0.4606, acc-0.8712\n",
      "Iter-50590, train loss-0.6455, acc-0.8200, valid loss-0.4568, acc-0.8742, test loss-0.4606, acc-0.8713\n",
      "Iter-50600, train loss-0.4654, acc-0.8400, valid loss-0.4568, acc-0.8742, test loss-0.4606, acc-0.8714\n",
      "Iter-50610, train loss-0.4922, acc-0.8600, valid loss-0.4567, acc-0.8742, test loss-0.4605, acc-0.8712\n",
      "Iter-50620, train loss-0.6634, acc-0.8000, valid loss-0.4567, acc-0.8744, test loss-0.4605, acc-0.8712\n",
      "Iter-50630, train loss-0.4989, acc-0.7800, valid loss-0.4566, acc-0.8742, test loss-0.4604, acc-0.8712\n",
      "Iter-50640, train loss-0.4672, acc-0.8800, valid loss-0.4566, acc-0.8746, test loss-0.4604, acc-0.8711\n",
      "Iter-50650, train loss-0.2694, acc-0.9600, valid loss-0.4566, acc-0.8746, test loss-0.4603, acc-0.8712\n",
      "Iter-50660, train loss-0.3049, acc-0.9200, valid loss-0.4565, acc-0.8748, test loss-0.4603, acc-0.8711\n",
      "Iter-50670, train loss-0.5876, acc-0.7800, valid loss-0.4565, acc-0.8746, test loss-0.4603, acc-0.8712\n",
      "Iter-50680, train loss-0.4847, acc-0.9000, valid loss-0.4564, acc-0.8746, test loss-0.4602, acc-0.8712\n",
      "Iter-50690, train loss-0.7449, acc-0.7600, valid loss-0.4564, acc-0.8746, test loss-0.4602, acc-0.8712\n",
      "Iter-50700, train loss-0.4738, acc-0.9000, valid loss-0.4563, acc-0.8746, test loss-0.4601, acc-0.8712\n",
      "Iter-50710, train loss-0.3848, acc-0.9000, valid loss-0.4563, acc-0.8746, test loss-0.4601, acc-0.8712\n",
      "Iter-50720, train loss-0.7263, acc-0.7800, valid loss-0.4563, acc-0.8744, test loss-0.4600, acc-0.8712\n",
      "Iter-50730, train loss-0.4372, acc-0.9000, valid loss-0.4562, acc-0.8744, test loss-0.4600, acc-0.8712\n",
      "Iter-50740, train loss-0.7191, acc-0.7800, valid loss-0.4562, acc-0.8744, test loss-0.4600, acc-0.8714\n",
      "Iter-50750, train loss-0.9142, acc-0.7800, valid loss-0.4561, acc-0.8748, test loss-0.4599, acc-0.8714\n",
      "Iter-50760, train loss-0.4477, acc-0.9200, valid loss-0.4561, acc-0.8746, test loss-0.4599, acc-0.8713\n",
      "Iter-50770, train loss-0.4752, acc-0.8200, valid loss-0.4560, acc-0.8748, test loss-0.4598, acc-0.8713\n",
      "Iter-50780, train loss-0.3937, acc-0.9000, valid loss-0.4560, acc-0.8748, test loss-0.4598, acc-0.8714\n",
      "Iter-50790, train loss-0.5223, acc-0.8200, valid loss-0.4560, acc-0.8748, test loss-0.4598, acc-0.8715\n",
      "Iter-50800, train loss-0.4149, acc-0.8800, valid loss-0.4559, acc-0.8748, test loss-0.4597, acc-0.8716\n",
      "Iter-50810, train loss-0.4333, acc-0.9000, valid loss-0.4559, acc-0.8746, test loss-0.4597, acc-0.8716\n",
      "Iter-50820, train loss-0.4138, acc-0.9200, valid loss-0.4559, acc-0.8746, test loss-0.4596, acc-0.8716\n",
      "Iter-50830, train loss-0.3978, acc-0.8800, valid loss-0.4558, acc-0.8748, test loss-0.4596, acc-0.8714\n",
      "Iter-50840, train loss-0.4946, acc-0.9000, valid loss-0.4558, acc-0.8748, test loss-0.4595, acc-0.8714\n",
      "Iter-50850, train loss-0.5399, acc-0.7800, valid loss-0.4558, acc-0.8748, test loss-0.4595, acc-0.8714\n",
      "Iter-50860, train loss-0.5461, acc-0.8000, valid loss-0.4557, acc-0.8750, test loss-0.4594, acc-0.8713\n",
      "Iter-50870, train loss-0.6718, acc-0.8000, valid loss-0.4557, acc-0.8750, test loss-0.4594, acc-0.8714\n",
      "Iter-50880, train loss-0.3831, acc-0.9200, valid loss-0.4556, acc-0.8750, test loss-0.4594, acc-0.8714\n",
      "Iter-50890, train loss-0.4395, acc-0.8800, valid loss-0.4556, acc-0.8750, test loss-0.4593, acc-0.8714\n",
      "Iter-50900, train loss-0.4172, acc-0.8800, valid loss-0.4555, acc-0.8750, test loss-0.4593, acc-0.8713\n",
      "Iter-50910, train loss-0.5760, acc-0.8400, valid loss-0.4555, acc-0.8750, test loss-0.4592, acc-0.8713\n",
      "Iter-50920, train loss-0.5601, acc-0.8200, valid loss-0.4554, acc-0.8750, test loss-0.4592, acc-0.8713\n",
      "Iter-50930, train loss-0.4416, acc-0.9200, valid loss-0.4554, acc-0.8752, test loss-0.4591, acc-0.8713\n",
      "Iter-50940, train loss-0.2760, acc-0.9400, valid loss-0.4554, acc-0.8752, test loss-0.4591, acc-0.8714\n",
      "Iter-50950, train loss-0.4841, acc-0.8400, valid loss-0.4553, acc-0.8752, test loss-0.4590, acc-0.8715\n",
      "Iter-50960, train loss-0.3173, acc-0.9600, valid loss-0.4553, acc-0.8752, test loss-0.4590, acc-0.8715\n",
      "Iter-50970, train loss-0.6188, acc-0.8000, valid loss-0.4552, acc-0.8752, test loss-0.4590, acc-0.8714\n",
      "Iter-50980, train loss-0.2531, acc-0.9200, valid loss-0.4552, acc-0.8752, test loss-0.4589, acc-0.8714\n",
      "Iter-50990, train loss-0.3172, acc-0.9400, valid loss-0.4552, acc-0.8752, test loss-0.4589, acc-0.8714\n",
      "Iter-51000, train loss-0.4665, acc-0.8600, valid loss-0.4551, acc-0.8754, test loss-0.4588, acc-0.8715\n",
      "Iter-51010, train loss-0.4007, acc-0.9000, valid loss-0.4551, acc-0.8754, test loss-0.4588, acc-0.8714\n",
      "Iter-51020, train loss-0.3948, acc-0.9000, valid loss-0.4550, acc-0.8754, test loss-0.4587, acc-0.8716\n",
      "Iter-51030, train loss-0.4367, acc-0.8400, valid loss-0.4550, acc-0.8754, test loss-0.4587, acc-0.8717\n",
      "Iter-51040, train loss-0.5146, acc-0.8400, valid loss-0.4550, acc-0.8752, test loss-0.4586, acc-0.8716\n",
      "Iter-51050, train loss-0.5285, acc-0.8200, valid loss-0.4549, acc-0.8752, test loss-0.4586, acc-0.8717\n",
      "Iter-51060, train loss-0.3697, acc-0.9200, valid loss-0.4549, acc-0.8752, test loss-0.4586, acc-0.8717\n",
      "Iter-51070, train loss-0.4784, acc-0.8600, valid loss-0.4548, acc-0.8752, test loss-0.4585, acc-0.8718\n",
      "Iter-51080, train loss-0.4734, acc-0.8600, valid loss-0.4548, acc-0.8752, test loss-0.4585, acc-0.8717\n",
      "Iter-51090, train loss-0.5617, acc-0.8600, valid loss-0.4548, acc-0.8752, test loss-0.4584, acc-0.8717\n",
      "Iter-51100, train loss-0.4491, acc-0.9000, valid loss-0.4547, acc-0.8752, test loss-0.4584, acc-0.8717\n",
      "Iter-51110, train loss-0.5474, acc-0.8600, valid loss-0.4547, acc-0.8752, test loss-0.4583, acc-0.8717\n",
      "Iter-51120, train loss-0.4725, acc-0.9200, valid loss-0.4546, acc-0.8752, test loss-0.4583, acc-0.8717\n",
      "Iter-51130, train loss-0.3976, acc-0.9000, valid loss-0.4546, acc-0.8752, test loss-0.4583, acc-0.8718\n",
      "Iter-51140, train loss-0.3391, acc-0.9400, valid loss-0.4545, acc-0.8752, test loss-0.4582, acc-0.8717\n",
      "Iter-51150, train loss-0.4713, acc-0.8800, valid loss-0.4545, acc-0.8752, test loss-0.4582, acc-0.8718\n",
      "Iter-51160, train loss-0.4715, acc-0.8600, valid loss-0.4544, acc-0.8752, test loss-0.4581, acc-0.8718\n",
      "Iter-51170, train loss-0.5593, acc-0.8600, valid loss-0.4544, acc-0.8752, test loss-0.4581, acc-0.8718\n",
      "Iter-51180, train loss-0.6713, acc-0.8200, valid loss-0.4544, acc-0.8752, test loss-0.4580, acc-0.8719\n",
      "Iter-51190, train loss-0.6826, acc-0.8400, valid loss-0.4543, acc-0.8752, test loss-0.4580, acc-0.8719\n",
      "Iter-51200, train loss-0.4083, acc-0.9200, valid loss-0.4543, acc-0.8752, test loss-0.4580, acc-0.8719\n",
      "Iter-51210, train loss-0.5245, acc-0.8200, valid loss-0.4542, acc-0.8752, test loss-0.4579, acc-0.8720\n",
      "Iter-51220, train loss-0.4433, acc-0.8400, valid loss-0.4542, acc-0.8754, test loss-0.4579, acc-0.8720\n",
      "Iter-51230, train loss-0.3588, acc-0.8800, valid loss-0.4541, acc-0.8754, test loss-0.4578, acc-0.8721\n",
      "Iter-51240, train loss-0.4746, acc-0.8600, valid loss-0.4541, acc-0.8754, test loss-0.4578, acc-0.8720\n",
      "Iter-51250, train loss-0.4588, acc-0.8800, valid loss-0.4541, acc-0.8752, test loss-0.4578, acc-0.8720\n",
      "Iter-51260, train loss-0.5546, acc-0.8200, valid loss-0.4540, acc-0.8752, test loss-0.4577, acc-0.8720\n",
      "Iter-51270, train loss-0.4230, acc-0.9000, valid loss-0.4540, acc-0.8752, test loss-0.4577, acc-0.8720\n",
      "Iter-51280, train loss-0.4575, acc-0.8400, valid loss-0.4540, acc-0.8752, test loss-0.4576, acc-0.8720\n",
      "Iter-51290, train loss-0.2756, acc-0.9400, valid loss-0.4539, acc-0.8752, test loss-0.4576, acc-0.8720\n",
      "Iter-51300, train loss-0.5643, acc-0.8000, valid loss-0.4539, acc-0.8754, test loss-0.4576, acc-0.8720\n",
      "Iter-51310, train loss-0.6119, acc-0.8000, valid loss-0.4538, acc-0.8754, test loss-0.4575, acc-0.8720\n",
      "Iter-51320, train loss-0.4000, acc-0.9000, valid loss-0.4538, acc-0.8754, test loss-0.4575, acc-0.8722\n",
      "Iter-51330, train loss-0.6561, acc-0.8000, valid loss-0.4537, acc-0.8754, test loss-0.4574, acc-0.8722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-51340, train loss-0.4561, acc-0.8600, valid loss-0.4537, acc-0.8754, test loss-0.4574, acc-0.8723\n",
      "Iter-51350, train loss-0.3748, acc-0.8600, valid loss-0.4537, acc-0.8754, test loss-0.4573, acc-0.8721\n",
      "Iter-51360, train loss-0.3891, acc-0.9000, valid loss-0.4536, acc-0.8754, test loss-0.4573, acc-0.8721\n",
      "Iter-51370, train loss-0.4078, acc-0.9200, valid loss-0.4536, acc-0.8754, test loss-0.4573, acc-0.8721\n",
      "Iter-51380, train loss-0.6438, acc-0.8600, valid loss-0.4535, acc-0.8754, test loss-0.4572, acc-0.8721\n",
      "Iter-51390, train loss-0.5439, acc-0.8200, valid loss-0.4535, acc-0.8754, test loss-0.4572, acc-0.8723\n",
      "Iter-51400, train loss-0.4925, acc-0.8800, valid loss-0.4535, acc-0.8754, test loss-0.4571, acc-0.8722\n",
      "Iter-51410, train loss-0.4097, acc-0.9000, valid loss-0.4534, acc-0.8754, test loss-0.4571, acc-0.8721\n",
      "Iter-51420, train loss-0.3459, acc-0.9200, valid loss-0.4534, acc-0.8754, test loss-0.4570, acc-0.8719\n",
      "Iter-51430, train loss-0.4557, acc-0.8600, valid loss-0.4533, acc-0.8754, test loss-0.4570, acc-0.8720\n",
      "Iter-51440, train loss-0.5113, acc-0.8200, valid loss-0.4533, acc-0.8754, test loss-0.4570, acc-0.8722\n",
      "Iter-51450, train loss-0.5205, acc-0.8400, valid loss-0.4533, acc-0.8754, test loss-0.4569, acc-0.8721\n",
      "Iter-51460, train loss-0.5452, acc-0.8400, valid loss-0.4532, acc-0.8754, test loss-0.4569, acc-0.8722\n",
      "Iter-51470, train loss-0.7780, acc-0.7600, valid loss-0.4532, acc-0.8754, test loss-0.4569, acc-0.8724\n",
      "Iter-51480, train loss-0.5162, acc-0.8800, valid loss-0.4531, acc-0.8754, test loss-0.4568, acc-0.8725\n",
      "Iter-51490, train loss-0.3683, acc-0.9400, valid loss-0.4531, acc-0.8752, test loss-0.4568, acc-0.8725\n",
      "Iter-51500, train loss-0.4081, acc-0.9000, valid loss-0.4531, acc-0.8754, test loss-0.4567, acc-0.8725\n",
      "Iter-51510, train loss-0.5267, acc-0.8400, valid loss-0.4530, acc-0.8754, test loss-0.4567, acc-0.8725\n",
      "Iter-51520, train loss-0.3712, acc-0.9000, valid loss-0.4530, acc-0.8752, test loss-0.4567, acc-0.8725\n",
      "Iter-51530, train loss-0.5415, acc-0.8000, valid loss-0.4529, acc-0.8754, test loss-0.4566, acc-0.8725\n",
      "Iter-51540, train loss-0.3626, acc-0.9200, valid loss-0.4529, acc-0.8754, test loss-0.4566, acc-0.8724\n",
      "Iter-51550, train loss-0.5306, acc-0.8600, valid loss-0.4529, acc-0.8754, test loss-0.4565, acc-0.8724\n",
      "Iter-51560, train loss-0.3809, acc-0.9200, valid loss-0.4528, acc-0.8754, test loss-0.4565, acc-0.8725\n",
      "Iter-51570, train loss-0.4724, acc-0.8600, valid loss-0.4527, acc-0.8754, test loss-0.4565, acc-0.8724\n",
      "Iter-51580, train loss-0.3656, acc-0.8800, valid loss-0.4527, acc-0.8754, test loss-0.4564, acc-0.8725\n",
      "Iter-51590, train loss-0.5554, acc-0.8600, valid loss-0.4527, acc-0.8754, test loss-0.4564, acc-0.8725\n",
      "Iter-51600, train loss-0.4468, acc-0.8600, valid loss-0.4526, acc-0.8754, test loss-0.4563, acc-0.8725\n",
      "Iter-51610, train loss-0.6057, acc-0.8200, valid loss-0.4526, acc-0.8754, test loss-0.4563, acc-0.8725\n",
      "Iter-51620, train loss-0.4666, acc-0.8600, valid loss-0.4525, acc-0.8754, test loss-0.4563, acc-0.8726\n",
      "Iter-51630, train loss-0.3661, acc-0.9000, valid loss-0.4525, acc-0.8754, test loss-0.4562, acc-0.8724\n",
      "Iter-51640, train loss-0.5097, acc-0.8600, valid loss-0.4525, acc-0.8754, test loss-0.4562, acc-0.8725\n",
      "Iter-51650, train loss-0.7068, acc-0.7600, valid loss-0.4524, acc-0.8754, test loss-0.4562, acc-0.8725\n",
      "Iter-51660, train loss-0.2881, acc-0.9400, valid loss-0.4524, acc-0.8754, test loss-0.4561, acc-0.8725\n",
      "Iter-51670, train loss-0.5781, acc-0.8000, valid loss-0.4523, acc-0.8754, test loss-0.4561, acc-0.8725\n",
      "Iter-51680, train loss-0.5068, acc-0.8400, valid loss-0.4523, acc-0.8754, test loss-0.4560, acc-0.8725\n",
      "Iter-51690, train loss-0.6723, acc-0.8000, valid loss-0.4522, acc-0.8754, test loss-0.4560, acc-0.8725\n",
      "Iter-51700, train loss-0.5332, acc-0.8400, valid loss-0.4522, acc-0.8754, test loss-0.4559, acc-0.8725\n",
      "Iter-51710, train loss-0.4916, acc-0.8600, valid loss-0.4521, acc-0.8754, test loss-0.4559, acc-0.8725\n",
      "Iter-51720, train loss-0.5739, acc-0.8400, valid loss-0.4521, acc-0.8754, test loss-0.4558, acc-0.8725\n",
      "Iter-51730, train loss-0.4309, acc-0.8400, valid loss-0.4521, acc-0.8754, test loss-0.4558, acc-0.8725\n",
      "Iter-51740, train loss-0.4414, acc-0.9000, valid loss-0.4520, acc-0.8754, test loss-0.4557, acc-0.8725\n",
      "Iter-51750, train loss-0.4649, acc-0.8800, valid loss-0.4520, acc-0.8754, test loss-0.4557, acc-0.8726\n",
      "Iter-51760, train loss-0.6312, acc-0.8200, valid loss-0.4519, acc-0.8754, test loss-0.4557, acc-0.8727\n",
      "Iter-51770, train loss-0.5290, acc-0.8600, valid loss-0.4519, acc-0.8754, test loss-0.4556, acc-0.8727\n",
      "Iter-51780, train loss-0.4249, acc-0.8400, valid loss-0.4518, acc-0.8754, test loss-0.4556, acc-0.8727\n",
      "Iter-51790, train loss-0.5528, acc-0.8200, valid loss-0.4518, acc-0.8754, test loss-0.4555, acc-0.8727\n",
      "Iter-51800, train loss-0.4129, acc-0.9000, valid loss-0.4518, acc-0.8754, test loss-0.4555, acc-0.8727\n",
      "Iter-51810, train loss-0.3656, acc-0.8800, valid loss-0.4517, acc-0.8754, test loss-0.4555, acc-0.8727\n",
      "Iter-51820, train loss-0.4044, acc-0.9000, valid loss-0.4517, acc-0.8754, test loss-0.4554, acc-0.8727\n",
      "Iter-51830, train loss-0.6493, acc-0.8400, valid loss-0.4516, acc-0.8754, test loss-0.4554, acc-0.8725\n",
      "Iter-51840, train loss-0.4634, acc-0.8200, valid loss-0.4516, acc-0.8754, test loss-0.4553, acc-0.8726\n",
      "Iter-51850, train loss-0.6689, acc-0.7800, valid loss-0.4516, acc-0.8754, test loss-0.4553, acc-0.8728\n",
      "Iter-51860, train loss-0.3180, acc-0.9000, valid loss-0.4515, acc-0.8754, test loss-0.4553, acc-0.8725\n",
      "Iter-51870, train loss-0.4703, acc-0.8600, valid loss-0.4515, acc-0.8754, test loss-0.4552, acc-0.8725\n",
      "Iter-51880, train loss-0.5239, acc-0.8200, valid loss-0.4514, acc-0.8756, test loss-0.4552, acc-0.8726\n",
      "Iter-51890, train loss-0.3961, acc-0.9000, valid loss-0.4514, acc-0.8754, test loss-0.4551, acc-0.8726\n",
      "Iter-51900, train loss-0.3936, acc-0.9000, valid loss-0.4514, acc-0.8754, test loss-0.4551, acc-0.8726\n",
      "Iter-51910, train loss-0.4397, acc-0.9000, valid loss-0.4513, acc-0.8754, test loss-0.4550, acc-0.8726\n",
      "Iter-51920, train loss-0.6738, acc-0.8200, valid loss-0.4513, acc-0.8756, test loss-0.4550, acc-0.8726\n",
      "Iter-51930, train loss-0.4604, acc-0.8800, valid loss-0.4513, acc-0.8754, test loss-0.4549, acc-0.8727\n",
      "Iter-51940, train loss-0.6899, acc-0.8200, valid loss-0.4512, acc-0.8754, test loss-0.4549, acc-0.8726\n",
      "Iter-51950, train loss-0.3778, acc-0.9000, valid loss-0.4512, acc-0.8754, test loss-0.4549, acc-0.8726\n",
      "Iter-51960, train loss-0.5409, acc-0.8600, valid loss-0.4511, acc-0.8752, test loss-0.4548, acc-0.8727\n",
      "Iter-51970, train loss-0.5466, acc-0.8600, valid loss-0.4511, acc-0.8754, test loss-0.4548, acc-0.8727\n",
      "Iter-51980, train loss-0.5081, acc-0.8600, valid loss-0.4511, acc-0.8752, test loss-0.4548, acc-0.8727\n",
      "Iter-51990, train loss-0.4967, acc-0.8800, valid loss-0.4510, acc-0.8752, test loss-0.4547, acc-0.8727\n",
      "Iter-52000, train loss-0.8598, acc-0.8200, valid loss-0.4510, acc-0.8752, test loss-0.4547, acc-0.8727\n",
      "Iter-52010, train loss-0.4471, acc-0.8800, valid loss-0.4509, acc-0.8752, test loss-0.4546, acc-0.8727\n",
      "Iter-52020, train loss-0.5132, acc-0.8400, valid loss-0.4509, acc-0.8756, test loss-0.4546, acc-0.8727\n",
      "Iter-52030, train loss-0.7235, acc-0.7600, valid loss-0.4509, acc-0.8756, test loss-0.4546, acc-0.8727\n",
      "Iter-52040, train loss-0.5175, acc-0.8000, valid loss-0.4508, acc-0.8758, test loss-0.4545, acc-0.8727\n",
      "Iter-52050, train loss-0.5733, acc-0.8200, valid loss-0.4508, acc-0.8758, test loss-0.4545, acc-0.8726\n",
      "Iter-52060, train loss-0.4838, acc-0.8400, valid loss-0.4507, acc-0.8758, test loss-0.4544, acc-0.8726\n",
      "Iter-52070, train loss-0.6006, acc-0.7600, valid loss-0.4507, acc-0.8758, test loss-0.4544, acc-0.8727\n",
      "Iter-52080, train loss-0.4403, acc-0.9000, valid loss-0.4507, acc-0.8758, test loss-0.4543, acc-0.8727\n",
      "Iter-52090, train loss-0.5903, acc-0.8000, valid loss-0.4506, acc-0.8758, test loss-0.4543, acc-0.8726\n",
      "Iter-52100, train loss-0.5481, acc-0.7800, valid loss-0.4506, acc-0.8756, test loss-0.4543, acc-0.8726\n",
      "Iter-52110, train loss-0.2910, acc-0.9400, valid loss-0.4505, acc-0.8756, test loss-0.4542, acc-0.8726\n",
      "Iter-52120, train loss-0.2807, acc-0.9400, valid loss-0.4505, acc-0.8756, test loss-0.4542, acc-0.8726\n",
      "Iter-52130, train loss-0.3628, acc-0.9200, valid loss-0.4505, acc-0.8756, test loss-0.4542, acc-0.8727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-52140, train loss-0.4432, acc-0.8600, valid loss-0.4504, acc-0.8756, test loss-0.4541, acc-0.8726\n",
      "Iter-52150, train loss-0.4496, acc-0.8200, valid loss-0.4504, acc-0.8754, test loss-0.4541, acc-0.8728\n",
      "Iter-52160, train loss-0.6539, acc-0.7600, valid loss-0.4503, acc-0.8754, test loss-0.4540, acc-0.8728\n",
      "Iter-52170, train loss-0.4780, acc-0.8400, valid loss-0.4503, acc-0.8754, test loss-0.4540, acc-0.8729\n",
      "Iter-52180, train loss-0.3807, acc-0.9000, valid loss-0.4502, acc-0.8754, test loss-0.4540, acc-0.8729\n",
      "Iter-52190, train loss-0.4305, acc-0.8800, valid loss-0.4502, acc-0.8754, test loss-0.4539, acc-0.8729\n",
      "Iter-52200, train loss-0.5171, acc-0.8600, valid loss-0.4502, acc-0.8754, test loss-0.4539, acc-0.8729\n",
      "Iter-52210, train loss-0.5845, acc-0.8000, valid loss-0.4501, acc-0.8754, test loss-0.4538, acc-0.8727\n",
      "Iter-52220, train loss-0.5421, acc-0.8400, valid loss-0.4501, acc-0.8752, test loss-0.4538, acc-0.8727\n",
      "Iter-52230, train loss-0.5515, acc-0.8400, valid loss-0.4500, acc-0.8754, test loss-0.4538, acc-0.8727\n",
      "Iter-52240, train loss-0.5704, acc-0.8200, valid loss-0.4500, acc-0.8752, test loss-0.4537, acc-0.8728\n",
      "Iter-52250, train loss-0.3911, acc-0.8600, valid loss-0.4500, acc-0.8752, test loss-0.4537, acc-0.8728\n",
      "Iter-52260, train loss-0.4152, acc-0.9200, valid loss-0.4499, acc-0.8752, test loss-0.4536, acc-0.8728\n",
      "Iter-52270, train loss-0.2712, acc-0.9600, valid loss-0.4499, acc-0.8752, test loss-0.4536, acc-0.8728\n",
      "Iter-52280, train loss-0.3999, acc-0.9000, valid loss-0.4499, acc-0.8752, test loss-0.4536, acc-0.8728\n",
      "Iter-52290, train loss-0.5385, acc-0.8600, valid loss-0.4498, acc-0.8752, test loss-0.4535, acc-0.8729\n",
      "Iter-52300, train loss-0.5524, acc-0.8200, valid loss-0.4498, acc-0.8752, test loss-0.4535, acc-0.8729\n",
      "Iter-52310, train loss-0.4932, acc-0.8400, valid loss-0.4497, acc-0.8752, test loss-0.4535, acc-0.8730\n",
      "Iter-52320, train loss-0.7015, acc-0.8000, valid loss-0.4497, acc-0.8752, test loss-0.4534, acc-0.8729\n",
      "Iter-52330, train loss-0.5615, acc-0.8400, valid loss-0.4497, acc-0.8754, test loss-0.4534, acc-0.8730\n",
      "Iter-52340, train loss-0.3480, acc-0.9200, valid loss-0.4496, acc-0.8752, test loss-0.4533, acc-0.8731\n",
      "Iter-52350, train loss-0.5785, acc-0.8400, valid loss-0.4496, acc-0.8754, test loss-0.4533, acc-0.8730\n",
      "Iter-52360, train loss-0.3644, acc-0.8600, valid loss-0.4495, acc-0.8754, test loss-0.4533, acc-0.8731\n",
      "Iter-52370, train loss-0.3373, acc-0.9400, valid loss-0.4495, acc-0.8754, test loss-0.4532, acc-0.8731\n",
      "Iter-52380, train loss-0.4210, acc-0.8600, valid loss-0.4494, acc-0.8754, test loss-0.4532, acc-0.8732\n",
      "Iter-52390, train loss-0.4372, acc-0.9000, valid loss-0.4494, acc-0.8754, test loss-0.4531, acc-0.8732\n",
      "Iter-52400, train loss-0.3675, acc-0.8800, valid loss-0.4494, acc-0.8754, test loss-0.4531, acc-0.8731\n",
      "Iter-52410, train loss-0.5340, acc-0.8400, valid loss-0.4493, acc-0.8758, test loss-0.4531, acc-0.8731\n",
      "Iter-52420, train loss-0.4824, acc-0.8000, valid loss-0.4493, acc-0.8756, test loss-0.4530, acc-0.8731\n",
      "Iter-52430, train loss-0.8925, acc-0.7400, valid loss-0.4492, acc-0.8758, test loss-0.4530, acc-0.8731\n",
      "Iter-52440, train loss-0.5676, acc-0.8200, valid loss-0.4492, acc-0.8756, test loss-0.4530, acc-0.8732\n",
      "Iter-52450, train loss-0.5850, acc-0.8400, valid loss-0.4492, acc-0.8756, test loss-0.4529, acc-0.8732\n",
      "Iter-52460, train loss-0.5589, acc-0.8400, valid loss-0.4491, acc-0.8756, test loss-0.4529, acc-0.8732\n",
      "Iter-52470, train loss-0.6975, acc-0.8400, valid loss-0.4491, acc-0.8756, test loss-0.4528, acc-0.8733\n",
      "Iter-52480, train loss-0.4852, acc-0.8200, valid loss-0.4490, acc-0.8756, test loss-0.4528, acc-0.8733\n",
      "Iter-52490, train loss-0.4283, acc-0.8600, valid loss-0.4490, acc-0.8756, test loss-0.4527, acc-0.8733\n",
      "Iter-52500, train loss-0.4158, acc-0.8600, valid loss-0.4490, acc-0.8756, test loss-0.4527, acc-0.8732\n",
      "Iter-52510, train loss-0.4162, acc-0.9200, valid loss-0.4489, acc-0.8756, test loss-0.4527, acc-0.8733\n",
      "Iter-52520, train loss-0.5924, acc-0.8000, valid loss-0.4489, acc-0.8756, test loss-0.4526, acc-0.8733\n",
      "Iter-52530, train loss-0.4595, acc-0.8600, valid loss-0.4488, acc-0.8758, test loss-0.4526, acc-0.8734\n",
      "Iter-52540, train loss-0.4719, acc-0.8800, valid loss-0.4488, acc-0.8756, test loss-0.4525, acc-0.8733\n",
      "Iter-52550, train loss-0.5544, acc-0.8200, valid loss-0.4487, acc-0.8756, test loss-0.4525, acc-0.8733\n",
      "Iter-52560, train loss-0.4928, acc-0.8800, valid loss-0.4487, acc-0.8756, test loss-0.4524, acc-0.8732\n",
      "Iter-52570, train loss-0.3343, acc-0.9400, valid loss-0.4487, acc-0.8756, test loss-0.4524, acc-0.8733\n",
      "Iter-52580, train loss-0.5214, acc-0.8600, valid loss-0.4486, acc-0.8756, test loss-0.4524, acc-0.8734\n",
      "Iter-52590, train loss-0.3063, acc-0.9400, valid loss-0.4486, acc-0.8756, test loss-0.4523, acc-0.8733\n",
      "Iter-52600, train loss-0.5474, acc-0.9000, valid loss-0.4486, acc-0.8756, test loss-0.4523, acc-0.8733\n",
      "Iter-52610, train loss-0.7415, acc-0.8000, valid loss-0.4485, acc-0.8756, test loss-0.4522, acc-0.8733\n",
      "Iter-52620, train loss-0.6107, acc-0.8400, valid loss-0.4485, acc-0.8756, test loss-0.4522, acc-0.8734\n",
      "Iter-52630, train loss-0.3734, acc-0.9000, valid loss-0.4484, acc-0.8756, test loss-0.4522, acc-0.8734\n",
      "Iter-52640, train loss-0.5760, acc-0.8400, valid loss-0.4484, acc-0.8756, test loss-0.4521, acc-0.8734\n",
      "Iter-52650, train loss-0.4285, acc-0.8800, valid loss-0.4484, acc-0.8756, test loss-0.4521, acc-0.8735\n",
      "Iter-52660, train loss-0.4486, acc-0.8800, valid loss-0.4483, acc-0.8756, test loss-0.4521, acc-0.8734\n",
      "Iter-52670, train loss-0.5655, acc-0.7400, valid loss-0.4483, acc-0.8756, test loss-0.4520, acc-0.8735\n",
      "Iter-52680, train loss-0.4880, acc-0.8200, valid loss-0.4482, acc-0.8756, test loss-0.4520, acc-0.8734\n",
      "Iter-52690, train loss-0.4898, acc-0.8600, valid loss-0.4482, acc-0.8756, test loss-0.4520, acc-0.8734\n",
      "Iter-52700, train loss-0.5435, acc-0.8000, valid loss-0.4482, acc-0.8756, test loss-0.4519, acc-0.8733\n",
      "Iter-52710, train loss-0.5437, acc-0.8600, valid loss-0.4481, acc-0.8756, test loss-0.4519, acc-0.8734\n",
      "Iter-52720, train loss-0.5360, acc-0.8400, valid loss-0.4481, acc-0.8756, test loss-0.4518, acc-0.8735\n",
      "Iter-52730, train loss-0.2591, acc-0.9400, valid loss-0.4481, acc-0.8756, test loss-0.4518, acc-0.8735\n",
      "Iter-52740, train loss-0.3184, acc-0.9400, valid loss-0.4480, acc-0.8754, test loss-0.4518, acc-0.8735\n",
      "Iter-52750, train loss-0.4751, acc-0.8400, valid loss-0.4480, acc-0.8756, test loss-0.4517, acc-0.8735\n",
      "Iter-52760, train loss-0.2976, acc-0.9200, valid loss-0.4480, acc-0.8756, test loss-0.4517, acc-0.8735\n",
      "Iter-52770, train loss-0.5444, acc-0.8200, valid loss-0.4479, acc-0.8756, test loss-0.4517, acc-0.8736\n",
      "Iter-52780, train loss-0.4485, acc-0.8400, valid loss-0.4479, acc-0.8756, test loss-0.4516, acc-0.8736\n",
      "Iter-52790, train loss-0.3825, acc-0.9000, valid loss-0.4478, acc-0.8756, test loss-0.4516, acc-0.8737\n",
      "Iter-52800, train loss-0.6504, acc-0.8600, valid loss-0.4478, acc-0.8756, test loss-0.4516, acc-0.8737\n",
      "Iter-52810, train loss-0.3103, acc-0.9200, valid loss-0.4477, acc-0.8756, test loss-0.4515, acc-0.8736\n",
      "Iter-52820, train loss-0.5322, acc-0.8400, valid loss-0.4477, acc-0.8756, test loss-0.4515, acc-0.8737\n",
      "Iter-52830, train loss-0.6497, acc-0.7600, valid loss-0.4477, acc-0.8756, test loss-0.4514, acc-0.8737\n",
      "Iter-52840, train loss-0.3460, acc-0.9400, valid loss-0.4476, acc-0.8756, test loss-0.4514, acc-0.8736\n",
      "Iter-52850, train loss-0.4417, acc-0.8400, valid loss-0.4476, acc-0.8756, test loss-0.4514, acc-0.8736\n",
      "Iter-52860, train loss-0.2479, acc-0.9600, valid loss-0.4475, acc-0.8756, test loss-0.4513, acc-0.8736\n",
      "Iter-52870, train loss-0.3991, acc-0.9000, valid loss-0.4475, acc-0.8756, test loss-0.4513, acc-0.8737\n",
      "Iter-52880, train loss-0.4380, acc-0.8800, valid loss-0.4474, acc-0.8756, test loss-0.4512, acc-0.8737\n",
      "Iter-52890, train loss-0.6772, acc-0.7800, valid loss-0.4474, acc-0.8756, test loss-0.4512, acc-0.8736\n",
      "Iter-52900, train loss-0.5185, acc-0.8800, valid loss-0.4474, acc-0.8756, test loss-0.4512, acc-0.8738\n",
      "Iter-52910, train loss-0.6762, acc-0.8600, valid loss-0.4473, acc-0.8756, test loss-0.4511, acc-0.8738\n",
      "Iter-52920, train loss-0.2697, acc-0.9600, valid loss-0.4473, acc-0.8758, test loss-0.4511, acc-0.8738\n",
      "Iter-52930, train loss-0.5272, acc-0.8600, valid loss-0.4472, acc-0.8758, test loss-0.4510, acc-0.8739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-52940, train loss-0.3244, acc-0.9400, valid loss-0.4472, acc-0.8758, test loss-0.4510, acc-0.8739\n",
      "Iter-52950, train loss-0.4108, acc-0.8800, valid loss-0.4471, acc-0.8760, test loss-0.4509, acc-0.8741\n",
      "Iter-52960, train loss-0.3824, acc-0.9000, valid loss-0.4471, acc-0.8762, test loss-0.4509, acc-0.8741\n",
      "Iter-52970, train loss-0.5556, acc-0.8000, valid loss-0.4471, acc-0.8760, test loss-0.4509, acc-0.8740\n",
      "Iter-52980, train loss-0.6197, acc-0.8200, valid loss-0.4470, acc-0.8760, test loss-0.4508, acc-0.8740\n",
      "Iter-52990, train loss-0.6157, acc-0.8600, valid loss-0.4470, acc-0.8762, test loss-0.4508, acc-0.8739\n",
      "Iter-53000, train loss-0.5960, acc-0.8200, valid loss-0.4470, acc-0.8760, test loss-0.4508, acc-0.8740\n",
      "Iter-53010, train loss-0.5596, acc-0.8000, valid loss-0.4469, acc-0.8764, test loss-0.4507, acc-0.8739\n",
      "Iter-53020, train loss-0.5745, acc-0.9000, valid loss-0.4469, acc-0.8764, test loss-0.4507, acc-0.8738\n",
      "Iter-53030, train loss-0.4787, acc-0.8600, valid loss-0.4468, acc-0.8764, test loss-0.4507, acc-0.8739\n",
      "Iter-53040, train loss-0.6615, acc-0.8000, valid loss-0.4468, acc-0.8764, test loss-0.4506, acc-0.8738\n",
      "Iter-53050, train loss-0.5103, acc-0.9000, valid loss-0.4468, acc-0.8764, test loss-0.4506, acc-0.8740\n",
      "Iter-53060, train loss-0.4882, acc-0.8800, valid loss-0.4467, acc-0.8766, test loss-0.4506, acc-0.8739\n",
      "Iter-53070, train loss-0.4463, acc-0.9200, valid loss-0.4467, acc-0.8768, test loss-0.4505, acc-0.8740\n",
      "Iter-53080, train loss-0.3547, acc-0.8600, valid loss-0.4467, acc-0.8766, test loss-0.4505, acc-0.8741\n",
      "Iter-53090, train loss-0.2628, acc-0.9600, valid loss-0.4466, acc-0.8768, test loss-0.4504, acc-0.8740\n",
      "Iter-53100, train loss-0.5019, acc-0.8400, valid loss-0.4466, acc-0.8768, test loss-0.4504, acc-0.8739\n",
      "Iter-53110, train loss-0.4081, acc-0.9200, valid loss-0.4465, acc-0.8768, test loss-0.4503, acc-0.8740\n",
      "Iter-53120, train loss-0.4631, acc-0.8800, valid loss-0.4465, acc-0.8766, test loss-0.4503, acc-0.8740\n",
      "Iter-53130, train loss-0.3635, acc-0.9200, valid loss-0.4464, acc-0.8764, test loss-0.4503, acc-0.8739\n",
      "Iter-53140, train loss-0.4963, acc-0.8600, valid loss-0.4464, acc-0.8764, test loss-0.4502, acc-0.8739\n",
      "Iter-53150, train loss-0.5945, acc-0.8400, valid loss-0.4464, acc-0.8764, test loss-0.4502, acc-0.8739\n",
      "Iter-53160, train loss-0.3924, acc-0.9000, valid loss-0.4463, acc-0.8764, test loss-0.4501, acc-0.8739\n",
      "Iter-53170, train loss-0.7007, acc-0.7800, valid loss-0.4463, acc-0.8766, test loss-0.4501, acc-0.8739\n",
      "Iter-53180, train loss-0.5474, acc-0.8000, valid loss-0.4463, acc-0.8764, test loss-0.4501, acc-0.8740\n",
      "Iter-53190, train loss-0.5580, acc-0.8400, valid loss-0.4462, acc-0.8764, test loss-0.4500, acc-0.8740\n",
      "Iter-53200, train loss-0.4264, acc-0.8800, valid loss-0.4462, acc-0.8764, test loss-0.4500, acc-0.8739\n",
      "Iter-53210, train loss-0.3937, acc-0.9200, valid loss-0.4462, acc-0.8764, test loss-0.4500, acc-0.8740\n",
      "Iter-53220, train loss-0.3644, acc-0.9200, valid loss-0.4461, acc-0.8766, test loss-0.4499, acc-0.8740\n",
      "Iter-53230, train loss-0.4709, acc-0.8600, valid loss-0.4461, acc-0.8766, test loss-0.4499, acc-0.8740\n",
      "Iter-53240, train loss-0.4732, acc-0.8400, valid loss-0.4460, acc-0.8766, test loss-0.4498, acc-0.8742\n",
      "Iter-53250, train loss-0.3665, acc-0.9400, valid loss-0.4460, acc-0.8766, test loss-0.4498, acc-0.8742\n",
      "Iter-53260, train loss-0.4083, acc-0.8800, valid loss-0.4460, acc-0.8766, test loss-0.4498, acc-0.8743\n",
      "Iter-53270, train loss-0.4148, acc-0.8800, valid loss-0.4459, acc-0.8766, test loss-0.4497, acc-0.8743\n",
      "Iter-53280, train loss-0.2981, acc-0.9600, valid loss-0.4459, acc-0.8766, test loss-0.4497, acc-0.8742\n",
      "Iter-53290, train loss-0.5993, acc-0.8200, valid loss-0.4458, acc-0.8766, test loss-0.4497, acc-0.8743\n",
      "Iter-53300, train loss-0.5584, acc-0.8400, valid loss-0.4458, acc-0.8766, test loss-0.4496, acc-0.8741\n",
      "Iter-53310, train loss-0.3879, acc-0.8800, valid loss-0.4457, acc-0.8768, test loss-0.4496, acc-0.8742\n",
      "Iter-53320, train loss-0.2877, acc-0.9400, valid loss-0.4457, acc-0.8768, test loss-0.4495, acc-0.8743\n",
      "Iter-53330, train loss-0.4910, acc-0.8600, valid loss-0.4457, acc-0.8766, test loss-0.4495, acc-0.8741\n",
      "Iter-53340, train loss-0.3126, acc-0.9200, valid loss-0.4457, acc-0.8768, test loss-0.4495, acc-0.8740\n",
      "Iter-53350, train loss-0.4252, acc-0.8400, valid loss-0.4456, acc-0.8768, test loss-0.4494, acc-0.8739\n",
      "Iter-53360, train loss-0.4120, acc-0.9400, valid loss-0.4456, acc-0.8768, test loss-0.4494, acc-0.8740\n",
      "Iter-53370, train loss-0.6369, acc-0.8200, valid loss-0.4456, acc-0.8768, test loss-0.4494, acc-0.8741\n",
      "Iter-53380, train loss-0.6790, acc-0.8200, valid loss-0.4455, acc-0.8768, test loss-0.4493, acc-0.8740\n",
      "Iter-53390, train loss-0.3214, acc-0.9000, valid loss-0.4455, acc-0.8768, test loss-0.4493, acc-0.8740\n",
      "Iter-53400, train loss-0.4878, acc-0.8400, valid loss-0.4454, acc-0.8768, test loss-0.4493, acc-0.8740\n",
      "Iter-53410, train loss-0.3773, acc-0.8800, valid loss-0.4454, acc-0.8768, test loss-0.4492, acc-0.8741\n",
      "Iter-53420, train loss-0.2619, acc-0.9400, valid loss-0.4454, acc-0.8768, test loss-0.4492, acc-0.8740\n",
      "Iter-53430, train loss-0.5875, acc-0.8400, valid loss-0.4453, acc-0.8770, test loss-0.4491, acc-0.8740\n",
      "Iter-53440, train loss-0.3125, acc-0.9600, valid loss-0.4453, acc-0.8772, test loss-0.4491, acc-0.8742\n",
      "Iter-53450, train loss-0.4458, acc-0.8800, valid loss-0.4453, acc-0.8772, test loss-0.4491, acc-0.8742\n",
      "Iter-53460, train loss-0.5406, acc-0.8800, valid loss-0.4452, acc-0.8772, test loss-0.4490, acc-0.8744\n",
      "Iter-53470, train loss-0.3325, acc-0.9400, valid loss-0.4452, acc-0.8772, test loss-0.4490, acc-0.8742\n",
      "Iter-53480, train loss-0.4796, acc-0.8600, valid loss-0.4452, acc-0.8772, test loss-0.4490, acc-0.8744\n",
      "Iter-53490, train loss-0.3428, acc-0.9200, valid loss-0.4451, acc-0.8772, test loss-0.4489, acc-0.8743\n",
      "Iter-53500, train loss-0.5154, acc-0.8400, valid loss-0.4451, acc-0.8772, test loss-0.4489, acc-0.8743\n",
      "Iter-53510, train loss-0.3603, acc-0.9000, valid loss-0.4450, acc-0.8772, test loss-0.4488, acc-0.8743\n",
      "Iter-53520, train loss-0.4983, acc-0.8600, valid loss-0.4450, acc-0.8772, test loss-0.4488, acc-0.8744\n",
      "Iter-53530, train loss-0.4594, acc-0.8600, valid loss-0.4450, acc-0.8772, test loss-0.4488, acc-0.8744\n",
      "Iter-53540, train loss-0.5497, acc-0.8800, valid loss-0.4449, acc-0.8772, test loss-0.4487, acc-0.8744\n",
      "Iter-53550, train loss-0.5397, acc-0.8400, valid loss-0.4449, acc-0.8772, test loss-0.4487, acc-0.8742\n",
      "Iter-53560, train loss-0.4123, acc-0.8400, valid loss-0.4448, acc-0.8772, test loss-0.4486, acc-0.8743\n",
      "Iter-53570, train loss-0.5686, acc-0.8400, valid loss-0.4448, acc-0.8772, test loss-0.4486, acc-0.8743\n",
      "Iter-53580, train loss-0.4737, acc-0.9200, valid loss-0.4447, acc-0.8774, test loss-0.4486, acc-0.8743\n",
      "Iter-53590, train loss-0.3574, acc-0.9400, valid loss-0.4447, acc-0.8774, test loss-0.4485, acc-0.8743\n",
      "Iter-53600, train loss-0.7460, acc-0.8000, valid loss-0.4447, acc-0.8774, test loss-0.4485, acc-0.8743\n",
      "Iter-53610, train loss-0.4350, acc-0.9000, valid loss-0.4446, acc-0.8774, test loss-0.4485, acc-0.8744\n",
      "Iter-53620, train loss-0.4309, acc-0.9400, valid loss-0.4446, acc-0.8774, test loss-0.4484, acc-0.8744\n",
      "Iter-53630, train loss-0.5419, acc-0.8200, valid loss-0.4445, acc-0.8774, test loss-0.4484, acc-0.8743\n",
      "Iter-53640, train loss-0.5070, acc-0.8000, valid loss-0.4445, acc-0.8774, test loss-0.4483, acc-0.8744\n",
      "Iter-53650, train loss-0.3870, acc-0.9400, valid loss-0.4444, acc-0.8774, test loss-0.4483, acc-0.8742\n",
      "Iter-53660, train loss-0.5436, acc-0.8200, valid loss-0.4444, acc-0.8774, test loss-0.4483, acc-0.8742\n",
      "Iter-53670, train loss-0.6052, acc-0.8000, valid loss-0.4444, acc-0.8774, test loss-0.4482, acc-0.8742\n",
      "Iter-53680, train loss-0.4571, acc-0.8400, valid loss-0.4443, acc-0.8774, test loss-0.4482, acc-0.8742\n",
      "Iter-53690, train loss-0.3827, acc-0.9000, valid loss-0.4443, acc-0.8774, test loss-0.4481, acc-0.8743\n",
      "Iter-53700, train loss-0.2440, acc-0.9400, valid loss-0.4443, acc-0.8774, test loss-0.4481, acc-0.8742\n",
      "Iter-53710, train loss-0.5080, acc-0.8800, valid loss-0.4442, acc-0.8774, test loss-0.4481, acc-0.8745\n",
      "Iter-53720, train loss-0.5537, acc-0.8200, valid loss-0.4442, acc-0.8774, test loss-0.4480, acc-0.8743\n",
      "Iter-53730, train loss-0.3804, acc-0.9000, valid loss-0.4441, acc-0.8774, test loss-0.4480, acc-0.8744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-53740, train loss-0.3216, acc-0.8800, valid loss-0.4441, acc-0.8774, test loss-0.4480, acc-0.8744\n",
      "Iter-53750, train loss-0.4151, acc-0.9400, valid loss-0.4440, acc-0.8774, test loss-0.4479, acc-0.8747\n",
      "Iter-53760, train loss-0.3682, acc-0.8800, valid loss-0.4440, acc-0.8774, test loss-0.4479, acc-0.8746\n",
      "Iter-53770, train loss-0.7306, acc-0.8600, valid loss-0.4440, acc-0.8774, test loss-0.4479, acc-0.8746\n",
      "Iter-53780, train loss-0.4264, acc-0.8800, valid loss-0.4439, acc-0.8774, test loss-0.4478, acc-0.8744\n",
      "Iter-53790, train loss-0.4088, acc-0.9000, valid loss-0.4439, acc-0.8774, test loss-0.4478, acc-0.8745\n",
      "Iter-53800, train loss-0.5680, acc-0.8200, valid loss-0.4439, acc-0.8772, test loss-0.4477, acc-0.8746\n",
      "Iter-53810, train loss-0.4503, acc-0.8600, valid loss-0.4438, acc-0.8776, test loss-0.4477, acc-0.8746\n",
      "Iter-53820, train loss-0.6056, acc-0.8200, valid loss-0.4438, acc-0.8774, test loss-0.4477, acc-0.8745\n",
      "Iter-53830, train loss-0.4637, acc-0.8600, valid loss-0.4437, acc-0.8776, test loss-0.4476, acc-0.8747\n",
      "Iter-53840, train loss-0.5537, acc-0.8600, valid loss-0.4437, acc-0.8776, test loss-0.4476, acc-0.8745\n",
      "Iter-53850, train loss-0.8056, acc-0.7200, valid loss-0.4437, acc-0.8776, test loss-0.4475, acc-0.8744\n",
      "Iter-53860, train loss-0.2127, acc-0.9800, valid loss-0.4436, acc-0.8776, test loss-0.4475, acc-0.8746\n",
      "Iter-53870, train loss-0.4633, acc-0.8400, valid loss-0.4436, acc-0.8774, test loss-0.4475, acc-0.8748\n",
      "Iter-53880, train loss-0.3617, acc-0.9200, valid loss-0.4436, acc-0.8774, test loss-0.4474, acc-0.8748\n",
      "Iter-53890, train loss-0.3179, acc-0.9000, valid loss-0.4435, acc-0.8774, test loss-0.4474, acc-0.8749\n",
      "Iter-53900, train loss-0.4842, acc-0.8800, valid loss-0.4435, acc-0.8774, test loss-0.4473, acc-0.8748\n",
      "Iter-53910, train loss-0.3818, acc-0.9000, valid loss-0.4434, acc-0.8774, test loss-0.4473, acc-0.8748\n",
      "Iter-53920, train loss-0.6042, acc-0.8200, valid loss-0.4434, acc-0.8774, test loss-0.4473, acc-0.8749\n",
      "Iter-53930, train loss-0.4141, acc-0.8800, valid loss-0.4433, acc-0.8774, test loss-0.4472, acc-0.8749\n",
      "Iter-53940, train loss-0.4056, acc-0.9400, valid loss-0.4433, acc-0.8774, test loss-0.4472, acc-0.8749\n",
      "Iter-53950, train loss-0.3339, acc-0.9200, valid loss-0.4433, acc-0.8774, test loss-0.4472, acc-0.8748\n",
      "Iter-53960, train loss-0.3940, acc-0.9000, valid loss-0.4432, acc-0.8774, test loss-0.4471, acc-0.8748\n",
      "Iter-53970, train loss-0.3857, acc-0.9200, valid loss-0.4432, acc-0.8774, test loss-0.4471, acc-0.8749\n",
      "Iter-53980, train loss-0.4998, acc-0.9000, valid loss-0.4432, acc-0.8774, test loss-0.4470, acc-0.8748\n",
      "Iter-53990, train loss-0.6602, acc-0.7600, valid loss-0.4431, acc-0.8774, test loss-0.4470, acc-0.8749\n",
      "Iter-54000, train loss-0.6529, acc-0.7800, valid loss-0.4431, acc-0.8774, test loss-0.4470, acc-0.8748\n",
      "Iter-54010, train loss-0.3952, acc-0.9000, valid loss-0.4431, acc-0.8774, test loss-0.4469, acc-0.8748\n",
      "Iter-54020, train loss-0.4516, acc-0.8800, valid loss-0.4431, acc-0.8774, test loss-0.4469, acc-0.8747\n",
      "Iter-54030, train loss-0.2526, acc-0.9800, valid loss-0.4430, acc-0.8776, test loss-0.4469, acc-0.8748\n",
      "Iter-54040, train loss-0.4746, acc-0.8000, valid loss-0.4430, acc-0.8776, test loss-0.4468, acc-0.8747\n",
      "Iter-54050, train loss-0.3324, acc-0.9200, valid loss-0.4429, acc-0.8776, test loss-0.4468, acc-0.8746\n",
      "Iter-54060, train loss-0.5988, acc-0.8800, valid loss-0.4429, acc-0.8776, test loss-0.4468, acc-0.8746\n",
      "Iter-54070, train loss-0.3164, acc-0.9200, valid loss-0.4429, acc-0.8776, test loss-0.4467, acc-0.8747\n",
      "Iter-54080, train loss-0.4925, acc-0.8800, valid loss-0.4428, acc-0.8776, test loss-0.4467, acc-0.8747\n",
      "Iter-54090, train loss-0.4868, acc-0.8600, valid loss-0.4428, acc-0.8776, test loss-0.4466, acc-0.8748\n",
      "Iter-54100, train loss-0.3427, acc-0.9400, valid loss-0.4427, acc-0.8776, test loss-0.4466, acc-0.8747\n",
      "Iter-54110, train loss-0.2975, acc-0.9400, valid loss-0.4427, acc-0.8776, test loss-0.4466, acc-0.8748\n",
      "Iter-54120, train loss-0.3614, acc-0.9400, valid loss-0.4427, acc-0.8776, test loss-0.4465, acc-0.8747\n",
      "Iter-54130, train loss-0.2336, acc-0.9600, valid loss-0.4426, acc-0.8776, test loss-0.4465, acc-0.8749\n",
      "Iter-54140, train loss-0.4033, acc-0.8800, valid loss-0.4426, acc-0.8776, test loss-0.4464, acc-0.8749\n",
      "Iter-54150, train loss-0.5171, acc-0.8800, valid loss-0.4425, acc-0.8776, test loss-0.4464, acc-0.8749\n",
      "Iter-54160, train loss-0.2187, acc-0.9400, valid loss-0.4425, acc-0.8776, test loss-0.4464, acc-0.8750\n",
      "Iter-54170, train loss-0.3886, acc-0.8600, valid loss-0.4425, acc-0.8776, test loss-0.4463, acc-0.8749\n",
      "Iter-54180, train loss-0.8220, acc-0.7600, valid loss-0.4424, acc-0.8776, test loss-0.4463, acc-0.8749\n",
      "Iter-54190, train loss-0.6683, acc-0.8000, valid loss-0.4424, acc-0.8776, test loss-0.4463, acc-0.8750\n",
      "Iter-54200, train loss-0.3873, acc-0.8800, valid loss-0.4424, acc-0.8776, test loss-0.4462, acc-0.8750\n",
      "Iter-54210, train loss-0.5287, acc-0.8600, valid loss-0.4423, acc-0.8776, test loss-0.4462, acc-0.8750\n",
      "Iter-54220, train loss-0.4429, acc-0.8200, valid loss-0.4423, acc-0.8776, test loss-0.4461, acc-0.8753\n",
      "Iter-54230, train loss-0.4438, acc-0.8200, valid loss-0.4423, acc-0.8776, test loss-0.4461, acc-0.8753\n",
      "Iter-54240, train loss-0.3925, acc-0.8800, valid loss-0.4422, acc-0.8776, test loss-0.4461, acc-0.8753\n",
      "Iter-54250, train loss-0.5531, acc-0.8200, valid loss-0.4422, acc-0.8776, test loss-0.4460, acc-0.8753\n",
      "Iter-54260, train loss-0.7538, acc-0.7800, valid loss-0.4422, acc-0.8776, test loss-0.4460, acc-0.8754\n",
      "Iter-54270, train loss-0.3727, acc-0.8600, valid loss-0.4421, acc-0.8776, test loss-0.4459, acc-0.8753\n",
      "Iter-54280, train loss-0.4339, acc-0.8600, valid loss-0.4421, acc-0.8776, test loss-0.4459, acc-0.8752\n",
      "Iter-54290, train loss-0.3548, acc-0.9200, valid loss-0.4420, acc-0.8776, test loss-0.4459, acc-0.8753\n",
      "Iter-54300, train loss-0.4856, acc-0.8600, valid loss-0.4420, acc-0.8774, test loss-0.4458, acc-0.8753\n",
      "Iter-54310, train loss-0.4228, acc-0.8800, valid loss-0.4420, acc-0.8774, test loss-0.4458, acc-0.8752\n",
      "Iter-54320, train loss-0.4731, acc-0.8200, valid loss-0.4419, acc-0.8776, test loss-0.4457, acc-0.8753\n",
      "Iter-54330, train loss-0.3780, acc-0.9600, valid loss-0.4419, acc-0.8776, test loss-0.4457, acc-0.8754\n",
      "Iter-54340, train loss-0.5140, acc-0.8800, valid loss-0.4418, acc-0.8776, test loss-0.4457, acc-0.8753\n",
      "Iter-54350, train loss-0.7347, acc-0.7800, valid loss-0.4418, acc-0.8776, test loss-0.4456, acc-0.8753\n",
      "Iter-54360, train loss-0.5853, acc-0.8400, valid loss-0.4418, acc-0.8776, test loss-0.4456, acc-0.8753\n",
      "Iter-54370, train loss-0.4313, acc-0.9000, valid loss-0.4417, acc-0.8776, test loss-0.4456, acc-0.8752\n",
      "Iter-54380, train loss-0.4816, acc-0.9000, valid loss-0.4417, acc-0.8776, test loss-0.4455, acc-0.8751\n",
      "Iter-54390, train loss-0.4425, acc-0.8400, valid loss-0.4417, acc-0.8776, test loss-0.4455, acc-0.8754\n",
      "Iter-54400, train loss-0.5553, acc-0.8200, valid loss-0.4416, acc-0.8776, test loss-0.4455, acc-0.8753\n",
      "Iter-54410, train loss-0.4074, acc-0.8600, valid loss-0.4416, acc-0.8774, test loss-0.4454, acc-0.8753\n",
      "Iter-54420, train loss-0.4427, acc-0.9000, valid loss-0.4416, acc-0.8774, test loss-0.4454, acc-0.8755\n",
      "Iter-54430, train loss-0.4354, acc-0.8800, valid loss-0.4415, acc-0.8774, test loss-0.4454, acc-0.8755\n",
      "Iter-54440, train loss-0.4844, acc-0.9000, valid loss-0.4415, acc-0.8774, test loss-0.4453, acc-0.8755\n",
      "Iter-54450, train loss-0.4224, acc-0.8600, valid loss-0.4414, acc-0.8772, test loss-0.4453, acc-0.8755\n",
      "Iter-54460, train loss-0.2265, acc-0.9400, valid loss-0.4414, acc-0.8774, test loss-0.4453, acc-0.8753\n",
      "Iter-54470, train loss-0.3445, acc-0.9000, valid loss-0.4414, acc-0.8774, test loss-0.4452, acc-0.8752\n",
      "Iter-54480, train loss-0.4072, acc-0.8600, valid loss-0.4413, acc-0.8772, test loss-0.4452, acc-0.8754\n",
      "Iter-54490, train loss-0.6509, acc-0.8000, valid loss-0.4413, acc-0.8772, test loss-0.4452, acc-0.8753\n",
      "Iter-54500, train loss-0.4220, acc-0.9000, valid loss-0.4413, acc-0.8772, test loss-0.4451, acc-0.8753\n",
      "Iter-54510, train loss-0.5206, acc-0.8600, valid loss-0.4412, acc-0.8772, test loss-0.4451, acc-0.8753\n",
      "Iter-54520, train loss-0.4156, acc-0.8600, valid loss-0.4412, acc-0.8774, test loss-0.4450, acc-0.8752\n",
      "Iter-54530, train loss-0.5026, acc-0.7800, valid loss-0.4411, acc-0.8774, test loss-0.4450, acc-0.8752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-54540, train loss-1.0057, acc-0.7600, valid loss-0.4411, acc-0.8774, test loss-0.4450, acc-0.8752\n",
      "Iter-54550, train loss-0.2628, acc-0.9800, valid loss-0.4411, acc-0.8774, test loss-0.4450, acc-0.8753\n",
      "Iter-54560, train loss-0.5017, acc-0.8800, valid loss-0.4410, acc-0.8774, test loss-0.4449, acc-0.8751\n",
      "Iter-54570, train loss-0.4488, acc-0.8800, valid loss-0.4410, acc-0.8774, test loss-0.4449, acc-0.8754\n",
      "Iter-54580, train loss-0.6327, acc-0.8000, valid loss-0.4409, acc-0.8774, test loss-0.4448, acc-0.8754\n",
      "Iter-54590, train loss-0.5416, acc-0.8400, valid loss-0.4409, acc-0.8774, test loss-0.4448, acc-0.8752\n",
      "Iter-54600, train loss-0.3446, acc-0.9400, valid loss-0.4409, acc-0.8774, test loss-0.4447, acc-0.8752\n",
      "Iter-54610, train loss-0.3181, acc-0.9200, valid loss-0.4408, acc-0.8774, test loss-0.4447, acc-0.8751\n",
      "Iter-54620, train loss-0.5057, acc-0.8000, valid loss-0.4408, acc-0.8776, test loss-0.4447, acc-0.8751\n",
      "Iter-54630, train loss-0.6856, acc-0.8000, valid loss-0.4408, acc-0.8774, test loss-0.4446, acc-0.8752\n",
      "Iter-54640, train loss-0.2738, acc-0.9200, valid loss-0.4408, acc-0.8774, test loss-0.4446, acc-0.8751\n",
      "Iter-54650, train loss-0.6042, acc-0.8400, valid loss-0.4407, acc-0.8774, test loss-0.4446, acc-0.8750\n",
      "Iter-54660, train loss-0.7000, acc-0.8000, valid loss-0.4407, acc-0.8774, test loss-0.4445, acc-0.8750\n",
      "Iter-54670, train loss-0.5386, acc-0.8200, valid loss-0.4407, acc-0.8776, test loss-0.4445, acc-0.8752\n",
      "Iter-54680, train loss-0.5567, acc-0.8200, valid loss-0.4406, acc-0.8776, test loss-0.4445, acc-0.8752\n",
      "Iter-54690, train loss-0.4527, acc-0.8800, valid loss-0.4406, acc-0.8780, test loss-0.4444, acc-0.8753\n",
      "Iter-54700, train loss-0.5306, acc-0.8800, valid loss-0.4406, acc-0.8778, test loss-0.4444, acc-0.8753\n",
      "Iter-54710, train loss-0.4892, acc-0.8400, valid loss-0.4405, acc-0.8778, test loss-0.4444, acc-0.8753\n",
      "Iter-54720, train loss-0.3771, acc-0.9000, valid loss-0.4405, acc-0.8778, test loss-0.4443, acc-0.8753\n",
      "Iter-54730, train loss-0.2805, acc-0.9200, valid loss-0.4404, acc-0.8778, test loss-0.4443, acc-0.8754\n",
      "Iter-54740, train loss-0.3911, acc-0.8600, valid loss-0.4404, acc-0.8778, test loss-0.4442, acc-0.8754\n",
      "Iter-54750, train loss-0.4597, acc-0.8800, valid loss-0.4404, acc-0.8778, test loss-0.4442, acc-0.8754\n",
      "Iter-54760, train loss-0.6021, acc-0.8400, valid loss-0.4403, acc-0.8778, test loss-0.4442, acc-0.8755\n",
      "Iter-54770, train loss-0.4661, acc-0.8600, valid loss-0.4403, acc-0.8778, test loss-0.4441, acc-0.8757\n",
      "Iter-54780, train loss-0.5213, acc-0.8800, valid loss-0.4402, acc-0.8780, test loss-0.4441, acc-0.8755\n",
      "Iter-54790, train loss-0.4192, acc-0.8600, valid loss-0.4402, acc-0.8780, test loss-0.4441, acc-0.8754\n",
      "Iter-54800, train loss-0.3686, acc-0.9000, valid loss-0.4402, acc-0.8780, test loss-0.4440, acc-0.8757\n",
      "Iter-54810, train loss-0.4337, acc-0.8800, valid loss-0.4401, acc-0.8782, test loss-0.4440, acc-0.8755\n",
      "Iter-54820, train loss-0.4391, acc-0.8800, valid loss-0.4401, acc-0.8782, test loss-0.4440, acc-0.8755\n",
      "Iter-54830, train loss-0.4621, acc-0.9200, valid loss-0.4400, acc-0.8780, test loss-0.4439, acc-0.8756\n",
      "Iter-54840, train loss-0.2735, acc-0.9400, valid loss-0.4400, acc-0.8782, test loss-0.4439, acc-0.8757\n",
      "Iter-54850, train loss-0.3084, acc-0.9200, valid loss-0.4399, acc-0.8780, test loss-0.4439, acc-0.8758\n",
      "Iter-54860, train loss-0.3876, acc-0.9000, valid loss-0.4399, acc-0.8782, test loss-0.4438, acc-0.8758\n",
      "Iter-54870, train loss-0.4810, acc-0.8200, valid loss-0.4399, acc-0.8782, test loss-0.4438, acc-0.8758\n",
      "Iter-54880, train loss-0.3619, acc-0.8800, valid loss-0.4398, acc-0.8782, test loss-0.4437, acc-0.8758\n",
      "Iter-54890, train loss-0.7206, acc-0.7800, valid loss-0.4398, acc-0.8782, test loss-0.4437, acc-0.8758\n",
      "Iter-54900, train loss-0.6481, acc-0.8600, valid loss-0.4398, acc-0.8780, test loss-0.4437, acc-0.8758\n",
      "Iter-54910, train loss-0.3286, acc-0.9400, valid loss-0.4397, acc-0.8782, test loss-0.4436, acc-0.8759\n",
      "Iter-54920, train loss-0.4679, acc-0.9000, valid loss-0.4397, acc-0.8780, test loss-0.4436, acc-0.8759\n",
      "Iter-54930, train loss-0.4407, acc-0.9000, valid loss-0.4397, acc-0.8778, test loss-0.4436, acc-0.8756\n",
      "Iter-54940, train loss-0.4596, acc-0.8600, valid loss-0.4396, acc-0.8778, test loss-0.4435, acc-0.8757\n",
      "Iter-54950, train loss-0.6625, acc-0.8000, valid loss-0.4396, acc-0.8778, test loss-0.4435, acc-0.8758\n",
      "Iter-54960, train loss-0.5520, acc-0.8600, valid loss-0.4396, acc-0.8780, test loss-0.4434, acc-0.8759\n",
      "Iter-54970, train loss-0.4772, acc-0.8200, valid loss-0.4395, acc-0.8780, test loss-0.4434, acc-0.8759\n",
      "Iter-54980, train loss-0.5181, acc-0.8800, valid loss-0.4395, acc-0.8780, test loss-0.4434, acc-0.8759\n",
      "Iter-54990, train loss-0.3765, acc-0.8800, valid loss-0.4395, acc-0.8780, test loss-0.4433, acc-0.8759\n",
      "Iter-55000, train loss-0.5385, acc-0.8800, valid loss-0.4394, acc-0.8780, test loss-0.4433, acc-0.8759\n",
      "Iter-55010, train loss-0.3727, acc-0.9000, valid loss-0.4394, acc-0.8780, test loss-0.4433, acc-0.8759\n",
      "Iter-55020, train loss-0.3527, acc-0.9000, valid loss-0.4394, acc-0.8782, test loss-0.4432, acc-0.8758\n",
      "Iter-55030, train loss-0.4273, acc-0.8600, valid loss-0.4393, acc-0.8780, test loss-0.4432, acc-0.8758\n",
      "Iter-55040, train loss-0.2030, acc-0.9600, valid loss-0.4393, acc-0.8780, test loss-0.4432, acc-0.8759\n",
      "Iter-55050, train loss-0.5616, acc-0.8400, valid loss-0.4393, acc-0.8780, test loss-0.4431, acc-0.8758\n",
      "Iter-55060, train loss-0.3707, acc-0.9400, valid loss-0.4392, acc-0.8780, test loss-0.4431, acc-0.8757\n",
      "Iter-55070, train loss-0.2397, acc-0.9600, valid loss-0.4392, acc-0.8780, test loss-0.4431, acc-0.8758\n",
      "Iter-55080, train loss-0.4369, acc-0.8600, valid loss-0.4391, acc-0.8780, test loss-0.4430, acc-0.8756\n",
      "Iter-55090, train loss-0.4372, acc-0.8800, valid loss-0.4391, acc-0.8782, test loss-0.4430, acc-0.8755\n",
      "Iter-55100, train loss-0.6114, acc-0.8600, valid loss-0.4391, acc-0.8782, test loss-0.4429, acc-0.8760\n",
      "Iter-55110, train loss-0.3317, acc-0.9400, valid loss-0.4390, acc-0.8780, test loss-0.4429, acc-0.8758\n",
      "Iter-55120, train loss-0.5638, acc-0.8400, valid loss-0.4390, acc-0.8780, test loss-0.4429, acc-0.8758\n",
      "Iter-55130, train loss-0.4036, acc-0.8600, valid loss-0.4390, acc-0.8780, test loss-0.4429, acc-0.8759\n",
      "Iter-55140, train loss-0.3676, acc-0.9000, valid loss-0.4389, acc-0.8778, test loss-0.4428, acc-0.8759\n",
      "Iter-55150, train loss-0.6058, acc-0.8200, valid loss-0.4389, acc-0.8778, test loss-0.4428, acc-0.8761\n",
      "Iter-55160, train loss-0.5372, acc-0.8800, valid loss-0.4388, acc-0.8778, test loss-0.4427, acc-0.8761\n",
      "Iter-55170, train loss-0.4337, acc-0.8800, valid loss-0.4388, acc-0.8778, test loss-0.4427, acc-0.8761\n",
      "Iter-55180, train loss-0.3721, acc-0.9200, valid loss-0.4388, acc-0.8780, test loss-0.4427, acc-0.8760\n",
      "Iter-55190, train loss-0.5748, acc-0.8400, valid loss-0.4387, acc-0.8778, test loss-0.4426, acc-0.8761\n",
      "Iter-55200, train loss-0.5276, acc-0.8400, valid loss-0.4387, acc-0.8778, test loss-0.4426, acc-0.8761\n",
      "Iter-55210, train loss-0.4883, acc-0.8200, valid loss-0.4387, acc-0.8778, test loss-0.4426, acc-0.8761\n",
      "Iter-55220, train loss-0.5563, acc-0.9000, valid loss-0.4386, acc-0.8778, test loss-0.4425, acc-0.8760\n",
      "Iter-55230, train loss-0.5110, acc-0.8600, valid loss-0.4386, acc-0.8778, test loss-0.4425, acc-0.8762\n",
      "Iter-55240, train loss-0.5811, acc-0.8600, valid loss-0.4385, acc-0.8780, test loss-0.4425, acc-0.8761\n",
      "Iter-55250, train loss-0.5161, acc-0.8400, valid loss-0.4385, acc-0.8780, test loss-0.4424, acc-0.8761\n",
      "Iter-55260, train loss-0.5778, acc-0.9000, valid loss-0.4385, acc-0.8780, test loss-0.4424, acc-0.8761\n",
      "Iter-55270, train loss-0.3907, acc-0.8200, valid loss-0.4385, acc-0.8780, test loss-0.4424, acc-0.8760\n",
      "Iter-55280, train loss-0.3057, acc-0.9400, valid loss-0.4384, acc-0.8780, test loss-0.4423, acc-0.8760\n",
      "Iter-55290, train loss-0.5105, acc-0.8200, valid loss-0.4384, acc-0.8780, test loss-0.4423, acc-0.8761\n",
      "Iter-55300, train loss-0.4315, acc-0.8600, valid loss-0.4383, acc-0.8782, test loss-0.4423, acc-0.8761\n",
      "Iter-55310, train loss-0.3065, acc-0.9200, valid loss-0.4383, acc-0.8778, test loss-0.4422, acc-0.8761\n",
      "Iter-55320, train loss-0.5427, acc-0.8200, valid loss-0.4383, acc-0.8780, test loss-0.4422, acc-0.8760\n",
      "Iter-55330, train loss-0.4610, acc-0.9000, valid loss-0.4382, acc-0.8782, test loss-0.4421, acc-0.8759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-55340, train loss-0.4550, acc-0.8200, valid loss-0.4382, acc-0.8782, test loss-0.4421, acc-0.8760\n",
      "Iter-55350, train loss-0.4568, acc-0.8200, valid loss-0.4381, acc-0.8780, test loss-0.4421, acc-0.8760\n",
      "Iter-55360, train loss-0.3442, acc-0.9000, valid loss-0.4381, acc-0.8780, test loss-0.4420, acc-0.8760\n",
      "Iter-55370, train loss-0.3848, acc-0.9000, valid loss-0.4381, acc-0.8780, test loss-0.4420, acc-0.8760\n",
      "Iter-55380, train loss-0.3733, acc-0.9600, valid loss-0.4380, acc-0.8782, test loss-0.4420, acc-0.8760\n",
      "Iter-55390, train loss-0.2965, acc-0.9400, valid loss-0.4380, acc-0.8782, test loss-0.4419, acc-0.8759\n",
      "Iter-55400, train loss-0.2817, acc-0.9200, valid loss-0.4379, acc-0.8782, test loss-0.4419, acc-0.8759\n",
      "Iter-55410, train loss-0.4147, acc-0.8400, valid loss-0.4379, acc-0.8780, test loss-0.4419, acc-0.8759\n",
      "Iter-55420, train loss-0.3856, acc-0.8200, valid loss-0.4379, acc-0.8780, test loss-0.4418, acc-0.8758\n",
      "Iter-55430, train loss-0.5220, acc-0.8200, valid loss-0.4378, acc-0.8782, test loss-0.4418, acc-0.8760\n",
      "Iter-55440, train loss-0.4932, acc-0.8600, valid loss-0.4378, acc-0.8780, test loss-0.4418, acc-0.8760\n",
      "Iter-55450, train loss-0.4488, acc-0.8800, valid loss-0.4378, acc-0.8780, test loss-0.4417, acc-0.8762\n",
      "Iter-55460, train loss-0.4332, acc-0.8600, valid loss-0.4377, acc-0.8782, test loss-0.4417, acc-0.8761\n",
      "Iter-55470, train loss-0.4162, acc-0.9200, valid loss-0.4377, acc-0.8782, test loss-0.4416, acc-0.8760\n",
      "Iter-55480, train loss-0.4838, acc-0.8600, valid loss-0.4376, acc-0.8780, test loss-0.4416, acc-0.8763\n",
      "Iter-55490, train loss-0.4528, acc-0.8800, valid loss-0.4376, acc-0.8780, test loss-0.4416, acc-0.8761\n",
      "Iter-55500, train loss-0.6468, acc-0.8000, valid loss-0.4376, acc-0.8780, test loss-0.4415, acc-0.8762\n",
      "Iter-55510, train loss-0.6333, acc-0.8600, valid loss-0.4375, acc-0.8780, test loss-0.4415, acc-0.8762\n",
      "Iter-55520, train loss-0.4506, acc-0.8600, valid loss-0.4375, acc-0.8780, test loss-0.4414, acc-0.8761\n",
      "Iter-55530, train loss-0.6455, acc-0.9000, valid loss-0.4375, acc-0.8780, test loss-0.4414, acc-0.8761\n",
      "Iter-55540, train loss-0.2844, acc-0.9400, valid loss-0.4374, acc-0.8778, test loss-0.4414, acc-0.8761\n",
      "Iter-55550, train loss-0.3853, acc-0.9200, valid loss-0.4374, acc-0.8780, test loss-0.4413, acc-0.8761\n",
      "Iter-55560, train loss-0.4256, acc-0.8400, valid loss-0.4374, acc-0.8780, test loss-0.4413, acc-0.8760\n",
      "Iter-55570, train loss-0.4238, acc-0.8600, valid loss-0.4373, acc-0.8780, test loss-0.4413, acc-0.8760\n",
      "Iter-55580, train loss-0.6009, acc-0.8200, valid loss-0.4373, acc-0.8780, test loss-0.4412, acc-0.8759\n",
      "Iter-55590, train loss-0.5208, acc-0.8600, valid loss-0.4372, acc-0.8780, test loss-0.4412, acc-0.8760\n",
      "Iter-55600, train loss-0.5899, acc-0.7800, valid loss-0.4372, acc-0.8780, test loss-0.4412, acc-0.8759\n",
      "Iter-55610, train loss-0.3157, acc-0.9400, valid loss-0.4372, acc-0.8780, test loss-0.4411, acc-0.8758\n",
      "Iter-55620, train loss-0.3733, acc-0.9000, valid loss-0.4372, acc-0.8780, test loss-0.4411, acc-0.8758\n",
      "Iter-55630, train loss-0.3052, acc-0.9600, valid loss-0.4371, acc-0.8784, test loss-0.4410, acc-0.8758\n",
      "Iter-55640, train loss-0.4151, acc-0.8600, valid loss-0.4371, acc-0.8784, test loss-0.4410, acc-0.8758\n",
      "Iter-55650, train loss-0.4669, acc-0.8800, valid loss-0.4370, acc-0.8784, test loss-0.4410, acc-0.8759\n",
      "Iter-55660, train loss-0.3549, acc-0.8800, valid loss-0.4370, acc-0.8784, test loss-0.4409, acc-0.8760\n",
      "Iter-55670, train loss-0.5949, acc-0.8200, valid loss-0.4370, acc-0.8782, test loss-0.4409, acc-0.8760\n",
      "Iter-55680, train loss-0.4954, acc-0.8800, valid loss-0.4369, acc-0.8782, test loss-0.4408, acc-0.8761\n",
      "Iter-55690, train loss-0.4691, acc-0.8400, valid loss-0.4369, acc-0.8780, test loss-0.4408, acc-0.8761\n",
      "Iter-55700, train loss-0.3456, acc-0.9200, valid loss-0.4369, acc-0.8782, test loss-0.4408, acc-0.8762\n",
      "Iter-55710, train loss-0.6056, acc-0.8000, valid loss-0.4368, acc-0.8784, test loss-0.4407, acc-0.8761\n",
      "Iter-55720, train loss-0.3979, acc-0.9000, valid loss-0.4368, acc-0.8784, test loss-0.4407, acc-0.8761\n",
      "Iter-55730, train loss-0.4034, acc-0.8600, valid loss-0.4368, acc-0.8784, test loss-0.4406, acc-0.8761\n",
      "Iter-55740, train loss-0.7079, acc-0.7800, valid loss-0.4368, acc-0.8782, test loss-0.4406, acc-0.8761\n",
      "Iter-55750, train loss-0.4013, acc-0.9000, valid loss-0.4367, acc-0.8786, test loss-0.4406, acc-0.8762\n",
      "Iter-55760, train loss-0.3076, acc-0.9400, valid loss-0.4367, acc-0.8786, test loss-0.4406, acc-0.8763\n",
      "Iter-55770, train loss-0.4126, acc-0.9000, valid loss-0.4367, acc-0.8788, test loss-0.4405, acc-0.8762\n",
      "Iter-55780, train loss-0.4234, acc-0.8600, valid loss-0.4366, acc-0.8786, test loss-0.4405, acc-0.8762\n",
      "Iter-55790, train loss-0.6480, acc-0.8000, valid loss-0.4366, acc-0.8788, test loss-0.4405, acc-0.8759\n",
      "Iter-55800, train loss-0.6211, acc-0.8000, valid loss-0.4366, acc-0.8788, test loss-0.4404, acc-0.8760\n",
      "Iter-55810, train loss-0.4370, acc-0.8800, valid loss-0.4365, acc-0.8788, test loss-0.4404, acc-0.8760\n",
      "Iter-55820, train loss-0.4353, acc-0.8800, valid loss-0.4365, acc-0.8786, test loss-0.4403, acc-0.8760\n",
      "Iter-55830, train loss-0.3545, acc-0.9400, valid loss-0.4364, acc-0.8788, test loss-0.4403, acc-0.8761\n",
      "Iter-55840, train loss-0.2965, acc-0.9000, valid loss-0.4364, acc-0.8790, test loss-0.4403, acc-0.8761\n",
      "Iter-55850, train loss-0.2730, acc-0.9400, valid loss-0.4364, acc-0.8788, test loss-0.4402, acc-0.8760\n",
      "Iter-55860, train loss-0.3268, acc-0.9400, valid loss-0.4363, acc-0.8790, test loss-0.4402, acc-0.8760\n",
      "Iter-55870, train loss-0.3795, acc-0.9000, valid loss-0.4363, acc-0.8788, test loss-0.4401, acc-0.8760\n",
      "Iter-55880, train loss-0.3144, acc-0.9000, valid loss-0.4363, acc-0.8790, test loss-0.4401, acc-0.8760\n",
      "Iter-55890, train loss-0.3709, acc-0.9400, valid loss-0.4362, acc-0.8788, test loss-0.4401, acc-0.8761\n",
      "Iter-55900, train loss-0.3842, acc-0.8800, valid loss-0.4362, acc-0.8792, test loss-0.4401, acc-0.8760\n",
      "Iter-55910, train loss-0.6529, acc-0.7800, valid loss-0.4362, acc-0.8790, test loss-0.4400, acc-0.8759\n",
      "Iter-55920, train loss-0.3107, acc-0.9000, valid loss-0.4361, acc-0.8792, test loss-0.4400, acc-0.8760\n",
      "Iter-55930, train loss-0.5440, acc-0.9000, valid loss-0.4361, acc-0.8786, test loss-0.4400, acc-0.8760\n",
      "Iter-55940, train loss-0.6319, acc-0.8000, valid loss-0.4361, acc-0.8790, test loss-0.4399, acc-0.8760\n",
      "Iter-55950, train loss-0.5056, acc-0.9000, valid loss-0.4360, acc-0.8792, test loss-0.4399, acc-0.8759\n",
      "Iter-55960, train loss-0.4442, acc-0.8800, valid loss-0.4360, acc-0.8788, test loss-0.4399, acc-0.8759\n",
      "Iter-55970, train loss-0.3643, acc-0.9000, valid loss-0.4360, acc-0.8788, test loss-0.4398, acc-0.8759\n",
      "Iter-55980, train loss-0.3981, acc-0.9000, valid loss-0.4359, acc-0.8788, test loss-0.4398, acc-0.8760\n",
      "Iter-55990, train loss-0.5280, acc-0.8600, valid loss-0.4359, acc-0.8788, test loss-0.4398, acc-0.8759\n",
      "Iter-56000, train loss-0.5787, acc-0.8000, valid loss-0.4359, acc-0.8788, test loss-0.4397, acc-0.8758\n",
      "Iter-56010, train loss-0.3403, acc-0.9200, valid loss-0.4358, acc-0.8788, test loss-0.4397, acc-0.8760\n",
      "Iter-56020, train loss-0.4681, acc-0.8600, valid loss-0.4358, acc-0.8788, test loss-0.4397, acc-0.8760\n",
      "Iter-56030, train loss-0.3716, acc-0.9000, valid loss-0.4358, acc-0.8790, test loss-0.4397, acc-0.8760\n",
      "Iter-56040, train loss-0.3826, acc-0.9000, valid loss-0.4357, acc-0.8790, test loss-0.4396, acc-0.8760\n",
      "Iter-56050, train loss-0.5403, acc-0.8000, valid loss-0.4357, acc-0.8792, test loss-0.4396, acc-0.8760\n",
      "Iter-56060, train loss-0.3770, acc-0.8800, valid loss-0.4357, acc-0.8792, test loss-0.4395, acc-0.8761\n",
      "Iter-56070, train loss-0.4883, acc-0.8400, valid loss-0.4356, acc-0.8792, test loss-0.4395, acc-0.8761\n",
      "Iter-56080, train loss-0.4048, acc-0.9000, valid loss-0.4356, acc-0.8792, test loss-0.4395, acc-0.8761\n",
      "Iter-56090, train loss-0.5171, acc-0.8400, valid loss-0.4356, acc-0.8790, test loss-0.4395, acc-0.8762\n",
      "Iter-56100, train loss-0.4664, acc-0.9000, valid loss-0.4355, acc-0.8790, test loss-0.4394, acc-0.8762\n",
      "Iter-56110, train loss-0.3500, acc-0.9000, valid loss-0.4355, acc-0.8790, test loss-0.4394, acc-0.8762\n",
      "Iter-56120, train loss-0.3249, acc-0.8800, valid loss-0.4355, acc-0.8792, test loss-0.4394, acc-0.8762\n",
      "Iter-56130, train loss-0.4734, acc-0.8800, valid loss-0.4354, acc-0.8792, test loss-0.4393, acc-0.8762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56140, train loss-0.4001, acc-0.8600, valid loss-0.4354, acc-0.8792, test loss-0.4393, acc-0.8761\n",
      "Iter-56150, train loss-0.5163, acc-0.8600, valid loss-0.4354, acc-0.8792, test loss-0.4392, acc-0.8760\n",
      "Iter-56160, train loss-0.2520, acc-0.9400, valid loss-0.4353, acc-0.8792, test loss-0.4392, acc-0.8760\n",
      "Iter-56170, train loss-0.3802, acc-0.9400, valid loss-0.4353, acc-0.8792, test loss-0.4392, acc-0.8760\n",
      "Iter-56180, train loss-0.3457, acc-0.8800, valid loss-0.4353, acc-0.8792, test loss-0.4391, acc-0.8760\n",
      "Iter-56190, train loss-0.3793, acc-0.9200, valid loss-0.4352, acc-0.8792, test loss-0.4391, acc-0.8760\n",
      "Iter-56200, train loss-0.4489, acc-0.8600, valid loss-0.4352, acc-0.8792, test loss-0.4390, acc-0.8761\n",
      "Iter-56210, train loss-0.4430, acc-0.8800, valid loss-0.4351, acc-0.8792, test loss-0.4390, acc-0.8761\n",
      "Iter-56220, train loss-0.6851, acc-0.8200, valid loss-0.4351, acc-0.8794, test loss-0.4390, acc-0.8762\n",
      "Iter-56230, train loss-0.4161, acc-0.8600, valid loss-0.4351, acc-0.8794, test loss-0.4389, acc-0.8762\n",
      "Iter-56240, train loss-0.4713, acc-0.8600, valid loss-0.4350, acc-0.8794, test loss-0.4389, acc-0.8762\n",
      "Iter-56250, train loss-0.5896, acc-0.8600, valid loss-0.4350, acc-0.8794, test loss-0.4389, acc-0.8762\n",
      "Iter-56260, train loss-0.4418, acc-0.8400, valid loss-0.4349, acc-0.8794, test loss-0.4388, acc-0.8763\n",
      "Iter-56270, train loss-0.6922, acc-0.8400, valid loss-0.4349, acc-0.8794, test loss-0.4388, acc-0.8764\n",
      "Iter-56280, train loss-0.6019, acc-0.8800, valid loss-0.4349, acc-0.8796, test loss-0.4388, acc-0.8764\n",
      "Iter-56290, train loss-0.5173, acc-0.8600, valid loss-0.4348, acc-0.8794, test loss-0.4387, acc-0.8763\n",
      "Iter-56300, train loss-0.2944, acc-0.9000, valid loss-0.4348, acc-0.8794, test loss-0.4387, acc-0.8764\n",
      "Iter-56310, train loss-0.3905, acc-0.9000, valid loss-0.4348, acc-0.8794, test loss-0.4387, acc-0.8765\n",
      "Iter-56320, train loss-0.5279, acc-0.8600, valid loss-0.4347, acc-0.8796, test loss-0.4386, acc-0.8766\n",
      "Iter-56330, train loss-0.4026, acc-0.8600, valid loss-0.4347, acc-0.8796, test loss-0.4386, acc-0.8766\n",
      "Iter-56340, train loss-0.3539, acc-0.9200, valid loss-0.4346, acc-0.8796, test loss-0.4386, acc-0.8766\n",
      "Iter-56350, train loss-0.3266, acc-0.8800, valid loss-0.4346, acc-0.8794, test loss-0.4385, acc-0.8765\n",
      "Iter-56360, train loss-0.4598, acc-0.8400, valid loss-0.4346, acc-0.8796, test loss-0.4385, acc-0.8764\n",
      "Iter-56370, train loss-0.5320, acc-0.8200, valid loss-0.4345, acc-0.8794, test loss-0.4385, acc-0.8764\n",
      "Iter-56380, train loss-0.2502, acc-0.9600, valid loss-0.4345, acc-0.8796, test loss-0.4384, acc-0.8765\n",
      "Iter-56390, train loss-0.3746, acc-0.9000, valid loss-0.4345, acc-0.8794, test loss-0.4384, acc-0.8765\n",
      "Iter-56400, train loss-0.5392, acc-0.8600, valid loss-0.4344, acc-0.8794, test loss-0.4384, acc-0.8765\n",
      "Iter-56410, train loss-0.4867, acc-0.8800, valid loss-0.4344, acc-0.8796, test loss-0.4384, acc-0.8765\n",
      "Iter-56420, train loss-0.3780, acc-0.9000, valid loss-0.4344, acc-0.8796, test loss-0.4383, acc-0.8765\n",
      "Iter-56430, train loss-0.4266, acc-0.8600, valid loss-0.4343, acc-0.8794, test loss-0.4383, acc-0.8766\n",
      "Iter-56440, train loss-0.4568, acc-0.8800, valid loss-0.4343, acc-0.8796, test loss-0.4382, acc-0.8765\n",
      "Iter-56450, train loss-0.4351, acc-0.9400, valid loss-0.4343, acc-0.8796, test loss-0.4382, acc-0.8765\n",
      "Iter-56460, train loss-0.3814, acc-0.9200, valid loss-0.4343, acc-0.8796, test loss-0.4382, acc-0.8765\n",
      "Iter-56470, train loss-0.3289, acc-0.9000, valid loss-0.4342, acc-0.8798, test loss-0.4382, acc-0.8765\n",
      "Iter-56480, train loss-0.4232, acc-0.8800, valid loss-0.4342, acc-0.8798, test loss-0.4381, acc-0.8765\n",
      "Iter-56490, train loss-0.5324, acc-0.7800, valid loss-0.4342, acc-0.8800, test loss-0.4381, acc-0.8765\n",
      "Iter-56500, train loss-0.3841, acc-0.9400, valid loss-0.4341, acc-0.8800, test loss-0.4380, acc-0.8765\n",
      "Iter-56510, train loss-0.4948, acc-0.8400, valid loss-0.4341, acc-0.8802, test loss-0.4380, acc-0.8766\n",
      "Iter-56520, train loss-0.7074, acc-0.8000, valid loss-0.4341, acc-0.8802, test loss-0.4380, acc-0.8765\n",
      "Iter-56530, train loss-0.3399, acc-0.8800, valid loss-0.4340, acc-0.8802, test loss-0.4379, acc-0.8765\n",
      "Iter-56540, train loss-0.4316, acc-0.9000, valid loss-0.4340, acc-0.8800, test loss-0.4379, acc-0.8765\n",
      "Iter-56550, train loss-0.2773, acc-0.9000, valid loss-0.4339, acc-0.8798, test loss-0.4379, acc-0.8765\n",
      "Iter-56560, train loss-0.4193, acc-0.9000, valid loss-0.4339, acc-0.8798, test loss-0.4378, acc-0.8765\n",
      "Iter-56570, train loss-0.6359, acc-0.8000, valid loss-0.4339, acc-0.8800, test loss-0.4378, acc-0.8765\n",
      "Iter-56580, train loss-0.2665, acc-0.9400, valid loss-0.4338, acc-0.8798, test loss-0.4378, acc-0.8765\n",
      "Iter-56590, train loss-0.4229, acc-0.8800, valid loss-0.4338, acc-0.8800, test loss-0.4377, acc-0.8765\n",
      "Iter-56600, train loss-0.4387, acc-0.9000, valid loss-0.4338, acc-0.8802, test loss-0.4377, acc-0.8765\n",
      "Iter-56610, train loss-0.4813, acc-0.8600, valid loss-0.4337, acc-0.8802, test loss-0.4376, acc-0.8765\n",
      "Iter-56620, train loss-0.4389, acc-0.9000, valid loss-0.4337, acc-0.8802, test loss-0.4376, acc-0.8765\n",
      "Iter-56630, train loss-0.4279, acc-0.8600, valid loss-0.4337, acc-0.8798, test loss-0.4376, acc-0.8767\n",
      "Iter-56640, train loss-0.4490, acc-0.8400, valid loss-0.4336, acc-0.8800, test loss-0.4375, acc-0.8767\n",
      "Iter-56650, train loss-0.6762, acc-0.8000, valid loss-0.4336, acc-0.8802, test loss-0.4375, acc-0.8766\n",
      "Iter-56660, train loss-0.3821, acc-0.8800, valid loss-0.4336, acc-0.8802, test loss-0.4374, acc-0.8766\n",
      "Iter-56670, train loss-0.5809, acc-0.8200, valid loss-0.4336, acc-0.8802, test loss-0.4374, acc-0.8766\n",
      "Iter-56680, train loss-0.4473, acc-0.9000, valid loss-0.4335, acc-0.8802, test loss-0.4374, acc-0.8765\n",
      "Iter-56690, train loss-0.3823, acc-0.8800, valid loss-0.4335, acc-0.8802, test loss-0.4374, acc-0.8765\n",
      "Iter-56700, train loss-0.4108, acc-0.8800, valid loss-0.4334, acc-0.8804, test loss-0.4373, acc-0.8764\n",
      "Iter-56710, train loss-0.2500, acc-0.9600, valid loss-0.4334, acc-0.8804, test loss-0.4373, acc-0.8764\n",
      "Iter-56720, train loss-0.4501, acc-0.8600, valid loss-0.4334, acc-0.8804, test loss-0.4373, acc-0.8764\n",
      "Iter-56730, train loss-0.5131, acc-0.8400, valid loss-0.4333, acc-0.8804, test loss-0.4372, acc-0.8764\n",
      "Iter-56740, train loss-0.3571, acc-0.9000, valid loss-0.4333, acc-0.8802, test loss-0.4372, acc-0.8765\n",
      "Iter-56750, train loss-0.4070, acc-0.8800, valid loss-0.4333, acc-0.8800, test loss-0.4372, acc-0.8766\n",
      "Iter-56760, train loss-0.5652, acc-0.8400, valid loss-0.4332, acc-0.8802, test loss-0.4372, acc-0.8767\n",
      "Iter-56770, train loss-0.4185, acc-0.9000, valid loss-0.4332, acc-0.8800, test loss-0.4371, acc-0.8767\n",
      "Iter-56780, train loss-0.4308, acc-0.7800, valid loss-0.4332, acc-0.8800, test loss-0.4371, acc-0.8768\n",
      "Iter-56790, train loss-0.2598, acc-0.9400, valid loss-0.4331, acc-0.8800, test loss-0.4371, acc-0.8768\n",
      "Iter-56800, train loss-0.4298, acc-0.8800, valid loss-0.4331, acc-0.8804, test loss-0.4370, acc-0.8766\n",
      "Iter-56810, train loss-0.2710, acc-0.9600, valid loss-0.4331, acc-0.8806, test loss-0.4370, acc-0.8767\n",
      "Iter-56820, train loss-0.4503, acc-0.8800, valid loss-0.4330, acc-0.8804, test loss-0.4370, acc-0.8768\n",
      "Iter-56830, train loss-0.4870, acc-0.8600, valid loss-0.4330, acc-0.8800, test loss-0.4369, acc-0.8768\n",
      "Iter-56840, train loss-0.4870, acc-0.8600, valid loss-0.4330, acc-0.8806, test loss-0.4369, acc-0.8769\n",
      "Iter-56850, train loss-0.5737, acc-0.8600, valid loss-0.4329, acc-0.8804, test loss-0.4368, acc-0.8768\n",
      "Iter-56860, train loss-0.5161, acc-0.8600, valid loss-0.4329, acc-0.8804, test loss-0.4368, acc-0.8768\n",
      "Iter-56870, train loss-0.3989, acc-0.9200, valid loss-0.4329, acc-0.8804, test loss-0.4368, acc-0.8768\n",
      "Iter-56880, train loss-0.6148, acc-0.8800, valid loss-0.4328, acc-0.8806, test loss-0.4368, acc-0.8767\n",
      "Iter-56890, train loss-0.5453, acc-0.9200, valid loss-0.4328, acc-0.8808, test loss-0.4367, acc-0.8769\n",
      "Iter-56900, train loss-0.3750, acc-0.9000, valid loss-0.4328, acc-0.8804, test loss-0.4367, acc-0.8768\n",
      "Iter-56910, train loss-0.3627, acc-0.9200, valid loss-0.4328, acc-0.8806, test loss-0.4367, acc-0.8768\n",
      "Iter-56920, train loss-0.3078, acc-0.9200, valid loss-0.4327, acc-0.8806, test loss-0.4367, acc-0.8769\n",
      "Iter-56930, train loss-0.5937, acc-0.8200, valid loss-0.4327, acc-0.8804, test loss-0.4366, acc-0.8771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56940, train loss-0.5976, acc-0.8200, valid loss-0.4326, acc-0.8810, test loss-0.4366, acc-0.8771\n",
      "Iter-56950, train loss-0.4472, acc-0.8600, valid loss-0.4326, acc-0.8810, test loss-0.4366, acc-0.8772\n",
      "Iter-56960, train loss-0.5433, acc-0.8400, valid loss-0.4326, acc-0.8808, test loss-0.4365, acc-0.8772\n",
      "Iter-56970, train loss-0.4195, acc-0.8600, valid loss-0.4325, acc-0.8808, test loss-0.4365, acc-0.8771\n",
      "Iter-56980, train loss-0.3141, acc-0.9400, valid loss-0.4325, acc-0.8808, test loss-0.4365, acc-0.8771\n",
      "Iter-56990, train loss-0.4550, acc-0.8800, valid loss-0.4325, acc-0.8808, test loss-0.4364, acc-0.8771\n",
      "Iter-57000, train loss-0.4836, acc-0.8600, valid loss-0.4324, acc-0.8810, test loss-0.4364, acc-0.8771\n",
      "Iter-57010, train loss-0.3009, acc-0.9600, valid loss-0.4324, acc-0.8810, test loss-0.4364, acc-0.8771\n",
      "Iter-57020, train loss-0.4687, acc-0.8800, valid loss-0.4324, acc-0.8810, test loss-0.4363, acc-0.8771\n",
      "Iter-57030, train loss-0.2438, acc-0.9600, valid loss-0.4323, acc-0.8810, test loss-0.4363, acc-0.8771\n",
      "Iter-57040, train loss-0.3876, acc-0.9200, valid loss-0.4323, acc-0.8810, test loss-0.4362, acc-0.8771\n",
      "Iter-57050, train loss-0.3454, acc-0.8800, valid loss-0.4322, acc-0.8810, test loss-0.4362, acc-0.8773\n",
      "Iter-57060, train loss-0.2717, acc-0.9400, valid loss-0.4322, acc-0.8810, test loss-0.4362, acc-0.8773\n",
      "Iter-57070, train loss-0.5328, acc-0.8200, valid loss-0.4322, acc-0.8808, test loss-0.4361, acc-0.8772\n",
      "Iter-57080, train loss-0.4499, acc-0.8800, valid loss-0.4321, acc-0.8808, test loss-0.4361, acc-0.8772\n",
      "Iter-57090, train loss-0.4775, acc-0.8200, valid loss-0.4321, acc-0.8810, test loss-0.4361, acc-0.8773\n",
      "Iter-57100, train loss-0.5271, acc-0.8200, valid loss-0.4321, acc-0.8808, test loss-0.4360, acc-0.8773\n",
      "Iter-57110, train loss-0.4561, acc-0.8800, valid loss-0.4320, acc-0.8810, test loss-0.4360, acc-0.8775\n",
      "Iter-57120, train loss-0.4267, acc-0.8600, valid loss-0.4320, acc-0.8810, test loss-0.4360, acc-0.8776\n",
      "Iter-57130, train loss-0.3951, acc-0.9000, valid loss-0.4320, acc-0.8810, test loss-0.4359, acc-0.8774\n",
      "Iter-57140, train loss-0.5048, acc-0.8800, valid loss-0.4320, acc-0.8812, test loss-0.4359, acc-0.8774\n",
      "Iter-57150, train loss-0.5398, acc-0.8200, valid loss-0.4319, acc-0.8808, test loss-0.4359, acc-0.8774\n",
      "Iter-57160, train loss-0.5463, acc-0.8400, valid loss-0.4319, acc-0.8810, test loss-0.4358, acc-0.8775\n",
      "Iter-57170, train loss-0.3900, acc-0.9000, valid loss-0.4318, acc-0.8808, test loss-0.4358, acc-0.8776\n",
      "Iter-57180, train loss-0.6039, acc-0.8000, valid loss-0.4318, acc-0.8810, test loss-0.4358, acc-0.8775\n",
      "Iter-57190, train loss-0.3222, acc-0.9000, valid loss-0.4318, acc-0.8812, test loss-0.4357, acc-0.8775\n",
      "Iter-57200, train loss-0.4761, acc-0.8800, valid loss-0.4317, acc-0.8808, test loss-0.4357, acc-0.8775\n",
      "Iter-57210, train loss-0.4310, acc-0.8600, valid loss-0.4317, acc-0.8810, test loss-0.4357, acc-0.8776\n",
      "Iter-57220, train loss-0.3517, acc-0.9000, valid loss-0.4317, acc-0.8812, test loss-0.4356, acc-0.8776\n",
      "Iter-57230, train loss-0.2336, acc-0.9400, valid loss-0.4317, acc-0.8814, test loss-0.4356, acc-0.8775\n",
      "Iter-57240, train loss-0.3790, acc-0.9200, valid loss-0.4316, acc-0.8814, test loss-0.4356, acc-0.8775\n",
      "Iter-57250, train loss-0.5549, acc-0.8200, valid loss-0.4316, acc-0.8814, test loss-0.4355, acc-0.8776\n",
      "Iter-57260, train loss-0.7985, acc-0.8200, valid loss-0.4316, acc-0.8814, test loss-0.4355, acc-0.8776\n",
      "Iter-57270, train loss-0.3945, acc-0.9200, valid loss-0.4315, acc-0.8812, test loss-0.4355, acc-0.8776\n",
      "Iter-57280, train loss-0.4206, acc-0.8200, valid loss-0.4315, acc-0.8812, test loss-0.4354, acc-0.8776\n",
      "Iter-57290, train loss-0.4993, acc-0.8600, valid loss-0.4315, acc-0.8812, test loss-0.4354, acc-0.8776\n",
      "Iter-57300, train loss-0.5061, acc-0.8400, valid loss-0.4314, acc-0.8812, test loss-0.4353, acc-0.8775\n",
      "Iter-57310, train loss-0.6046, acc-0.8600, valid loss-0.4314, acc-0.8812, test loss-0.4353, acc-0.8775\n",
      "Iter-57320, train loss-0.5010, acc-0.8400, valid loss-0.4314, acc-0.8812, test loss-0.4353, acc-0.8776\n",
      "Iter-57330, train loss-0.2157, acc-0.9800, valid loss-0.4313, acc-0.8814, test loss-0.4352, acc-0.8775\n",
      "Iter-57340, train loss-0.3799, acc-0.9000, valid loss-0.4313, acc-0.8814, test loss-0.4352, acc-0.8775\n",
      "Iter-57350, train loss-0.5698, acc-0.8000, valid loss-0.4313, acc-0.8814, test loss-0.4352, acc-0.8775\n",
      "Iter-57360, train loss-0.4084, acc-0.8800, valid loss-0.4313, acc-0.8814, test loss-0.4352, acc-0.8775\n",
      "Iter-57370, train loss-0.6616, acc-0.8200, valid loss-0.4312, acc-0.8814, test loss-0.4351, acc-0.8776\n",
      "Iter-57380, train loss-0.3361, acc-0.9000, valid loss-0.4312, acc-0.8814, test loss-0.4351, acc-0.8776\n",
      "Iter-57390, train loss-0.4343, acc-0.8800, valid loss-0.4311, acc-0.8814, test loss-0.4351, acc-0.8776\n",
      "Iter-57400, train loss-0.4953, acc-0.8600, valid loss-0.4311, acc-0.8818, test loss-0.4350, acc-0.8776\n",
      "Iter-57410, train loss-0.7902, acc-0.7800, valid loss-0.4311, acc-0.8818, test loss-0.4350, acc-0.8777\n",
      "Iter-57420, train loss-0.5792, acc-0.8400, valid loss-0.4311, acc-0.8818, test loss-0.4350, acc-0.8775\n",
      "Iter-57430, train loss-0.4825, acc-0.8400, valid loss-0.4310, acc-0.8818, test loss-0.4349, acc-0.8775\n",
      "Iter-57440, train loss-0.3822, acc-0.8600, valid loss-0.4310, acc-0.8818, test loss-0.4349, acc-0.8776\n",
      "Iter-57450, train loss-0.4015, acc-0.9000, valid loss-0.4309, acc-0.8818, test loss-0.4349, acc-0.8776\n",
      "Iter-57460, train loss-0.5858, acc-0.8200, valid loss-0.4309, acc-0.8818, test loss-0.4348, acc-0.8776\n",
      "Iter-57470, train loss-0.4127, acc-0.8800, valid loss-0.4309, acc-0.8818, test loss-0.4348, acc-0.8776\n",
      "Iter-57480, train loss-0.3738, acc-0.8600, valid loss-0.4308, acc-0.8818, test loss-0.4348, acc-0.8777\n",
      "Iter-57490, train loss-0.3430, acc-0.9000, valid loss-0.4308, acc-0.8818, test loss-0.4348, acc-0.8776\n",
      "Iter-57500, train loss-0.3884, acc-0.9200, valid loss-0.4308, acc-0.8820, test loss-0.4347, acc-0.8776\n",
      "Iter-57510, train loss-0.5907, acc-0.7800, valid loss-0.4307, acc-0.8820, test loss-0.4347, acc-0.8777\n",
      "Iter-57520, train loss-0.3450, acc-0.9200, valid loss-0.4307, acc-0.8820, test loss-0.4347, acc-0.8778\n",
      "Iter-57530, train loss-0.3859, acc-0.8800, valid loss-0.4307, acc-0.8818, test loss-0.4346, acc-0.8777\n",
      "Iter-57540, train loss-0.3965, acc-0.9200, valid loss-0.4306, acc-0.8820, test loss-0.4346, acc-0.8776\n",
      "Iter-57550, train loss-0.8878, acc-0.7600, valid loss-0.4306, acc-0.8818, test loss-0.4345, acc-0.8778\n",
      "Iter-57560, train loss-0.5808, acc-0.7800, valid loss-0.4306, acc-0.8818, test loss-0.4345, acc-0.8777\n",
      "Iter-57570, train loss-0.3975, acc-0.9000, valid loss-0.4305, acc-0.8820, test loss-0.4345, acc-0.8777\n",
      "Iter-57580, train loss-0.4774, acc-0.8800, valid loss-0.4305, acc-0.8818, test loss-0.4345, acc-0.8777\n",
      "Iter-57590, train loss-0.4378, acc-0.8800, valid loss-0.4305, acc-0.8818, test loss-0.4344, acc-0.8777\n",
      "Iter-57600, train loss-0.5740, acc-0.8600, valid loss-0.4304, acc-0.8816, test loss-0.4344, acc-0.8778\n",
      "Iter-57610, train loss-0.2994, acc-0.9400, valid loss-0.4304, acc-0.8818, test loss-0.4344, acc-0.8776\n",
      "Iter-57620, train loss-0.4029, acc-0.8800, valid loss-0.4304, acc-0.8818, test loss-0.4343, acc-0.8777\n",
      "Iter-57630, train loss-0.3313, acc-0.9000, valid loss-0.4303, acc-0.8818, test loss-0.4343, acc-0.8779\n",
      "Iter-57640, train loss-0.6023, acc-0.8200, valid loss-0.4303, acc-0.8816, test loss-0.4342, acc-0.8781\n",
      "Iter-57650, train loss-0.5182, acc-0.7800, valid loss-0.4302, acc-0.8816, test loss-0.4342, acc-0.8782\n",
      "Iter-57660, train loss-0.3872, acc-0.9400, valid loss-0.4302, acc-0.8814, test loss-0.4341, acc-0.8781\n",
      "Iter-57670, train loss-0.3219, acc-0.9400, valid loss-0.4302, acc-0.8814, test loss-0.4341, acc-0.8778\n",
      "Iter-57680, train loss-0.7064, acc-0.8400, valid loss-0.4301, acc-0.8816, test loss-0.4341, acc-0.8778\n",
      "Iter-57690, train loss-0.6662, acc-0.8200, valid loss-0.4301, acc-0.8816, test loss-0.4340, acc-0.8779\n",
      "Iter-57700, train loss-0.6472, acc-0.8000, valid loss-0.4301, acc-0.8818, test loss-0.4340, acc-0.8780\n",
      "Iter-57710, train loss-0.3872, acc-0.9200, valid loss-0.4300, acc-0.8816, test loss-0.4340, acc-0.8780\n",
      "Iter-57720, train loss-0.8240, acc-0.8200, valid loss-0.4300, acc-0.8816, test loss-0.4339, acc-0.8778\n",
      "Iter-57730, train loss-0.4515, acc-0.8400, valid loss-0.4300, acc-0.8816, test loss-0.4339, acc-0.8778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-57740, train loss-0.3940, acc-0.8800, valid loss-0.4299, acc-0.8816, test loss-0.4339, acc-0.8779\n",
      "Iter-57750, train loss-0.5867, acc-0.8600, valid loss-0.4299, acc-0.8816, test loss-0.4338, acc-0.8779\n",
      "Iter-57760, train loss-0.3799, acc-0.8600, valid loss-0.4299, acc-0.8818, test loss-0.4338, acc-0.8780\n",
      "Iter-57770, train loss-0.2698, acc-0.9400, valid loss-0.4298, acc-0.8818, test loss-0.4338, acc-0.8779\n",
      "Iter-57780, train loss-0.3401, acc-0.9000, valid loss-0.4298, acc-0.8818, test loss-0.4337, acc-0.8779\n",
      "Iter-57790, train loss-0.3292, acc-0.9200, valid loss-0.4298, acc-0.8818, test loss-0.4337, acc-0.8780\n",
      "Iter-57800, train loss-0.3511, acc-0.9600, valid loss-0.4297, acc-0.8818, test loss-0.4337, acc-0.8779\n",
      "Iter-57810, train loss-0.2876, acc-0.9400, valid loss-0.4297, acc-0.8820, test loss-0.4336, acc-0.8779\n",
      "Iter-57820, train loss-0.4007, acc-0.8400, valid loss-0.4297, acc-0.8820, test loss-0.4336, acc-0.8778\n",
      "Iter-57830, train loss-0.4137, acc-0.9000, valid loss-0.4296, acc-0.8820, test loss-0.4336, acc-0.8778\n",
      "Iter-57840, train loss-0.4370, acc-0.8000, valid loss-0.4296, acc-0.8820, test loss-0.4335, acc-0.8778\n",
      "Iter-57850, train loss-0.5606, acc-0.8200, valid loss-0.4296, acc-0.8820, test loss-0.4335, acc-0.8778\n",
      "Iter-57860, train loss-0.4298, acc-0.8600, valid loss-0.4295, acc-0.8818, test loss-0.4335, acc-0.8777\n",
      "Iter-57870, train loss-0.3822, acc-0.9400, valid loss-0.4295, acc-0.8820, test loss-0.4334, acc-0.8778\n",
      "Iter-57880, train loss-0.4416, acc-0.9000, valid loss-0.4294, acc-0.8820, test loss-0.4334, acc-0.8778\n",
      "Iter-57890, train loss-0.4659, acc-0.8600, valid loss-0.4294, acc-0.8820, test loss-0.4334, acc-0.8777\n",
      "Iter-57900, train loss-0.4524, acc-0.8400, valid loss-0.4293, acc-0.8818, test loss-0.4333, acc-0.8778\n",
      "Iter-57910, train loss-0.3782, acc-0.9200, valid loss-0.4293, acc-0.8818, test loss-0.4333, acc-0.8780\n",
      "Iter-57920, train loss-0.2490, acc-0.9400, valid loss-0.4293, acc-0.8818, test loss-0.4333, acc-0.8780\n",
      "Iter-57930, train loss-0.6112, acc-0.8800, valid loss-0.4292, acc-0.8816, test loss-0.4332, acc-0.8781\n",
      "Iter-57940, train loss-0.5454, acc-0.8200, valid loss-0.4292, acc-0.8818, test loss-0.4332, acc-0.8781\n",
      "Iter-57950, train loss-0.5083, acc-0.8400, valid loss-0.4292, acc-0.8818, test loss-0.4332, acc-0.8780\n",
      "Iter-57960, train loss-0.3626, acc-0.8800, valid loss-0.4291, acc-0.8818, test loss-0.4331, acc-0.8781\n",
      "Iter-57970, train loss-0.4744, acc-0.9200, valid loss-0.4291, acc-0.8818, test loss-0.4331, acc-0.8779\n",
      "Iter-57980, train loss-0.5676, acc-0.8600, valid loss-0.4291, acc-0.8818, test loss-0.4331, acc-0.8780\n",
      "Iter-57990, train loss-0.6294, acc-0.8000, valid loss-0.4291, acc-0.8818, test loss-0.4330, acc-0.8780\n",
      "Iter-58000, train loss-0.3404, acc-0.9000, valid loss-0.4291, acc-0.8818, test loss-0.4330, acc-0.8781\n",
      "Iter-58010, train loss-0.5684, acc-0.7800, valid loss-0.4290, acc-0.8818, test loss-0.4330, acc-0.8781\n",
      "Iter-58020, train loss-0.5148, acc-0.8800, valid loss-0.4290, acc-0.8818, test loss-0.4330, acc-0.8781\n",
      "Iter-58030, train loss-0.5819, acc-0.8600, valid loss-0.4290, acc-0.8814, test loss-0.4329, acc-0.8781\n",
      "Iter-58040, train loss-0.3843, acc-0.9000, valid loss-0.4289, acc-0.8818, test loss-0.4329, acc-0.8781\n",
      "Iter-58050, train loss-0.4724, acc-0.8600, valid loss-0.4289, acc-0.8816, test loss-0.4329, acc-0.8781\n",
      "Iter-58060, train loss-0.5099, acc-0.8600, valid loss-0.4289, acc-0.8820, test loss-0.4328, acc-0.8780\n",
      "Iter-58070, train loss-0.5730, acc-0.7800, valid loss-0.4288, acc-0.8818, test loss-0.4328, acc-0.8780\n",
      "Iter-58080, train loss-0.4083, acc-0.8600, valid loss-0.4288, acc-0.8816, test loss-0.4328, acc-0.8781\n",
      "Iter-58090, train loss-0.4391, acc-0.8800, valid loss-0.4288, acc-0.8816, test loss-0.4327, acc-0.8781\n",
      "Iter-58100, train loss-0.4705, acc-0.9200, valid loss-0.4287, acc-0.8816, test loss-0.4327, acc-0.8782\n",
      "Iter-58110, train loss-0.2890, acc-0.9200, valid loss-0.4287, acc-0.8816, test loss-0.4327, acc-0.8782\n",
      "Iter-58120, train loss-0.3354, acc-0.9000, valid loss-0.4287, acc-0.8818, test loss-0.4326, acc-0.8782\n",
      "Iter-58130, train loss-0.5078, acc-0.8600, valid loss-0.4286, acc-0.8818, test loss-0.4326, acc-0.8783\n",
      "Iter-58140, train loss-0.3295, acc-0.8800, valid loss-0.4286, acc-0.8818, test loss-0.4326, acc-0.8783\n",
      "Iter-58150, train loss-0.6570, acc-0.8000, valid loss-0.4285, acc-0.8820, test loss-0.4326, acc-0.8783\n",
      "Iter-58160, train loss-0.7048, acc-0.7600, valid loss-0.4285, acc-0.8820, test loss-0.4325, acc-0.8782\n",
      "Iter-58170, train loss-0.6416, acc-0.8400, valid loss-0.4285, acc-0.8820, test loss-0.4325, acc-0.8783\n",
      "Iter-58180, train loss-0.3946, acc-0.9000, valid loss-0.4284, acc-0.8820, test loss-0.4325, acc-0.8784\n",
      "Iter-58190, train loss-0.3674, acc-0.8800, valid loss-0.4284, acc-0.8818, test loss-0.4324, acc-0.8784\n",
      "Iter-58200, train loss-0.2964, acc-0.9600, valid loss-0.4284, acc-0.8820, test loss-0.4324, acc-0.8785\n",
      "Iter-58210, train loss-0.4228, acc-0.9200, valid loss-0.4283, acc-0.8820, test loss-0.4324, acc-0.8784\n",
      "Iter-58220, train loss-0.4525, acc-0.9000, valid loss-0.4283, acc-0.8820, test loss-0.4323, acc-0.8784\n",
      "Iter-58230, train loss-0.3812, acc-0.8800, valid loss-0.4283, acc-0.8820, test loss-0.4323, acc-0.8784\n",
      "Iter-58240, train loss-0.2778, acc-0.9200, valid loss-0.4283, acc-0.8822, test loss-0.4323, acc-0.8784\n",
      "Iter-58250, train loss-0.3907, acc-0.9200, valid loss-0.4282, acc-0.8822, test loss-0.4323, acc-0.8784\n",
      "Iter-58260, train loss-0.5657, acc-0.8400, valid loss-0.4282, acc-0.8822, test loss-0.4322, acc-0.8784\n",
      "Iter-58270, train loss-0.4959, acc-0.8800, valid loss-0.4282, acc-0.8820, test loss-0.4322, acc-0.8785\n",
      "Iter-58280, train loss-0.5801, acc-0.8000, valid loss-0.4281, acc-0.8822, test loss-0.4322, acc-0.8785\n",
      "Iter-58290, train loss-0.4776, acc-0.9000, valid loss-0.4281, acc-0.8820, test loss-0.4321, acc-0.8786\n",
      "Iter-58300, train loss-0.3678, acc-0.9200, valid loss-0.4281, acc-0.8822, test loss-0.4321, acc-0.8786\n",
      "Iter-58310, train loss-0.6032, acc-0.8400, valid loss-0.4280, acc-0.8822, test loss-0.4321, acc-0.8786\n",
      "Iter-58320, train loss-0.3923, acc-0.8800, valid loss-0.4280, acc-0.8822, test loss-0.4320, acc-0.8786\n",
      "Iter-58330, train loss-0.3988, acc-0.9000, valid loss-0.4280, acc-0.8822, test loss-0.4320, acc-0.8786\n",
      "Iter-58340, train loss-0.6394, acc-0.8400, valid loss-0.4280, acc-0.8822, test loss-0.4320, acc-0.8786\n",
      "Iter-58350, train loss-0.4873, acc-0.8200, valid loss-0.4279, acc-0.8822, test loss-0.4319, acc-0.8787\n",
      "Iter-58360, train loss-0.3159, acc-0.9000, valid loss-0.4279, acc-0.8822, test loss-0.4319, acc-0.8787\n",
      "Iter-58370, train loss-0.4759, acc-0.8200, valid loss-0.4279, acc-0.8822, test loss-0.4319, acc-0.8787\n",
      "Iter-58380, train loss-0.6257, acc-0.8400, valid loss-0.4278, acc-0.8822, test loss-0.4318, acc-0.8787\n",
      "Iter-58390, train loss-0.4463, acc-0.8800, valid loss-0.4278, acc-0.8822, test loss-0.4318, acc-0.8788\n",
      "Iter-58400, train loss-0.6059, acc-0.8600, valid loss-0.4278, acc-0.8822, test loss-0.4318, acc-0.8788\n",
      "Iter-58410, train loss-0.3492, acc-0.9400, valid loss-0.4278, acc-0.8822, test loss-0.4317, acc-0.8788\n",
      "Iter-58420, train loss-0.4212, acc-0.8400, valid loss-0.4277, acc-0.8822, test loss-0.4317, acc-0.8788\n",
      "Iter-58430, train loss-0.3227, acc-0.9200, valid loss-0.4277, acc-0.8822, test loss-0.4317, acc-0.8788\n",
      "Iter-58440, train loss-0.4268, acc-0.8800, valid loss-0.4277, acc-0.8822, test loss-0.4316, acc-0.8788\n",
      "Iter-58450, train loss-0.3464, acc-0.9000, valid loss-0.4276, acc-0.8822, test loss-0.4316, acc-0.8788\n",
      "Iter-58460, train loss-0.2745, acc-0.9200, valid loss-0.4276, acc-0.8822, test loss-0.4316, acc-0.8787\n",
      "Iter-58470, train loss-0.4382, acc-0.8600, valid loss-0.4276, acc-0.8822, test loss-0.4315, acc-0.8787\n",
      "Iter-58480, train loss-0.3731, acc-0.9000, valid loss-0.4275, acc-0.8822, test loss-0.4315, acc-0.8787\n",
      "Iter-58490, train loss-0.5663, acc-0.8400, valid loss-0.4275, acc-0.8822, test loss-0.4315, acc-0.8787\n",
      "Iter-58500, train loss-0.7541, acc-0.7400, valid loss-0.4275, acc-0.8822, test loss-0.4314, acc-0.8786\n",
      "Iter-58510, train loss-0.4719, acc-0.8600, valid loss-0.4274, acc-0.8822, test loss-0.4314, acc-0.8786\n",
      "Iter-58520, train loss-0.4534, acc-0.9200, valid loss-0.4274, acc-0.8822, test loss-0.4314, acc-0.8786\n",
      "Iter-58530, train loss-0.3508, acc-0.8800, valid loss-0.4274, acc-0.8822, test loss-0.4313, acc-0.8786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-58540, train loss-0.7001, acc-0.7600, valid loss-0.4273, acc-0.8820, test loss-0.4313, acc-0.8787\n",
      "Iter-58550, train loss-0.3198, acc-0.9200, valid loss-0.4273, acc-0.8820, test loss-0.4313, acc-0.8787\n",
      "Iter-58560, train loss-0.4727, acc-0.8800, valid loss-0.4273, acc-0.8822, test loss-0.4312, acc-0.8787\n",
      "Iter-58570, train loss-0.3446, acc-0.9600, valid loss-0.4272, acc-0.8822, test loss-0.4312, acc-0.8787\n",
      "Iter-58580, train loss-0.3913, acc-0.8600, valid loss-0.4272, acc-0.8824, test loss-0.4312, acc-0.8786\n",
      "Iter-58590, train loss-0.5171, acc-0.8400, valid loss-0.4272, acc-0.8824, test loss-0.4311, acc-0.8787\n",
      "Iter-58600, train loss-0.5797, acc-0.8600, valid loss-0.4271, acc-0.8824, test loss-0.4311, acc-0.8787\n",
      "Iter-58610, train loss-0.4191, acc-0.9200, valid loss-0.4271, acc-0.8824, test loss-0.4311, acc-0.8788\n",
      "Iter-58620, train loss-0.3507, acc-0.9000, valid loss-0.4271, acc-0.8824, test loss-0.4310, acc-0.8790\n",
      "Iter-58630, train loss-0.2958, acc-0.9600, valid loss-0.4270, acc-0.8824, test loss-0.4310, acc-0.8788\n",
      "Iter-58640, train loss-0.3669, acc-0.9200, valid loss-0.4270, acc-0.8824, test loss-0.4310, acc-0.8787\n",
      "Iter-58650, train loss-0.2678, acc-0.9000, valid loss-0.4270, acc-0.8824, test loss-0.4309, acc-0.8787\n",
      "Iter-58660, train loss-0.4613, acc-0.8400, valid loss-0.4270, acc-0.8824, test loss-0.4309, acc-0.8786\n",
      "Iter-58670, train loss-0.5003, acc-0.8800, valid loss-0.4269, acc-0.8824, test loss-0.4309, acc-0.8789\n",
      "Iter-58680, train loss-0.3846, acc-0.8400, valid loss-0.4269, acc-0.8824, test loss-0.4309, acc-0.8788\n",
      "Iter-58690, train loss-0.5606, acc-0.8400, valid loss-0.4268, acc-0.8824, test loss-0.4308, acc-0.8791\n",
      "Iter-58700, train loss-0.8224, acc-0.7600, valid loss-0.4268, acc-0.8824, test loss-0.4308, acc-0.8791\n",
      "Iter-58710, train loss-0.4769, acc-0.8800, valid loss-0.4268, acc-0.8824, test loss-0.4308, acc-0.8790\n",
      "Iter-58720, train loss-0.3805, acc-0.8600, valid loss-0.4267, acc-0.8824, test loss-0.4307, acc-0.8790\n",
      "Iter-58730, train loss-0.6861, acc-0.8200, valid loss-0.4267, acc-0.8824, test loss-0.4307, acc-0.8791\n",
      "Iter-58740, train loss-0.5860, acc-0.8200, valid loss-0.4267, acc-0.8822, test loss-0.4307, acc-0.8790\n",
      "Iter-58750, train loss-0.3916, acc-0.9200, valid loss-0.4266, acc-0.8822, test loss-0.4306, acc-0.8789\n",
      "Iter-58760, train loss-0.3395, acc-0.9000, valid loss-0.4266, acc-0.8822, test loss-0.4306, acc-0.8790\n",
      "Iter-58770, train loss-0.2234, acc-0.9600, valid loss-0.4266, acc-0.8822, test loss-0.4306, acc-0.8790\n",
      "Iter-58780, train loss-0.3968, acc-0.8600, valid loss-0.4266, acc-0.8820, test loss-0.4305, acc-0.8790\n",
      "Iter-58790, train loss-0.4575, acc-0.8600, valid loss-0.4265, acc-0.8822, test loss-0.4305, acc-0.8790\n",
      "Iter-58800, train loss-0.6682, acc-0.7800, valid loss-0.4265, acc-0.8822, test loss-0.4305, acc-0.8792\n",
      "Iter-58810, train loss-0.4995, acc-0.8600, valid loss-0.4265, acc-0.8822, test loss-0.4304, acc-0.8789\n",
      "Iter-58820, train loss-0.5227, acc-0.8000, valid loss-0.4265, acc-0.8820, test loss-0.4304, acc-0.8790\n",
      "Iter-58830, train loss-0.3318, acc-0.8800, valid loss-0.4264, acc-0.8822, test loss-0.4304, acc-0.8790\n",
      "Iter-58840, train loss-0.3974, acc-0.8400, valid loss-0.4264, acc-0.8820, test loss-0.4303, acc-0.8790\n",
      "Iter-58850, train loss-0.4356, acc-0.9000, valid loss-0.4263, acc-0.8822, test loss-0.4303, acc-0.8791\n",
      "Iter-58860, train loss-0.3318, acc-0.9200, valid loss-0.4263, acc-0.8822, test loss-0.4303, acc-0.8792\n",
      "Iter-58870, train loss-0.4551, acc-0.8600, valid loss-0.4263, acc-0.8822, test loss-0.4302, acc-0.8792\n",
      "Iter-58880, train loss-0.5211, acc-0.8400, valid loss-0.4262, acc-0.8822, test loss-0.4302, acc-0.8793\n",
      "Iter-58890, train loss-0.5219, acc-0.8600, valid loss-0.4262, acc-0.8822, test loss-0.4301, acc-0.8793\n",
      "Iter-58900, train loss-0.3167, acc-0.9200, valid loss-0.4262, acc-0.8824, test loss-0.4301, acc-0.8795\n",
      "Iter-58910, train loss-0.2976, acc-0.9400, valid loss-0.4261, acc-0.8824, test loss-0.4301, acc-0.8795\n",
      "Iter-58920, train loss-0.3613, acc-0.9000, valid loss-0.4261, acc-0.8824, test loss-0.4300, acc-0.8794\n",
      "Iter-58930, train loss-0.3916, acc-0.9200, valid loss-0.4261, acc-0.8824, test loss-0.4300, acc-0.8794\n",
      "Iter-58940, train loss-0.5478, acc-0.8400, valid loss-0.4261, acc-0.8824, test loss-0.4300, acc-0.8796\n",
      "Iter-58950, train loss-0.4006, acc-0.8600, valid loss-0.4260, acc-0.8824, test loss-0.4299, acc-0.8796\n",
      "Iter-58960, train loss-0.4389, acc-0.8600, valid loss-0.4260, acc-0.8826, test loss-0.4299, acc-0.8795\n",
      "Iter-58970, train loss-0.3433, acc-0.9200, valid loss-0.4260, acc-0.8826, test loss-0.4299, acc-0.8796\n",
      "Iter-58980, train loss-0.8206, acc-0.7200, valid loss-0.4259, acc-0.8826, test loss-0.4298, acc-0.8794\n",
      "Iter-58990, train loss-0.5042, acc-0.8400, valid loss-0.4259, acc-0.8826, test loss-0.4298, acc-0.8794\n",
      "Iter-59000, train loss-0.4392, acc-0.8800, valid loss-0.4259, acc-0.8824, test loss-0.4298, acc-0.8796\n",
      "Iter-59010, train loss-0.4147, acc-0.8800, valid loss-0.4258, acc-0.8826, test loss-0.4297, acc-0.8796\n",
      "Iter-59020, train loss-0.5460, acc-0.8400, valid loss-0.4258, acc-0.8826, test loss-0.4297, acc-0.8795\n",
      "Iter-59030, train loss-0.3794, acc-0.9200, valid loss-0.4258, acc-0.8824, test loss-0.4297, acc-0.8796\n",
      "Iter-59040, train loss-0.4379, acc-0.9000, valid loss-0.4257, acc-0.8824, test loss-0.4296, acc-0.8797\n",
      "Iter-59050, train loss-0.3788, acc-0.9000, valid loss-0.4257, acc-0.8824, test loss-0.4296, acc-0.8796\n",
      "Iter-59060, train loss-0.5982, acc-0.8200, valid loss-0.4257, acc-0.8824, test loss-0.4296, acc-0.8796\n",
      "Iter-59070, train loss-0.3721, acc-0.9200, valid loss-0.4256, acc-0.8822, test loss-0.4295, acc-0.8796\n",
      "Iter-59080, train loss-0.3955, acc-0.9200, valid loss-0.4256, acc-0.8822, test loss-0.4295, acc-0.8795\n",
      "Iter-59090, train loss-0.6073, acc-0.8200, valid loss-0.4256, acc-0.8822, test loss-0.4295, acc-0.8796\n",
      "Iter-59100, train loss-0.4364, acc-0.8800, valid loss-0.4256, acc-0.8826, test loss-0.4294, acc-0.8798\n",
      "Iter-59110, train loss-0.5314, acc-0.8600, valid loss-0.4255, acc-0.8824, test loss-0.4294, acc-0.8799\n",
      "Iter-59120, train loss-0.4115, acc-0.9000, valid loss-0.4255, acc-0.8828, test loss-0.4294, acc-0.8797\n",
      "Iter-59130, train loss-0.3141, acc-0.9400, valid loss-0.4255, acc-0.8828, test loss-0.4293, acc-0.8798\n",
      "Iter-59140, train loss-0.4886, acc-0.8600, valid loss-0.4254, acc-0.8828, test loss-0.4293, acc-0.8798\n",
      "Iter-59150, train loss-0.4494, acc-0.8400, valid loss-0.4254, acc-0.8828, test loss-0.4293, acc-0.8797\n",
      "Iter-59160, train loss-0.3636, acc-0.9200, valid loss-0.4254, acc-0.8826, test loss-0.4292, acc-0.8799\n",
      "Iter-59170, train loss-0.3453, acc-0.9400, valid loss-0.4253, acc-0.8826, test loss-0.4292, acc-0.8799\n",
      "Iter-59180, train loss-0.3312, acc-0.9200, valid loss-0.4253, acc-0.8826, test loss-0.4292, acc-0.8799\n",
      "Iter-59190, train loss-0.4414, acc-0.8800, valid loss-0.4253, acc-0.8824, test loss-0.4291, acc-0.8800\n",
      "Iter-59200, train loss-0.3659, acc-0.9200, valid loss-0.4252, acc-0.8826, test loss-0.4291, acc-0.8800\n",
      "Iter-59210, train loss-0.3448, acc-0.8600, valid loss-0.4252, acc-0.8826, test loss-0.4291, acc-0.8800\n",
      "Iter-59220, train loss-0.4105, acc-0.8800, valid loss-0.4252, acc-0.8826, test loss-0.4291, acc-0.8798\n",
      "Iter-59230, train loss-0.3323, acc-0.9400, valid loss-0.4252, acc-0.8824, test loss-0.4290, acc-0.8798\n",
      "Iter-59240, train loss-0.3511, acc-0.8600, valid loss-0.4251, acc-0.8824, test loss-0.4290, acc-0.8797\n",
      "Iter-59250, train loss-0.4562, acc-0.8800, valid loss-0.4251, acc-0.8824, test loss-0.4290, acc-0.8797\n",
      "Iter-59260, train loss-0.5934, acc-0.8600, valid loss-0.4251, acc-0.8824, test loss-0.4290, acc-0.8798\n",
      "Iter-59270, train loss-0.3723, acc-0.8800, valid loss-0.4251, acc-0.8822, test loss-0.4289, acc-0.8799\n",
      "Iter-59280, train loss-0.7186, acc-0.8000, valid loss-0.4250, acc-0.8822, test loss-0.4289, acc-0.8798\n",
      "Iter-59290, train loss-0.3669, acc-0.9400, valid loss-0.4250, acc-0.8824, test loss-0.4289, acc-0.8798\n",
      "Iter-59300, train loss-0.2933, acc-0.9400, valid loss-0.4250, acc-0.8822, test loss-0.4288, acc-0.8798\n",
      "Iter-59310, train loss-0.3132, acc-0.9600, valid loss-0.4249, acc-0.8822, test loss-0.4288, acc-0.8798\n",
      "Iter-59320, train loss-0.4193, acc-0.9000, valid loss-0.4249, acc-0.8822, test loss-0.4288, acc-0.8800\n",
      "Iter-59330, train loss-0.3086, acc-0.9200, valid loss-0.4249, acc-0.8822, test loss-0.4287, acc-0.8800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-59340, train loss-0.4048, acc-0.8800, valid loss-0.4248, acc-0.8822, test loss-0.4287, acc-0.8800\n",
      "Iter-59350, train loss-0.3830, acc-0.9000, valid loss-0.4248, acc-0.8822, test loss-0.4287, acc-0.8799\n",
      "Iter-59360, train loss-0.4917, acc-0.8600, valid loss-0.4248, acc-0.8822, test loss-0.4286, acc-0.8801\n",
      "Iter-59370, train loss-0.5349, acc-0.8600, valid loss-0.4247, acc-0.8822, test loss-0.4286, acc-0.8801\n",
      "Iter-59380, train loss-0.3519, acc-0.9400, valid loss-0.4247, acc-0.8822, test loss-0.4286, acc-0.8800\n",
      "Iter-59390, train loss-0.3753, acc-0.9000, valid loss-0.4247, acc-0.8822, test loss-0.4286, acc-0.8799\n",
      "Iter-59400, train loss-0.6362, acc-0.8600, valid loss-0.4246, acc-0.8822, test loss-0.4285, acc-0.8799\n",
      "Iter-59410, train loss-0.5079, acc-0.8000, valid loss-0.4246, acc-0.8824, test loss-0.4285, acc-0.8799\n",
      "Iter-59420, train loss-0.7180, acc-0.8400, valid loss-0.4246, acc-0.8820, test loss-0.4285, acc-0.8802\n",
      "Iter-59430, train loss-0.2330, acc-0.9600, valid loss-0.4246, acc-0.8820, test loss-0.4284, acc-0.8801\n",
      "Iter-59440, train loss-0.4313, acc-0.8600, valid loss-0.4245, acc-0.8820, test loss-0.4284, acc-0.8802\n",
      "Iter-59450, train loss-0.2606, acc-0.9800, valid loss-0.4245, acc-0.8820, test loss-0.4284, acc-0.8802\n",
      "Iter-59460, train loss-0.4002, acc-0.9000, valid loss-0.4245, acc-0.8822, test loss-0.4283, acc-0.8800\n",
      "Iter-59470, train loss-0.6467, acc-0.8200, valid loss-0.4245, acc-0.8822, test loss-0.4283, acc-0.8803\n",
      "Iter-59480, train loss-0.4176, acc-0.9000, valid loss-0.4244, acc-0.8822, test loss-0.4283, acc-0.8802\n",
      "Iter-59490, train loss-0.3543, acc-0.8800, valid loss-0.4244, acc-0.8822, test loss-0.4283, acc-0.8801\n",
      "Iter-59500, train loss-0.5363, acc-0.8200, valid loss-0.4244, acc-0.8822, test loss-0.4282, acc-0.8801\n",
      "Iter-59510, train loss-0.3066, acc-0.9200, valid loss-0.4243, acc-0.8822, test loss-0.4282, acc-0.8801\n",
      "Iter-59520, train loss-0.2563, acc-0.9400, valid loss-0.4243, acc-0.8820, test loss-0.4282, acc-0.8803\n",
      "Iter-59530, train loss-0.6995, acc-0.7600, valid loss-0.4243, acc-0.8820, test loss-0.4281, acc-0.8803\n",
      "Iter-59540, train loss-0.3351, acc-0.8600, valid loss-0.4243, acc-0.8820, test loss-0.4281, acc-0.8803\n",
      "Iter-59550, train loss-0.3246, acc-0.9400, valid loss-0.4242, acc-0.8820, test loss-0.4281, acc-0.8801\n",
      "Iter-59560, train loss-0.6171, acc-0.7800, valid loss-0.4242, acc-0.8820, test loss-0.4280, acc-0.8802\n",
      "Iter-59570, train loss-0.5825, acc-0.8200, valid loss-0.4242, acc-0.8822, test loss-0.4280, acc-0.8804\n",
      "Iter-59580, train loss-0.6418, acc-0.8000, valid loss-0.4241, acc-0.8820, test loss-0.4280, acc-0.8804\n",
      "Iter-59590, train loss-0.4743, acc-0.8600, valid loss-0.4241, acc-0.8820, test loss-0.4279, acc-0.8804\n",
      "Iter-59600, train loss-0.3603, acc-0.9000, valid loss-0.4241, acc-0.8820, test loss-0.4279, acc-0.8804\n",
      "Iter-59610, train loss-0.3254, acc-0.9000, valid loss-0.4241, acc-0.8820, test loss-0.4279, acc-0.8805\n",
      "Iter-59620, train loss-0.2720, acc-0.9400, valid loss-0.4240, acc-0.8820, test loss-0.4279, acc-0.8805\n",
      "Iter-59630, train loss-0.2042, acc-0.9800, valid loss-0.4240, acc-0.8822, test loss-0.4278, acc-0.8805\n",
      "Iter-59640, train loss-0.2908, acc-0.9200, valid loss-0.4240, acc-0.8822, test loss-0.4278, acc-0.8805\n",
      "Iter-59650, train loss-0.3917, acc-0.9200, valid loss-0.4239, acc-0.8822, test loss-0.4278, acc-0.8804\n",
      "Iter-59660, train loss-0.3313, acc-0.8800, valid loss-0.4239, acc-0.8822, test loss-0.4277, acc-0.8804\n",
      "Iter-59670, train loss-0.2373, acc-0.9600, valid loss-0.4239, acc-0.8822, test loss-0.4277, acc-0.8804\n",
      "Iter-59680, train loss-0.4239, acc-0.8800, valid loss-0.4238, acc-0.8822, test loss-0.4277, acc-0.8805\n",
      "Iter-59690, train loss-0.4084, acc-0.8800, valid loss-0.4238, acc-0.8822, test loss-0.4277, acc-0.8806\n",
      "Iter-59700, train loss-0.2713, acc-0.9600, valid loss-0.4238, acc-0.8822, test loss-0.4276, acc-0.8806\n",
      "Iter-59710, train loss-0.6537, acc-0.8000, valid loss-0.4238, acc-0.8822, test loss-0.4276, acc-0.8808\n",
      "Iter-59720, train loss-0.4328, acc-0.9000, valid loss-0.4237, acc-0.8822, test loss-0.4276, acc-0.8807\n",
      "Iter-59730, train loss-0.3587, acc-0.9400, valid loss-0.4237, acc-0.8822, test loss-0.4276, acc-0.8807\n",
      "Iter-59740, train loss-0.4289, acc-0.9000, valid loss-0.4237, acc-0.8822, test loss-0.4275, acc-0.8807\n",
      "Iter-59750, train loss-0.4121, acc-0.8800, valid loss-0.4237, acc-0.8822, test loss-0.4275, acc-0.8807\n",
      "Iter-59760, train loss-0.2915, acc-0.9200, valid loss-0.4236, acc-0.8822, test loss-0.4275, acc-0.8806\n",
      "Iter-59770, train loss-0.5375, acc-0.8800, valid loss-0.4236, acc-0.8822, test loss-0.4274, acc-0.8806\n",
      "Iter-59780, train loss-0.4821, acc-0.8600, valid loss-0.4236, acc-0.8822, test loss-0.4274, acc-0.8807\n",
      "Iter-59790, train loss-0.5074, acc-0.8800, valid loss-0.4236, acc-0.8822, test loss-0.4274, acc-0.8807\n",
      "Iter-59800, train loss-0.5918, acc-0.8200, valid loss-0.4235, acc-0.8822, test loss-0.4274, acc-0.8807\n",
      "Iter-59810, train loss-0.3921, acc-0.8800, valid loss-0.4235, acc-0.8822, test loss-0.4273, acc-0.8807\n",
      "Iter-59820, train loss-0.3225, acc-0.9400, valid loss-0.4235, acc-0.8820, test loss-0.4273, acc-0.8807\n",
      "Iter-59830, train loss-0.4481, acc-0.8600, valid loss-0.4234, acc-0.8820, test loss-0.4273, acc-0.8807\n",
      "Iter-59840, train loss-0.5822, acc-0.8200, valid loss-0.4234, acc-0.8820, test loss-0.4272, acc-0.8806\n",
      "Iter-59850, train loss-0.3691, acc-0.9000, valid loss-0.4233, acc-0.8820, test loss-0.4272, acc-0.8806\n",
      "Iter-59860, train loss-0.3384, acc-0.9200, valid loss-0.4233, acc-0.8820, test loss-0.4272, acc-0.8807\n",
      "Iter-59870, train loss-0.4732, acc-0.8400, valid loss-0.4233, acc-0.8820, test loss-0.4271, acc-0.8808\n",
      "Iter-59880, train loss-0.2577, acc-0.9600, valid loss-0.4233, acc-0.8820, test loss-0.4271, acc-0.8806\n",
      "Iter-59890, train loss-0.4269, acc-0.9000, valid loss-0.4232, acc-0.8820, test loss-0.4271, acc-0.8806\n",
      "Iter-59900, train loss-0.2422, acc-0.9600, valid loss-0.4232, acc-0.8820, test loss-0.4270, acc-0.8808\n",
      "Iter-59910, train loss-0.3226, acc-0.8600, valid loss-0.4232, acc-0.8820, test loss-0.4270, acc-0.8807\n",
      "Iter-59920, train loss-0.3476, acc-0.9000, valid loss-0.4231, acc-0.8820, test loss-0.4269, acc-0.8807\n",
      "Iter-59930, train loss-0.3589, acc-0.9000, valid loss-0.4231, acc-0.8820, test loss-0.4269, acc-0.8807\n",
      "Iter-59940, train loss-0.4741, acc-0.8800, valid loss-0.4231, acc-0.8822, test loss-0.4269, acc-0.8808\n",
      "Iter-59950, train loss-0.4414, acc-0.8400, valid loss-0.4230, acc-0.8822, test loss-0.4268, acc-0.8806\n",
      "Iter-59960, train loss-0.5201, acc-0.8800, valid loss-0.4230, acc-0.8822, test loss-0.4268, acc-0.8807\n",
      "Iter-59970, train loss-0.4075, acc-0.8800, valid loss-0.4230, acc-0.8822, test loss-0.4268, acc-0.8807\n",
      "Iter-59980, train loss-0.3775, acc-0.8800, valid loss-0.4229, acc-0.8822, test loss-0.4267, acc-0.8806\n",
      "Iter-59990, train loss-0.3657, acc-0.9000, valid loss-0.4229, acc-0.8822, test loss-0.4267, acc-0.8807\n",
      "Iter-60000, train loss-0.7260, acc-0.8000, valid loss-0.4229, acc-0.8822, test loss-0.4267, acc-0.8806\n",
      "Iter-60010, train loss-0.4101, acc-0.8400, valid loss-0.4229, acc-0.8822, test loss-0.4267, acc-0.8807\n",
      "Iter-60020, train loss-0.2916, acc-0.9400, valid loss-0.4228, acc-0.8822, test loss-0.4266, acc-0.8807\n",
      "Iter-60030, train loss-0.4750, acc-0.8000, valid loss-0.4228, acc-0.8822, test loss-0.4266, acc-0.8807\n",
      "Iter-60040, train loss-0.3715, acc-0.8800, valid loss-0.4228, acc-0.8822, test loss-0.4266, acc-0.8808\n",
      "Iter-60050, train loss-0.4847, acc-0.8600, valid loss-0.4227, acc-0.8822, test loss-0.4265, acc-0.8808\n",
      "Iter-60060, train loss-0.4326, acc-0.9000, valid loss-0.4227, acc-0.8824, test loss-0.4265, acc-0.8807\n",
      "Iter-60070, train loss-0.5487, acc-0.7800, valid loss-0.4227, acc-0.8824, test loss-0.4265, acc-0.8807\n",
      "Iter-60080, train loss-0.5284, acc-0.8200, valid loss-0.4226, acc-0.8824, test loss-0.4265, acc-0.8807\n",
      "Iter-60090, train loss-0.3145, acc-0.9000, valid loss-0.4226, acc-0.8824, test loss-0.4264, acc-0.8809\n",
      "Iter-60100, train loss-0.4238, acc-0.8800, valid loss-0.4226, acc-0.8824, test loss-0.4264, acc-0.8809\n",
      "Iter-60110, train loss-0.4036, acc-0.8600, valid loss-0.4225, acc-0.8826, test loss-0.4264, acc-0.8809\n",
      "Iter-60120, train loss-0.4348, acc-0.8600, valid loss-0.4225, acc-0.8826, test loss-0.4263, acc-0.8810\n",
      "Iter-60130, train loss-0.5320, acc-0.8600, valid loss-0.4225, acc-0.8826, test loss-0.4263, acc-0.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-60140, train loss-0.4768, acc-0.8600, valid loss-0.4224, acc-0.8826, test loss-0.4263, acc-0.8809\n",
      "Iter-60150, train loss-0.2002, acc-0.9800, valid loss-0.4224, acc-0.8826, test loss-0.4262, acc-0.8809\n",
      "Iter-60160, train loss-0.5251, acc-0.8600, valid loss-0.4224, acc-0.8826, test loss-0.4262, acc-0.8809\n",
      "Iter-60170, train loss-0.6442, acc-0.7800, valid loss-0.4223, acc-0.8826, test loss-0.4262, acc-0.8809\n",
      "Iter-60180, train loss-0.4895, acc-0.8600, valid loss-0.4223, acc-0.8824, test loss-0.4261, acc-0.8809\n",
      "Iter-60190, train loss-0.5981, acc-0.8200, valid loss-0.4223, acc-0.8826, test loss-0.4261, acc-0.8809\n",
      "Iter-60200, train loss-0.3327, acc-0.9000, valid loss-0.4222, acc-0.8826, test loss-0.4261, acc-0.8810\n",
      "Iter-60210, train loss-0.4950, acc-0.8600, valid loss-0.4222, acc-0.8826, test loss-0.4260, acc-0.8811\n",
      "Iter-60220, train loss-0.2715, acc-0.9400, valid loss-0.4222, acc-0.8826, test loss-0.4260, acc-0.8808\n",
      "Iter-60230, train loss-0.3566, acc-0.8800, valid loss-0.4221, acc-0.8826, test loss-0.4260, acc-0.8808\n",
      "Iter-60240, train loss-0.3700, acc-0.9200, valid loss-0.4221, acc-0.8828, test loss-0.4260, acc-0.8807\n",
      "Iter-60250, train loss-0.4411, acc-0.8400, valid loss-0.4221, acc-0.8826, test loss-0.4259, acc-0.8807\n",
      "Iter-60260, train loss-0.6067, acc-0.8400, valid loss-0.4220, acc-0.8826, test loss-0.4259, acc-0.8808\n",
      "Iter-60270, train loss-0.4912, acc-0.8600, valid loss-0.4220, acc-0.8826, test loss-0.4259, acc-0.8808\n",
      "Iter-60280, train loss-0.4101, acc-0.9200, valid loss-0.4220, acc-0.8824, test loss-0.4259, acc-0.8809\n",
      "Iter-60290, train loss-0.3599, acc-0.9200, valid loss-0.4220, acc-0.8824, test loss-0.4258, acc-0.8809\n",
      "Iter-60300, train loss-0.5939, acc-0.7800, valid loss-0.4219, acc-0.8822, test loss-0.4258, acc-0.8809\n",
      "Iter-60310, train loss-0.5430, acc-0.8600, valid loss-0.4219, acc-0.8824, test loss-0.4258, acc-0.8809\n",
      "Iter-60320, train loss-0.4693, acc-0.8600, valid loss-0.4219, acc-0.8824, test loss-0.4257, acc-0.8809\n",
      "Iter-60330, train loss-0.5441, acc-0.8400, valid loss-0.4218, acc-0.8822, test loss-0.4257, acc-0.8809\n",
      "Iter-60340, train loss-0.4042, acc-0.9000, valid loss-0.4218, acc-0.8822, test loss-0.4257, acc-0.8809\n",
      "Iter-60350, train loss-0.2151, acc-0.9800, valid loss-0.4218, acc-0.8822, test loss-0.4256, acc-0.8809\n",
      "Iter-60360, train loss-0.3138, acc-0.9600, valid loss-0.4218, acc-0.8822, test loss-0.4256, acc-0.8809\n",
      "Iter-60370, train loss-0.4762, acc-0.8600, valid loss-0.4217, acc-0.8822, test loss-0.4256, acc-0.8809\n",
      "Iter-60380, train loss-0.4351, acc-0.9200, valid loss-0.4217, acc-0.8822, test loss-0.4256, acc-0.8809\n",
      "Iter-60390, train loss-0.3661, acc-0.9400, valid loss-0.4217, acc-0.8822, test loss-0.4255, acc-0.8808\n",
      "Iter-60400, train loss-0.4017, acc-0.9400, valid loss-0.4217, acc-0.8822, test loss-0.4255, acc-0.8808\n",
      "Iter-60410, train loss-0.4880, acc-0.9000, valid loss-0.4216, acc-0.8822, test loss-0.4255, acc-0.8808\n",
      "Iter-60420, train loss-0.3928, acc-0.9200, valid loss-0.4216, acc-0.8822, test loss-0.4255, acc-0.8808\n",
      "Iter-60430, train loss-0.3823, acc-0.9000, valid loss-0.4216, acc-0.8824, test loss-0.4254, acc-0.8809\n",
      "Iter-60440, train loss-0.6313, acc-0.8000, valid loss-0.4216, acc-0.8824, test loss-0.4254, acc-0.8809\n",
      "Iter-60450, train loss-0.4783, acc-0.8400, valid loss-0.4215, acc-0.8822, test loss-0.4254, acc-0.8809\n",
      "Iter-60460, train loss-0.2417, acc-0.9400, valid loss-0.4215, acc-0.8826, test loss-0.4253, acc-0.8809\n",
      "Iter-60470, train loss-0.2970, acc-0.8800, valid loss-0.4215, acc-0.8824, test loss-0.4253, acc-0.8810\n",
      "Iter-60480, train loss-0.3624, acc-0.9200, valid loss-0.4214, acc-0.8826, test loss-0.4253, acc-0.8811\n",
      "Iter-60490, train loss-0.4947, acc-0.8400, valid loss-0.4214, acc-0.8826, test loss-0.4253, acc-0.8812\n",
      "Iter-60500, train loss-0.4579, acc-0.8800, valid loss-0.4214, acc-0.8822, test loss-0.4252, acc-0.8811\n",
      "Iter-60510, train loss-0.4691, acc-0.9000, valid loss-0.4214, acc-0.8824, test loss-0.4252, acc-0.8812\n",
      "Iter-60520, train loss-0.7705, acc-0.7600, valid loss-0.4213, acc-0.8824, test loss-0.4252, acc-0.8813\n",
      "Iter-60530, train loss-0.4717, acc-0.8400, valid loss-0.4213, acc-0.8824, test loss-0.4251, acc-0.8813\n",
      "Iter-60540, train loss-0.3149, acc-0.9400, valid loss-0.4213, acc-0.8824, test loss-0.4251, acc-0.8812\n",
      "Iter-60550, train loss-0.4347, acc-0.8400, valid loss-0.4212, acc-0.8824, test loss-0.4251, acc-0.8812\n",
      "Iter-60560, train loss-0.4856, acc-0.8400, valid loss-0.4212, acc-0.8824, test loss-0.4250, acc-0.8812\n",
      "Iter-60570, train loss-0.2518, acc-0.9800, valid loss-0.4211, acc-0.8824, test loss-0.4250, acc-0.8812\n",
      "Iter-60580, train loss-0.7031, acc-0.7400, valid loss-0.4211, acc-0.8824, test loss-0.4250, acc-0.8811\n",
      "Iter-60590, train loss-0.4128, acc-0.9200, valid loss-0.4211, acc-0.8824, test loss-0.4249, acc-0.8812\n",
      "Iter-60600, train loss-0.3766, acc-0.9200, valid loss-0.4210, acc-0.8824, test loss-0.4249, acc-0.8812\n",
      "Iter-60610, train loss-0.3253, acc-0.9200, valid loss-0.4210, acc-0.8826, test loss-0.4249, acc-0.8812\n",
      "Iter-60620, train loss-0.4966, acc-0.8600, valid loss-0.4210, acc-0.8824, test loss-0.4248, acc-0.8812\n",
      "Iter-60630, train loss-0.3417, acc-0.9000, valid loss-0.4210, acc-0.8824, test loss-0.4248, acc-0.8812\n",
      "Iter-60640, train loss-0.5160, acc-0.8400, valid loss-0.4209, acc-0.8824, test loss-0.4248, acc-0.8812\n",
      "Iter-60650, train loss-0.2984, acc-0.9200, valid loss-0.4209, acc-0.8824, test loss-0.4247, acc-0.8812\n",
      "Iter-60660, train loss-0.6134, acc-0.8200, valid loss-0.4208, acc-0.8826, test loss-0.4247, acc-0.8812\n",
      "Iter-60670, train loss-0.4677, acc-0.8800, valid loss-0.4208, acc-0.8828, test loss-0.4247, acc-0.8810\n",
      "Iter-60680, train loss-0.4117, acc-0.9000, valid loss-0.4208, acc-0.8828, test loss-0.4246, acc-0.8812\n",
      "Iter-60690, train loss-0.3870, acc-0.9000, valid loss-0.4207, acc-0.8826, test loss-0.4246, acc-0.8811\n",
      "Iter-60700, train loss-0.3436, acc-0.9200, valid loss-0.4207, acc-0.8826, test loss-0.4246, acc-0.8813\n",
      "Iter-60710, train loss-0.4753, acc-0.8600, valid loss-0.4207, acc-0.8824, test loss-0.4246, acc-0.8812\n",
      "Iter-60720, train loss-0.4154, acc-0.9000, valid loss-0.4207, acc-0.8824, test loss-0.4245, acc-0.8813\n",
      "Iter-60730, train loss-0.2281, acc-0.9400, valid loss-0.4206, acc-0.8826, test loss-0.4245, acc-0.8813\n",
      "Iter-60740, train loss-0.3922, acc-0.8400, valid loss-0.4206, acc-0.8824, test loss-0.4245, acc-0.8812\n",
      "Iter-60750, train loss-0.3202, acc-0.9400, valid loss-0.4206, acc-0.8826, test loss-0.4245, acc-0.8813\n",
      "Iter-60760, train loss-0.3544, acc-0.9000, valid loss-0.4205, acc-0.8828, test loss-0.4244, acc-0.8813\n",
      "Iter-60770, train loss-0.4273, acc-0.8400, valid loss-0.4205, acc-0.8826, test loss-0.4244, acc-0.8814\n",
      "Iter-60780, train loss-0.4748, acc-0.8800, valid loss-0.4205, acc-0.8828, test loss-0.4243, acc-0.8813\n",
      "Iter-60790, train loss-0.4860, acc-0.9000, valid loss-0.4204, acc-0.8826, test loss-0.4243, acc-0.8813\n",
      "Iter-60800, train loss-0.5310, acc-0.8400, valid loss-0.4204, acc-0.8828, test loss-0.4243, acc-0.8812\n",
      "Iter-60810, train loss-0.2983, acc-0.9200, valid loss-0.4204, acc-0.8828, test loss-0.4243, acc-0.8812\n",
      "Iter-60820, train loss-0.3063, acc-0.9600, valid loss-0.4203, acc-0.8830, test loss-0.4243, acc-0.8812\n",
      "Iter-60830, train loss-0.4381, acc-0.8800, valid loss-0.4203, acc-0.8826, test loss-0.4242, acc-0.8812\n",
      "Iter-60840, train loss-0.5031, acc-0.8600, valid loss-0.4203, acc-0.8830, test loss-0.4242, acc-0.8813\n",
      "Iter-60850, train loss-0.5154, acc-0.8400, valid loss-0.4203, acc-0.8830, test loss-0.4242, acc-0.8813\n",
      "Iter-60860, train loss-0.4909, acc-0.9000, valid loss-0.4202, acc-0.8830, test loss-0.4241, acc-0.8813\n",
      "Iter-60870, train loss-0.4106, acc-0.8600, valid loss-0.4202, acc-0.8828, test loss-0.4241, acc-0.8813\n",
      "Iter-60880, train loss-0.5214, acc-0.8600, valid loss-0.4201, acc-0.8828, test loss-0.4241, acc-0.8813\n",
      "Iter-60890, train loss-0.5577, acc-0.8800, valid loss-0.4201, acc-0.8830, test loss-0.4240, acc-0.8813\n",
      "Iter-60900, train loss-0.5330, acc-0.8000, valid loss-0.4201, acc-0.8832, test loss-0.4240, acc-0.8813\n",
      "Iter-60910, train loss-0.7762, acc-0.7800, valid loss-0.4201, acc-0.8832, test loss-0.4240, acc-0.8813\n",
      "Iter-60920, train loss-0.2877, acc-0.9400, valid loss-0.4200, acc-0.8830, test loss-0.4239, acc-0.8814\n",
      "Iter-60930, train loss-0.3448, acc-0.9000, valid loss-0.4200, acc-0.8830, test loss-0.4239, acc-0.8814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-60940, train loss-0.3380, acc-0.9400, valid loss-0.4200, acc-0.8830, test loss-0.4239, acc-0.8814\n",
      "Iter-60950, train loss-0.3232, acc-0.9000, valid loss-0.4200, acc-0.8830, test loss-0.4239, acc-0.8814\n",
      "Iter-60960, train loss-0.3628, acc-0.9400, valid loss-0.4199, acc-0.8832, test loss-0.4238, acc-0.8814\n",
      "Iter-60970, train loss-0.4604, acc-0.8200, valid loss-0.4199, acc-0.8832, test loss-0.4238, acc-0.8814\n",
      "Iter-60980, train loss-0.3591, acc-0.8800, valid loss-0.4199, acc-0.8832, test loss-0.4238, acc-0.8814\n",
      "Iter-60990, train loss-0.3934, acc-0.8600, valid loss-0.4198, acc-0.8832, test loss-0.4238, acc-0.8814\n",
      "Iter-61000, train loss-0.4448, acc-0.8800, valid loss-0.4198, acc-0.8830, test loss-0.4237, acc-0.8814\n",
      "Iter-61010, train loss-0.3943, acc-0.9000, valid loss-0.4198, acc-0.8830, test loss-0.4237, acc-0.8814\n",
      "Iter-61020, train loss-0.4948, acc-0.8000, valid loss-0.4197, acc-0.8832, test loss-0.4236, acc-0.8814\n",
      "Iter-61030, train loss-0.5507, acc-0.8200, valid loss-0.4197, acc-0.8832, test loss-0.4236, acc-0.8814\n",
      "Iter-61040, train loss-0.4255, acc-0.9000, valid loss-0.4197, acc-0.8832, test loss-0.4236, acc-0.8814\n",
      "Iter-61050, train loss-0.5298, acc-0.8400, valid loss-0.4196, acc-0.8830, test loss-0.4235, acc-0.8814\n",
      "Iter-61060, train loss-0.4560, acc-0.8600, valid loss-0.4196, acc-0.8828, test loss-0.4235, acc-0.8814\n",
      "Iter-61070, train loss-0.3375, acc-0.9200, valid loss-0.4196, acc-0.8826, test loss-0.4235, acc-0.8814\n",
      "Iter-61080, train loss-0.4855, acc-0.8600, valid loss-0.4196, acc-0.8824, test loss-0.4234, acc-0.8814\n",
      "Iter-61090, train loss-0.5226, acc-0.8400, valid loss-0.4195, acc-0.8824, test loss-0.4234, acc-0.8814\n",
      "Iter-61100, train loss-0.3998, acc-0.8800, valid loss-0.4195, acc-0.8824, test loss-0.4234, acc-0.8814\n",
      "Iter-61110, train loss-0.5526, acc-0.7600, valid loss-0.4195, acc-0.8824, test loss-0.4233, acc-0.8814\n",
      "Iter-61120, train loss-0.5362, acc-0.8800, valid loss-0.4194, acc-0.8826, test loss-0.4233, acc-0.8815\n",
      "Iter-61130, train loss-0.4331, acc-0.8400, valid loss-0.4194, acc-0.8828, test loss-0.4233, acc-0.8815\n",
      "Iter-61140, train loss-0.2397, acc-0.9200, valid loss-0.4194, acc-0.8824, test loss-0.4232, acc-0.8814\n",
      "Iter-61150, train loss-0.3117, acc-0.9400, valid loss-0.4193, acc-0.8828, test loss-0.4232, acc-0.8815\n",
      "Iter-61160, train loss-0.3759, acc-0.9000, valid loss-0.4193, acc-0.8824, test loss-0.4232, acc-0.8814\n",
      "Iter-61170, train loss-0.3949, acc-0.8400, valid loss-0.4193, acc-0.8826, test loss-0.4231, acc-0.8814\n",
      "Iter-61180, train loss-0.6389, acc-0.8800, valid loss-0.4192, acc-0.8830, test loss-0.4231, acc-0.8814\n",
      "Iter-61190, train loss-0.5878, acc-0.8400, valid loss-0.4192, acc-0.8828, test loss-0.4231, acc-0.8814\n",
      "Iter-61200, train loss-0.4530, acc-0.9000, valid loss-0.4192, acc-0.8828, test loss-0.4231, acc-0.8815\n",
      "Iter-61210, train loss-0.4973, acc-0.8400, valid loss-0.4192, acc-0.8828, test loss-0.4230, acc-0.8815\n",
      "Iter-61220, train loss-0.3684, acc-0.8400, valid loss-0.4191, acc-0.8828, test loss-0.4230, acc-0.8815\n",
      "Iter-61230, train loss-0.3227, acc-0.8800, valid loss-0.4191, acc-0.8830, test loss-0.4230, acc-0.8815\n",
      "Iter-61240, train loss-0.6043, acc-0.7400, valid loss-0.4191, acc-0.8830, test loss-0.4230, acc-0.8815\n",
      "Iter-61250, train loss-0.5019, acc-0.8400, valid loss-0.4190, acc-0.8830, test loss-0.4229, acc-0.8815\n",
      "Iter-61260, train loss-0.3413, acc-0.8800, valid loss-0.4190, acc-0.8828, test loss-0.4229, acc-0.8815\n",
      "Iter-61270, train loss-0.4162, acc-0.9200, valid loss-0.4190, acc-0.8828, test loss-0.4229, acc-0.8815\n",
      "Iter-61280, train loss-0.7505, acc-0.7400, valid loss-0.4190, acc-0.8828, test loss-0.4228, acc-0.8815\n",
      "Iter-61290, train loss-0.4577, acc-0.8800, valid loss-0.4189, acc-0.8830, test loss-0.4228, acc-0.8815\n",
      "Iter-61300, train loss-0.3617, acc-0.8400, valid loss-0.4189, acc-0.8830, test loss-0.4228, acc-0.8815\n",
      "Iter-61310, train loss-0.5149, acc-0.8400, valid loss-0.4189, acc-0.8830, test loss-0.4228, acc-0.8815\n",
      "Iter-61320, train loss-0.3051, acc-0.9000, valid loss-0.4188, acc-0.8830, test loss-0.4227, acc-0.8815\n",
      "Iter-61330, train loss-0.5795, acc-0.7800, valid loss-0.4188, acc-0.8830, test loss-0.4227, acc-0.8815\n",
      "Iter-61340, train loss-0.4383, acc-0.9000, valid loss-0.4188, acc-0.8830, test loss-0.4227, acc-0.8815\n",
      "Iter-61350, train loss-0.4803, acc-0.8600, valid loss-0.4188, acc-0.8828, test loss-0.4226, acc-0.8815\n",
      "Iter-61360, train loss-0.4779, acc-0.8400, valid loss-0.4187, acc-0.8828, test loss-0.4226, acc-0.8815\n",
      "Iter-61370, train loss-0.4155, acc-0.9000, valid loss-0.4187, acc-0.8832, test loss-0.4226, acc-0.8814\n",
      "Iter-61380, train loss-0.3626, acc-0.9000, valid loss-0.4187, acc-0.8830, test loss-0.4226, acc-0.8814\n",
      "Iter-61390, train loss-0.3795, acc-0.9400, valid loss-0.4186, acc-0.8828, test loss-0.4225, acc-0.8814\n",
      "Iter-61400, train loss-0.4357, acc-0.8600, valid loss-0.4186, acc-0.8828, test loss-0.4225, acc-0.8815\n",
      "Iter-61410, train loss-0.4004, acc-0.9200, valid loss-0.4186, acc-0.8830, test loss-0.4225, acc-0.8815\n",
      "Iter-61420, train loss-0.4108, acc-0.9200, valid loss-0.4185, acc-0.8830, test loss-0.4225, acc-0.8815\n",
      "Iter-61430, train loss-0.4101, acc-0.9000, valid loss-0.4185, acc-0.8830, test loss-0.4224, acc-0.8816\n",
      "Iter-61440, train loss-0.2744, acc-0.9200, valid loss-0.4185, acc-0.8832, test loss-0.4224, acc-0.8816\n",
      "Iter-61450, train loss-0.4157, acc-0.9200, valid loss-0.4185, acc-0.8834, test loss-0.4224, acc-0.8818\n",
      "Iter-61460, train loss-0.3830, acc-0.8600, valid loss-0.4184, acc-0.8834, test loss-0.4223, acc-0.8816\n",
      "Iter-61470, train loss-0.3743, acc-0.8600, valid loss-0.4184, acc-0.8832, test loss-0.4223, acc-0.8816\n",
      "Iter-61480, train loss-0.4230, acc-0.9000, valid loss-0.4184, acc-0.8832, test loss-0.4223, acc-0.8816\n",
      "Iter-61490, train loss-0.4087, acc-0.9200, valid loss-0.4183, acc-0.8830, test loss-0.4223, acc-0.8816\n",
      "Iter-61500, train loss-0.3867, acc-0.8600, valid loss-0.4183, acc-0.8830, test loss-0.4222, acc-0.8816\n",
      "Iter-61510, train loss-0.4289, acc-0.8800, valid loss-0.4183, acc-0.8830, test loss-0.4222, acc-0.8816\n",
      "Iter-61520, train loss-0.3918, acc-0.9000, valid loss-0.4182, acc-0.8832, test loss-0.4222, acc-0.8816\n",
      "Iter-61530, train loss-0.3638, acc-0.8600, valid loss-0.4182, acc-0.8832, test loss-0.4221, acc-0.8816\n",
      "Iter-61540, train loss-0.6084, acc-0.8400, valid loss-0.4182, acc-0.8832, test loss-0.4221, acc-0.8815\n",
      "Iter-61550, train loss-0.2892, acc-0.9000, valid loss-0.4182, acc-0.8834, test loss-0.4221, acc-0.8815\n",
      "Iter-61560, train loss-0.5437, acc-0.8400, valid loss-0.4181, acc-0.8834, test loss-0.4221, acc-0.8815\n",
      "Iter-61570, train loss-0.4897, acc-0.8000, valid loss-0.4181, acc-0.8834, test loss-0.4220, acc-0.8815\n",
      "Iter-61580, train loss-0.4601, acc-0.8600, valid loss-0.4181, acc-0.8832, test loss-0.4220, acc-0.8815\n",
      "Iter-61590, train loss-0.2742, acc-0.9400, valid loss-0.4181, acc-0.8836, test loss-0.4220, acc-0.8815\n",
      "Iter-61600, train loss-0.3996, acc-0.9200, valid loss-0.4180, acc-0.8834, test loss-0.4220, acc-0.8816\n",
      "Iter-61610, train loss-0.5377, acc-0.9000, valid loss-0.4180, acc-0.8836, test loss-0.4219, acc-0.8816\n",
      "Iter-61620, train loss-0.4610, acc-0.8400, valid loss-0.4180, acc-0.8838, test loss-0.4219, acc-0.8816\n",
      "Iter-61630, train loss-0.5657, acc-0.8600, valid loss-0.4179, acc-0.8836, test loss-0.4219, acc-0.8816\n",
      "Iter-61640, train loss-0.5542, acc-0.8000, valid loss-0.4179, acc-0.8834, test loss-0.4219, acc-0.8816\n",
      "Iter-61650, train loss-0.3552, acc-0.9200, valid loss-0.4179, acc-0.8836, test loss-0.4218, acc-0.8816\n",
      "Iter-61660, train loss-0.3652, acc-0.9200, valid loss-0.4179, acc-0.8836, test loss-0.4218, acc-0.8816\n",
      "Iter-61670, train loss-0.3920, acc-0.8800, valid loss-0.4178, acc-0.8836, test loss-0.4218, acc-0.8817\n",
      "Iter-61680, train loss-0.4122, acc-0.8800, valid loss-0.4178, acc-0.8836, test loss-0.4217, acc-0.8817\n",
      "Iter-61690, train loss-0.4612, acc-0.8400, valid loss-0.4178, acc-0.8836, test loss-0.4217, acc-0.8818\n",
      "Iter-61700, train loss-0.3413, acc-0.9200, valid loss-0.4178, acc-0.8836, test loss-0.4217, acc-0.8818\n",
      "Iter-61710, train loss-0.2992, acc-0.9400, valid loss-0.4177, acc-0.8834, test loss-0.4216, acc-0.8818\n",
      "Iter-61720, train loss-0.3860, acc-0.8800, valid loss-0.4177, acc-0.8836, test loss-0.4216, acc-0.8819\n",
      "Iter-61730, train loss-0.4024, acc-0.9000, valid loss-0.4177, acc-0.8838, test loss-0.4216, acc-0.8819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-61740, train loss-0.3656, acc-0.9000, valid loss-0.4177, acc-0.8838, test loss-0.4216, acc-0.8819\n",
      "Iter-61750, train loss-0.4077, acc-0.8800, valid loss-0.4176, acc-0.8836, test loss-0.4215, acc-0.8819\n",
      "Iter-61760, train loss-0.4323, acc-0.9200, valid loss-0.4176, acc-0.8836, test loss-0.4215, acc-0.8820\n",
      "Iter-61770, train loss-0.3589, acc-0.8800, valid loss-0.4176, acc-0.8834, test loss-0.4215, acc-0.8820\n",
      "Iter-61780, train loss-0.4163, acc-0.9000, valid loss-0.4175, acc-0.8836, test loss-0.4214, acc-0.8821\n",
      "Iter-61790, train loss-0.2853, acc-0.9600, valid loss-0.4175, acc-0.8834, test loss-0.4214, acc-0.8820\n",
      "Iter-61800, train loss-0.4977, acc-0.8400, valid loss-0.4175, acc-0.8834, test loss-0.4214, acc-0.8821\n",
      "Iter-61810, train loss-0.4642, acc-0.9400, valid loss-0.4175, acc-0.8834, test loss-0.4214, acc-0.8820\n",
      "Iter-61820, train loss-0.3411, acc-0.9000, valid loss-0.4174, acc-0.8834, test loss-0.4213, acc-0.8820\n",
      "Iter-61830, train loss-0.4398, acc-0.8400, valid loss-0.4174, acc-0.8834, test loss-0.4213, acc-0.8821\n",
      "Iter-61840, train loss-0.3883, acc-0.8600, valid loss-0.4174, acc-0.8832, test loss-0.4213, acc-0.8820\n",
      "Iter-61850, train loss-0.4172, acc-0.9000, valid loss-0.4173, acc-0.8832, test loss-0.4212, acc-0.8819\n",
      "Iter-61860, train loss-0.5046, acc-0.8800, valid loss-0.4173, acc-0.8832, test loss-0.4212, acc-0.8821\n",
      "Iter-61870, train loss-0.4013, acc-0.8800, valid loss-0.4173, acc-0.8832, test loss-0.4212, acc-0.8821\n",
      "Iter-61880, train loss-0.4959, acc-0.8200, valid loss-0.4172, acc-0.8832, test loss-0.4212, acc-0.8820\n",
      "Iter-61890, train loss-0.5662, acc-0.8200, valid loss-0.4172, acc-0.8832, test loss-0.4211, acc-0.8820\n",
      "Iter-61900, train loss-0.3482, acc-0.8800, valid loss-0.4172, acc-0.8830, test loss-0.4211, acc-0.8821\n",
      "Iter-61910, train loss-0.3880, acc-0.9000, valid loss-0.4172, acc-0.8830, test loss-0.4211, acc-0.8820\n",
      "Iter-61920, train loss-0.7334, acc-0.7800, valid loss-0.4171, acc-0.8832, test loss-0.4211, acc-0.8820\n",
      "Iter-61930, train loss-0.4536, acc-0.8400, valid loss-0.4171, acc-0.8834, test loss-0.4211, acc-0.8820\n",
      "Iter-61940, train loss-0.4260, acc-0.9000, valid loss-0.4171, acc-0.8836, test loss-0.4210, acc-0.8820\n",
      "Iter-61950, train loss-0.3344, acc-0.8800, valid loss-0.4171, acc-0.8834, test loss-0.4210, acc-0.8820\n",
      "Iter-61960, train loss-0.3094, acc-0.9400, valid loss-0.4170, acc-0.8836, test loss-0.4210, acc-0.8819\n",
      "Iter-61970, train loss-0.3739, acc-0.9000, valid loss-0.4170, acc-0.8836, test loss-0.4210, acc-0.8818\n",
      "Iter-61980, train loss-0.4859, acc-0.8600, valid loss-0.4169, acc-0.8836, test loss-0.4209, acc-0.8818\n",
      "Iter-61990, train loss-0.4864, acc-0.8800, valid loss-0.4169, acc-0.8838, test loss-0.4209, acc-0.8818\n",
      "Iter-62000, train loss-0.3992, acc-0.9200, valid loss-0.4169, acc-0.8838, test loss-0.4209, acc-0.8820\n",
      "Iter-62010, train loss-0.4381, acc-0.9400, valid loss-0.4169, acc-0.8836, test loss-0.4208, acc-0.8819\n",
      "Iter-62020, train loss-0.4818, acc-0.8400, valid loss-0.4169, acc-0.8836, test loss-0.4208, acc-0.8819\n",
      "Iter-62030, train loss-0.4828, acc-0.8000, valid loss-0.4168, acc-0.8836, test loss-0.4208, acc-0.8821\n",
      "Iter-62040, train loss-0.4737, acc-0.8600, valid loss-0.4168, acc-0.8836, test loss-0.4208, acc-0.8819\n",
      "Iter-62050, train loss-0.3474, acc-0.8800, valid loss-0.4168, acc-0.8836, test loss-0.4207, acc-0.8820\n",
      "Iter-62060, train loss-0.3657, acc-0.9200, valid loss-0.4167, acc-0.8836, test loss-0.4207, acc-0.8820\n",
      "Iter-62070, train loss-0.3273, acc-0.9000, valid loss-0.4167, acc-0.8836, test loss-0.4207, acc-0.8819\n",
      "Iter-62080, train loss-0.3713, acc-0.8800, valid loss-0.4167, acc-0.8834, test loss-0.4206, acc-0.8819\n",
      "Iter-62090, train loss-0.5047, acc-0.9000, valid loss-0.4166, acc-0.8836, test loss-0.4206, acc-0.8820\n",
      "Iter-62100, train loss-0.4502, acc-0.8200, valid loss-0.4166, acc-0.8834, test loss-0.4206, acc-0.8820\n",
      "Iter-62110, train loss-0.4328, acc-0.9000, valid loss-0.4166, acc-0.8830, test loss-0.4206, acc-0.8819\n",
      "Iter-62120, train loss-0.4852, acc-0.8400, valid loss-0.4165, acc-0.8830, test loss-0.4205, acc-0.8819\n",
      "Iter-62130, train loss-0.5584, acc-0.8600, valid loss-0.4165, acc-0.8832, test loss-0.4205, acc-0.8819\n",
      "Iter-62140, train loss-0.3536, acc-0.8600, valid loss-0.4165, acc-0.8836, test loss-0.4205, acc-0.8819\n",
      "Iter-62150, train loss-0.5306, acc-0.8800, valid loss-0.4164, acc-0.8836, test loss-0.4205, acc-0.8819\n",
      "Iter-62160, train loss-0.3158, acc-0.9400, valid loss-0.4164, acc-0.8836, test loss-0.4204, acc-0.8819\n",
      "Iter-62170, train loss-0.4706, acc-0.9000, valid loss-0.4164, acc-0.8836, test loss-0.4204, acc-0.8820\n",
      "Iter-62180, train loss-0.3856, acc-0.9000, valid loss-0.4163, acc-0.8836, test loss-0.4204, acc-0.8820\n",
      "Iter-62190, train loss-0.6828, acc-0.8400, valid loss-0.4163, acc-0.8836, test loss-0.4204, acc-0.8819\n",
      "Iter-62200, train loss-0.4573, acc-0.9200, valid loss-0.4163, acc-0.8836, test loss-0.4203, acc-0.8819\n",
      "Iter-62210, train loss-0.3892, acc-0.8800, valid loss-0.4163, acc-0.8834, test loss-0.4203, acc-0.8819\n",
      "Iter-62220, train loss-0.4540, acc-0.8600, valid loss-0.4162, acc-0.8834, test loss-0.4203, acc-0.8820\n",
      "Iter-62230, train loss-0.4851, acc-0.8600, valid loss-0.4162, acc-0.8836, test loss-0.4203, acc-0.8820\n",
      "Iter-62240, train loss-0.4630, acc-0.8800, valid loss-0.4162, acc-0.8832, test loss-0.4202, acc-0.8821\n",
      "Iter-62250, train loss-0.5893, acc-0.8600, valid loss-0.4162, acc-0.8830, test loss-0.4202, acc-0.8821\n",
      "Iter-62260, train loss-0.4314, acc-0.9000, valid loss-0.4161, acc-0.8834, test loss-0.4202, acc-0.8820\n",
      "Iter-62270, train loss-0.4171, acc-0.8800, valid loss-0.4161, acc-0.8830, test loss-0.4201, acc-0.8820\n",
      "Iter-62280, train loss-0.5492, acc-0.8200, valid loss-0.4161, acc-0.8836, test loss-0.4201, acc-0.8820\n",
      "Iter-62290, train loss-0.2853, acc-0.9200, valid loss-0.4161, acc-0.8834, test loss-0.4201, acc-0.8820\n",
      "Iter-62300, train loss-0.4878, acc-0.8600, valid loss-0.4160, acc-0.8834, test loss-0.4200, acc-0.8820\n",
      "Iter-62310, train loss-0.3966, acc-0.9000, valid loss-0.4160, acc-0.8832, test loss-0.4200, acc-0.8820\n",
      "Iter-62320, train loss-0.3200, acc-0.9200, valid loss-0.4160, acc-0.8832, test loss-0.4200, acc-0.8820\n",
      "Iter-62330, train loss-0.5163, acc-0.8600, valid loss-0.4159, acc-0.8836, test loss-0.4200, acc-0.8820\n",
      "Iter-62340, train loss-0.3493, acc-0.9200, valid loss-0.4159, acc-0.8836, test loss-0.4199, acc-0.8820\n",
      "Iter-62350, train loss-0.4934, acc-0.8400, valid loss-0.4159, acc-0.8836, test loss-0.4199, acc-0.8820\n",
      "Iter-62360, train loss-0.2515, acc-0.9200, valid loss-0.4159, acc-0.8838, test loss-0.4199, acc-0.8820\n",
      "Iter-62370, train loss-0.3333, acc-0.9200, valid loss-0.4159, acc-0.8836, test loss-0.4199, acc-0.8819\n",
      "Iter-62380, train loss-0.2041, acc-0.9600, valid loss-0.4158, acc-0.8838, test loss-0.4198, acc-0.8820\n",
      "Iter-62390, train loss-0.5837, acc-0.8000, valid loss-0.4158, acc-0.8838, test loss-0.4198, acc-0.8819\n",
      "Iter-62400, train loss-0.5340, acc-0.8600, valid loss-0.4158, acc-0.8836, test loss-0.4198, acc-0.8820\n",
      "Iter-62410, train loss-0.7332, acc-0.8200, valid loss-0.4158, acc-0.8840, test loss-0.4198, acc-0.8820\n",
      "Iter-62420, train loss-0.5335, acc-0.8600, valid loss-0.4157, acc-0.8836, test loss-0.4197, acc-0.8820\n",
      "Iter-62430, train loss-0.4319, acc-0.9000, valid loss-0.4157, acc-0.8838, test loss-0.4197, acc-0.8820\n",
      "Iter-62440, train loss-0.3193, acc-0.9200, valid loss-0.4157, acc-0.8834, test loss-0.4197, acc-0.8821\n",
      "Iter-62450, train loss-0.5595, acc-0.8400, valid loss-0.4157, acc-0.8834, test loss-0.4196, acc-0.8821\n",
      "Iter-62460, train loss-0.3701, acc-0.9400, valid loss-0.4156, acc-0.8834, test loss-0.4196, acc-0.8821\n",
      "Iter-62470, train loss-0.3667, acc-0.8800, valid loss-0.4156, acc-0.8832, test loss-0.4196, acc-0.8821\n",
      "Iter-62480, train loss-0.3559, acc-0.9200, valid loss-0.4156, acc-0.8832, test loss-0.4196, acc-0.8821\n",
      "Iter-62490, train loss-0.3503, acc-0.8600, valid loss-0.4156, acc-0.8832, test loss-0.4195, acc-0.8821\n",
      "Iter-62500, train loss-0.3081, acc-0.9400, valid loss-0.4155, acc-0.8832, test loss-0.4195, acc-0.8822\n",
      "Iter-62510, train loss-0.3312, acc-0.8800, valid loss-0.4155, acc-0.8832, test loss-0.4195, acc-0.8821\n",
      "Iter-62520, train loss-0.2627, acc-0.9600, valid loss-0.4155, acc-0.8832, test loss-0.4195, acc-0.8822\n",
      "Iter-62530, train loss-0.5200, acc-0.8400, valid loss-0.4155, acc-0.8832, test loss-0.4194, acc-0.8822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-62540, train loss-0.3812, acc-0.9200, valid loss-0.4154, acc-0.8832, test loss-0.4194, acc-0.8822\n",
      "Iter-62550, train loss-0.5963, acc-0.7800, valid loss-0.4154, acc-0.8836, test loss-0.4194, acc-0.8823\n",
      "Iter-62560, train loss-0.3214, acc-0.9000, valid loss-0.4154, acc-0.8836, test loss-0.4193, acc-0.8823\n",
      "Iter-62570, train loss-0.3776, acc-0.9200, valid loss-0.4154, acc-0.8836, test loss-0.4193, acc-0.8824\n",
      "Iter-62580, train loss-0.2732, acc-0.9600, valid loss-0.4153, acc-0.8840, test loss-0.4193, acc-0.8823\n",
      "Iter-62590, train loss-0.5462, acc-0.8000, valid loss-0.4153, acc-0.8840, test loss-0.4193, acc-0.8823\n",
      "Iter-62600, train loss-0.3521, acc-0.9000, valid loss-0.4153, acc-0.8840, test loss-0.4192, acc-0.8824\n",
      "Iter-62610, train loss-0.4584, acc-0.9000, valid loss-0.4153, acc-0.8842, test loss-0.4192, acc-0.8825\n",
      "Iter-62620, train loss-0.6191, acc-0.8600, valid loss-0.4152, acc-0.8840, test loss-0.4192, acc-0.8824\n",
      "Iter-62630, train loss-0.5824, acc-0.8400, valid loss-0.4152, acc-0.8838, test loss-0.4191, acc-0.8825\n",
      "Iter-62640, train loss-0.5035, acc-0.8000, valid loss-0.4152, acc-0.8838, test loss-0.4191, acc-0.8824\n",
      "Iter-62650, train loss-0.4544, acc-0.8400, valid loss-0.4151, acc-0.8840, test loss-0.4191, acc-0.8824\n",
      "Iter-62660, train loss-0.3484, acc-0.8800, valid loss-0.4151, acc-0.8840, test loss-0.4190, acc-0.8825\n",
      "Iter-62670, train loss-0.4013, acc-0.9000, valid loss-0.4151, acc-0.8844, test loss-0.4190, acc-0.8825\n",
      "Iter-62680, train loss-0.4802, acc-0.8400, valid loss-0.4150, acc-0.8844, test loss-0.4190, acc-0.8825\n",
      "Iter-62690, train loss-0.3396, acc-0.9000, valid loss-0.4150, acc-0.8846, test loss-0.4189, acc-0.8824\n",
      "Iter-62700, train loss-0.6442, acc-0.8200, valid loss-0.4150, acc-0.8844, test loss-0.4189, acc-0.8823\n",
      "Iter-62710, train loss-0.2766, acc-0.9400, valid loss-0.4150, acc-0.8846, test loss-0.4189, acc-0.8823\n",
      "Iter-62720, train loss-0.4295, acc-0.8200, valid loss-0.4149, acc-0.8846, test loss-0.4189, acc-0.8823\n",
      "Iter-62730, train loss-0.5635, acc-0.8200, valid loss-0.4149, acc-0.8844, test loss-0.4188, acc-0.8823\n",
      "Iter-62740, train loss-0.6095, acc-0.8800, valid loss-0.4149, acc-0.8842, test loss-0.4188, acc-0.8823\n",
      "Iter-62750, train loss-0.4958, acc-0.8400, valid loss-0.4149, acc-0.8846, test loss-0.4188, acc-0.8823\n",
      "Iter-62760, train loss-0.7239, acc-0.8400, valid loss-0.4148, acc-0.8844, test loss-0.4188, acc-0.8825\n",
      "Iter-62770, train loss-0.3010, acc-0.9200, valid loss-0.4148, acc-0.8844, test loss-0.4188, acc-0.8824\n",
      "Iter-62780, train loss-0.4579, acc-0.8600, valid loss-0.4148, acc-0.8844, test loss-0.4187, acc-0.8823\n",
      "Iter-62790, train loss-0.4751, acc-0.8200, valid loss-0.4148, acc-0.8842, test loss-0.4187, acc-0.8822\n",
      "Iter-62800, train loss-0.5028, acc-0.8600, valid loss-0.4147, acc-0.8842, test loss-0.4187, acc-0.8823\n",
      "Iter-62810, train loss-0.4249, acc-0.8800, valid loss-0.4147, acc-0.8844, test loss-0.4187, acc-0.8823\n",
      "Iter-62820, train loss-0.4608, acc-0.9000, valid loss-0.4147, acc-0.8844, test loss-0.4187, acc-0.8823\n",
      "Iter-62830, train loss-0.5217, acc-0.8400, valid loss-0.4147, acc-0.8842, test loss-0.4186, acc-0.8825\n",
      "Iter-62840, train loss-0.4160, acc-0.9000, valid loss-0.4146, acc-0.8842, test loss-0.4186, acc-0.8823\n",
      "Iter-62850, train loss-0.3920, acc-0.8800, valid loss-0.4146, acc-0.8842, test loss-0.4186, acc-0.8825\n",
      "Iter-62860, train loss-0.3330, acc-0.9000, valid loss-0.4146, acc-0.8844, test loss-0.4186, acc-0.8824\n",
      "Iter-62870, train loss-0.3583, acc-0.9400, valid loss-0.4145, acc-0.8844, test loss-0.4185, acc-0.8825\n",
      "Iter-62880, train loss-0.4454, acc-0.8200, valid loss-0.4145, acc-0.8844, test loss-0.4185, acc-0.8825\n",
      "Iter-62890, train loss-0.3658, acc-0.9000, valid loss-0.4145, acc-0.8844, test loss-0.4185, acc-0.8825\n",
      "Iter-62900, train loss-0.6833, acc-0.8000, valid loss-0.4145, acc-0.8844, test loss-0.4184, acc-0.8825\n",
      "Iter-62910, train loss-0.4111, acc-0.8400, valid loss-0.4144, acc-0.8844, test loss-0.4184, acc-0.8825\n",
      "Iter-62920, train loss-0.4483, acc-0.8800, valid loss-0.4144, acc-0.8844, test loss-0.4184, acc-0.8824\n",
      "Iter-62930, train loss-0.4029, acc-0.8800, valid loss-0.4144, acc-0.8844, test loss-0.4183, acc-0.8825\n",
      "Iter-62940, train loss-0.3302, acc-0.9000, valid loss-0.4144, acc-0.8844, test loss-0.4183, acc-0.8825\n",
      "Iter-62950, train loss-0.4009, acc-0.9200, valid loss-0.4144, acc-0.8842, test loss-0.4183, acc-0.8825\n",
      "Iter-62960, train loss-0.2845, acc-0.9200, valid loss-0.4143, acc-0.8842, test loss-0.4183, acc-0.8825\n",
      "Iter-62970, train loss-0.2817, acc-0.9200, valid loss-0.4143, acc-0.8844, test loss-0.4182, acc-0.8824\n",
      "Iter-62980, train loss-0.3875, acc-0.8600, valid loss-0.4143, acc-0.8842, test loss-0.4182, acc-0.8824\n",
      "Iter-62990, train loss-0.6311, acc-0.8800, valid loss-0.4142, acc-0.8844, test loss-0.4182, acc-0.8824\n",
      "Iter-63000, train loss-0.4452, acc-0.9200, valid loss-0.4142, acc-0.8844, test loss-0.4182, acc-0.8824\n",
      "Iter-63010, train loss-0.3353, acc-0.9000, valid loss-0.4142, acc-0.8844, test loss-0.4181, acc-0.8824\n",
      "Iter-63020, train loss-0.5334, acc-0.8200, valid loss-0.4142, acc-0.8844, test loss-0.4181, acc-0.8824\n",
      "Iter-63030, train loss-0.4481, acc-0.8600, valid loss-0.4141, acc-0.8842, test loss-0.4181, acc-0.8824\n",
      "Iter-63040, train loss-0.3796, acc-0.9200, valid loss-0.4141, acc-0.8844, test loss-0.4181, acc-0.8824\n",
      "Iter-63050, train loss-0.5824, acc-0.8800, valid loss-0.4141, acc-0.8844, test loss-0.4180, acc-0.8824\n",
      "Iter-63060, train loss-0.3872, acc-0.8600, valid loss-0.4140, acc-0.8846, test loss-0.4180, acc-0.8824\n",
      "Iter-63070, train loss-0.6321, acc-0.8800, valid loss-0.4140, acc-0.8846, test loss-0.4180, acc-0.8824\n",
      "Iter-63080, train loss-0.3017, acc-0.9200, valid loss-0.4140, acc-0.8846, test loss-0.4180, acc-0.8825\n",
      "Iter-63090, train loss-0.4807, acc-0.8800, valid loss-0.4140, acc-0.8842, test loss-0.4180, acc-0.8824\n",
      "Iter-63100, train loss-0.5397, acc-0.8800, valid loss-0.4139, acc-0.8842, test loss-0.4179, acc-0.8824\n",
      "Iter-63110, train loss-0.5601, acc-0.7600, valid loss-0.4139, acc-0.8844, test loss-0.4179, acc-0.8824\n",
      "Iter-63120, train loss-0.5254, acc-0.8200, valid loss-0.4139, acc-0.8846, test loss-0.4179, acc-0.8825\n",
      "Iter-63130, train loss-0.3229, acc-0.8800, valid loss-0.4138, acc-0.8846, test loss-0.4179, acc-0.8825\n",
      "Iter-63140, train loss-0.4214, acc-0.9000, valid loss-0.4138, acc-0.8846, test loss-0.4178, acc-0.8825\n",
      "Iter-63150, train loss-0.4013, acc-0.9400, valid loss-0.4138, acc-0.8846, test loss-0.4178, acc-0.8825\n",
      "Iter-63160, train loss-0.4269, acc-0.8600, valid loss-0.4137, acc-0.8846, test loss-0.4178, acc-0.8825\n",
      "Iter-63170, train loss-0.5321, acc-0.8200, valid loss-0.4137, acc-0.8846, test loss-0.4178, acc-0.8825\n",
      "Iter-63180, train loss-0.4398, acc-0.8400, valid loss-0.4137, acc-0.8846, test loss-0.4177, acc-0.8825\n",
      "Iter-63190, train loss-0.3458, acc-0.8600, valid loss-0.4137, acc-0.8846, test loss-0.4177, acc-0.8825\n",
      "Iter-63200, train loss-0.3441, acc-0.9200, valid loss-0.4136, acc-0.8848, test loss-0.4177, acc-0.8825\n",
      "Iter-63210, train loss-0.3379, acc-0.9000, valid loss-0.4136, acc-0.8848, test loss-0.4176, acc-0.8825\n",
      "Iter-63220, train loss-0.5930, acc-0.8600, valid loss-0.4136, acc-0.8848, test loss-0.4176, acc-0.8825\n",
      "Iter-63230, train loss-0.5346, acc-0.8400, valid loss-0.4135, acc-0.8848, test loss-0.4176, acc-0.8825\n",
      "Iter-63240, train loss-0.5194, acc-0.8600, valid loss-0.4135, acc-0.8848, test loss-0.4175, acc-0.8825\n",
      "Iter-63250, train loss-0.3915, acc-0.8600, valid loss-0.4135, acc-0.8848, test loss-0.4175, acc-0.8825\n",
      "Iter-63260, train loss-0.2233, acc-0.9200, valid loss-0.4134, acc-0.8848, test loss-0.4175, acc-0.8825\n",
      "Iter-63270, train loss-0.4733, acc-0.8800, valid loss-0.4134, acc-0.8848, test loss-0.4175, acc-0.8825\n",
      "Iter-63280, train loss-0.6687, acc-0.8200, valid loss-0.4134, acc-0.8848, test loss-0.4174, acc-0.8825\n",
      "Iter-63290, train loss-0.5850, acc-0.8400, valid loss-0.4134, acc-0.8848, test loss-0.4174, acc-0.8826\n",
      "Iter-63300, train loss-0.4987, acc-0.8800, valid loss-0.4133, acc-0.8848, test loss-0.4174, acc-0.8826\n",
      "Iter-63310, train loss-0.4302, acc-0.8600, valid loss-0.4133, acc-0.8850, test loss-0.4173, acc-0.8826\n",
      "Iter-63320, train loss-0.6977, acc-0.7800, valid loss-0.4133, acc-0.8850, test loss-0.4173, acc-0.8825\n",
      "Iter-63330, train loss-0.2734, acc-0.9600, valid loss-0.4132, acc-0.8850, test loss-0.4173, acc-0.8825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-63340, train loss-0.4568, acc-0.8800, valid loss-0.4132, acc-0.8850, test loss-0.4173, acc-0.8825\n",
      "Iter-63350, train loss-0.2866, acc-0.9000, valid loss-0.4132, acc-0.8848, test loss-0.4172, acc-0.8825\n",
      "Iter-63360, train loss-0.5992, acc-0.7600, valid loss-0.4132, acc-0.8850, test loss-0.4172, acc-0.8825\n",
      "Iter-63370, train loss-0.4180, acc-0.8400, valid loss-0.4131, acc-0.8848, test loss-0.4172, acc-0.8824\n",
      "Iter-63380, train loss-0.4708, acc-0.8400, valid loss-0.4131, acc-0.8848, test loss-0.4172, acc-0.8825\n",
      "Iter-63390, train loss-0.5463, acc-0.8800, valid loss-0.4131, acc-0.8848, test loss-0.4171, acc-0.8825\n",
      "Iter-63400, train loss-0.6401, acc-0.8400, valid loss-0.4131, acc-0.8848, test loss-0.4171, acc-0.8826\n",
      "Iter-63410, train loss-0.3900, acc-0.8800, valid loss-0.4130, acc-0.8848, test loss-0.4171, acc-0.8826\n",
      "Iter-63420, train loss-0.4011, acc-0.9000, valid loss-0.4130, acc-0.8848, test loss-0.4170, acc-0.8826\n",
      "Iter-63430, train loss-0.3920, acc-0.9000, valid loss-0.4130, acc-0.8848, test loss-0.4170, acc-0.8825\n",
      "Iter-63440, train loss-0.6358, acc-0.8200, valid loss-0.4130, acc-0.8850, test loss-0.4170, acc-0.8825\n",
      "Iter-63450, train loss-0.3821, acc-0.8600, valid loss-0.4129, acc-0.8848, test loss-0.4169, acc-0.8826\n",
      "Iter-63460, train loss-0.3441, acc-0.8800, valid loss-0.4129, acc-0.8850, test loss-0.4169, acc-0.8826\n",
      "Iter-63470, train loss-0.3251, acc-0.9200, valid loss-0.4129, acc-0.8850, test loss-0.4169, acc-0.8826\n",
      "Iter-63480, train loss-0.4238, acc-0.8800, valid loss-0.4128, acc-0.8850, test loss-0.4169, acc-0.8826\n",
      "Iter-63490, train loss-0.2474, acc-0.9400, valid loss-0.4128, acc-0.8850, test loss-0.4168, acc-0.8826\n",
      "Iter-63500, train loss-0.2342, acc-0.9400, valid loss-0.4128, acc-0.8850, test loss-0.4168, acc-0.8827\n",
      "Iter-63510, train loss-0.5868, acc-0.8200, valid loss-0.4127, acc-0.8850, test loss-0.4168, acc-0.8827\n",
      "Iter-63520, train loss-0.4892, acc-0.9000, valid loss-0.4127, acc-0.8852, test loss-0.4167, acc-0.8827\n",
      "Iter-63530, train loss-0.5304, acc-0.8200, valid loss-0.4127, acc-0.8852, test loss-0.4167, acc-0.8828\n",
      "Iter-63540, train loss-0.3726, acc-0.9000, valid loss-0.4127, acc-0.8852, test loss-0.4167, acc-0.8827\n",
      "Iter-63550, train loss-0.5258, acc-0.8400, valid loss-0.4126, acc-0.8850, test loss-0.4167, acc-0.8828\n",
      "Iter-63560, train loss-0.3800, acc-0.8600, valid loss-0.4126, acc-0.8850, test loss-0.4166, acc-0.8828\n",
      "Iter-63570, train loss-0.5421, acc-0.8600, valid loss-0.4126, acc-0.8850, test loss-0.4166, acc-0.8828\n",
      "Iter-63580, train loss-0.3280, acc-0.9000, valid loss-0.4125, acc-0.8852, test loss-0.4166, acc-0.8828\n",
      "Iter-63590, train loss-0.5798, acc-0.8200, valid loss-0.4125, acc-0.8850, test loss-0.4166, acc-0.8828\n",
      "Iter-63600, train loss-0.3130, acc-0.9400, valid loss-0.4125, acc-0.8850, test loss-0.4165, acc-0.8829\n",
      "Iter-63610, train loss-0.5688, acc-0.8200, valid loss-0.4124, acc-0.8852, test loss-0.4165, acc-0.8829\n",
      "Iter-63620, train loss-0.4437, acc-0.8600, valid loss-0.4124, acc-0.8852, test loss-0.4165, acc-0.8829\n",
      "Iter-63630, train loss-0.3308, acc-0.9200, valid loss-0.4124, acc-0.8852, test loss-0.4165, acc-0.8829\n",
      "Iter-63640, train loss-0.2626, acc-0.9400, valid loss-0.4124, acc-0.8852, test loss-0.4164, acc-0.8828\n",
      "Iter-63650, train loss-0.3840, acc-0.9200, valid loss-0.4123, acc-0.8852, test loss-0.4164, acc-0.8828\n",
      "Iter-63660, train loss-0.4366, acc-0.9000, valid loss-0.4123, acc-0.8852, test loss-0.4164, acc-0.8828\n",
      "Iter-63670, train loss-0.4675, acc-0.9000, valid loss-0.4123, acc-0.8852, test loss-0.4164, acc-0.8828\n",
      "Iter-63680, train loss-0.6021, acc-0.8200, valid loss-0.4123, acc-0.8850, test loss-0.4163, acc-0.8828\n",
      "Iter-63690, train loss-0.3757, acc-0.8600, valid loss-0.4122, acc-0.8850, test loss-0.4163, acc-0.8828\n",
      "Iter-63700, train loss-0.4289, acc-0.8600, valid loss-0.4122, acc-0.8852, test loss-0.4163, acc-0.8828\n",
      "Iter-63710, train loss-0.5353, acc-0.8600, valid loss-0.4122, acc-0.8854, test loss-0.4163, acc-0.8829\n",
      "Iter-63720, train loss-0.2815, acc-0.9000, valid loss-0.4122, acc-0.8854, test loss-0.4162, acc-0.8828\n",
      "Iter-63730, train loss-0.4571, acc-0.9000, valid loss-0.4121, acc-0.8856, test loss-0.4162, acc-0.8828\n",
      "Iter-63740, train loss-0.3979, acc-0.8600, valid loss-0.4121, acc-0.8854, test loss-0.4162, acc-0.8828\n",
      "Iter-63750, train loss-0.3462, acc-0.9000, valid loss-0.4121, acc-0.8854, test loss-0.4162, acc-0.8828\n",
      "Iter-63760, train loss-0.4762, acc-0.8400, valid loss-0.4121, acc-0.8854, test loss-0.4161, acc-0.8829\n",
      "Iter-63770, train loss-0.3302, acc-0.8600, valid loss-0.4120, acc-0.8854, test loss-0.4161, acc-0.8829\n",
      "Iter-63780, train loss-0.5035, acc-0.8000, valid loss-0.4120, acc-0.8854, test loss-0.4161, acc-0.8829\n",
      "Iter-63790, train loss-0.4974, acc-0.8600, valid loss-0.4120, acc-0.8856, test loss-0.4161, acc-0.8829\n",
      "Iter-63800, train loss-0.4673, acc-0.8400, valid loss-0.4120, acc-0.8856, test loss-0.4160, acc-0.8828\n",
      "Iter-63810, train loss-0.5219, acc-0.8400, valid loss-0.4119, acc-0.8856, test loss-0.4160, acc-0.8828\n",
      "Iter-63820, train loss-0.4083, acc-0.8800, valid loss-0.4119, acc-0.8858, test loss-0.4160, acc-0.8828\n",
      "Iter-63830, train loss-0.2794, acc-0.9400, valid loss-0.4119, acc-0.8856, test loss-0.4160, acc-0.8828\n",
      "Iter-63840, train loss-0.3413, acc-0.9000, valid loss-0.4119, acc-0.8858, test loss-0.4159, acc-0.8828\n",
      "Iter-63850, train loss-0.5542, acc-0.8000, valid loss-0.4118, acc-0.8858, test loss-0.4159, acc-0.8829\n",
      "Iter-63860, train loss-0.5300, acc-0.8400, valid loss-0.4118, acc-0.8858, test loss-0.4159, acc-0.8828\n",
      "Iter-63870, train loss-0.7793, acc-0.7400, valid loss-0.4118, acc-0.8858, test loss-0.4159, acc-0.8828\n",
      "Iter-63880, train loss-0.4527, acc-0.8800, valid loss-0.4118, acc-0.8858, test loss-0.4158, acc-0.8828\n",
      "Iter-63890, train loss-0.4416, acc-0.8600, valid loss-0.4117, acc-0.8858, test loss-0.4158, acc-0.8828\n",
      "Iter-63900, train loss-0.2966, acc-0.9200, valid loss-0.4117, acc-0.8858, test loss-0.4158, acc-0.8828\n",
      "Iter-63910, train loss-0.5176, acc-0.8800, valid loss-0.4117, acc-0.8858, test loss-0.4158, acc-0.8829\n",
      "Iter-63920, train loss-0.5373, acc-0.7600, valid loss-0.4116, acc-0.8858, test loss-0.4157, acc-0.8829\n",
      "Iter-63930, train loss-0.4135, acc-0.9200, valid loss-0.4116, acc-0.8858, test loss-0.4157, acc-0.8829\n",
      "Iter-63940, train loss-0.3930, acc-0.8400, valid loss-0.4116, acc-0.8858, test loss-0.4157, acc-0.8828\n",
      "Iter-63950, train loss-0.2953, acc-0.9400, valid loss-0.4116, acc-0.8858, test loss-0.4156, acc-0.8828\n",
      "Iter-63960, train loss-0.4407, acc-0.8600, valid loss-0.4115, acc-0.8856, test loss-0.4156, acc-0.8828\n",
      "Iter-63970, train loss-0.3947, acc-0.9000, valid loss-0.4115, acc-0.8856, test loss-0.4156, acc-0.8829\n",
      "Iter-63980, train loss-0.4402, acc-0.9000, valid loss-0.4115, acc-0.8856, test loss-0.4156, acc-0.8830\n",
      "Iter-63990, train loss-0.7547, acc-0.7200, valid loss-0.4115, acc-0.8856, test loss-0.4155, acc-0.8831\n",
      "Iter-64000, train loss-0.3019, acc-0.9400, valid loss-0.4115, acc-0.8856, test loss-0.4155, acc-0.8831\n",
      "Iter-64010, train loss-0.4695, acc-0.8400, valid loss-0.4114, acc-0.8856, test loss-0.4155, acc-0.8831\n",
      "Iter-64020, train loss-0.3771, acc-0.8800, valid loss-0.4114, acc-0.8856, test loss-0.4155, acc-0.8831\n",
      "Iter-64030, train loss-0.4111, acc-0.9000, valid loss-0.4114, acc-0.8858, test loss-0.4154, acc-0.8832\n",
      "Iter-64040, train loss-0.4979, acc-0.8200, valid loss-0.4114, acc-0.8858, test loss-0.4154, acc-0.8832\n",
      "Iter-64050, train loss-0.7179, acc-0.7800, valid loss-0.4113, acc-0.8858, test loss-0.4154, acc-0.8832\n",
      "Iter-64060, train loss-0.4136, acc-0.8600, valid loss-0.4113, acc-0.8856, test loss-0.4153, acc-0.8832\n",
      "Iter-64070, train loss-0.3357, acc-0.9000, valid loss-0.4113, acc-0.8858, test loss-0.4153, acc-0.8832\n",
      "Iter-64080, train loss-0.4139, acc-0.8600, valid loss-0.4113, acc-0.8858, test loss-0.4153, acc-0.8833\n",
      "Iter-64090, train loss-0.4495, acc-0.8400, valid loss-0.4113, acc-0.8858, test loss-0.4153, acc-0.8833\n",
      "Iter-64100, train loss-0.3271, acc-0.8800, valid loss-0.4112, acc-0.8858, test loss-0.4152, acc-0.8833\n",
      "Iter-64110, train loss-0.3924, acc-0.8600, valid loss-0.4112, acc-0.8856, test loss-0.4152, acc-0.8833\n",
      "Iter-64120, train loss-0.5020, acc-0.8600, valid loss-0.4112, acc-0.8856, test loss-0.4152, acc-0.8832\n",
      "Iter-64130, train loss-0.4339, acc-0.8400, valid loss-0.4112, acc-0.8856, test loss-0.4152, acc-0.8833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-64140, train loss-0.4239, acc-0.8800, valid loss-0.4111, acc-0.8856, test loss-0.4151, acc-0.8834\n",
      "Iter-64150, train loss-0.6686, acc-0.8400, valid loss-0.4111, acc-0.8856, test loss-0.4151, acc-0.8834\n",
      "Iter-64160, train loss-0.4732, acc-0.9000, valid loss-0.4111, acc-0.8858, test loss-0.4151, acc-0.8834\n",
      "Iter-64170, train loss-0.3818, acc-0.8600, valid loss-0.4110, acc-0.8858, test loss-0.4151, acc-0.8833\n",
      "Iter-64180, train loss-0.3744, acc-0.8600, valid loss-0.4110, acc-0.8858, test loss-0.4150, acc-0.8833\n",
      "Iter-64190, train loss-0.4023, acc-0.9000, valid loss-0.4110, acc-0.8858, test loss-0.4150, acc-0.8833\n",
      "Iter-64200, train loss-0.2738, acc-0.9600, valid loss-0.4110, acc-0.8858, test loss-0.4150, acc-0.8833\n",
      "Iter-64210, train loss-0.5346, acc-0.8600, valid loss-0.4109, acc-0.8858, test loss-0.4150, acc-0.8834\n",
      "Iter-64220, train loss-0.3228, acc-0.9000, valid loss-0.4109, acc-0.8858, test loss-0.4149, acc-0.8835\n",
      "Iter-64230, train loss-0.3679, acc-0.9200, valid loss-0.4109, acc-0.8858, test loss-0.4149, acc-0.8835\n",
      "Iter-64240, train loss-0.3790, acc-0.8800, valid loss-0.4109, acc-0.8858, test loss-0.4149, acc-0.8834\n",
      "Iter-64250, train loss-0.3490, acc-0.9200, valid loss-0.4108, acc-0.8856, test loss-0.4148, acc-0.8834\n",
      "Iter-64260, train loss-0.6862, acc-0.8400, valid loss-0.4108, acc-0.8856, test loss-0.4148, acc-0.8833\n",
      "Iter-64270, train loss-0.3697, acc-0.8800, valid loss-0.4108, acc-0.8856, test loss-0.4148, acc-0.8834\n",
      "Iter-64280, train loss-0.3471, acc-0.8800, valid loss-0.4107, acc-0.8856, test loss-0.4147, acc-0.8834\n",
      "Iter-64290, train loss-0.5300, acc-0.8600, valid loss-0.4107, acc-0.8858, test loss-0.4147, acc-0.8834\n",
      "Iter-64300, train loss-0.3317, acc-0.9200, valid loss-0.4107, acc-0.8856, test loss-0.4147, acc-0.8835\n",
      "Iter-64310, train loss-0.5943, acc-0.8000, valid loss-0.4107, acc-0.8856, test loss-0.4147, acc-0.8835\n",
      "Iter-64320, train loss-0.2687, acc-0.9600, valid loss-0.4106, acc-0.8860, test loss-0.4147, acc-0.8835\n",
      "Iter-64330, train loss-0.2767, acc-0.9000, valid loss-0.4106, acc-0.8858, test loss-0.4146, acc-0.8835\n",
      "Iter-64340, train loss-0.3531, acc-0.9600, valid loss-0.4106, acc-0.8858, test loss-0.4146, acc-0.8834\n",
      "Iter-64350, train loss-0.4932, acc-0.9200, valid loss-0.4105, acc-0.8858, test loss-0.4146, acc-0.8835\n",
      "Iter-64360, train loss-0.4948, acc-0.8600, valid loss-0.4105, acc-0.8858, test loss-0.4146, acc-0.8835\n",
      "Iter-64370, train loss-0.2287, acc-0.9600, valid loss-0.4105, acc-0.8858, test loss-0.4145, acc-0.8835\n",
      "Iter-64380, train loss-0.4598, acc-0.8800, valid loss-0.4105, acc-0.8858, test loss-0.4145, acc-0.8834\n",
      "Iter-64390, train loss-0.5211, acc-0.8000, valid loss-0.4104, acc-0.8858, test loss-0.4145, acc-0.8834\n",
      "Iter-64400, train loss-0.6081, acc-0.8000, valid loss-0.4104, acc-0.8858, test loss-0.4145, acc-0.8836\n",
      "Iter-64410, train loss-0.5480, acc-0.7800, valid loss-0.4104, acc-0.8858, test loss-0.4144, acc-0.8836\n",
      "Iter-64420, train loss-0.3911, acc-0.8600, valid loss-0.4103, acc-0.8858, test loss-0.4144, acc-0.8836\n",
      "Iter-64430, train loss-0.4452, acc-0.8800, valid loss-0.4103, acc-0.8858, test loss-0.4144, acc-0.8836\n",
      "Iter-64440, train loss-0.5544, acc-0.8800, valid loss-0.4102, acc-0.8858, test loss-0.4143, acc-0.8835\n",
      "Iter-64450, train loss-0.5181, acc-0.8600, valid loss-0.4102, acc-0.8860, test loss-0.4143, acc-0.8835\n",
      "Iter-64460, train loss-0.3134, acc-0.9000, valid loss-0.4102, acc-0.8862, test loss-0.4143, acc-0.8836\n",
      "Iter-64470, train loss-0.2981, acc-0.9600, valid loss-0.4102, acc-0.8858, test loss-0.4143, acc-0.8836\n",
      "Iter-64480, train loss-0.3313, acc-0.9200, valid loss-0.4102, acc-0.8862, test loss-0.4142, acc-0.8836\n",
      "Iter-64490, train loss-0.4003, acc-0.8600, valid loss-0.4101, acc-0.8862, test loss-0.4142, acc-0.8836\n",
      "Iter-64500, train loss-0.5456, acc-0.8400, valid loss-0.4101, acc-0.8862, test loss-0.4142, acc-0.8835\n",
      "Iter-64510, train loss-0.4697, acc-0.8600, valid loss-0.4101, acc-0.8864, test loss-0.4141, acc-0.8836\n",
      "Iter-64520, train loss-0.3573, acc-0.8800, valid loss-0.4100, acc-0.8864, test loss-0.4141, acc-0.8835\n",
      "Iter-64530, train loss-0.4911, acc-0.8000, valid loss-0.4100, acc-0.8866, test loss-0.4141, acc-0.8835\n",
      "Iter-64540, train loss-0.4503, acc-0.8800, valid loss-0.4100, acc-0.8866, test loss-0.4141, acc-0.8836\n",
      "Iter-64550, train loss-0.4878, acc-0.8600, valid loss-0.4099, acc-0.8864, test loss-0.4140, acc-0.8836\n",
      "Iter-64560, train loss-0.3405, acc-0.8400, valid loss-0.4099, acc-0.8862, test loss-0.4140, acc-0.8835\n",
      "Iter-64570, train loss-0.3702, acc-0.8800, valid loss-0.4099, acc-0.8862, test loss-0.4140, acc-0.8835\n",
      "Iter-64580, train loss-0.1993, acc-0.9600, valid loss-0.4099, acc-0.8862, test loss-0.4139, acc-0.8834\n",
      "Iter-64590, train loss-0.3295, acc-0.8800, valid loss-0.4098, acc-0.8862, test loss-0.4139, acc-0.8834\n",
      "Iter-64600, train loss-0.3799, acc-0.8800, valid loss-0.4098, acc-0.8860, test loss-0.4139, acc-0.8836\n",
      "Iter-64610, train loss-0.2927, acc-0.9200, valid loss-0.4098, acc-0.8862, test loss-0.4139, acc-0.8835\n",
      "Iter-64620, train loss-0.6755, acc-0.8200, valid loss-0.4097, acc-0.8862, test loss-0.4138, acc-0.8834\n",
      "Iter-64630, train loss-0.5090, acc-0.8400, valid loss-0.4097, acc-0.8864, test loss-0.4138, acc-0.8834\n",
      "Iter-64640, train loss-0.3157, acc-0.9400, valid loss-0.4097, acc-0.8862, test loss-0.4138, acc-0.8835\n",
      "Iter-64650, train loss-0.3938, acc-0.8800, valid loss-0.4097, acc-0.8864, test loss-0.4137, acc-0.8835\n",
      "Iter-64660, train loss-0.4863, acc-0.8400, valid loss-0.4096, acc-0.8862, test loss-0.4137, acc-0.8835\n",
      "Iter-64670, train loss-0.3772, acc-0.8200, valid loss-0.4096, acc-0.8864, test loss-0.4137, acc-0.8835\n",
      "Iter-64680, train loss-0.5569, acc-0.7800, valid loss-0.4096, acc-0.8866, test loss-0.4136, acc-0.8834\n",
      "Iter-64690, train loss-0.4488, acc-0.8800, valid loss-0.4095, acc-0.8868, test loss-0.4136, acc-0.8834\n",
      "Iter-64700, train loss-0.3566, acc-0.9000, valid loss-0.4095, acc-0.8868, test loss-0.4136, acc-0.8834\n",
      "Iter-64710, train loss-0.4060, acc-0.8800, valid loss-0.4095, acc-0.8868, test loss-0.4136, acc-0.8834\n",
      "Iter-64720, train loss-0.4774, acc-0.9000, valid loss-0.4095, acc-0.8866, test loss-0.4136, acc-0.8836\n",
      "Iter-64730, train loss-0.2621, acc-0.9600, valid loss-0.4094, acc-0.8868, test loss-0.4135, acc-0.8834\n",
      "Iter-64740, train loss-0.8164, acc-0.7200, valid loss-0.4094, acc-0.8868, test loss-0.4135, acc-0.8834\n",
      "Iter-64750, train loss-0.5598, acc-0.8000, valid loss-0.4094, acc-0.8868, test loss-0.4135, acc-0.8835\n",
      "Iter-64760, train loss-0.4675, acc-0.8400, valid loss-0.4094, acc-0.8866, test loss-0.4135, acc-0.8836\n",
      "Iter-64770, train loss-0.7287, acc-0.8800, valid loss-0.4093, acc-0.8864, test loss-0.4134, acc-0.8836\n",
      "Iter-64780, train loss-0.4933, acc-0.8600, valid loss-0.4093, acc-0.8866, test loss-0.4134, acc-0.8836\n",
      "Iter-64790, train loss-0.3915, acc-0.9000, valid loss-0.4093, acc-0.8864, test loss-0.4134, acc-0.8836\n",
      "Iter-64800, train loss-0.4723, acc-0.8800, valid loss-0.4093, acc-0.8864, test loss-0.4133, acc-0.8836\n",
      "Iter-64810, train loss-0.3644, acc-0.8600, valid loss-0.4092, acc-0.8864, test loss-0.4133, acc-0.8837\n",
      "Iter-64820, train loss-0.5931, acc-0.7800, valid loss-0.4092, acc-0.8864, test loss-0.4133, acc-0.8837\n",
      "Iter-64830, train loss-0.2502, acc-0.9200, valid loss-0.4092, acc-0.8864, test loss-0.4132, acc-0.8837\n",
      "Iter-64840, train loss-0.5082, acc-0.8600, valid loss-0.4091, acc-0.8866, test loss-0.4132, acc-0.8838\n",
      "Iter-64850, train loss-0.4393, acc-0.8800, valid loss-0.4091, acc-0.8864, test loss-0.4132, acc-0.8838\n",
      "Iter-64860, train loss-0.3871, acc-0.9000, valid loss-0.4091, acc-0.8866, test loss-0.4132, acc-0.8837\n",
      "Iter-64870, train loss-0.4088, acc-0.8800, valid loss-0.4090, acc-0.8868, test loss-0.4131, acc-0.8837\n",
      "Iter-64880, train loss-0.4570, acc-0.8600, valid loss-0.4090, acc-0.8864, test loss-0.4131, acc-0.8837\n",
      "Iter-64890, train loss-0.7296, acc-0.8200, valid loss-0.4090, acc-0.8868, test loss-0.4131, acc-0.8838\n",
      "Iter-64900, train loss-0.4458, acc-0.8400, valid loss-0.4090, acc-0.8868, test loss-0.4131, acc-0.8839\n",
      "Iter-64910, train loss-0.5733, acc-0.8400, valid loss-0.4089, acc-0.8868, test loss-0.4130, acc-0.8840\n",
      "Iter-64920, train loss-0.3898, acc-0.9200, valid loss-0.4089, acc-0.8868, test loss-0.4130, acc-0.8839\n",
      "Iter-64930, train loss-0.4373, acc-0.8600, valid loss-0.4089, acc-0.8868, test loss-0.4130, acc-0.8840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-64940, train loss-0.4555, acc-0.8400, valid loss-0.4089, acc-0.8868, test loss-0.4130, acc-0.8838\n",
      "Iter-64950, train loss-0.7103, acc-0.8200, valid loss-0.4088, acc-0.8868, test loss-0.4129, acc-0.8839\n",
      "Iter-64960, train loss-0.3556, acc-0.9200, valid loss-0.4088, acc-0.8868, test loss-0.4129, acc-0.8841\n",
      "Iter-64970, train loss-0.3859, acc-0.9000, valid loss-0.4088, acc-0.8868, test loss-0.4129, acc-0.8840\n",
      "Iter-64980, train loss-0.4270, acc-0.8200, valid loss-0.4087, acc-0.8868, test loss-0.4129, acc-0.8841\n",
      "Iter-64990, train loss-0.4058, acc-0.9200, valid loss-0.4087, acc-0.8868, test loss-0.4128, acc-0.8840\n",
      "Iter-65000, train loss-0.4019, acc-0.8800, valid loss-0.4087, acc-0.8868, test loss-0.4128, acc-0.8840\n",
      "Iter-65010, train loss-0.4841, acc-0.8600, valid loss-0.4087, acc-0.8868, test loss-0.4128, acc-0.8840\n",
      "Iter-65020, train loss-0.3542, acc-0.9200, valid loss-0.4086, acc-0.8868, test loss-0.4127, acc-0.8840\n",
      "Iter-65030, train loss-0.4962, acc-0.8600, valid loss-0.4086, acc-0.8866, test loss-0.4127, acc-0.8840\n",
      "Iter-65040, train loss-0.3018, acc-0.9200, valid loss-0.4086, acc-0.8864, test loss-0.4127, acc-0.8841\n",
      "Iter-65050, train loss-0.3693, acc-0.9000, valid loss-0.4086, acc-0.8864, test loss-0.4127, acc-0.8841\n",
      "Iter-65060, train loss-0.3490, acc-0.8800, valid loss-0.4085, acc-0.8862, test loss-0.4126, acc-0.8841\n",
      "Iter-65070, train loss-0.4399, acc-0.8400, valid loss-0.4085, acc-0.8866, test loss-0.4126, acc-0.8841\n",
      "Iter-65080, train loss-0.4711, acc-0.9000, valid loss-0.4085, acc-0.8862, test loss-0.4126, acc-0.8842\n",
      "Iter-65090, train loss-0.3964, acc-0.9000, valid loss-0.4085, acc-0.8862, test loss-0.4126, acc-0.8842\n",
      "Iter-65100, train loss-0.4027, acc-0.8600, valid loss-0.4084, acc-0.8862, test loss-0.4125, acc-0.8842\n",
      "Iter-65110, train loss-0.5762, acc-0.8200, valid loss-0.4084, acc-0.8864, test loss-0.4125, acc-0.8842\n",
      "Iter-65120, train loss-0.7407, acc-0.8000, valid loss-0.4084, acc-0.8864, test loss-0.4125, acc-0.8842\n",
      "Iter-65130, train loss-0.3448, acc-0.8800, valid loss-0.4084, acc-0.8864, test loss-0.4125, acc-0.8842\n",
      "Iter-65140, train loss-0.5106, acc-0.7800, valid loss-0.4083, acc-0.8864, test loss-0.4124, acc-0.8842\n",
      "Iter-65150, train loss-0.3629, acc-0.9200, valid loss-0.4083, acc-0.8866, test loss-0.4124, acc-0.8841\n",
      "Iter-65160, train loss-0.2774, acc-0.8800, valid loss-0.4083, acc-0.8866, test loss-0.4124, acc-0.8841\n",
      "Iter-65170, train loss-0.5506, acc-0.8600, valid loss-0.4082, acc-0.8864, test loss-0.4124, acc-0.8841\n",
      "Iter-65180, train loss-0.2739, acc-0.9400, valid loss-0.4082, acc-0.8866, test loss-0.4123, acc-0.8841\n",
      "Iter-65190, train loss-0.3384, acc-0.8800, valid loss-0.4082, acc-0.8866, test loss-0.4123, acc-0.8841\n",
      "Iter-65200, train loss-0.2461, acc-0.9600, valid loss-0.4082, acc-0.8866, test loss-0.4123, acc-0.8841\n",
      "Iter-65210, train loss-0.2933, acc-0.9200, valid loss-0.4081, acc-0.8864, test loss-0.4122, acc-0.8840\n",
      "Iter-65220, train loss-0.4297, acc-0.8800, valid loss-0.4081, acc-0.8866, test loss-0.4122, acc-0.8841\n",
      "Iter-65230, train loss-0.4534, acc-0.8400, valid loss-0.4081, acc-0.8866, test loss-0.4122, acc-0.8840\n",
      "Iter-65240, train loss-0.6718, acc-0.8000, valid loss-0.4080, acc-0.8866, test loss-0.4122, acc-0.8840\n",
      "Iter-65250, train loss-0.5047, acc-0.9000, valid loss-0.4080, acc-0.8866, test loss-0.4121, acc-0.8841\n",
      "Iter-65260, train loss-0.5160, acc-0.8200, valid loss-0.4080, acc-0.8866, test loss-0.4121, acc-0.8842\n",
      "Iter-65270, train loss-0.5516, acc-0.8600, valid loss-0.4080, acc-0.8866, test loss-0.4121, acc-0.8842\n",
      "Iter-65280, train loss-0.4338, acc-0.8800, valid loss-0.4080, acc-0.8866, test loss-0.4121, acc-0.8842\n",
      "Iter-65290, train loss-0.1997, acc-0.9600, valid loss-0.4079, acc-0.8866, test loss-0.4120, acc-0.8842\n",
      "Iter-65300, train loss-0.2929, acc-0.9200, valid loss-0.4079, acc-0.8866, test loss-0.4120, acc-0.8842\n",
      "Iter-65310, train loss-0.3634, acc-0.9000, valid loss-0.4079, acc-0.8868, test loss-0.4120, acc-0.8842\n",
      "Iter-65320, train loss-0.3549, acc-0.8600, valid loss-0.4078, acc-0.8868, test loss-0.4119, acc-0.8842\n",
      "Iter-65330, train loss-0.5885, acc-0.8000, valid loss-0.4078, acc-0.8868, test loss-0.4119, acc-0.8843\n",
      "Iter-65340, train loss-0.5627, acc-0.8200, valid loss-0.4078, acc-0.8868, test loss-0.4119, acc-0.8843\n",
      "Iter-65350, train loss-0.3692, acc-0.9200, valid loss-0.4078, acc-0.8868, test loss-0.4119, acc-0.8843\n",
      "Iter-65360, train loss-0.4112, acc-0.8600, valid loss-0.4078, acc-0.8868, test loss-0.4118, acc-0.8843\n",
      "Iter-65370, train loss-0.5249, acc-0.8600, valid loss-0.4077, acc-0.8868, test loss-0.4118, acc-0.8843\n",
      "Iter-65380, train loss-0.4037, acc-0.8600, valid loss-0.4077, acc-0.8868, test loss-0.4118, acc-0.8843\n",
      "Iter-65390, train loss-0.5415, acc-0.8400, valid loss-0.4077, acc-0.8868, test loss-0.4117, acc-0.8843\n",
      "Iter-65400, train loss-0.5099, acc-0.8600, valid loss-0.4077, acc-0.8868, test loss-0.4117, acc-0.8843\n",
      "Iter-65410, train loss-0.5167, acc-0.8400, valid loss-0.4076, acc-0.8870, test loss-0.4117, acc-0.8843\n",
      "Iter-65420, train loss-0.3368, acc-0.9400, valid loss-0.4076, acc-0.8868, test loss-0.4117, acc-0.8843\n",
      "Iter-65430, train loss-0.2275, acc-0.9800, valid loss-0.4076, acc-0.8870, test loss-0.4116, acc-0.8843\n",
      "Iter-65440, train loss-0.3888, acc-0.9000, valid loss-0.4076, acc-0.8868, test loss-0.4116, acc-0.8843\n",
      "Iter-65450, train loss-0.5506, acc-0.9200, valid loss-0.4075, acc-0.8870, test loss-0.4116, acc-0.8843\n",
      "Iter-65460, train loss-0.3103, acc-0.9400, valid loss-0.4075, acc-0.8870, test loss-0.4116, acc-0.8843\n",
      "Iter-65470, train loss-0.3756, acc-0.8800, valid loss-0.4075, acc-0.8868, test loss-0.4116, acc-0.8844\n",
      "Iter-65480, train loss-0.5590, acc-0.8000, valid loss-0.4074, acc-0.8868, test loss-0.4115, acc-0.8844\n",
      "Iter-65490, train loss-0.3217, acc-0.8800, valid loss-0.4074, acc-0.8868, test loss-0.4115, acc-0.8843\n",
      "Iter-65500, train loss-0.2870, acc-0.9000, valid loss-0.4074, acc-0.8868, test loss-0.4115, acc-0.8844\n",
      "Iter-65510, train loss-0.5219, acc-0.8800, valid loss-0.4074, acc-0.8876, test loss-0.4114, acc-0.8844\n",
      "Iter-65520, train loss-0.2904, acc-0.9400, valid loss-0.4073, acc-0.8874, test loss-0.4114, acc-0.8845\n",
      "Iter-65530, train loss-0.5384, acc-0.8600, valid loss-0.4073, acc-0.8874, test loss-0.4114, acc-0.8844\n",
      "Iter-65540, train loss-0.5492, acc-0.8600, valid loss-0.4073, acc-0.8876, test loss-0.4114, acc-0.8846\n",
      "Iter-65550, train loss-0.5106, acc-0.8600, valid loss-0.4072, acc-0.8876, test loss-0.4113, acc-0.8846\n",
      "Iter-65560, train loss-0.3894, acc-0.8200, valid loss-0.4072, acc-0.8874, test loss-0.4113, acc-0.8846\n",
      "Iter-65570, train loss-0.4167, acc-0.8800, valid loss-0.4072, acc-0.8874, test loss-0.4113, acc-0.8846\n",
      "Iter-65580, train loss-0.3807, acc-0.9200, valid loss-0.4072, acc-0.8874, test loss-0.4113, acc-0.8847\n",
      "Iter-65590, train loss-0.3385, acc-0.8600, valid loss-0.4071, acc-0.8874, test loss-0.4112, acc-0.8847\n",
      "Iter-65600, train loss-0.4094, acc-0.8800, valid loss-0.4071, acc-0.8876, test loss-0.4112, acc-0.8846\n",
      "Iter-65610, train loss-0.6418, acc-0.8400, valid loss-0.4071, acc-0.8876, test loss-0.4112, acc-0.8847\n",
      "Iter-65620, train loss-0.4254, acc-0.8400, valid loss-0.4071, acc-0.8876, test loss-0.4111, acc-0.8848\n",
      "Iter-65630, train loss-0.5076, acc-0.9200, valid loss-0.4070, acc-0.8874, test loss-0.4111, acc-0.8848\n",
      "Iter-65640, train loss-0.4700, acc-0.8800, valid loss-0.4070, acc-0.8876, test loss-0.4111, acc-0.8847\n",
      "Iter-65650, train loss-0.3960, acc-0.8600, valid loss-0.4070, acc-0.8874, test loss-0.4111, acc-0.8847\n",
      "Iter-65660, train loss-0.3494, acc-0.9200, valid loss-0.4070, acc-0.8874, test loss-0.4110, acc-0.8847\n",
      "Iter-65670, train loss-0.1885, acc-0.9600, valid loss-0.4070, acc-0.8876, test loss-0.4110, acc-0.8847\n",
      "Iter-65680, train loss-0.4848, acc-0.8400, valid loss-0.4069, acc-0.8876, test loss-0.4110, acc-0.8848\n",
      "Iter-65690, train loss-0.3486, acc-0.9000, valid loss-0.4069, acc-0.8876, test loss-0.4110, acc-0.8848\n",
      "Iter-65700, train loss-0.5723, acc-0.8800, valid loss-0.4069, acc-0.8876, test loss-0.4110, acc-0.8848\n",
      "Iter-65710, train loss-0.3007, acc-0.9400, valid loss-0.4068, acc-0.8876, test loss-0.4109, acc-0.8848\n",
      "Iter-65720, train loss-0.4505, acc-0.8400, valid loss-0.4068, acc-0.8876, test loss-0.4109, acc-0.8848\n",
      "Iter-65730, train loss-0.5855, acc-0.8400, valid loss-0.4068, acc-0.8876, test loss-0.4109, acc-0.8847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-65740, train loss-0.5736, acc-0.8400, valid loss-0.4067, acc-0.8876, test loss-0.4109, acc-0.8847\n",
      "Iter-65750, train loss-0.1599, acc-0.9600, valid loss-0.4067, acc-0.8878, test loss-0.4108, acc-0.8846\n",
      "Iter-65760, train loss-0.4027, acc-0.8400, valid loss-0.4067, acc-0.8878, test loss-0.4108, acc-0.8846\n",
      "Iter-65770, train loss-0.3785, acc-0.9000, valid loss-0.4066, acc-0.8878, test loss-0.4108, acc-0.8846\n",
      "Iter-65780, train loss-0.4225, acc-0.8800, valid loss-0.4066, acc-0.8878, test loss-0.4108, acc-0.8845\n",
      "Iter-65790, train loss-0.7038, acc-0.8000, valid loss-0.4066, acc-0.8878, test loss-0.4108, acc-0.8846\n",
      "Iter-65800, train loss-0.4437, acc-0.9000, valid loss-0.4066, acc-0.8878, test loss-0.4107, acc-0.8846\n",
      "Iter-65810, train loss-0.4948, acc-0.8600, valid loss-0.4066, acc-0.8876, test loss-0.4107, acc-0.8847\n",
      "Iter-65820, train loss-0.4655, acc-0.8600, valid loss-0.4065, acc-0.8878, test loss-0.4107, acc-0.8847\n",
      "Iter-65830, train loss-0.4198, acc-0.8800, valid loss-0.4065, acc-0.8878, test loss-0.4106, acc-0.8847\n",
      "Iter-65840, train loss-0.7333, acc-0.8000, valid loss-0.4065, acc-0.8876, test loss-0.4106, acc-0.8846\n",
      "Iter-65850, train loss-0.3368, acc-0.9200, valid loss-0.4065, acc-0.8876, test loss-0.4106, acc-0.8846\n",
      "Iter-65860, train loss-0.4710, acc-0.8600, valid loss-0.4064, acc-0.8878, test loss-0.4106, acc-0.8846\n",
      "Iter-65870, train loss-0.3320, acc-0.9000, valid loss-0.4064, acc-0.8876, test loss-0.4105, acc-0.8847\n",
      "Iter-65880, train loss-0.4074, acc-0.8600, valid loss-0.4064, acc-0.8878, test loss-0.4105, acc-0.8847\n",
      "Iter-65890, train loss-0.3196, acc-0.9000, valid loss-0.4063, acc-0.8876, test loss-0.4105, acc-0.8847\n",
      "Iter-65900, train loss-0.3883, acc-0.9400, valid loss-0.4063, acc-0.8876, test loss-0.4105, acc-0.8848\n",
      "Iter-65910, train loss-0.3461, acc-0.9400, valid loss-0.4063, acc-0.8876, test loss-0.4104, acc-0.8848\n",
      "Iter-65920, train loss-0.4177, acc-0.8800, valid loss-0.4063, acc-0.8876, test loss-0.4104, acc-0.8848\n",
      "Iter-65930, train loss-0.3869, acc-0.8800, valid loss-0.4062, acc-0.8876, test loss-0.4104, acc-0.8847\n",
      "Iter-65940, train loss-0.3073, acc-0.9000, valid loss-0.4062, acc-0.8876, test loss-0.4104, acc-0.8847\n",
      "Iter-65950, train loss-0.3490, acc-0.9000, valid loss-0.4062, acc-0.8876, test loss-0.4103, acc-0.8847\n",
      "Iter-65960, train loss-0.4751, acc-0.8400, valid loss-0.4061, acc-0.8876, test loss-0.4103, acc-0.8847\n",
      "Iter-65970, train loss-0.2244, acc-0.9800, valid loss-0.4061, acc-0.8876, test loss-0.4103, acc-0.8848\n",
      "Iter-65980, train loss-0.5315, acc-0.8400, valid loss-0.4061, acc-0.8876, test loss-0.4103, acc-0.8848\n",
      "Iter-65990, train loss-0.3292, acc-0.9000, valid loss-0.4061, acc-0.8880, test loss-0.4102, acc-0.8848\n",
      "Iter-66000, train loss-0.3759, acc-0.9000, valid loss-0.4060, acc-0.8880, test loss-0.4102, acc-0.8848\n",
      "Iter-66010, train loss-0.5033, acc-0.8600, valid loss-0.4060, acc-0.8880, test loss-0.4102, acc-0.8848\n",
      "Iter-66020, train loss-0.4048, acc-0.8800, valid loss-0.4060, acc-0.8880, test loss-0.4101, acc-0.8847\n",
      "Iter-66030, train loss-0.3607, acc-0.8800, valid loss-0.4059, acc-0.8880, test loss-0.4101, acc-0.8847\n",
      "Iter-66040, train loss-0.5928, acc-0.8600, valid loss-0.4059, acc-0.8880, test loss-0.4101, acc-0.8847\n",
      "Iter-66050, train loss-0.5218, acc-0.8600, valid loss-0.4059, acc-0.8880, test loss-0.4101, acc-0.8847\n",
      "Iter-66060, train loss-0.4373, acc-0.8400, valid loss-0.4059, acc-0.8880, test loss-0.4100, acc-0.8847\n",
      "Iter-66070, train loss-0.3455, acc-0.9200, valid loss-0.4058, acc-0.8880, test loss-0.4100, acc-0.8849\n",
      "Iter-66080, train loss-0.4330, acc-0.9000, valid loss-0.4058, acc-0.8880, test loss-0.4100, acc-0.8849\n",
      "Iter-66090, train loss-0.3962, acc-0.8600, valid loss-0.4058, acc-0.8880, test loss-0.4100, acc-0.8848\n",
      "Iter-66100, train loss-0.7237, acc-0.8200, valid loss-0.4058, acc-0.8880, test loss-0.4099, acc-0.8848\n",
      "Iter-66110, train loss-0.4286, acc-0.8800, valid loss-0.4058, acc-0.8880, test loss-0.4099, acc-0.8848\n",
      "Iter-66120, train loss-0.2565, acc-0.9200, valid loss-0.4057, acc-0.8880, test loss-0.4099, acc-0.8848\n",
      "Iter-66130, train loss-0.3640, acc-0.8800, valid loss-0.4057, acc-0.8882, test loss-0.4099, acc-0.8848\n",
      "Iter-66140, train loss-0.2272, acc-0.9400, valid loss-0.4057, acc-0.8880, test loss-0.4099, acc-0.8848\n",
      "Iter-66150, train loss-0.3353, acc-0.9000, valid loss-0.4057, acc-0.8880, test loss-0.4098, acc-0.8848\n",
      "Iter-66160, train loss-0.4115, acc-0.9000, valid loss-0.4056, acc-0.8878, test loss-0.4098, acc-0.8848\n",
      "Iter-66170, train loss-0.3297, acc-0.9000, valid loss-0.4056, acc-0.8880, test loss-0.4098, acc-0.8848\n",
      "Iter-66180, train loss-0.5207, acc-0.8400, valid loss-0.4056, acc-0.8880, test loss-0.4098, acc-0.8847\n",
      "Iter-66190, train loss-0.5552, acc-0.8400, valid loss-0.4056, acc-0.8880, test loss-0.4097, acc-0.8849\n",
      "Iter-66200, train loss-0.5727, acc-0.8600, valid loss-0.4055, acc-0.8880, test loss-0.4097, acc-0.8849\n",
      "Iter-66210, train loss-0.5522, acc-0.8000, valid loss-0.4055, acc-0.8880, test loss-0.4097, acc-0.8849\n",
      "Iter-66220, train loss-0.3498, acc-0.9200, valid loss-0.4055, acc-0.8880, test loss-0.4097, acc-0.8849\n",
      "Iter-66230, train loss-0.4601, acc-0.8600, valid loss-0.4055, acc-0.8882, test loss-0.4097, acc-0.8849\n",
      "Iter-66240, train loss-0.5177, acc-0.8800, valid loss-0.4054, acc-0.8880, test loss-0.4096, acc-0.8850\n",
      "Iter-66250, train loss-0.3338, acc-0.8800, valid loss-0.4054, acc-0.8880, test loss-0.4096, acc-0.8849\n",
      "Iter-66260, train loss-0.5429, acc-0.8600, valid loss-0.4054, acc-0.8880, test loss-0.4096, acc-0.8849\n",
      "Iter-66270, train loss-0.4314, acc-0.8800, valid loss-0.4054, acc-0.8880, test loss-0.4096, acc-0.8850\n",
      "Iter-66280, train loss-0.3973, acc-0.9000, valid loss-0.4054, acc-0.8880, test loss-0.4096, acc-0.8851\n",
      "Iter-66290, train loss-0.5388, acc-0.8200, valid loss-0.4054, acc-0.8878, test loss-0.4095, acc-0.8850\n",
      "Iter-66300, train loss-0.7245, acc-0.7600, valid loss-0.4054, acc-0.8878, test loss-0.4095, acc-0.8851\n",
      "Iter-66310, train loss-0.4885, acc-0.8600, valid loss-0.4053, acc-0.8876, test loss-0.4095, acc-0.8849\n",
      "Iter-66320, train loss-0.4698, acc-0.8600, valid loss-0.4053, acc-0.8878, test loss-0.4094, acc-0.8850\n",
      "Iter-66330, train loss-0.4169, acc-0.8800, valid loss-0.4053, acc-0.8880, test loss-0.4094, acc-0.8851\n",
      "Iter-66340, train loss-0.4515, acc-0.8400, valid loss-0.4053, acc-0.8878, test loss-0.4094, acc-0.8851\n",
      "Iter-66350, train loss-0.4651, acc-0.8800, valid loss-0.4052, acc-0.8878, test loss-0.4094, acc-0.8851\n",
      "Iter-66360, train loss-0.3381, acc-0.8600, valid loss-0.4052, acc-0.8876, test loss-0.4093, acc-0.8849\n",
      "Iter-66370, train loss-0.5669, acc-0.8200, valid loss-0.4052, acc-0.8876, test loss-0.4093, acc-0.8848\n",
      "Iter-66380, train loss-0.3408, acc-0.8800, valid loss-0.4051, acc-0.8876, test loss-0.4093, acc-0.8847\n",
      "Iter-66390, train loss-0.5235, acc-0.8600, valid loss-0.4051, acc-0.8876, test loss-0.4092, acc-0.8848\n",
      "Iter-66400, train loss-0.4202, acc-0.8600, valid loss-0.4051, acc-0.8876, test loss-0.4092, acc-0.8849\n",
      "Iter-66410, train loss-0.5941, acc-0.8600, valid loss-0.4050, acc-0.8876, test loss-0.4092, acc-0.8849\n",
      "Iter-66420, train loss-0.3415, acc-0.9000, valid loss-0.4050, acc-0.8876, test loss-0.4092, acc-0.8850\n",
      "Iter-66430, train loss-0.4926, acc-0.8800, valid loss-0.4050, acc-0.8876, test loss-0.4091, acc-0.8851\n",
      "Iter-66440, train loss-0.5455, acc-0.8000, valid loss-0.4050, acc-0.8876, test loss-0.4091, acc-0.8850\n",
      "Iter-66450, train loss-0.3125, acc-0.9200, valid loss-0.4049, acc-0.8880, test loss-0.4091, acc-0.8850\n",
      "Iter-66460, train loss-0.3935, acc-0.9000, valid loss-0.4049, acc-0.8880, test loss-0.4091, acc-0.8850\n",
      "Iter-66470, train loss-0.5958, acc-0.8400, valid loss-0.4049, acc-0.8884, test loss-0.4091, acc-0.8850\n",
      "Iter-66480, train loss-0.3324, acc-0.9200, valid loss-0.4049, acc-0.8886, test loss-0.4090, acc-0.8850\n",
      "Iter-66490, train loss-0.2177, acc-0.9800, valid loss-0.4048, acc-0.8886, test loss-0.4090, acc-0.8850\n",
      "Iter-66500, train loss-0.4533, acc-0.9200, valid loss-0.4048, acc-0.8886, test loss-0.4090, acc-0.8851\n",
      "Iter-66510, train loss-0.5390, acc-0.8200, valid loss-0.4048, acc-0.8884, test loss-0.4090, acc-0.8851\n",
      "Iter-66520, train loss-0.2732, acc-0.9000, valid loss-0.4048, acc-0.8884, test loss-0.4089, acc-0.8851\n",
      "Iter-66530, train loss-0.3602, acc-0.9200, valid loss-0.4047, acc-0.8884, test loss-0.4089, acc-0.8850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-66540, train loss-0.3255, acc-0.9400, valid loss-0.4047, acc-0.8884, test loss-0.4089, acc-0.8850\n",
      "Iter-66550, train loss-0.5018, acc-0.8200, valid loss-0.4047, acc-0.8884, test loss-0.4089, acc-0.8850\n",
      "Iter-66560, train loss-0.4788, acc-0.8800, valid loss-0.4047, acc-0.8884, test loss-0.4089, acc-0.8851\n",
      "Iter-66570, train loss-0.3893, acc-0.8800, valid loss-0.4047, acc-0.8880, test loss-0.4089, acc-0.8851\n",
      "Iter-66580, train loss-0.4450, acc-0.8800, valid loss-0.4047, acc-0.8880, test loss-0.4088, acc-0.8852\n",
      "Iter-66590, train loss-0.3051, acc-0.9400, valid loss-0.4046, acc-0.8880, test loss-0.4088, acc-0.8852\n",
      "Iter-66600, train loss-0.3396, acc-0.8600, valid loss-0.4046, acc-0.8882, test loss-0.4088, acc-0.8851\n",
      "Iter-66610, train loss-0.2956, acc-0.9200, valid loss-0.4046, acc-0.8882, test loss-0.4088, acc-0.8851\n",
      "Iter-66620, train loss-0.6046, acc-0.8800, valid loss-0.4046, acc-0.8882, test loss-0.4087, acc-0.8851\n",
      "Iter-66630, train loss-0.3027, acc-0.9200, valid loss-0.4045, acc-0.8882, test loss-0.4087, acc-0.8852\n",
      "Iter-66640, train loss-0.4304, acc-0.8800, valid loss-0.4045, acc-0.8880, test loss-0.4087, acc-0.8852\n",
      "Iter-66650, train loss-0.4736, acc-0.8600, valid loss-0.4045, acc-0.8880, test loss-0.4087, acc-0.8851\n",
      "Iter-66660, train loss-0.5156, acc-0.8800, valid loss-0.4045, acc-0.8880, test loss-0.4086, acc-0.8851\n",
      "Iter-66670, train loss-0.3666, acc-0.8800, valid loss-0.4044, acc-0.8880, test loss-0.4086, acc-0.8851\n",
      "Iter-66680, train loss-0.1754, acc-0.9800, valid loss-0.4044, acc-0.8882, test loss-0.4086, acc-0.8853\n",
      "Iter-66690, train loss-0.4488, acc-0.9000, valid loss-0.4044, acc-0.8882, test loss-0.4086, acc-0.8853\n",
      "Iter-66700, train loss-0.4929, acc-0.9000, valid loss-0.4043, acc-0.8882, test loss-0.4085, acc-0.8853\n",
      "Iter-66710, train loss-0.3861, acc-0.8800, valid loss-0.4043, acc-0.8884, test loss-0.4085, acc-0.8853\n",
      "Iter-66720, train loss-0.5724, acc-0.8200, valid loss-0.4043, acc-0.8884, test loss-0.4085, acc-0.8853\n",
      "Iter-66730, train loss-0.5986, acc-0.8000, valid loss-0.4043, acc-0.8884, test loss-0.4085, acc-0.8853\n",
      "Iter-66740, train loss-0.6651, acc-0.7800, valid loss-0.4042, acc-0.8884, test loss-0.4084, acc-0.8852\n",
      "Iter-66750, train loss-0.3212, acc-0.9200, valid loss-0.4042, acc-0.8882, test loss-0.4084, acc-0.8852\n",
      "Iter-66760, train loss-0.4087, acc-0.9000, valid loss-0.4042, acc-0.8882, test loss-0.4084, acc-0.8851\n",
      "Iter-66770, train loss-0.4196, acc-0.8600, valid loss-0.4042, acc-0.8882, test loss-0.4084, acc-0.8853\n",
      "Iter-66780, train loss-0.3099, acc-0.9200, valid loss-0.4041, acc-0.8880, test loss-0.4084, acc-0.8853\n",
      "Iter-66790, train loss-0.3924, acc-0.9200, valid loss-0.4041, acc-0.8882, test loss-0.4083, acc-0.8853\n",
      "Iter-66800, train loss-0.4054, acc-0.8600, valid loss-0.4041, acc-0.8884, test loss-0.4083, acc-0.8853\n",
      "Iter-66810, train loss-0.4088, acc-0.8600, valid loss-0.4041, acc-0.8884, test loss-0.4083, acc-0.8852\n",
      "Iter-66820, train loss-0.2667, acc-0.9000, valid loss-0.4040, acc-0.8884, test loss-0.4083, acc-0.8852\n",
      "Iter-66830, train loss-0.4672, acc-0.9200, valid loss-0.4040, acc-0.8886, test loss-0.4082, acc-0.8853\n",
      "Iter-66840, train loss-0.3837, acc-0.8600, valid loss-0.4040, acc-0.8886, test loss-0.4082, acc-0.8853\n",
      "Iter-66850, train loss-0.4415, acc-0.8800, valid loss-0.4040, acc-0.8884, test loss-0.4082, acc-0.8852\n",
      "Iter-66860, train loss-0.4457, acc-0.8600, valid loss-0.4040, acc-0.8884, test loss-0.4082, acc-0.8853\n",
      "Iter-66870, train loss-0.3838, acc-0.9000, valid loss-0.4039, acc-0.8886, test loss-0.4082, acc-0.8853\n",
      "Iter-66880, train loss-0.3673, acc-0.9000, valid loss-0.4039, acc-0.8886, test loss-0.4081, acc-0.8853\n",
      "Iter-66890, train loss-0.4262, acc-0.8800, valid loss-0.4039, acc-0.8886, test loss-0.4081, acc-0.8853\n",
      "Iter-66900, train loss-0.4482, acc-0.8800, valid loss-0.4038, acc-0.8886, test loss-0.4081, acc-0.8854\n",
      "Iter-66910, train loss-0.6710, acc-0.8200, valid loss-0.4038, acc-0.8886, test loss-0.4081, acc-0.8854\n",
      "Iter-66920, train loss-0.5261, acc-0.7800, valid loss-0.4038, acc-0.8888, test loss-0.4080, acc-0.8853\n",
      "Iter-66930, train loss-0.3135, acc-0.9200, valid loss-0.4038, acc-0.8884, test loss-0.4080, acc-0.8854\n",
      "Iter-66940, train loss-0.4053, acc-0.8800, valid loss-0.4038, acc-0.8884, test loss-0.4080, acc-0.8855\n",
      "Iter-66950, train loss-0.4922, acc-0.8600, valid loss-0.4037, acc-0.8886, test loss-0.4080, acc-0.8854\n",
      "Iter-66960, train loss-0.4065, acc-0.8600, valid loss-0.4037, acc-0.8886, test loss-0.4079, acc-0.8854\n",
      "Iter-66970, train loss-0.3883, acc-0.8800, valid loss-0.4037, acc-0.8884, test loss-0.4079, acc-0.8854\n",
      "Iter-66980, train loss-0.4177, acc-0.8800, valid loss-0.4037, acc-0.8884, test loss-0.4079, acc-0.8854\n",
      "Iter-66990, train loss-0.5221, acc-0.8600, valid loss-0.4036, acc-0.8884, test loss-0.4079, acc-0.8854\n",
      "Iter-67000, train loss-0.5124, acc-0.8400, valid loss-0.4036, acc-0.8884, test loss-0.4078, acc-0.8854\n",
      "Iter-67010, train loss-0.3105, acc-0.9200, valid loss-0.4036, acc-0.8884, test loss-0.4078, acc-0.8854\n",
      "Iter-67020, train loss-0.3039, acc-0.9400, valid loss-0.4035, acc-0.8884, test loss-0.4078, acc-0.8854\n",
      "Iter-67030, train loss-0.7282, acc-0.7400, valid loss-0.4035, acc-0.8884, test loss-0.4078, acc-0.8855\n",
      "Iter-67040, train loss-0.2289, acc-0.9600, valid loss-0.4035, acc-0.8886, test loss-0.4077, acc-0.8854\n",
      "Iter-67050, train loss-0.4275, acc-0.8600, valid loss-0.4035, acc-0.8884, test loss-0.4077, acc-0.8853\n",
      "Iter-67060, train loss-0.5821, acc-0.8200, valid loss-0.4035, acc-0.8882, test loss-0.4077, acc-0.8854\n",
      "Iter-67070, train loss-0.5366, acc-0.7800, valid loss-0.4034, acc-0.8882, test loss-0.4077, acc-0.8854\n",
      "Iter-67080, train loss-0.5266, acc-0.8800, valid loss-0.4034, acc-0.8884, test loss-0.4076, acc-0.8855\n",
      "Iter-67090, train loss-0.4112, acc-0.8400, valid loss-0.4034, acc-0.8882, test loss-0.4076, acc-0.8855\n",
      "Iter-67100, train loss-0.4299, acc-0.8400, valid loss-0.4034, acc-0.8882, test loss-0.4076, acc-0.8855\n",
      "Iter-67110, train loss-0.4494, acc-0.8800, valid loss-0.4033, acc-0.8882, test loss-0.4076, acc-0.8854\n",
      "Iter-67120, train loss-0.4010, acc-0.8600, valid loss-0.4033, acc-0.8882, test loss-0.4075, acc-0.8855\n",
      "Iter-67130, train loss-0.4193, acc-0.9000, valid loss-0.4033, acc-0.8884, test loss-0.4075, acc-0.8855\n",
      "Iter-67140, train loss-0.6078, acc-0.8000, valid loss-0.4033, acc-0.8884, test loss-0.4075, acc-0.8855\n",
      "Iter-67150, train loss-0.4674, acc-0.8800, valid loss-0.4032, acc-0.8884, test loss-0.4074, acc-0.8854\n",
      "Iter-67160, train loss-0.4511, acc-0.8600, valid loss-0.4032, acc-0.8884, test loss-0.4074, acc-0.8854\n",
      "Iter-67170, train loss-0.5108, acc-0.8600, valid loss-0.4032, acc-0.8884, test loss-0.4074, acc-0.8856\n",
      "Iter-67180, train loss-0.5009, acc-0.8200, valid loss-0.4032, acc-0.8884, test loss-0.4074, acc-0.8856\n",
      "Iter-67190, train loss-0.4188, acc-0.8400, valid loss-0.4031, acc-0.8884, test loss-0.4073, acc-0.8856\n",
      "Iter-67200, train loss-0.3319, acc-0.9000, valid loss-0.4031, acc-0.8884, test loss-0.4073, acc-0.8855\n",
      "Iter-67210, train loss-0.2922, acc-0.9400, valid loss-0.4031, acc-0.8882, test loss-0.4073, acc-0.8856\n",
      "Iter-67220, train loss-0.5921, acc-0.8600, valid loss-0.4031, acc-0.8886, test loss-0.4073, acc-0.8857\n",
      "Iter-67230, train loss-0.2826, acc-0.9200, valid loss-0.4030, acc-0.8884, test loss-0.4072, acc-0.8857\n",
      "Iter-67240, train loss-0.2965, acc-0.9400, valid loss-0.4030, acc-0.8886, test loss-0.4072, acc-0.8856\n",
      "Iter-67250, train loss-0.3341, acc-0.9000, valid loss-0.4030, acc-0.8886, test loss-0.4072, acc-0.8857\n",
      "Iter-67260, train loss-0.4045, acc-0.8600, valid loss-0.4030, acc-0.8886, test loss-0.4072, acc-0.8857\n",
      "Iter-67270, train loss-0.2558, acc-0.9200, valid loss-0.4030, acc-0.8886, test loss-0.4072, acc-0.8857\n",
      "Iter-67280, train loss-0.2889, acc-0.9400, valid loss-0.4029, acc-0.8888, test loss-0.4071, acc-0.8856\n",
      "Iter-67290, train loss-0.4338, acc-0.8800, valid loss-0.4029, acc-0.8886, test loss-0.4071, acc-0.8857\n",
      "Iter-67300, train loss-0.6629, acc-0.8200, valid loss-0.4029, acc-0.8886, test loss-0.4071, acc-0.8857\n",
      "Iter-67310, train loss-0.4350, acc-0.8800, valid loss-0.4029, acc-0.8886, test loss-0.4071, acc-0.8857\n",
      "Iter-67320, train loss-0.5226, acc-0.8600, valid loss-0.4028, acc-0.8886, test loss-0.4070, acc-0.8856\n",
      "Iter-67330, train loss-0.4083, acc-0.8600, valid loss-0.4028, acc-0.8886, test loss-0.4070, acc-0.8856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-67340, train loss-0.4964, acc-0.8200, valid loss-0.4028, acc-0.8886, test loss-0.4070, acc-0.8856\n",
      "Iter-67350, train loss-0.5297, acc-0.8800, valid loss-0.4028, acc-0.8886, test loss-0.4070, acc-0.8856\n",
      "Iter-67360, train loss-0.5699, acc-0.8200, valid loss-0.4027, acc-0.8886, test loss-0.4069, acc-0.8856\n",
      "Iter-67370, train loss-0.4259, acc-0.8800, valid loss-0.4027, acc-0.8886, test loss-0.4069, acc-0.8856\n",
      "Iter-67380, train loss-0.6582, acc-0.8400, valid loss-0.4027, acc-0.8886, test loss-0.4069, acc-0.8856\n",
      "Iter-67390, train loss-0.3975, acc-0.9000, valid loss-0.4027, acc-0.8886, test loss-0.4069, acc-0.8856\n",
      "Iter-67400, train loss-0.5638, acc-0.8600, valid loss-0.4027, acc-0.8886, test loss-0.4068, acc-0.8857\n",
      "Iter-67410, train loss-0.4684, acc-0.8800, valid loss-0.4026, acc-0.8886, test loss-0.4068, acc-0.8857\n",
      "Iter-67420, train loss-0.3442, acc-0.8800, valid loss-0.4026, acc-0.8886, test loss-0.4068, acc-0.8857\n",
      "Iter-67430, train loss-0.3186, acc-0.9200, valid loss-0.4026, acc-0.8886, test loss-0.4068, acc-0.8857\n",
      "Iter-67440, train loss-0.4453, acc-0.9000, valid loss-0.4026, acc-0.8886, test loss-0.4068, acc-0.8856\n",
      "Iter-67450, train loss-0.3882, acc-0.8800, valid loss-0.4025, acc-0.8886, test loss-0.4067, acc-0.8857\n",
      "Iter-67460, train loss-0.1918, acc-0.9600, valid loss-0.4025, acc-0.8886, test loss-0.4067, acc-0.8858\n",
      "Iter-67470, train loss-0.4040, acc-0.8800, valid loss-0.4025, acc-0.8886, test loss-0.4067, acc-0.8859\n",
      "Iter-67480, train loss-0.6597, acc-0.8600, valid loss-0.4025, acc-0.8886, test loss-0.4067, acc-0.8858\n",
      "Iter-67490, train loss-0.3757, acc-0.8600, valid loss-0.4024, acc-0.8886, test loss-0.4066, acc-0.8858\n",
      "Iter-67500, train loss-0.5310, acc-0.8000, valid loss-0.4024, acc-0.8886, test loss-0.4066, acc-0.8857\n",
      "Iter-67510, train loss-0.4457, acc-0.9400, valid loss-0.4024, acc-0.8886, test loss-0.4066, acc-0.8857\n",
      "Iter-67520, train loss-0.4655, acc-0.8600, valid loss-0.4024, acc-0.8884, test loss-0.4066, acc-0.8857\n",
      "Iter-67530, train loss-0.3678, acc-0.9000, valid loss-0.4023, acc-0.8884, test loss-0.4065, acc-0.8857\n",
      "Iter-67540, train loss-0.5338, acc-0.8600, valid loss-0.4023, acc-0.8884, test loss-0.4065, acc-0.8858\n",
      "Iter-67550, train loss-0.4933, acc-0.8800, valid loss-0.4023, acc-0.8886, test loss-0.4065, acc-0.8858\n",
      "Iter-67560, train loss-0.5780, acc-0.8600, valid loss-0.4023, acc-0.8886, test loss-0.4065, acc-0.8858\n",
      "Iter-67570, train loss-0.2134, acc-0.9600, valid loss-0.4022, acc-0.8886, test loss-0.4064, acc-0.8858\n",
      "Iter-67580, train loss-0.6344, acc-0.8000, valid loss-0.4022, acc-0.8886, test loss-0.4064, acc-0.8858\n",
      "Iter-67590, train loss-0.2855, acc-0.9200, valid loss-0.4022, acc-0.8888, test loss-0.4064, acc-0.8858\n",
      "Iter-67600, train loss-0.4129, acc-0.8800, valid loss-0.4022, acc-0.8886, test loss-0.4064, acc-0.8857\n",
      "Iter-67610, train loss-0.5235, acc-0.8400, valid loss-0.4022, acc-0.8886, test loss-0.4064, acc-0.8855\n",
      "Iter-67620, train loss-0.3667, acc-0.9400, valid loss-0.4021, acc-0.8884, test loss-0.4063, acc-0.8856\n",
      "Iter-67630, train loss-0.3960, acc-0.8600, valid loss-0.4021, acc-0.8888, test loss-0.4063, acc-0.8856\n",
      "Iter-67640, train loss-0.5428, acc-0.8000, valid loss-0.4021, acc-0.8886, test loss-0.4063, acc-0.8856\n",
      "Iter-67650, train loss-0.2825, acc-0.8800, valid loss-0.4021, acc-0.8886, test loss-0.4062, acc-0.8856\n",
      "Iter-67660, train loss-0.4324, acc-0.9000, valid loss-0.4021, acc-0.8886, test loss-0.4062, acc-0.8857\n",
      "Iter-67670, train loss-0.4172, acc-0.8800, valid loss-0.4020, acc-0.8886, test loss-0.4062, acc-0.8858\n",
      "Iter-67680, train loss-0.4711, acc-0.8000, valid loss-0.4020, acc-0.8886, test loss-0.4062, acc-0.8858\n",
      "Iter-67690, train loss-0.5885, acc-0.8000, valid loss-0.4020, acc-0.8886, test loss-0.4061, acc-0.8859\n",
      "Iter-67700, train loss-0.2827, acc-0.9400, valid loss-0.4020, acc-0.8886, test loss-0.4061, acc-0.8860\n",
      "Iter-67710, train loss-0.4844, acc-0.8600, valid loss-0.4019, acc-0.8886, test loss-0.4061, acc-0.8858\n",
      "Iter-67720, train loss-0.2841, acc-0.9400, valid loss-0.4019, acc-0.8886, test loss-0.4061, acc-0.8857\n",
      "Iter-67730, train loss-0.4331, acc-0.8400, valid loss-0.4019, acc-0.8884, test loss-0.4061, acc-0.8858\n",
      "Iter-67740, train loss-0.4088, acc-0.9000, valid loss-0.4019, acc-0.8886, test loss-0.4060, acc-0.8859\n",
      "Iter-67750, train loss-0.5284, acc-0.8400, valid loss-0.4019, acc-0.8886, test loss-0.4060, acc-0.8859\n",
      "Iter-67760, train loss-0.2754, acc-0.9400, valid loss-0.4018, acc-0.8886, test loss-0.4060, acc-0.8857\n",
      "Iter-67770, train loss-0.2870, acc-0.9400, valid loss-0.4018, acc-0.8884, test loss-0.4060, acc-0.8858\n",
      "Iter-67780, train loss-0.2823, acc-0.9200, valid loss-0.4018, acc-0.8886, test loss-0.4060, acc-0.8857\n",
      "Iter-67790, train loss-0.6262, acc-0.8600, valid loss-0.4018, acc-0.8886, test loss-0.4059, acc-0.8858\n",
      "Iter-67800, train loss-0.3828, acc-0.9000, valid loss-0.4017, acc-0.8886, test loss-0.4059, acc-0.8857\n",
      "Iter-67810, train loss-0.4813, acc-0.8800, valid loss-0.4017, acc-0.8886, test loss-0.4059, acc-0.8857\n",
      "Iter-67820, train loss-0.4120, acc-0.8800, valid loss-0.4017, acc-0.8886, test loss-0.4059, acc-0.8856\n",
      "Iter-67830, train loss-0.2618, acc-0.9400, valid loss-0.4017, acc-0.8886, test loss-0.4058, acc-0.8856\n",
      "Iter-67840, train loss-0.5398, acc-0.8800, valid loss-0.4017, acc-0.8886, test loss-0.4058, acc-0.8856\n",
      "Iter-67850, train loss-0.2521, acc-0.9400, valid loss-0.4016, acc-0.8886, test loss-0.4058, acc-0.8857\n",
      "Iter-67860, train loss-0.2771, acc-0.9600, valid loss-0.4016, acc-0.8886, test loss-0.4058, acc-0.8857\n",
      "Iter-67870, train loss-0.3469, acc-0.9400, valid loss-0.4016, acc-0.8886, test loss-0.4057, acc-0.8857\n",
      "Iter-67880, train loss-0.5864, acc-0.8000, valid loss-0.4016, acc-0.8888, test loss-0.4057, acc-0.8856\n",
      "Iter-67890, train loss-0.5024, acc-0.9000, valid loss-0.4016, acc-0.8886, test loss-0.4057, acc-0.8857\n",
      "Iter-67900, train loss-0.4264, acc-0.8600, valid loss-0.4015, acc-0.8886, test loss-0.4057, acc-0.8857\n",
      "Iter-67910, train loss-0.1961, acc-0.9800, valid loss-0.4015, acc-0.8886, test loss-0.4056, acc-0.8856\n",
      "Iter-67920, train loss-0.4576, acc-0.9000, valid loss-0.4015, acc-0.8886, test loss-0.4056, acc-0.8858\n",
      "Iter-67930, train loss-0.3485, acc-0.8800, valid loss-0.4015, acc-0.8888, test loss-0.4056, acc-0.8858\n",
      "Iter-67940, train loss-0.2883, acc-0.9000, valid loss-0.4014, acc-0.8886, test loss-0.4056, acc-0.8857\n",
      "Iter-67950, train loss-0.3498, acc-0.9000, valid loss-0.4014, acc-0.8886, test loss-0.4056, acc-0.8858\n",
      "Iter-67960, train loss-0.3745, acc-0.9200, valid loss-0.4014, acc-0.8886, test loss-0.4055, acc-0.8859\n",
      "Iter-67970, train loss-0.3254, acc-0.9000, valid loss-0.4014, acc-0.8888, test loss-0.4055, acc-0.8858\n",
      "Iter-67980, train loss-0.6580, acc-0.7800, valid loss-0.4013, acc-0.8888, test loss-0.4055, acc-0.8859\n",
      "Iter-67990, train loss-0.5180, acc-0.8400, valid loss-0.4013, acc-0.8886, test loss-0.4055, acc-0.8859\n",
      "Iter-68000, train loss-0.4608, acc-0.8600, valid loss-0.4013, acc-0.8886, test loss-0.4054, acc-0.8859\n",
      "Iter-68010, train loss-0.2054, acc-0.9400, valid loss-0.4012, acc-0.8886, test loss-0.4054, acc-0.8859\n",
      "Iter-68020, train loss-0.3014, acc-0.9400, valid loss-0.4012, acc-0.8888, test loss-0.4054, acc-0.8859\n",
      "Iter-68030, train loss-0.6541, acc-0.8000, valid loss-0.4012, acc-0.8886, test loss-0.4054, acc-0.8859\n",
      "Iter-68040, train loss-0.5120, acc-0.8200, valid loss-0.4012, acc-0.8888, test loss-0.4054, acc-0.8859\n",
      "Iter-68050, train loss-0.2619, acc-0.9200, valid loss-0.4011, acc-0.8886, test loss-0.4053, acc-0.8859\n",
      "Iter-68060, train loss-0.3373, acc-0.9400, valid loss-0.4011, acc-0.8886, test loss-0.4053, acc-0.8859\n",
      "Iter-68070, train loss-0.2987, acc-0.9200, valid loss-0.4011, acc-0.8886, test loss-0.4053, acc-0.8860\n",
      "Iter-68080, train loss-0.4319, acc-0.9000, valid loss-0.4011, acc-0.8886, test loss-0.4053, acc-0.8859\n",
      "Iter-68090, train loss-0.2288, acc-0.9400, valid loss-0.4010, acc-0.8886, test loss-0.4052, acc-0.8859\n",
      "Iter-68100, train loss-0.4110, acc-0.9200, valid loss-0.4010, acc-0.8886, test loss-0.4052, acc-0.8859\n",
      "Iter-68110, train loss-0.4386, acc-0.9000, valid loss-0.4010, acc-0.8886, test loss-0.4052, acc-0.8860\n",
      "Iter-68120, train loss-0.6698, acc-0.8400, valid loss-0.4010, acc-0.8886, test loss-0.4052, acc-0.8860\n",
      "Iter-68130, train loss-0.4081, acc-0.8600, valid loss-0.4009, acc-0.8886, test loss-0.4051, acc-0.8860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68140, train loss-0.4324, acc-0.8400, valid loss-0.4009, acc-0.8888, test loss-0.4051, acc-0.8861\n",
      "Iter-68150, train loss-0.3145, acc-0.8800, valid loss-0.4009, acc-0.8890, test loss-0.4051, acc-0.8861\n",
      "Iter-68160, train loss-0.4550, acc-0.8600, valid loss-0.4009, acc-0.8890, test loss-0.4051, acc-0.8861\n",
      "Iter-68170, train loss-0.4227, acc-0.9000, valid loss-0.4009, acc-0.8890, test loss-0.4050, acc-0.8861\n",
      "Iter-68180, train loss-0.4940, acc-0.8000, valid loss-0.4008, acc-0.8890, test loss-0.4050, acc-0.8861\n",
      "Iter-68190, train loss-0.3067, acc-0.9400, valid loss-0.4008, acc-0.8888, test loss-0.4050, acc-0.8860\n",
      "Iter-68200, train loss-0.5278, acc-0.8400, valid loss-0.4008, acc-0.8888, test loss-0.4050, acc-0.8860\n",
      "Iter-68210, train loss-0.3638, acc-0.8800, valid loss-0.4007, acc-0.8888, test loss-0.4049, acc-0.8860\n",
      "Iter-68220, train loss-0.4117, acc-0.9000, valid loss-0.4007, acc-0.8888, test loss-0.4049, acc-0.8860\n",
      "Iter-68230, train loss-0.5672, acc-0.8400, valid loss-0.4007, acc-0.8888, test loss-0.4049, acc-0.8860\n",
      "Iter-68240, train loss-0.3063, acc-0.9400, valid loss-0.4007, acc-0.8888, test loss-0.4049, acc-0.8860\n",
      "Iter-68250, train loss-0.4823, acc-0.9000, valid loss-0.4007, acc-0.8888, test loss-0.4049, acc-0.8860\n",
      "Iter-68260, train loss-0.2977, acc-0.9000, valid loss-0.4006, acc-0.8888, test loss-0.4049, acc-0.8860\n",
      "Iter-68270, train loss-0.4546, acc-0.8600, valid loss-0.4006, acc-0.8888, test loss-0.4048, acc-0.8861\n",
      "Iter-68280, train loss-0.3766, acc-0.9200, valid loss-0.4005, acc-0.8888, test loss-0.4048, acc-0.8861\n",
      "Iter-68290, train loss-0.4341, acc-0.8200, valid loss-0.4005, acc-0.8888, test loss-0.4048, acc-0.8861\n",
      "Iter-68300, train loss-0.5093, acc-0.9000, valid loss-0.4005, acc-0.8886, test loss-0.4048, acc-0.8861\n",
      "Iter-68310, train loss-0.3957, acc-0.9200, valid loss-0.4005, acc-0.8888, test loss-0.4047, acc-0.8861\n",
      "Iter-68320, train loss-0.4578, acc-0.8800, valid loss-0.4005, acc-0.8888, test loss-0.4047, acc-0.8861\n",
      "Iter-68330, train loss-0.2986, acc-0.9600, valid loss-0.4004, acc-0.8886, test loss-0.4047, acc-0.8861\n",
      "Iter-68340, train loss-0.3489, acc-0.8800, valid loss-0.4004, acc-0.8886, test loss-0.4047, acc-0.8861\n",
      "Iter-68350, train loss-0.6161, acc-0.7600, valid loss-0.4004, acc-0.8886, test loss-0.4047, acc-0.8862\n",
      "Iter-68360, train loss-0.2103, acc-0.9600, valid loss-0.4004, acc-0.8886, test loss-0.4046, acc-0.8861\n",
      "Iter-68370, train loss-0.4937, acc-0.9000, valid loss-0.4003, acc-0.8886, test loss-0.4046, acc-0.8863\n",
      "Iter-68380, train loss-0.3384, acc-0.9000, valid loss-0.4003, acc-0.8886, test loss-0.4046, acc-0.8863\n",
      "Iter-68390, train loss-0.6976, acc-0.7600, valid loss-0.4003, acc-0.8886, test loss-0.4046, acc-0.8863\n",
      "Iter-68400, train loss-0.4887, acc-0.8800, valid loss-0.4002, acc-0.8886, test loss-0.4046, acc-0.8862\n",
      "Iter-68410, train loss-0.6160, acc-0.7600, valid loss-0.4002, acc-0.8886, test loss-0.4045, acc-0.8863\n",
      "Iter-68420, train loss-0.3770, acc-0.9400, valid loss-0.4002, acc-0.8886, test loss-0.4045, acc-0.8862\n",
      "Iter-68430, train loss-0.2599, acc-0.9600, valid loss-0.4002, acc-0.8886, test loss-0.4045, acc-0.8862\n",
      "Iter-68440, train loss-0.3074, acc-0.9200, valid loss-0.4002, acc-0.8886, test loss-0.4045, acc-0.8862\n",
      "Iter-68450, train loss-0.3871, acc-0.8800, valid loss-0.4002, acc-0.8886, test loss-0.4044, acc-0.8863\n",
      "Iter-68460, train loss-0.5563, acc-0.8400, valid loss-0.4001, acc-0.8886, test loss-0.4044, acc-0.8863\n",
      "Iter-68470, train loss-0.3502, acc-0.9000, valid loss-0.4001, acc-0.8886, test loss-0.4044, acc-0.8863\n",
      "Iter-68480, train loss-0.4363, acc-0.8000, valid loss-0.4001, acc-0.8886, test loss-0.4044, acc-0.8862\n",
      "Iter-68490, train loss-0.6351, acc-0.8600, valid loss-0.4001, acc-0.8886, test loss-0.4043, acc-0.8862\n",
      "Iter-68500, train loss-0.4929, acc-0.8400, valid loss-0.4001, acc-0.8886, test loss-0.4043, acc-0.8862\n",
      "Iter-68510, train loss-0.4689, acc-0.9000, valid loss-0.4000, acc-0.8886, test loss-0.4043, acc-0.8862\n",
      "Iter-68520, train loss-0.3069, acc-0.9200, valid loss-0.4000, acc-0.8886, test loss-0.4043, acc-0.8862\n",
      "Iter-68530, train loss-0.3438, acc-0.9000, valid loss-0.4000, acc-0.8886, test loss-0.4043, acc-0.8863\n",
      "Iter-68540, train loss-0.3316, acc-0.9200, valid loss-0.4000, acc-0.8886, test loss-0.4042, acc-0.8863\n",
      "Iter-68550, train loss-0.4010, acc-0.8800, valid loss-0.3999, acc-0.8886, test loss-0.4042, acc-0.8863\n",
      "Iter-68560, train loss-0.5570, acc-0.8800, valid loss-0.3999, acc-0.8886, test loss-0.4042, acc-0.8864\n",
      "Iter-68570, train loss-0.3538, acc-0.9400, valid loss-0.3999, acc-0.8886, test loss-0.4042, acc-0.8863\n",
      "Iter-68580, train loss-0.5757, acc-0.8600, valid loss-0.3999, acc-0.8888, test loss-0.4042, acc-0.8863\n",
      "Iter-68590, train loss-0.3443, acc-0.9200, valid loss-0.3999, acc-0.8888, test loss-0.4041, acc-0.8863\n",
      "Iter-68600, train loss-0.4388, acc-0.8800, valid loss-0.3998, acc-0.8886, test loss-0.4041, acc-0.8863\n",
      "Iter-68610, train loss-0.5237, acc-0.8400, valid loss-0.3998, acc-0.8886, test loss-0.4041, acc-0.8863\n",
      "Iter-68620, train loss-0.4391, acc-0.9000, valid loss-0.3998, acc-0.8888, test loss-0.4041, acc-0.8863\n",
      "Iter-68630, train loss-0.5424, acc-0.8400, valid loss-0.3998, acc-0.8886, test loss-0.4040, acc-0.8863\n",
      "Iter-68640, train loss-0.5436, acc-0.8400, valid loss-0.3997, acc-0.8888, test loss-0.4040, acc-0.8863\n",
      "Iter-68650, train loss-0.3309, acc-0.9000, valid loss-0.3997, acc-0.8888, test loss-0.4040, acc-0.8863\n",
      "Iter-68660, train loss-0.4132, acc-0.8800, valid loss-0.3997, acc-0.8888, test loss-0.4040, acc-0.8863\n",
      "Iter-68670, train loss-0.3591, acc-0.8600, valid loss-0.3997, acc-0.8888, test loss-0.4039, acc-0.8863\n",
      "Iter-68680, train loss-0.7496, acc-0.8200, valid loss-0.3996, acc-0.8888, test loss-0.4039, acc-0.8863\n",
      "Iter-68690, train loss-0.4227, acc-0.8800, valid loss-0.3996, acc-0.8888, test loss-0.4039, acc-0.8863\n",
      "Iter-68700, train loss-0.5920, acc-0.8000, valid loss-0.3996, acc-0.8888, test loss-0.4039, acc-0.8863\n",
      "Iter-68710, train loss-0.3741, acc-0.9000, valid loss-0.3996, acc-0.8888, test loss-0.4039, acc-0.8863\n",
      "Iter-68720, train loss-0.3714, acc-0.9200, valid loss-0.3996, acc-0.8888, test loss-0.4038, acc-0.8863\n",
      "Iter-68730, train loss-0.4749, acc-0.8600, valid loss-0.3995, acc-0.8888, test loss-0.4038, acc-0.8862\n",
      "Iter-68740, train loss-0.2767, acc-0.9400, valid loss-0.3995, acc-0.8888, test loss-0.4038, acc-0.8863\n",
      "Iter-68750, train loss-0.4178, acc-0.8800, valid loss-0.3995, acc-0.8888, test loss-0.4038, acc-0.8864\n",
      "Iter-68760, train loss-0.3992, acc-0.9000, valid loss-0.3995, acc-0.8888, test loss-0.4038, acc-0.8864\n",
      "Iter-68770, train loss-0.3403, acc-0.9600, valid loss-0.3995, acc-0.8890, test loss-0.4037, acc-0.8864\n",
      "Iter-68780, train loss-0.3913, acc-0.8800, valid loss-0.3995, acc-0.8890, test loss-0.4037, acc-0.8864\n",
      "Iter-68790, train loss-0.2271, acc-0.9600, valid loss-0.3994, acc-0.8888, test loss-0.4037, acc-0.8866\n",
      "Iter-68800, train loss-0.3181, acc-0.8800, valid loss-0.3994, acc-0.8888, test loss-0.4037, acc-0.8864\n",
      "Iter-68810, train loss-0.3813, acc-0.8800, valid loss-0.3994, acc-0.8888, test loss-0.4037, acc-0.8864\n",
      "Iter-68820, train loss-0.3989, acc-0.8800, valid loss-0.3993, acc-0.8888, test loss-0.4036, acc-0.8863\n",
      "Iter-68830, train loss-0.3853, acc-0.9400, valid loss-0.3993, acc-0.8888, test loss-0.4036, acc-0.8864\n",
      "Iter-68840, train loss-0.4226, acc-0.8600, valid loss-0.3993, acc-0.8888, test loss-0.4036, acc-0.8863\n",
      "Iter-68850, train loss-0.4061, acc-0.9200, valid loss-0.3993, acc-0.8886, test loss-0.4036, acc-0.8865\n",
      "Iter-68860, train loss-0.3405, acc-0.8600, valid loss-0.3993, acc-0.8890, test loss-0.4036, acc-0.8865\n",
      "Iter-68870, train loss-0.4155, acc-0.9000, valid loss-0.3992, acc-0.8890, test loss-0.4035, acc-0.8864\n",
      "Iter-68880, train loss-0.4372, acc-0.8200, valid loss-0.3992, acc-0.8888, test loss-0.4035, acc-0.8863\n",
      "Iter-68890, train loss-0.3863, acc-0.8600, valid loss-0.3992, acc-0.8890, test loss-0.4035, acc-0.8864\n",
      "Iter-68900, train loss-0.4832, acc-0.8400, valid loss-0.3992, acc-0.8890, test loss-0.4035, acc-0.8865\n",
      "Iter-68910, train loss-0.5021, acc-0.8800, valid loss-0.3991, acc-0.8888, test loss-0.4034, acc-0.8865\n",
      "Iter-68920, train loss-0.6363, acc-0.8200, valid loss-0.3991, acc-0.8890, test loss-0.4034, acc-0.8865\n",
      "Iter-68930, train loss-0.4821, acc-0.8600, valid loss-0.3991, acc-0.8888, test loss-0.4034, acc-0.8865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68940, train loss-0.3549, acc-0.9200, valid loss-0.3991, acc-0.8888, test loss-0.4034, acc-0.8865\n",
      "Iter-68950, train loss-0.3739, acc-0.8800, valid loss-0.3990, acc-0.8888, test loss-0.4033, acc-0.8865\n",
      "Iter-68960, train loss-0.3138, acc-0.8800, valid loss-0.3990, acc-0.8890, test loss-0.4033, acc-0.8865\n",
      "Iter-68970, train loss-0.4585, acc-0.8200, valid loss-0.3990, acc-0.8886, test loss-0.4033, acc-0.8864\n",
      "Iter-68980, train loss-0.4375, acc-0.9400, valid loss-0.3990, acc-0.8886, test loss-0.4033, acc-0.8864\n",
      "Iter-68990, train loss-0.3338, acc-0.9200, valid loss-0.3990, acc-0.8886, test loss-0.4032, acc-0.8864\n",
      "Iter-69000, train loss-0.6790, acc-0.8000, valid loss-0.3989, acc-0.8886, test loss-0.4032, acc-0.8865\n",
      "Iter-69010, train loss-0.4612, acc-0.8000, valid loss-0.3989, acc-0.8886, test loss-0.4032, acc-0.8866\n",
      "Iter-69020, train loss-0.3593, acc-0.8600, valid loss-0.3989, acc-0.8886, test loss-0.4032, acc-0.8866\n",
      "Iter-69030, train loss-0.4615, acc-0.8800, valid loss-0.3988, acc-0.8886, test loss-0.4031, acc-0.8866\n",
      "Iter-69040, train loss-0.3750, acc-0.8400, valid loss-0.3988, acc-0.8886, test loss-0.4031, acc-0.8865\n",
      "Iter-69050, train loss-0.4989, acc-0.8600, valid loss-0.3988, acc-0.8886, test loss-0.4031, acc-0.8864\n",
      "Iter-69060, train loss-0.4807, acc-0.8200, valid loss-0.3988, acc-0.8886, test loss-0.4031, acc-0.8866\n",
      "Iter-69070, train loss-0.3419, acc-0.9200, valid loss-0.3988, acc-0.8886, test loss-0.4031, acc-0.8865\n",
      "Iter-69080, train loss-0.3408, acc-0.9400, valid loss-0.3988, acc-0.8886, test loss-0.4031, acc-0.8865\n",
      "Iter-69090, train loss-0.3281, acc-0.9000, valid loss-0.3987, acc-0.8886, test loss-0.4030, acc-0.8864\n",
      "Iter-69100, train loss-0.3748, acc-0.8600, valid loss-0.3987, acc-0.8886, test loss-0.4030, acc-0.8864\n",
      "Iter-69110, train loss-0.3304, acc-0.9200, valid loss-0.3987, acc-0.8886, test loss-0.4030, acc-0.8864\n",
      "Iter-69120, train loss-0.4021, acc-0.8400, valid loss-0.3987, acc-0.8886, test loss-0.4030, acc-0.8864\n",
      "Iter-69130, train loss-0.4579, acc-0.8800, valid loss-0.3987, acc-0.8886, test loss-0.4030, acc-0.8863\n",
      "Iter-69140, train loss-0.5836, acc-0.8200, valid loss-0.3986, acc-0.8886, test loss-0.4029, acc-0.8865\n",
      "Iter-69150, train loss-0.3443, acc-0.9400, valid loss-0.3986, acc-0.8886, test loss-0.4029, acc-0.8864\n",
      "Iter-69160, train loss-0.5045, acc-0.8600, valid loss-0.3986, acc-0.8886, test loss-0.4029, acc-0.8864\n",
      "Iter-69170, train loss-0.7364, acc-0.7800, valid loss-0.3986, acc-0.8886, test loss-0.4029, acc-0.8864\n",
      "Iter-69180, train loss-0.4964, acc-0.8600, valid loss-0.3986, acc-0.8886, test loss-0.4029, acc-0.8864\n",
      "Iter-69190, train loss-0.6002, acc-0.8000, valid loss-0.3985, acc-0.8886, test loss-0.4028, acc-0.8864\n",
      "Iter-69200, train loss-0.3840, acc-0.9000, valid loss-0.3985, acc-0.8886, test loss-0.4028, acc-0.8865\n",
      "Iter-69210, train loss-0.3576, acc-0.9000, valid loss-0.3985, acc-0.8886, test loss-0.4028, acc-0.8865\n",
      "Iter-69220, train loss-0.4454, acc-0.8600, valid loss-0.3985, acc-0.8886, test loss-0.4028, acc-0.8864\n",
      "Iter-69230, train loss-0.4242, acc-0.8600, valid loss-0.3984, acc-0.8886, test loss-0.4027, acc-0.8864\n",
      "Iter-69240, train loss-0.6487, acc-0.8000, valid loss-0.3984, acc-0.8886, test loss-0.4027, acc-0.8864\n",
      "Iter-69250, train loss-0.2684, acc-0.9200, valid loss-0.3984, acc-0.8886, test loss-0.4027, acc-0.8863\n",
      "Iter-69260, train loss-0.3575, acc-0.9000, valid loss-0.3984, acc-0.8886, test loss-0.4027, acc-0.8863\n",
      "Iter-69270, train loss-0.5616, acc-0.8200, valid loss-0.3984, acc-0.8886, test loss-0.4027, acc-0.8863\n",
      "Iter-69280, train loss-0.5751, acc-0.8000, valid loss-0.3983, acc-0.8886, test loss-0.4026, acc-0.8863\n",
      "Iter-69290, train loss-0.3080, acc-0.8800, valid loss-0.3983, acc-0.8886, test loss-0.4026, acc-0.8865\n",
      "Iter-69300, train loss-0.4173, acc-0.8600, valid loss-0.3983, acc-0.8886, test loss-0.4026, acc-0.8866\n",
      "Iter-69310, train loss-0.3703, acc-0.9000, valid loss-0.3983, acc-0.8886, test loss-0.4026, acc-0.8865\n",
      "Iter-69320, train loss-0.2394, acc-0.9400, valid loss-0.3982, acc-0.8886, test loss-0.4026, acc-0.8865\n",
      "Iter-69330, train loss-0.5192, acc-0.8000, valid loss-0.3982, acc-0.8888, test loss-0.4025, acc-0.8865\n",
      "Iter-69340, train loss-0.5875, acc-0.8600, valid loss-0.3981, acc-0.8886, test loss-0.4025, acc-0.8866\n",
      "Iter-69350, train loss-0.3188, acc-0.9200, valid loss-0.3981, acc-0.8888, test loss-0.4025, acc-0.8865\n",
      "Iter-69360, train loss-0.3944, acc-0.9000, valid loss-0.3981, acc-0.8888, test loss-0.4025, acc-0.8865\n",
      "Iter-69370, train loss-0.4140, acc-0.9000, valid loss-0.3981, acc-0.8888, test loss-0.4024, acc-0.8866\n",
      "Iter-69380, train loss-0.4559, acc-0.8400, valid loss-0.3981, acc-0.8888, test loss-0.4024, acc-0.8866\n",
      "Iter-69390, train loss-0.4319, acc-0.9000, valid loss-0.3980, acc-0.8888, test loss-0.4024, acc-0.8865\n",
      "Iter-69400, train loss-0.4429, acc-0.8600, valid loss-0.3980, acc-0.8888, test loss-0.4024, acc-0.8864\n",
      "Iter-69410, train loss-0.3686, acc-0.8800, valid loss-0.3980, acc-0.8888, test loss-0.4023, acc-0.8865\n",
      "Iter-69420, train loss-0.4749, acc-0.8400, valid loss-0.3980, acc-0.8888, test loss-0.4023, acc-0.8866\n",
      "Iter-69430, train loss-0.4121, acc-0.9000, valid loss-0.3980, acc-0.8888, test loss-0.4023, acc-0.8865\n",
      "Iter-69440, train loss-0.3858, acc-0.9200, valid loss-0.3979, acc-0.8890, test loss-0.4023, acc-0.8864\n",
      "Iter-69450, train loss-0.4059, acc-0.8800, valid loss-0.3979, acc-0.8890, test loss-0.4023, acc-0.8865\n",
      "Iter-69460, train loss-0.3203, acc-0.9200, valid loss-0.3979, acc-0.8890, test loss-0.4022, acc-0.8867\n",
      "Iter-69470, train loss-0.3320, acc-0.9400, valid loss-0.3979, acc-0.8888, test loss-0.4022, acc-0.8867\n",
      "Iter-69480, train loss-0.5154, acc-0.8600, valid loss-0.3978, acc-0.8890, test loss-0.4022, acc-0.8868\n",
      "Iter-69490, train loss-0.3393, acc-0.9200, valid loss-0.3978, acc-0.8890, test loss-0.4022, acc-0.8867\n",
      "Iter-69500, train loss-0.4125, acc-0.9000, valid loss-0.3978, acc-0.8890, test loss-0.4021, acc-0.8867\n",
      "Iter-69510, train loss-0.3798, acc-0.8800, valid loss-0.3977, acc-0.8890, test loss-0.4021, acc-0.8867\n",
      "Iter-69520, train loss-0.3000, acc-0.9200, valid loss-0.3977, acc-0.8892, test loss-0.4021, acc-0.8868\n",
      "Iter-69530, train loss-0.4938, acc-0.8400, valid loss-0.3977, acc-0.8890, test loss-0.4021, acc-0.8868\n",
      "Iter-69540, train loss-0.3212, acc-0.9400, valid loss-0.3977, acc-0.8888, test loss-0.4021, acc-0.8868\n",
      "Iter-69550, train loss-0.3168, acc-0.9400, valid loss-0.3977, acc-0.8888, test loss-0.4020, acc-0.8868\n",
      "Iter-69560, train loss-0.4464, acc-0.8800, valid loss-0.3977, acc-0.8888, test loss-0.4020, acc-0.8868\n",
      "Iter-69570, train loss-0.4848, acc-0.8800, valid loss-0.3976, acc-0.8888, test loss-0.4020, acc-0.8868\n",
      "Iter-69580, train loss-0.4302, acc-0.8800, valid loss-0.3976, acc-0.8888, test loss-0.4020, acc-0.8866\n",
      "Iter-69590, train loss-0.4691, acc-0.9200, valid loss-0.3976, acc-0.8890, test loss-0.4019, acc-0.8866\n",
      "Iter-69600, train loss-0.5592, acc-0.7800, valid loss-0.3976, acc-0.8890, test loss-0.4019, acc-0.8866\n",
      "Iter-69610, train loss-0.2743, acc-0.9400, valid loss-0.3975, acc-0.8892, test loss-0.4019, acc-0.8865\n",
      "Iter-69620, train loss-0.5105, acc-0.8400, valid loss-0.3975, acc-0.8892, test loss-0.4019, acc-0.8865\n",
      "Iter-69630, train loss-0.3930, acc-0.9000, valid loss-0.3975, acc-0.8892, test loss-0.4018, acc-0.8865\n",
      "Iter-69640, train loss-0.3787, acc-0.8600, valid loss-0.3975, acc-0.8890, test loss-0.4018, acc-0.8865\n",
      "Iter-69650, train loss-0.4435, acc-0.8400, valid loss-0.3974, acc-0.8890, test loss-0.4018, acc-0.8865\n",
      "Iter-69660, train loss-0.3605, acc-0.8800, valid loss-0.3974, acc-0.8890, test loss-0.4018, acc-0.8864\n",
      "Iter-69670, train loss-0.4520, acc-0.9000, valid loss-0.3974, acc-0.8890, test loss-0.4018, acc-0.8863\n",
      "Iter-69680, train loss-0.3845, acc-0.8600, valid loss-0.3974, acc-0.8890, test loss-0.4017, acc-0.8863\n",
      "Iter-69690, train loss-0.6726, acc-0.8200, valid loss-0.3974, acc-0.8890, test loss-0.4017, acc-0.8864\n",
      "Iter-69700, train loss-0.2228, acc-0.9600, valid loss-0.3974, acc-0.8890, test loss-0.4017, acc-0.8864\n",
      "Iter-69710, train loss-0.2933, acc-0.8800, valid loss-0.3973, acc-0.8890, test loss-0.4017, acc-0.8864\n",
      "Iter-69720, train loss-0.5502, acc-0.8000, valid loss-0.3973, acc-0.8890, test loss-0.4017, acc-0.8864\n",
      "Iter-69730, train loss-0.3080, acc-0.8800, valid loss-0.3973, acc-0.8890, test loss-0.4016, acc-0.8866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-69740, train loss-0.4822, acc-0.8400, valid loss-0.3973, acc-0.8892, test loss-0.4016, acc-0.8867\n",
      "Iter-69750, train loss-0.3040, acc-0.9000, valid loss-0.3972, acc-0.8892, test loss-0.4016, acc-0.8867\n",
      "Iter-69760, train loss-0.3139, acc-0.9000, valid loss-0.3972, acc-0.8892, test loss-0.4016, acc-0.8867\n",
      "Iter-69770, train loss-0.4608, acc-0.9000, valid loss-0.3972, acc-0.8892, test loss-0.4016, acc-0.8867\n",
      "Iter-69780, train loss-0.3590, acc-0.9200, valid loss-0.3972, acc-0.8892, test loss-0.4015, acc-0.8867\n",
      "Iter-69790, train loss-0.5288, acc-0.8200, valid loss-0.3972, acc-0.8892, test loss-0.4015, acc-0.8867\n",
      "Iter-69800, train loss-0.2817, acc-0.9800, valid loss-0.3971, acc-0.8892, test loss-0.4015, acc-0.8866\n",
      "Iter-69810, train loss-0.5608, acc-0.8000, valid loss-0.3971, acc-0.8892, test loss-0.4015, acc-0.8865\n",
      "Iter-69820, train loss-0.4668, acc-0.8200, valid loss-0.3971, acc-0.8892, test loss-0.4015, acc-0.8866\n",
      "Iter-69830, train loss-0.4159, acc-0.8800, valid loss-0.3971, acc-0.8892, test loss-0.4015, acc-0.8866\n",
      "Iter-69840, train loss-0.3915, acc-0.9000, valid loss-0.3970, acc-0.8892, test loss-0.4014, acc-0.8867\n",
      "Iter-69850, train loss-0.5479, acc-0.8000, valid loss-0.3970, acc-0.8892, test loss-0.4014, acc-0.8867\n",
      "Iter-69860, train loss-0.4263, acc-0.8600, valid loss-0.3970, acc-0.8892, test loss-0.4014, acc-0.8867\n",
      "Iter-69870, train loss-0.2478, acc-0.9200, valid loss-0.3970, acc-0.8892, test loss-0.4014, acc-0.8867\n",
      "Iter-69880, train loss-0.2989, acc-0.9000, valid loss-0.3970, acc-0.8892, test loss-0.4014, acc-0.8866\n",
      "Iter-69890, train loss-0.3317, acc-0.9200, valid loss-0.3970, acc-0.8892, test loss-0.4013, acc-0.8866\n",
      "Iter-69900, train loss-0.3503, acc-0.9000, valid loss-0.3970, acc-0.8892, test loss-0.4013, acc-0.8866\n",
      "Iter-69910, train loss-0.2823, acc-0.9000, valid loss-0.3969, acc-0.8892, test loss-0.4013, acc-0.8866\n",
      "Iter-69920, train loss-0.4151, acc-0.8600, valid loss-0.3969, acc-0.8894, test loss-0.4013, acc-0.8866\n",
      "Iter-69930, train loss-0.3427, acc-0.9000, valid loss-0.3969, acc-0.8894, test loss-0.4012, acc-0.8864\n",
      "Iter-69940, train loss-0.2924, acc-0.9200, valid loss-0.3969, acc-0.8894, test loss-0.4012, acc-0.8864\n",
      "Iter-69950, train loss-0.3798, acc-0.9200, valid loss-0.3969, acc-0.8894, test loss-0.4012, acc-0.8865\n",
      "Iter-69960, train loss-0.4822, acc-0.8800, valid loss-0.3968, acc-0.8894, test loss-0.4012, acc-0.8864\n",
      "Iter-69970, train loss-0.3885, acc-0.9200, valid loss-0.3968, acc-0.8894, test loss-0.4011, acc-0.8864\n",
      "Iter-69980, train loss-0.2751, acc-0.9400, valid loss-0.3968, acc-0.8894, test loss-0.4011, acc-0.8864\n",
      "Iter-69990, train loss-0.5259, acc-0.8000, valid loss-0.3968, acc-0.8894, test loss-0.4011, acc-0.8864\n",
      "Iter-70000, train loss-0.4284, acc-0.8800, valid loss-0.3967, acc-0.8894, test loss-0.4011, acc-0.8865\n",
      "Iter-70010, train loss-0.4647, acc-0.8800, valid loss-0.3967, acc-0.8894, test loss-0.4011, acc-0.8864\n",
      "Iter-70020, train loss-0.2620, acc-0.9600, valid loss-0.3967, acc-0.8896, test loss-0.4011, acc-0.8864\n",
      "Iter-70030, train loss-0.4000, acc-0.9000, valid loss-0.3967, acc-0.8894, test loss-0.4010, acc-0.8866\n",
      "Iter-70040, train loss-0.3188, acc-0.9200, valid loss-0.3967, acc-0.8896, test loss-0.4010, acc-0.8866\n",
      "Iter-70050, train loss-0.2210, acc-0.9200, valid loss-0.3966, acc-0.8894, test loss-0.4010, acc-0.8865\n",
      "Iter-70060, train loss-0.3600, acc-0.8400, valid loss-0.3966, acc-0.8894, test loss-0.4010, acc-0.8866\n",
      "Iter-70070, train loss-0.3994, acc-0.9000, valid loss-0.3966, acc-0.8894, test loss-0.4009, acc-0.8866\n",
      "Iter-70080, train loss-0.3121, acc-0.9200, valid loss-0.3966, acc-0.8896, test loss-0.4009, acc-0.8866\n",
      "Iter-70090, train loss-0.2917, acc-0.8800, valid loss-0.3966, acc-0.8894, test loss-0.4009, acc-0.8866\n",
      "Iter-70100, train loss-0.4886, acc-0.8600, valid loss-0.3965, acc-0.8894, test loss-0.4009, acc-0.8866\n",
      "Iter-70110, train loss-0.3902, acc-0.8600, valid loss-0.3965, acc-0.8894, test loss-0.4008, acc-0.8866\n",
      "Iter-70120, train loss-0.2848, acc-0.9400, valid loss-0.3965, acc-0.8894, test loss-0.4008, acc-0.8866\n",
      "Iter-70130, train loss-0.5085, acc-0.8600, valid loss-0.3964, acc-0.8894, test loss-0.4008, acc-0.8866\n",
      "Iter-70140, train loss-0.3529, acc-0.9200, valid loss-0.3964, acc-0.8894, test loss-0.4008, acc-0.8866\n",
      "Iter-70150, train loss-0.4318, acc-0.8800, valid loss-0.3964, acc-0.8894, test loss-0.4007, acc-0.8866\n",
      "Iter-70160, train loss-0.5016, acc-0.9000, valid loss-0.3964, acc-0.8894, test loss-0.4007, acc-0.8866\n",
      "Iter-70170, train loss-0.3949, acc-0.8600, valid loss-0.3964, acc-0.8894, test loss-0.4007, acc-0.8865\n",
      "Iter-70180, train loss-0.3838, acc-0.8600, valid loss-0.3964, acc-0.8894, test loss-0.4007, acc-0.8866\n",
      "Iter-70190, train loss-0.3384, acc-0.9200, valid loss-0.3964, acc-0.8894, test loss-0.4007, acc-0.8866\n",
      "Iter-70200, train loss-0.4704, acc-0.8800, valid loss-0.3963, acc-0.8896, test loss-0.4006, acc-0.8866\n",
      "Iter-70210, train loss-0.6205, acc-0.8200, valid loss-0.3963, acc-0.8896, test loss-0.4006, acc-0.8866\n",
      "Iter-70220, train loss-0.5064, acc-0.8800, valid loss-0.3963, acc-0.8894, test loss-0.4006, acc-0.8865\n",
      "Iter-70230, train loss-0.3478, acc-0.9200, valid loss-0.3963, acc-0.8894, test loss-0.4006, acc-0.8866\n",
      "Iter-70240, train loss-0.4306, acc-0.8600, valid loss-0.3962, acc-0.8896, test loss-0.4005, acc-0.8866\n",
      "Iter-70250, train loss-0.3958, acc-0.8800, valid loss-0.3962, acc-0.8896, test loss-0.4005, acc-0.8866\n",
      "Iter-70260, train loss-0.4324, acc-0.8800, valid loss-0.3962, acc-0.8896, test loss-0.4005, acc-0.8866\n",
      "Iter-70270, train loss-0.2984, acc-0.9200, valid loss-0.3962, acc-0.8896, test loss-0.4005, acc-0.8866\n",
      "Iter-70280, train loss-0.3811, acc-0.9000, valid loss-0.3961, acc-0.8896, test loss-0.4005, acc-0.8866\n",
      "Iter-70290, train loss-0.2869, acc-0.9400, valid loss-0.3961, acc-0.8894, test loss-0.4004, acc-0.8866\n",
      "Iter-70300, train loss-0.3713, acc-0.9400, valid loss-0.3961, acc-0.8894, test loss-0.4004, acc-0.8866\n",
      "Iter-70310, train loss-0.3395, acc-0.9000, valid loss-0.3961, acc-0.8894, test loss-0.4004, acc-0.8866\n",
      "Iter-70320, train loss-0.3592, acc-0.9200, valid loss-0.3960, acc-0.8894, test loss-0.4004, acc-0.8865\n",
      "Iter-70330, train loss-0.3918, acc-0.8800, valid loss-0.3960, acc-0.8896, test loss-0.4003, acc-0.8866\n",
      "Iter-70340, train loss-0.4215, acc-0.8800, valid loss-0.3960, acc-0.8896, test loss-0.4003, acc-0.8866\n",
      "Iter-70350, train loss-0.5302, acc-0.8200, valid loss-0.3960, acc-0.8894, test loss-0.4003, acc-0.8866\n",
      "Iter-70360, train loss-0.4899, acc-0.8400, valid loss-0.3959, acc-0.8896, test loss-0.4003, acc-0.8865\n",
      "Iter-70370, train loss-0.5373, acc-0.8200, valid loss-0.3959, acc-0.8896, test loss-0.4003, acc-0.8865\n",
      "Iter-70380, train loss-0.4041, acc-0.9000, valid loss-0.3959, acc-0.8896, test loss-0.4002, acc-0.8865\n",
      "Iter-70390, train loss-0.3390, acc-0.9000, valid loss-0.3959, acc-0.8894, test loss-0.4002, acc-0.8865\n",
      "Iter-70400, train loss-0.3032, acc-0.9000, valid loss-0.3958, acc-0.8894, test loss-0.4002, acc-0.8865\n",
      "Iter-70410, train loss-0.3741, acc-0.9000, valid loss-0.3958, acc-0.8894, test loss-0.4002, acc-0.8865\n",
      "Iter-70420, train loss-0.4027, acc-0.8600, valid loss-0.3958, acc-0.8894, test loss-0.4001, acc-0.8866\n",
      "Iter-70430, train loss-0.2701, acc-0.9000, valid loss-0.3958, acc-0.8894, test loss-0.4001, acc-0.8865\n",
      "Iter-70440, train loss-0.5003, acc-0.8600, valid loss-0.3957, acc-0.8894, test loss-0.4001, acc-0.8865\n",
      "Iter-70450, train loss-0.4705, acc-0.8800, valid loss-0.3957, acc-0.8894, test loss-0.4001, acc-0.8865\n",
      "Iter-70460, train loss-0.7950, acc-0.8400, valid loss-0.3957, acc-0.8894, test loss-0.4001, acc-0.8866\n",
      "Iter-70470, train loss-0.4771, acc-0.8200, valid loss-0.3957, acc-0.8894, test loss-0.4000, acc-0.8865\n",
      "Iter-70480, train loss-0.3889, acc-0.8800, valid loss-0.3956, acc-0.8894, test loss-0.4000, acc-0.8865\n",
      "Iter-70490, train loss-0.5424, acc-0.8800, valid loss-0.3956, acc-0.8896, test loss-0.4000, acc-0.8865\n",
      "Iter-70500, train loss-0.5685, acc-0.8400, valid loss-0.3956, acc-0.8896, test loss-0.4000, acc-0.8865\n",
      "Iter-70510, train loss-0.4224, acc-0.9000, valid loss-0.3956, acc-0.8896, test loss-0.3999, acc-0.8865\n",
      "Iter-70520, train loss-0.5197, acc-0.9200, valid loss-0.3956, acc-0.8896, test loss-0.3999, acc-0.8867\n",
      "Iter-70530, train loss-0.5299, acc-0.8400, valid loss-0.3955, acc-0.8894, test loss-0.3999, acc-0.8867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-70540, train loss-0.3636, acc-0.9000, valid loss-0.3955, acc-0.8896, test loss-0.3999, acc-0.8868\n",
      "Iter-70550, train loss-0.3904, acc-0.9400, valid loss-0.3955, acc-0.8894, test loss-0.3999, acc-0.8868\n",
      "Iter-70560, train loss-0.2838, acc-0.9600, valid loss-0.3955, acc-0.8894, test loss-0.3999, acc-0.8869\n",
      "Iter-70570, train loss-0.4434, acc-0.8800, valid loss-0.3955, acc-0.8896, test loss-0.3998, acc-0.8868\n",
      "Iter-70580, train loss-0.4995, acc-0.9000, valid loss-0.3954, acc-0.8896, test loss-0.3998, acc-0.8869\n",
      "Iter-70590, train loss-0.3740, acc-0.9200, valid loss-0.3954, acc-0.8898, test loss-0.3998, acc-0.8868\n",
      "Iter-70600, train loss-0.4250, acc-0.8400, valid loss-0.3954, acc-0.8900, test loss-0.3998, acc-0.8869\n",
      "Iter-70610, train loss-0.4341, acc-0.9000, valid loss-0.3954, acc-0.8900, test loss-0.3997, acc-0.8868\n",
      "Iter-70620, train loss-0.5331, acc-0.8600, valid loss-0.3953, acc-0.8900, test loss-0.3997, acc-0.8868\n",
      "Iter-70630, train loss-0.2565, acc-0.9200, valid loss-0.3953, acc-0.8898, test loss-0.3997, acc-0.8869\n",
      "Iter-70640, train loss-0.5296, acc-0.8200, valid loss-0.3953, acc-0.8900, test loss-0.3997, acc-0.8868\n",
      "Iter-70650, train loss-0.4616, acc-0.8600, valid loss-0.3953, acc-0.8898, test loss-0.3997, acc-0.8867\n",
      "Iter-70660, train loss-0.3675, acc-0.9000, valid loss-0.3953, acc-0.8900, test loss-0.3997, acc-0.8867\n",
      "Iter-70670, train loss-0.3005, acc-0.9200, valid loss-0.3953, acc-0.8898, test loss-0.3996, acc-0.8867\n",
      "Iter-70680, train loss-0.4187, acc-0.8600, valid loss-0.3953, acc-0.8900, test loss-0.3996, acc-0.8867\n",
      "Iter-70690, train loss-0.3592, acc-0.9400, valid loss-0.3952, acc-0.8898, test loss-0.3996, acc-0.8867\n",
      "Iter-70700, train loss-0.4887, acc-0.8800, valid loss-0.3952, acc-0.8898, test loss-0.3996, acc-0.8867\n",
      "Iter-70710, train loss-0.4847, acc-0.8600, valid loss-0.3952, acc-0.8898, test loss-0.3995, acc-0.8867\n",
      "Iter-70720, train loss-0.3836, acc-0.8800, valid loss-0.3952, acc-0.8900, test loss-0.3995, acc-0.8867\n",
      "Iter-70730, train loss-0.4191, acc-0.8600, valid loss-0.3952, acc-0.8900, test loss-0.3995, acc-0.8867\n",
      "Iter-70740, train loss-0.6801, acc-0.8400, valid loss-0.3951, acc-0.8900, test loss-0.3995, acc-0.8868\n",
      "Iter-70750, train loss-0.4596, acc-0.8600, valid loss-0.3951, acc-0.8900, test loss-0.3994, acc-0.8868\n",
      "Iter-70760, train loss-0.2090, acc-0.9600, valid loss-0.3951, acc-0.8900, test loss-0.3994, acc-0.8868\n",
      "Iter-70770, train loss-0.9135, acc-0.7400, valid loss-0.3951, acc-0.8900, test loss-0.3994, acc-0.8868\n",
      "Iter-70780, train loss-0.7465, acc-0.7400, valid loss-0.3950, acc-0.8900, test loss-0.3994, acc-0.8869\n",
      "Iter-70790, train loss-0.4990, acc-0.8600, valid loss-0.3950, acc-0.8900, test loss-0.3994, acc-0.8869\n",
      "Iter-70800, train loss-0.5125, acc-0.8400, valid loss-0.3950, acc-0.8900, test loss-0.3993, acc-0.8869\n",
      "Iter-70810, train loss-0.3047, acc-0.9000, valid loss-0.3950, acc-0.8900, test loss-0.3993, acc-0.8869\n",
      "Iter-70820, train loss-0.4742, acc-0.8000, valid loss-0.3950, acc-0.8900, test loss-0.3993, acc-0.8869\n",
      "Iter-70830, train loss-0.3368, acc-0.9200, valid loss-0.3949, acc-0.8898, test loss-0.3993, acc-0.8869\n",
      "Iter-70840, train loss-0.3824, acc-0.9200, valid loss-0.3949, acc-0.8900, test loss-0.3992, acc-0.8869\n",
      "Iter-70850, train loss-0.3176, acc-0.9400, valid loss-0.3949, acc-0.8900, test loss-0.3992, acc-0.8869\n",
      "Iter-70860, train loss-0.5937, acc-0.8000, valid loss-0.3949, acc-0.8900, test loss-0.3992, acc-0.8869\n",
      "Iter-70870, train loss-0.3923, acc-0.9200, valid loss-0.3948, acc-0.8900, test loss-0.3992, acc-0.8869\n",
      "Iter-70880, train loss-0.2556, acc-0.9600, valid loss-0.3948, acc-0.8900, test loss-0.3992, acc-0.8869\n",
      "Iter-70890, train loss-0.3244, acc-0.9200, valid loss-0.3948, acc-0.8900, test loss-0.3991, acc-0.8868\n",
      "Iter-70900, train loss-0.5528, acc-0.8600, valid loss-0.3948, acc-0.8900, test loss-0.3991, acc-0.8868\n",
      "Iter-70910, train loss-0.4598, acc-0.8600, valid loss-0.3947, acc-0.8900, test loss-0.3991, acc-0.8868\n",
      "Iter-70920, train loss-0.5636, acc-0.8400, valid loss-0.3947, acc-0.8898, test loss-0.3991, acc-0.8867\n",
      "Iter-70930, train loss-0.2814, acc-0.9200, valid loss-0.3947, acc-0.8898, test loss-0.3991, acc-0.8867\n",
      "Iter-70940, train loss-0.5609, acc-0.8800, valid loss-0.3947, acc-0.8898, test loss-0.3990, acc-0.8867\n",
      "Iter-70950, train loss-0.4457, acc-0.8800, valid loss-0.3947, acc-0.8898, test loss-0.3990, acc-0.8867\n",
      "Iter-70960, train loss-0.3819, acc-0.9000, valid loss-0.3946, acc-0.8898, test loss-0.3990, acc-0.8867\n",
      "Iter-70970, train loss-0.3062, acc-0.9600, valid loss-0.3946, acc-0.8898, test loss-0.3990, acc-0.8867\n",
      "Iter-70980, train loss-0.4838, acc-0.8800, valid loss-0.3946, acc-0.8898, test loss-0.3989, acc-0.8867\n",
      "Iter-70990, train loss-0.4748, acc-0.8600, valid loss-0.3946, acc-0.8898, test loss-0.3989, acc-0.8867\n",
      "Iter-71000, train loss-0.3529, acc-0.8800, valid loss-0.3946, acc-0.8898, test loss-0.3989, acc-0.8867\n",
      "Iter-71010, train loss-0.2747, acc-0.8800, valid loss-0.3945, acc-0.8898, test loss-0.3989, acc-0.8867\n",
      "Iter-71020, train loss-0.3616, acc-0.9200, valid loss-0.3945, acc-0.8898, test loss-0.3989, acc-0.8867\n",
      "Iter-71030, train loss-0.5338, acc-0.8000, valid loss-0.3945, acc-0.8898, test loss-0.3989, acc-0.8867\n",
      "Iter-71040, train loss-0.2502, acc-0.9400, valid loss-0.3945, acc-0.8898, test loss-0.3988, acc-0.8867\n",
      "Iter-71050, train loss-0.4304, acc-0.9000, valid loss-0.3945, acc-0.8898, test loss-0.3988, acc-0.8867\n",
      "Iter-71060, train loss-0.4775, acc-0.8200, valid loss-0.3945, acc-0.8898, test loss-0.3988, acc-0.8868\n",
      "Iter-71070, train loss-0.2961, acc-0.9200, valid loss-0.3944, acc-0.8898, test loss-0.3988, acc-0.8868\n",
      "Iter-71080, train loss-0.3816, acc-0.8800, valid loss-0.3944, acc-0.8898, test loss-0.3987, acc-0.8868\n",
      "Iter-71090, train loss-0.2922, acc-0.9400, valid loss-0.3944, acc-0.8898, test loss-0.3987, acc-0.8868\n",
      "Iter-71100, train loss-0.7233, acc-0.7800, valid loss-0.3944, acc-0.8898, test loss-0.3987, acc-0.8868\n",
      "Iter-71110, train loss-0.2373, acc-0.9400, valid loss-0.3943, acc-0.8898, test loss-0.3987, acc-0.8867\n",
      "Iter-71120, train loss-0.4309, acc-0.8800, valid loss-0.3943, acc-0.8898, test loss-0.3986, acc-0.8868\n",
      "Iter-71130, train loss-0.3699, acc-0.8600, valid loss-0.3943, acc-0.8898, test loss-0.3986, acc-0.8868\n",
      "Iter-71140, train loss-0.3391, acc-0.9600, valid loss-0.3943, acc-0.8898, test loss-0.3986, acc-0.8868\n",
      "Iter-71150, train loss-0.4011, acc-0.8600, valid loss-0.3943, acc-0.8898, test loss-0.3986, acc-0.8868\n",
      "Iter-71160, train loss-0.2481, acc-0.9400, valid loss-0.3942, acc-0.8898, test loss-0.3985, acc-0.8869\n",
      "Iter-71170, train loss-0.4337, acc-0.9000, valid loss-0.3942, acc-0.8898, test loss-0.3985, acc-0.8869\n",
      "Iter-71180, train loss-0.4967, acc-0.8400, valid loss-0.3942, acc-0.8898, test loss-0.3985, acc-0.8868\n",
      "Iter-71190, train loss-0.4333, acc-0.8600, valid loss-0.3942, acc-0.8898, test loss-0.3985, acc-0.8869\n",
      "Iter-71200, train loss-0.5628, acc-0.8600, valid loss-0.3942, acc-0.8898, test loss-0.3985, acc-0.8870\n",
      "Iter-71210, train loss-0.5065, acc-0.8200, valid loss-0.3941, acc-0.8898, test loss-0.3985, acc-0.8870\n",
      "Iter-71220, train loss-0.3275, acc-0.9000, valid loss-0.3941, acc-0.8900, test loss-0.3984, acc-0.8870\n",
      "Iter-71230, train loss-0.4253, acc-0.8400, valid loss-0.3941, acc-0.8900, test loss-0.3984, acc-0.8870\n",
      "Iter-71240, train loss-0.4290, acc-0.8600, valid loss-0.3941, acc-0.8902, test loss-0.3984, acc-0.8870\n",
      "Iter-71250, train loss-0.3534, acc-0.8800, valid loss-0.3940, acc-0.8900, test loss-0.3984, acc-0.8870\n",
      "Iter-71260, train loss-0.5956, acc-0.8400, valid loss-0.3940, acc-0.8900, test loss-0.3984, acc-0.8871\n",
      "Iter-71270, train loss-0.5093, acc-0.8200, valid loss-0.3940, acc-0.8900, test loss-0.3983, acc-0.8869\n",
      "Iter-71280, train loss-0.2565, acc-0.9400, valid loss-0.3940, acc-0.8900, test loss-0.3983, acc-0.8869\n",
      "Iter-71290, train loss-0.3616, acc-0.8400, valid loss-0.3940, acc-0.8900, test loss-0.3983, acc-0.8870\n",
      "Iter-71300, train loss-0.5289, acc-0.8400, valid loss-0.3940, acc-0.8900, test loss-0.3983, acc-0.8870\n",
      "Iter-71310, train loss-0.5580, acc-0.7400, valid loss-0.3939, acc-0.8900, test loss-0.3982, acc-0.8870\n",
      "Iter-71320, train loss-0.5935, acc-0.9000, valid loss-0.3939, acc-0.8902, test loss-0.3982, acc-0.8870\n",
      "Iter-71330, train loss-0.3909, acc-0.8800, valid loss-0.3939, acc-0.8902, test loss-0.3982, acc-0.8870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-71340, train loss-0.3941, acc-0.8600, valid loss-0.3939, acc-0.8900, test loss-0.3982, acc-0.8869\n",
      "Iter-71350, train loss-0.6711, acc-0.8400, valid loss-0.3939, acc-0.8902, test loss-0.3982, acc-0.8869\n",
      "Iter-71360, train loss-0.2908, acc-0.9200, valid loss-0.3938, acc-0.8902, test loss-0.3982, acc-0.8869\n",
      "Iter-71370, train loss-0.6035, acc-0.8600, valid loss-0.3938, acc-0.8902, test loss-0.3981, acc-0.8870\n",
      "Iter-71380, train loss-0.3095, acc-0.9400, valid loss-0.3938, acc-0.8900, test loss-0.3981, acc-0.8870\n",
      "Iter-71390, train loss-0.4833, acc-0.8200, valid loss-0.3938, acc-0.8902, test loss-0.3981, acc-0.8869\n",
      "Iter-71400, train loss-0.2780, acc-0.9400, valid loss-0.3937, acc-0.8902, test loss-0.3981, acc-0.8870\n",
      "Iter-71410, train loss-0.3662, acc-0.8600, valid loss-0.3937, acc-0.8902, test loss-0.3980, acc-0.8870\n",
      "Iter-71420, train loss-0.5631, acc-0.8400, valid loss-0.3937, acc-0.8902, test loss-0.3980, acc-0.8871\n",
      "Iter-71430, train loss-0.3516, acc-0.8400, valid loss-0.3937, acc-0.8902, test loss-0.3980, acc-0.8871\n",
      "Iter-71440, train loss-0.3770, acc-0.9200, valid loss-0.3936, acc-0.8902, test loss-0.3980, acc-0.8871\n",
      "Iter-71450, train loss-0.3753, acc-0.8600, valid loss-0.3936, acc-0.8902, test loss-0.3979, acc-0.8869\n",
      "Iter-71460, train loss-0.3464, acc-0.9200, valid loss-0.3936, acc-0.8902, test loss-0.3979, acc-0.8871\n",
      "Iter-71470, train loss-0.2566, acc-0.9600, valid loss-0.3936, acc-0.8904, test loss-0.3979, acc-0.8870\n",
      "Iter-71480, train loss-0.5427, acc-0.9000, valid loss-0.3935, acc-0.8902, test loss-0.3979, acc-0.8870\n",
      "Iter-71490, train loss-0.3308, acc-0.9200, valid loss-0.3935, acc-0.8902, test loss-0.3979, acc-0.8870\n",
      "Iter-71500, train loss-0.3787, acc-0.9200, valid loss-0.3935, acc-0.8902, test loss-0.3978, acc-0.8871\n",
      "Iter-71510, train loss-0.4186, acc-0.9000, valid loss-0.3935, acc-0.8902, test loss-0.3978, acc-0.8871\n",
      "Iter-71520, train loss-0.2377, acc-0.9400, valid loss-0.3934, acc-0.8904, test loss-0.3978, acc-0.8871\n",
      "Iter-71530, train loss-0.6191, acc-0.7600, valid loss-0.3934, acc-0.8902, test loss-0.3978, acc-0.8872\n",
      "Iter-71540, train loss-0.6654, acc-0.7600, valid loss-0.3934, acc-0.8902, test loss-0.3978, acc-0.8870\n",
      "Iter-71550, train loss-0.5005, acc-0.8400, valid loss-0.3934, acc-0.8902, test loss-0.3978, acc-0.8870\n",
      "Iter-71560, train loss-0.5353, acc-0.8200, valid loss-0.3934, acc-0.8902, test loss-0.3977, acc-0.8871\n",
      "Iter-71570, train loss-0.5643, acc-0.8400, valid loss-0.3933, acc-0.8902, test loss-0.3977, acc-0.8871\n",
      "Iter-71580, train loss-0.3852, acc-0.8800, valid loss-0.3933, acc-0.8902, test loss-0.3977, acc-0.8871\n",
      "Iter-71590, train loss-0.4139, acc-0.8800, valid loss-0.3933, acc-0.8902, test loss-0.3977, acc-0.8871\n",
      "Iter-71600, train loss-0.4383, acc-0.8800, valid loss-0.3933, acc-0.8904, test loss-0.3977, acc-0.8871\n",
      "Iter-71610, train loss-0.4130, acc-0.8600, valid loss-0.3933, acc-0.8904, test loss-0.3976, acc-0.8873\n",
      "Iter-71620, train loss-0.3516, acc-0.9000, valid loss-0.3932, acc-0.8904, test loss-0.3976, acc-0.8871\n",
      "Iter-71630, train loss-0.8615, acc-0.7600, valid loss-0.3932, acc-0.8904, test loss-0.3976, acc-0.8871\n",
      "Iter-71640, train loss-0.3808, acc-0.8200, valid loss-0.3932, acc-0.8904, test loss-0.3976, acc-0.8870\n",
      "Iter-71650, train loss-0.4758, acc-0.8800, valid loss-0.3932, acc-0.8904, test loss-0.3975, acc-0.8870\n",
      "Iter-71660, train loss-0.3305, acc-0.9000, valid loss-0.3932, acc-0.8904, test loss-0.3975, acc-0.8870\n",
      "Iter-71670, train loss-0.4782, acc-0.8400, valid loss-0.3931, acc-0.8904, test loss-0.3975, acc-0.8871\n",
      "Iter-71680, train loss-0.4518, acc-0.8600, valid loss-0.3931, acc-0.8904, test loss-0.3975, acc-0.8871\n",
      "Iter-71690, train loss-0.6158, acc-0.8400, valid loss-0.3931, acc-0.8902, test loss-0.3974, acc-0.8871\n",
      "Iter-71700, train loss-0.2295, acc-0.9600, valid loss-0.3930, acc-0.8904, test loss-0.3974, acc-0.8871\n",
      "Iter-71710, train loss-0.5673, acc-0.8600, valid loss-0.3930, acc-0.8904, test loss-0.3974, acc-0.8871\n",
      "Iter-71720, train loss-0.2548, acc-0.9600, valid loss-0.3930, acc-0.8904, test loss-0.3974, acc-0.8871\n",
      "Iter-71730, train loss-0.4836, acc-0.8200, valid loss-0.3930, acc-0.8904, test loss-0.3974, acc-0.8871\n",
      "Iter-71740, train loss-0.4358, acc-0.8800, valid loss-0.3930, acc-0.8904, test loss-0.3973, acc-0.8871\n",
      "Iter-71750, train loss-0.3708, acc-0.9000, valid loss-0.3929, acc-0.8904, test loss-0.3973, acc-0.8872\n",
      "Iter-71760, train loss-0.5455, acc-0.8400, valid loss-0.3929, acc-0.8904, test loss-0.3973, acc-0.8872\n",
      "Iter-71770, train loss-0.2597, acc-0.9800, valid loss-0.3929, acc-0.8904, test loss-0.3973, acc-0.8872\n",
      "Iter-71780, train loss-0.4449, acc-0.9000, valid loss-0.3929, acc-0.8904, test loss-0.3973, acc-0.8872\n",
      "Iter-71790, train loss-0.3599, acc-0.9000, valid loss-0.3928, acc-0.8904, test loss-0.3972, acc-0.8871\n",
      "Iter-71800, train loss-0.2573, acc-0.9400, valid loss-0.3928, acc-0.8904, test loss-0.3972, acc-0.8872\n",
      "Iter-71810, train loss-0.3999, acc-0.9000, valid loss-0.3928, acc-0.8904, test loss-0.3972, acc-0.8872\n",
      "Iter-71820, train loss-0.3884, acc-0.8800, valid loss-0.3928, acc-0.8904, test loss-0.3972, acc-0.8872\n",
      "Iter-71830, train loss-0.2709, acc-0.9200, valid loss-0.3927, acc-0.8904, test loss-0.3971, acc-0.8872\n",
      "Iter-71840, train loss-0.7579, acc-0.7600, valid loss-0.3927, acc-0.8904, test loss-0.3971, acc-0.8872\n",
      "Iter-71850, train loss-0.4352, acc-0.8800, valid loss-0.3927, acc-0.8904, test loss-0.3971, acc-0.8873\n",
      "Iter-71860, train loss-0.2838, acc-0.9400, valid loss-0.3927, acc-0.8904, test loss-0.3971, acc-0.8873\n",
      "Iter-71870, train loss-0.4420, acc-0.8200, valid loss-0.3927, acc-0.8904, test loss-0.3971, acc-0.8873\n",
      "Iter-71880, train loss-0.3667, acc-0.9200, valid loss-0.3926, acc-0.8904, test loss-0.3970, acc-0.8873\n",
      "Iter-71890, train loss-0.4355, acc-0.8800, valid loss-0.3926, acc-0.8904, test loss-0.3970, acc-0.8873\n",
      "Iter-71900, train loss-0.3420, acc-0.9200, valid loss-0.3926, acc-0.8904, test loss-0.3970, acc-0.8873\n",
      "Iter-71910, train loss-0.3692, acc-0.9400, valid loss-0.3926, acc-0.8904, test loss-0.3970, acc-0.8873\n",
      "Iter-71920, train loss-0.3343, acc-0.9400, valid loss-0.3925, acc-0.8904, test loss-0.3970, acc-0.8873\n",
      "Iter-71930, train loss-0.5082, acc-0.8000, valid loss-0.3925, acc-0.8904, test loss-0.3970, acc-0.8873\n",
      "Iter-71940, train loss-0.4877, acc-0.8400, valid loss-0.3925, acc-0.8904, test loss-0.3969, acc-0.8873\n",
      "Iter-71950, train loss-0.3209, acc-0.9400, valid loss-0.3925, acc-0.8904, test loss-0.3969, acc-0.8874\n",
      "Iter-71960, train loss-0.2826, acc-0.9200, valid loss-0.3924, acc-0.8904, test loss-0.3969, acc-0.8874\n",
      "Iter-71970, train loss-0.5883, acc-0.7400, valid loss-0.3924, acc-0.8904, test loss-0.3969, acc-0.8874\n",
      "Iter-71980, train loss-0.6387, acc-0.7800, valid loss-0.3924, acc-0.8904, test loss-0.3968, acc-0.8873\n",
      "Iter-71990, train loss-0.4018, acc-0.8400, valid loss-0.3924, acc-0.8904, test loss-0.3968, acc-0.8873\n",
      "Iter-72000, train loss-0.3722, acc-0.8800, valid loss-0.3924, acc-0.8904, test loss-0.3968, acc-0.8873\n",
      "Iter-72010, train loss-0.4338, acc-0.8800, valid loss-0.3924, acc-0.8904, test loss-0.3968, acc-0.8873\n",
      "Iter-72020, train loss-0.6137, acc-0.8200, valid loss-0.3923, acc-0.8904, test loss-0.3968, acc-0.8874\n",
      "Iter-72030, train loss-0.3919, acc-0.9200, valid loss-0.3923, acc-0.8904, test loss-0.3967, acc-0.8874\n",
      "Iter-72040, train loss-0.7231, acc-0.7200, valid loss-0.3923, acc-0.8904, test loss-0.3967, acc-0.8873\n",
      "Iter-72050, train loss-0.3917, acc-0.9600, valid loss-0.3922, acc-0.8904, test loss-0.3967, acc-0.8873\n",
      "Iter-72060, train loss-0.3380, acc-0.8800, valid loss-0.3922, acc-0.8904, test loss-0.3966, acc-0.8874\n",
      "Iter-72070, train loss-0.6427, acc-0.8000, valid loss-0.3922, acc-0.8904, test loss-0.3966, acc-0.8873\n",
      "Iter-72080, train loss-0.4297, acc-0.8600, valid loss-0.3922, acc-0.8904, test loss-0.3966, acc-0.8873\n",
      "Iter-72090, train loss-0.4668, acc-0.8400, valid loss-0.3922, acc-0.8904, test loss-0.3966, acc-0.8874\n",
      "Iter-72100, train loss-0.6230, acc-0.8600, valid loss-0.3921, acc-0.8904, test loss-0.3966, acc-0.8875\n",
      "Iter-72110, train loss-0.4143, acc-0.9000, valid loss-0.3921, acc-0.8908, test loss-0.3965, acc-0.8875\n",
      "Iter-72120, train loss-0.3612, acc-0.9000, valid loss-0.3921, acc-0.8908, test loss-0.3965, acc-0.8875\n",
      "Iter-72130, train loss-0.2882, acc-0.9200, valid loss-0.3921, acc-0.8906, test loss-0.3965, acc-0.8875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-72140, train loss-0.5309, acc-0.8400, valid loss-0.3920, acc-0.8908, test loss-0.3965, acc-0.8875\n",
      "Iter-72150, train loss-0.3619, acc-0.9200, valid loss-0.3920, acc-0.8906, test loss-0.3965, acc-0.8874\n",
      "Iter-72160, train loss-0.3696, acc-0.9200, valid loss-0.3920, acc-0.8904, test loss-0.3965, acc-0.8874\n",
      "Iter-72170, train loss-0.3353, acc-0.9000, valid loss-0.3920, acc-0.8908, test loss-0.3964, acc-0.8873\n",
      "Iter-72180, train loss-0.3085, acc-0.9200, valid loss-0.3920, acc-0.8906, test loss-0.3964, acc-0.8873\n",
      "Iter-72190, train loss-0.5095, acc-0.8600, valid loss-0.3920, acc-0.8908, test loss-0.3964, acc-0.8874\n",
      "Iter-72200, train loss-0.4269, acc-0.8800, valid loss-0.3919, acc-0.8904, test loss-0.3964, acc-0.8874\n",
      "Iter-72210, train loss-0.2265, acc-0.9200, valid loss-0.3919, acc-0.8904, test loss-0.3964, acc-0.8874\n",
      "Iter-72220, train loss-0.3012, acc-0.9400, valid loss-0.3919, acc-0.8904, test loss-0.3964, acc-0.8874\n",
      "Iter-72230, train loss-0.4193, acc-0.8800, valid loss-0.3919, acc-0.8904, test loss-0.3963, acc-0.8873\n",
      "Iter-72240, train loss-0.3286, acc-0.9200, valid loss-0.3919, acc-0.8904, test loss-0.3963, acc-0.8874\n",
      "Iter-72250, train loss-0.4480, acc-0.8800, valid loss-0.3919, acc-0.8904, test loss-0.3963, acc-0.8873\n",
      "Iter-72260, train loss-0.5018, acc-0.8600, valid loss-0.3918, acc-0.8904, test loss-0.3963, acc-0.8873\n",
      "Iter-72270, train loss-0.2758, acc-0.9200, valid loss-0.3918, acc-0.8904, test loss-0.3962, acc-0.8873\n",
      "Iter-72280, train loss-0.4474, acc-0.8400, valid loss-0.3918, acc-0.8908, test loss-0.3962, acc-0.8873\n",
      "Iter-72290, train loss-0.2769, acc-0.9600, valid loss-0.3918, acc-0.8906, test loss-0.3962, acc-0.8873\n",
      "Iter-72300, train loss-0.5091, acc-0.8000, valid loss-0.3918, acc-0.8908, test loss-0.3962, acc-0.8873\n",
      "Iter-72310, train loss-0.2847, acc-0.9400, valid loss-0.3917, acc-0.8908, test loss-0.3962, acc-0.8874\n",
      "Iter-72320, train loss-0.4426, acc-0.8600, valid loss-0.3917, acc-0.8908, test loss-0.3961, acc-0.8873\n",
      "Iter-72330, train loss-0.2793, acc-0.9400, valid loss-0.3917, acc-0.8908, test loss-0.3961, acc-0.8872\n",
      "Iter-72340, train loss-0.3289, acc-0.9400, valid loss-0.3917, acc-0.8904, test loss-0.3961, acc-0.8872\n",
      "Iter-72350, train loss-0.2715, acc-0.9000, valid loss-0.3916, acc-0.8908, test loss-0.3961, acc-0.8872\n",
      "Iter-72360, train loss-0.3954, acc-0.9000, valid loss-0.3916, acc-0.8908, test loss-0.3960, acc-0.8872\n",
      "Iter-72370, train loss-0.2957, acc-0.9000, valid loss-0.3916, acc-0.8904, test loss-0.3960, acc-0.8873\n",
      "Iter-72380, train loss-0.3733, acc-0.9200, valid loss-0.3916, acc-0.8904, test loss-0.3960, acc-0.8874\n",
      "Iter-72390, train loss-0.2998, acc-0.9000, valid loss-0.3916, acc-0.8904, test loss-0.3960, acc-0.8874\n",
      "Iter-72400, train loss-0.3636, acc-0.9200, valid loss-0.3915, acc-0.8904, test loss-0.3960, acc-0.8874\n",
      "Iter-72410, train loss-0.3301, acc-0.8800, valid loss-0.3915, acc-0.8904, test loss-0.3959, acc-0.8874\n",
      "Iter-72420, train loss-0.4442, acc-0.8200, valid loss-0.3915, acc-0.8904, test loss-0.3959, acc-0.8874\n",
      "Iter-72430, train loss-0.3329, acc-0.9400, valid loss-0.3915, acc-0.8904, test loss-0.3959, acc-0.8874\n",
      "Iter-72440, train loss-0.3295, acc-0.9000, valid loss-0.3915, acc-0.8904, test loss-0.3959, acc-0.8874\n",
      "Iter-72450, train loss-0.4799, acc-0.8400, valid loss-0.3915, acc-0.8904, test loss-0.3958, acc-0.8874\n",
      "Iter-72460, train loss-0.3979, acc-0.8800, valid loss-0.3914, acc-0.8906, test loss-0.3958, acc-0.8874\n",
      "Iter-72470, train loss-0.4177, acc-0.9000, valid loss-0.3914, acc-0.8906, test loss-0.3958, acc-0.8874\n",
      "Iter-72480, train loss-0.3021, acc-0.9200, valid loss-0.3914, acc-0.8906, test loss-0.3958, acc-0.8874\n",
      "Iter-72490, train loss-0.6106, acc-0.8200, valid loss-0.3914, acc-0.8906, test loss-0.3958, acc-0.8874\n",
      "Iter-72500, train loss-0.4423, acc-0.8400, valid loss-0.3914, acc-0.8904, test loss-0.3957, acc-0.8873\n",
      "Iter-72510, train loss-0.3786, acc-0.9000, valid loss-0.3913, acc-0.8906, test loss-0.3957, acc-0.8873\n",
      "Iter-72520, train loss-0.3889, acc-0.9200, valid loss-0.3913, acc-0.8904, test loss-0.3957, acc-0.8874\n",
      "Iter-72530, train loss-0.2333, acc-0.9600, valid loss-0.3913, acc-0.8904, test loss-0.3957, acc-0.8875\n",
      "Iter-72540, train loss-0.5659, acc-0.9000, valid loss-0.3912, acc-0.8904, test loss-0.3956, acc-0.8875\n",
      "Iter-72550, train loss-0.3777, acc-0.8800, valid loss-0.3912, acc-0.8904, test loss-0.3956, acc-0.8875\n",
      "Iter-72560, train loss-0.4363, acc-0.8800, valid loss-0.3912, acc-0.8906, test loss-0.3956, acc-0.8875\n",
      "Iter-72570, train loss-0.5881, acc-0.8600, valid loss-0.3912, acc-0.8906, test loss-0.3956, acc-0.8874\n",
      "Iter-72580, train loss-0.3341, acc-0.8800, valid loss-0.3912, acc-0.8906, test loss-0.3956, acc-0.8874\n",
      "Iter-72590, train loss-0.2859, acc-0.9600, valid loss-0.3911, acc-0.8904, test loss-0.3955, acc-0.8874\n",
      "Iter-72600, train loss-0.4910, acc-0.8200, valid loss-0.3911, acc-0.8904, test loss-0.3955, acc-0.8875\n",
      "Iter-72610, train loss-0.6076, acc-0.8600, valid loss-0.3911, acc-0.8904, test loss-0.3955, acc-0.8875\n",
      "Iter-72620, train loss-0.1813, acc-0.9600, valid loss-0.3911, acc-0.8904, test loss-0.3955, acc-0.8875\n",
      "Iter-72630, train loss-0.4629, acc-0.8800, valid loss-0.3911, acc-0.8904, test loss-0.3955, acc-0.8875\n",
      "Iter-72640, train loss-0.4390, acc-0.8600, valid loss-0.3911, acc-0.8904, test loss-0.3955, acc-0.8875\n",
      "Iter-72650, train loss-0.5078, acc-0.8200, valid loss-0.3911, acc-0.8904, test loss-0.3954, acc-0.8875\n",
      "Iter-72660, train loss-0.3626, acc-0.9000, valid loss-0.3910, acc-0.8904, test loss-0.3954, acc-0.8876\n",
      "Iter-72670, train loss-0.3221, acc-0.9000, valid loss-0.3910, acc-0.8904, test loss-0.3954, acc-0.8875\n",
      "Iter-72680, train loss-0.3965, acc-0.8800, valid loss-0.3910, acc-0.8904, test loss-0.3954, acc-0.8874\n",
      "Iter-72690, train loss-0.5927, acc-0.9000, valid loss-0.3910, acc-0.8906, test loss-0.3954, acc-0.8876\n",
      "Iter-72700, train loss-0.2601, acc-0.9400, valid loss-0.3909, acc-0.8906, test loss-0.3954, acc-0.8876\n",
      "Iter-72710, train loss-0.5229, acc-0.8400, valid loss-0.3909, acc-0.8904, test loss-0.3953, acc-0.8876\n",
      "Iter-72720, train loss-0.3358, acc-0.9200, valid loss-0.3909, acc-0.8904, test loss-0.3953, acc-0.8876\n",
      "Iter-72730, train loss-0.5302, acc-0.7800, valid loss-0.3909, acc-0.8906, test loss-0.3953, acc-0.8876\n",
      "Iter-72740, train loss-0.3206, acc-0.9200, valid loss-0.3908, acc-0.8904, test loss-0.3953, acc-0.8876\n",
      "Iter-72750, train loss-0.6445, acc-0.8000, valid loss-0.3908, acc-0.8906, test loss-0.3953, acc-0.8876\n",
      "Iter-72760, train loss-0.4529, acc-0.8200, valid loss-0.3908, acc-0.8908, test loss-0.3952, acc-0.8876\n",
      "Iter-72770, train loss-0.2589, acc-0.9200, valid loss-0.3908, acc-0.8908, test loss-0.3952, acc-0.8876\n",
      "Iter-72780, train loss-0.3036, acc-0.9200, valid loss-0.3908, acc-0.8908, test loss-0.3952, acc-0.8876\n",
      "Iter-72790, train loss-0.3285, acc-0.9000, valid loss-0.3908, acc-0.8908, test loss-0.3952, acc-0.8876\n",
      "Iter-72800, train loss-0.2069, acc-0.9800, valid loss-0.3907, acc-0.8908, test loss-0.3952, acc-0.8876\n",
      "Iter-72810, train loss-0.4440, acc-0.8400, valid loss-0.3907, acc-0.8908, test loss-0.3951, acc-0.8876\n",
      "Iter-72820, train loss-0.1876, acc-0.9800, valid loss-0.3907, acc-0.8908, test loss-0.3951, acc-0.8876\n",
      "Iter-72830, train loss-0.2748, acc-0.9200, valid loss-0.3907, acc-0.8908, test loss-0.3951, acc-0.8876\n",
      "Iter-72840, train loss-0.4271, acc-0.8200, valid loss-0.3907, acc-0.8908, test loss-0.3951, acc-0.8876\n",
      "Iter-72850, train loss-0.4657, acc-0.8800, valid loss-0.3906, acc-0.8908, test loss-0.3950, acc-0.8876\n",
      "Iter-72860, train loss-0.4218, acc-0.8600, valid loss-0.3906, acc-0.8908, test loss-0.3950, acc-0.8876\n",
      "Iter-72870, train loss-0.4518, acc-0.8600, valid loss-0.3906, acc-0.8908, test loss-0.3950, acc-0.8876\n",
      "Iter-72880, train loss-0.5243, acc-0.8000, valid loss-0.3906, acc-0.8908, test loss-0.3950, acc-0.8876\n",
      "Iter-72890, train loss-0.4122, acc-0.8600, valid loss-0.3906, acc-0.8908, test loss-0.3950, acc-0.8876\n",
      "Iter-72900, train loss-0.3691, acc-0.8600, valid loss-0.3905, acc-0.8908, test loss-0.3950, acc-0.8875\n",
      "Iter-72910, train loss-0.3971, acc-0.8600, valid loss-0.3905, acc-0.8910, test loss-0.3950, acc-0.8876\n",
      "Iter-72920, train loss-0.2606, acc-0.9400, valid loss-0.3905, acc-0.8908, test loss-0.3950, acc-0.8875\n",
      "Iter-72930, train loss-0.3029, acc-0.9000, valid loss-0.3905, acc-0.8908, test loss-0.3949, acc-0.8875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-72940, train loss-0.3191, acc-0.9400, valid loss-0.3905, acc-0.8908, test loss-0.3949, acc-0.8875\n",
      "Iter-72950, train loss-0.7619, acc-0.7600, valid loss-0.3905, acc-0.8908, test loss-0.3949, acc-0.8875\n",
      "Iter-72960, train loss-0.2353, acc-0.9200, valid loss-0.3904, acc-0.8908, test loss-0.3949, acc-0.8875\n",
      "Iter-72970, train loss-0.5644, acc-0.8600, valid loss-0.3904, acc-0.8908, test loss-0.3949, acc-0.8875\n",
      "Iter-72980, train loss-0.4323, acc-0.8400, valid loss-0.3904, acc-0.8908, test loss-0.3948, acc-0.8875\n",
      "Iter-72990, train loss-0.4522, acc-0.9200, valid loss-0.3904, acc-0.8908, test loss-0.3948, acc-0.8875\n",
      "Iter-73000, train loss-0.5501, acc-0.8200, valid loss-0.3903, acc-0.8908, test loss-0.3948, acc-0.8875\n",
      "Iter-73010, train loss-0.2208, acc-0.9600, valid loss-0.3903, acc-0.8908, test loss-0.3948, acc-0.8875\n",
      "Iter-73020, train loss-0.2250, acc-0.9600, valid loss-0.3903, acc-0.8908, test loss-0.3948, acc-0.8875\n",
      "Iter-73030, train loss-0.4324, acc-0.8400, valid loss-0.3903, acc-0.8908, test loss-0.3947, acc-0.8875\n",
      "Iter-73040, train loss-0.3251, acc-0.9000, valid loss-0.3902, acc-0.8908, test loss-0.3947, acc-0.8875\n",
      "Iter-73050, train loss-0.5197, acc-0.8000, valid loss-0.3902, acc-0.8908, test loss-0.3947, acc-0.8876\n",
      "Iter-73060, train loss-0.4571, acc-0.8800, valid loss-0.3902, acc-0.8908, test loss-0.3947, acc-0.8875\n",
      "Iter-73070, train loss-0.6337, acc-0.8400, valid loss-0.3902, acc-0.8908, test loss-0.3947, acc-0.8875\n",
      "Iter-73080, train loss-0.3567, acc-0.9000, valid loss-0.3901, acc-0.8908, test loss-0.3946, acc-0.8875\n",
      "Iter-73090, train loss-0.4602, acc-0.8400, valid loss-0.3901, acc-0.8908, test loss-0.3946, acc-0.8875\n",
      "Iter-73100, train loss-0.5226, acc-0.8600, valid loss-0.3901, acc-0.8910, test loss-0.3946, acc-0.8875\n",
      "Iter-73110, train loss-0.3394, acc-0.9200, valid loss-0.3901, acc-0.8910, test loss-0.3946, acc-0.8875\n",
      "Iter-73120, train loss-0.3309, acc-0.9000, valid loss-0.3901, acc-0.8910, test loss-0.3945, acc-0.8875\n",
      "Iter-73130, train loss-0.2937, acc-0.9000, valid loss-0.3901, acc-0.8910, test loss-0.3945, acc-0.8875\n",
      "Iter-73140, train loss-0.4469, acc-0.8600, valid loss-0.3900, acc-0.8910, test loss-0.3945, acc-0.8874\n",
      "Iter-73150, train loss-0.6183, acc-0.8600, valid loss-0.3900, acc-0.8910, test loss-0.3944, acc-0.8874\n",
      "Iter-73160, train loss-0.4609, acc-0.8800, valid loss-0.3900, acc-0.8910, test loss-0.3944, acc-0.8874\n",
      "Iter-73170, train loss-0.3060, acc-0.8800, valid loss-0.3900, acc-0.8910, test loss-0.3944, acc-0.8874\n",
      "Iter-73180, train loss-0.4652, acc-0.9200, valid loss-0.3899, acc-0.8910, test loss-0.3944, acc-0.8874\n",
      "Iter-73190, train loss-0.5682, acc-0.8400, valid loss-0.3899, acc-0.8910, test loss-0.3944, acc-0.8874\n",
      "Iter-73200, train loss-0.8258, acc-0.8200, valid loss-0.3899, acc-0.8912, test loss-0.3943, acc-0.8874\n",
      "Iter-73210, train loss-0.2247, acc-0.9600, valid loss-0.3899, acc-0.8910, test loss-0.3943, acc-0.8874\n",
      "Iter-73220, train loss-0.4596, acc-0.8600, valid loss-0.3898, acc-0.8912, test loss-0.3943, acc-0.8874\n",
      "Iter-73230, train loss-0.3254, acc-0.8800, valid loss-0.3898, acc-0.8912, test loss-0.3943, acc-0.8874\n",
      "Iter-73240, train loss-0.4132, acc-0.8600, valid loss-0.3898, acc-0.8912, test loss-0.3942, acc-0.8874\n",
      "Iter-73250, train loss-0.2602, acc-0.9200, valid loss-0.3898, acc-0.8912, test loss-0.3942, acc-0.8874\n",
      "Iter-73260, train loss-0.4075, acc-0.9200, valid loss-0.3898, acc-0.8912, test loss-0.3942, acc-0.8874\n",
      "Iter-73270, train loss-0.4560, acc-0.8800, valid loss-0.3898, acc-0.8912, test loss-0.3942, acc-0.8874\n",
      "Iter-73280, train loss-0.5076, acc-0.8400, valid loss-0.3897, acc-0.8912, test loss-0.3942, acc-0.8874\n",
      "Iter-73290, train loss-0.4193, acc-0.8600, valid loss-0.3897, acc-0.8910, test loss-0.3942, acc-0.8874\n",
      "Iter-73300, train loss-0.5378, acc-0.8200, valid loss-0.3897, acc-0.8912, test loss-0.3941, acc-0.8874\n",
      "Iter-73310, train loss-0.3337, acc-0.8800, valid loss-0.3897, acc-0.8912, test loss-0.3941, acc-0.8874\n",
      "Iter-73320, train loss-0.2642, acc-0.9400, valid loss-0.3896, acc-0.8912, test loss-0.3941, acc-0.8874\n",
      "Iter-73330, train loss-0.1935, acc-0.9600, valid loss-0.3896, acc-0.8912, test loss-0.3941, acc-0.8874\n",
      "Iter-73340, train loss-0.2809, acc-0.9600, valid loss-0.3896, acc-0.8912, test loss-0.3940, acc-0.8874\n",
      "Iter-73350, train loss-0.3692, acc-0.8600, valid loss-0.3896, acc-0.8912, test loss-0.3940, acc-0.8874\n",
      "Iter-73360, train loss-0.6176, acc-0.7200, valid loss-0.3896, acc-0.8912, test loss-0.3940, acc-0.8874\n",
      "Iter-73370, train loss-0.4979, acc-0.8600, valid loss-0.3895, acc-0.8912, test loss-0.3940, acc-0.8874\n",
      "Iter-73380, train loss-0.3546, acc-0.9400, valid loss-0.3895, acc-0.8912, test loss-0.3940, acc-0.8874\n",
      "Iter-73390, train loss-0.3570, acc-0.9000, valid loss-0.3895, acc-0.8912, test loss-0.3939, acc-0.8875\n",
      "Iter-73400, train loss-0.3601, acc-0.8800, valid loss-0.3895, acc-0.8912, test loss-0.3939, acc-0.8874\n",
      "Iter-73410, train loss-0.5068, acc-0.9000, valid loss-0.3894, acc-0.8912, test loss-0.3939, acc-0.8875\n",
      "Iter-73420, train loss-0.8115, acc-0.7400, valid loss-0.3894, acc-0.8912, test loss-0.3939, acc-0.8875\n",
      "Iter-73430, train loss-0.3753, acc-0.9000, valid loss-0.3894, acc-0.8912, test loss-0.3939, acc-0.8875\n",
      "Iter-73440, train loss-0.4606, acc-0.9200, valid loss-0.3894, acc-0.8912, test loss-0.3938, acc-0.8875\n",
      "Iter-73450, train loss-0.5343, acc-0.8600, valid loss-0.3893, acc-0.8912, test loss-0.3938, acc-0.8875\n",
      "Iter-73460, train loss-0.3271, acc-0.9200, valid loss-0.3893, acc-0.8912, test loss-0.3938, acc-0.8875\n",
      "Iter-73470, train loss-0.3502, acc-0.8800, valid loss-0.3893, acc-0.8912, test loss-0.3938, acc-0.8874\n",
      "Iter-73480, train loss-0.2869, acc-0.9000, valid loss-0.3893, acc-0.8912, test loss-0.3938, acc-0.8874\n",
      "Iter-73490, train loss-0.4860, acc-0.9000, valid loss-0.3893, acc-0.8914, test loss-0.3937, acc-0.8874\n",
      "Iter-73500, train loss-0.3894, acc-0.8800, valid loss-0.3892, acc-0.8916, test loss-0.3937, acc-0.8874\n",
      "Iter-73510, train loss-0.4220, acc-0.8400, valid loss-0.3892, acc-0.8914, test loss-0.3937, acc-0.8876\n",
      "Iter-73520, train loss-0.3533, acc-0.8400, valid loss-0.3892, acc-0.8914, test loss-0.3937, acc-0.8876\n",
      "Iter-73530, train loss-0.3589, acc-0.9200, valid loss-0.3891, acc-0.8912, test loss-0.3936, acc-0.8876\n",
      "Iter-73540, train loss-0.3862, acc-0.8800, valid loss-0.3891, acc-0.8912, test loss-0.3936, acc-0.8876\n",
      "Iter-73550, train loss-0.5948, acc-0.8600, valid loss-0.3891, acc-0.8912, test loss-0.3936, acc-0.8876\n",
      "Iter-73560, train loss-0.4534, acc-0.8800, valid loss-0.3891, acc-0.8912, test loss-0.3935, acc-0.8875\n",
      "Iter-73570, train loss-0.4433, acc-0.9000, valid loss-0.3890, acc-0.8912, test loss-0.3935, acc-0.8876\n",
      "Iter-73580, train loss-0.4970, acc-0.8000, valid loss-0.3890, acc-0.8912, test loss-0.3935, acc-0.8875\n",
      "Iter-73590, train loss-0.2314, acc-0.9400, valid loss-0.3890, acc-0.8912, test loss-0.3935, acc-0.8875\n",
      "Iter-73600, train loss-0.1765, acc-0.9600, valid loss-0.3890, acc-0.8912, test loss-0.3935, acc-0.8876\n",
      "Iter-73610, train loss-0.4357, acc-0.8800, valid loss-0.3890, acc-0.8912, test loss-0.3935, acc-0.8875\n",
      "Iter-73620, train loss-0.3731, acc-0.9000, valid loss-0.3890, acc-0.8912, test loss-0.3934, acc-0.8875\n",
      "Iter-73630, train loss-0.2991, acc-0.9200, valid loss-0.3889, acc-0.8914, test loss-0.3934, acc-0.8876\n",
      "Iter-73640, train loss-0.3772, acc-0.8800, valid loss-0.3889, acc-0.8914, test loss-0.3934, acc-0.8876\n",
      "Iter-73650, train loss-0.3906, acc-0.9000, valid loss-0.3889, acc-0.8914, test loss-0.3934, acc-0.8876\n",
      "Iter-73660, train loss-0.5749, acc-0.8200, valid loss-0.3889, acc-0.8914, test loss-0.3934, acc-0.8875\n",
      "Iter-73670, train loss-0.5430, acc-0.8800, valid loss-0.3889, acc-0.8914, test loss-0.3933, acc-0.8876\n",
      "Iter-73680, train loss-0.3719, acc-0.9000, valid loss-0.3889, acc-0.8914, test loss-0.3933, acc-0.8875\n",
      "Iter-73690, train loss-0.2885, acc-0.9800, valid loss-0.3888, acc-0.8914, test loss-0.3933, acc-0.8875\n",
      "Iter-73700, train loss-0.6181, acc-0.8000, valid loss-0.3888, acc-0.8914, test loss-0.3933, acc-0.8875\n",
      "Iter-73710, train loss-0.4033, acc-0.8600, valid loss-0.3888, acc-0.8914, test loss-0.3933, acc-0.8874\n",
      "Iter-73720, train loss-0.4258, acc-0.8400, valid loss-0.3888, acc-0.8916, test loss-0.3933, acc-0.8874\n",
      "Iter-73730, train loss-0.2107, acc-0.9400, valid loss-0.3888, acc-0.8914, test loss-0.3932, acc-0.8874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-73740, train loss-0.4470, acc-0.8800, valid loss-0.3887, acc-0.8914, test loss-0.3932, acc-0.8874\n",
      "Iter-73750, train loss-0.3500, acc-0.9200, valid loss-0.3887, acc-0.8914, test loss-0.3932, acc-0.8875\n",
      "Iter-73760, train loss-0.4320, acc-0.9000, valid loss-0.3887, acc-0.8914, test loss-0.3932, acc-0.8875\n",
      "Iter-73770, train loss-0.3394, acc-0.9000, valid loss-0.3887, acc-0.8914, test loss-0.3932, acc-0.8875\n",
      "Iter-73780, train loss-0.2997, acc-0.8800, valid loss-0.3887, acc-0.8918, test loss-0.3931, acc-0.8876\n",
      "Iter-73790, train loss-0.5935, acc-0.8200, valid loss-0.3887, acc-0.8918, test loss-0.3931, acc-0.8875\n",
      "Iter-73800, train loss-0.4485, acc-0.8400, valid loss-0.3887, acc-0.8920, test loss-0.3931, acc-0.8875\n",
      "Iter-73810, train loss-0.3861, acc-0.9000, valid loss-0.3886, acc-0.8920, test loss-0.3931, acc-0.8875\n",
      "Iter-73820, train loss-0.5763, acc-0.8200, valid loss-0.3886, acc-0.8920, test loss-0.3931, acc-0.8877\n",
      "Iter-73830, train loss-0.5291, acc-0.8400, valid loss-0.3886, acc-0.8918, test loss-0.3930, acc-0.8877\n",
      "Iter-73840, train loss-0.2708, acc-0.9600, valid loss-0.3886, acc-0.8918, test loss-0.3930, acc-0.8877\n",
      "Iter-73850, train loss-0.4421, acc-0.9000, valid loss-0.3886, acc-0.8916, test loss-0.3930, acc-0.8876\n",
      "Iter-73860, train loss-0.3450, acc-0.9200, valid loss-0.3885, acc-0.8920, test loss-0.3930, acc-0.8875\n",
      "Iter-73870, train loss-0.3512, acc-0.9200, valid loss-0.3885, acc-0.8920, test loss-0.3930, acc-0.8876\n",
      "Iter-73880, train loss-0.4685, acc-0.8800, valid loss-0.3885, acc-0.8920, test loss-0.3930, acc-0.8877\n",
      "Iter-73890, train loss-0.1440, acc-0.9800, valid loss-0.3885, acc-0.8918, test loss-0.3929, acc-0.8877\n",
      "Iter-73900, train loss-0.2483, acc-0.9200, valid loss-0.3884, acc-0.8918, test loss-0.3929, acc-0.8876\n",
      "Iter-73910, train loss-0.4766, acc-0.8400, valid loss-0.3884, acc-0.8920, test loss-0.3929, acc-0.8877\n",
      "Iter-73920, train loss-0.6110, acc-0.8400, valid loss-0.3884, acc-0.8918, test loss-0.3929, acc-0.8878\n",
      "Iter-73930, train loss-0.3150, acc-0.9000, valid loss-0.3884, acc-0.8918, test loss-0.3929, acc-0.8878\n",
      "Iter-73940, train loss-0.3443, acc-0.8800, valid loss-0.3884, acc-0.8918, test loss-0.3928, acc-0.8878\n",
      "Iter-73950, train loss-0.4435, acc-0.8800, valid loss-0.3883, acc-0.8920, test loss-0.3928, acc-0.8878\n",
      "Iter-73960, train loss-0.3319, acc-0.9000, valid loss-0.3883, acc-0.8920, test loss-0.3928, acc-0.8878\n",
      "Iter-73970, train loss-0.4769, acc-0.8800, valid loss-0.3883, acc-0.8920, test loss-0.3928, acc-0.8878\n",
      "Iter-73980, train loss-0.5516, acc-0.8600, valid loss-0.3883, acc-0.8920, test loss-0.3928, acc-0.8878\n",
      "Iter-73990, train loss-0.4879, acc-0.8800, valid loss-0.3882, acc-0.8920, test loss-0.3927, acc-0.8877\n",
      "Iter-74000, train loss-0.5352, acc-0.8600, valid loss-0.3882, acc-0.8920, test loss-0.3927, acc-0.8877\n",
      "Iter-74010, train loss-0.3736, acc-0.9000, valid loss-0.3882, acc-0.8920, test loss-0.3927, acc-0.8878\n",
      "Iter-74020, train loss-0.5109, acc-0.8600, valid loss-0.3882, acc-0.8920, test loss-0.3927, acc-0.8878\n",
      "Iter-74030, train loss-0.6327, acc-0.8000, valid loss-0.3882, acc-0.8920, test loss-0.3927, acc-0.8878\n",
      "Iter-74040, train loss-0.6244, acc-0.8200, valid loss-0.3881, acc-0.8920, test loss-0.3927, acc-0.8877\n",
      "Iter-74050, train loss-0.3627, acc-0.9400, valid loss-0.3881, acc-0.8920, test loss-0.3926, acc-0.8877\n",
      "Iter-74060, train loss-0.3881, acc-0.9200, valid loss-0.3881, acc-0.8920, test loss-0.3926, acc-0.8877\n",
      "Iter-74070, train loss-0.3445, acc-0.8600, valid loss-0.3881, acc-0.8920, test loss-0.3926, acc-0.8877\n",
      "Iter-74080, train loss-0.4059, acc-0.8600, valid loss-0.3881, acc-0.8920, test loss-0.3926, acc-0.8877\n",
      "Iter-74090, train loss-0.3803, acc-0.9000, valid loss-0.3880, acc-0.8920, test loss-0.3926, acc-0.8877\n",
      "Iter-74100, train loss-0.3157, acc-0.9400, valid loss-0.3880, acc-0.8920, test loss-0.3925, acc-0.8877\n",
      "Iter-74110, train loss-0.5269, acc-0.8400, valid loss-0.3880, acc-0.8920, test loss-0.3925, acc-0.8877\n",
      "Iter-74120, train loss-0.3764, acc-0.9400, valid loss-0.3880, acc-0.8920, test loss-0.3925, acc-0.8877\n",
      "Iter-74130, train loss-0.2923, acc-0.9200, valid loss-0.3880, acc-0.8920, test loss-0.3925, acc-0.8877\n",
      "Iter-74140, train loss-0.6963, acc-0.7800, valid loss-0.3879, acc-0.8920, test loss-0.3925, acc-0.8877\n",
      "Iter-74150, train loss-0.4682, acc-0.8800, valid loss-0.3879, acc-0.8920, test loss-0.3924, acc-0.8877\n",
      "Iter-74160, train loss-0.2841, acc-0.9400, valid loss-0.3879, acc-0.8920, test loss-0.3924, acc-0.8876\n",
      "Iter-74170, train loss-0.2503, acc-0.9400, valid loss-0.3879, acc-0.8920, test loss-0.3924, acc-0.8877\n",
      "Iter-74180, train loss-0.4837, acc-0.9000, valid loss-0.3879, acc-0.8920, test loss-0.3924, acc-0.8877\n",
      "Iter-74190, train loss-0.4635, acc-0.9000, valid loss-0.3878, acc-0.8920, test loss-0.3924, acc-0.8877\n",
      "Iter-74200, train loss-0.4907, acc-0.8600, valid loss-0.3878, acc-0.8920, test loss-0.3923, acc-0.8877\n",
      "Iter-74210, train loss-0.3554, acc-0.8800, valid loss-0.3878, acc-0.8920, test loss-0.3923, acc-0.8877\n",
      "Iter-74220, train loss-0.4931, acc-0.8600, valid loss-0.3878, acc-0.8920, test loss-0.3923, acc-0.8877\n",
      "Iter-74230, train loss-0.6477, acc-0.7800, valid loss-0.3878, acc-0.8920, test loss-0.3923, acc-0.8877\n",
      "Iter-74240, train loss-0.6808, acc-0.7800, valid loss-0.3877, acc-0.8920, test loss-0.3923, acc-0.8878\n",
      "Iter-74250, train loss-0.2999, acc-0.9400, valid loss-0.3877, acc-0.8920, test loss-0.3923, acc-0.8878\n",
      "Iter-74260, train loss-0.5541, acc-0.8400, valid loss-0.3877, acc-0.8920, test loss-0.3922, acc-0.8878\n",
      "Iter-74270, train loss-0.4466, acc-0.9000, valid loss-0.3877, acc-0.8922, test loss-0.3922, acc-0.8878\n",
      "Iter-74280, train loss-0.4247, acc-0.8800, valid loss-0.3877, acc-0.8922, test loss-0.3922, acc-0.8878\n",
      "Iter-74290, train loss-0.3820, acc-0.9200, valid loss-0.3876, acc-0.8920, test loss-0.3922, acc-0.8877\n",
      "Iter-74300, train loss-0.4952, acc-0.8000, valid loss-0.3876, acc-0.8920, test loss-0.3922, acc-0.8877\n",
      "Iter-74310, train loss-0.3467, acc-0.8800, valid loss-0.3876, acc-0.8922, test loss-0.3921, acc-0.8878\n",
      "Iter-74320, train loss-0.4412, acc-0.8600, valid loss-0.3876, acc-0.8922, test loss-0.3921, acc-0.8879\n",
      "Iter-74330, train loss-0.3034, acc-0.9400, valid loss-0.3876, acc-0.8922, test loss-0.3921, acc-0.8877\n",
      "Iter-74340, train loss-0.4340, acc-0.8800, valid loss-0.3876, acc-0.8920, test loss-0.3921, acc-0.8877\n",
      "Iter-74350, train loss-0.3756, acc-0.9200, valid loss-0.3875, acc-0.8920, test loss-0.3921, acc-0.8877\n",
      "Iter-74360, train loss-0.5671, acc-0.7800, valid loss-0.3875, acc-0.8920, test loss-0.3921, acc-0.8877\n",
      "Iter-74370, train loss-0.5533, acc-0.8400, valid loss-0.3875, acc-0.8920, test loss-0.3920, acc-0.8879\n",
      "Iter-74380, train loss-0.2639, acc-0.9600, valid loss-0.3875, acc-0.8920, test loss-0.3920, acc-0.8878\n",
      "Iter-74390, train loss-0.5186, acc-0.8200, valid loss-0.3875, acc-0.8920, test loss-0.3920, acc-0.8879\n",
      "Iter-74400, train loss-0.1850, acc-0.9600, valid loss-0.3875, acc-0.8920, test loss-0.3920, acc-0.8880\n",
      "Iter-74410, train loss-0.5652, acc-0.8200, valid loss-0.3874, acc-0.8922, test loss-0.3920, acc-0.8879\n",
      "Iter-74420, train loss-0.2874, acc-0.9200, valid loss-0.3874, acc-0.8922, test loss-0.3919, acc-0.8879\n",
      "Iter-74430, train loss-0.3468, acc-0.9000, valid loss-0.3874, acc-0.8920, test loss-0.3919, acc-0.8879\n",
      "Iter-74440, train loss-0.3223, acc-0.9200, valid loss-0.3874, acc-0.8918, test loss-0.3919, acc-0.8879\n",
      "Iter-74450, train loss-0.3624, acc-0.9000, valid loss-0.3874, acc-0.8918, test loss-0.3919, acc-0.8879\n",
      "Iter-74460, train loss-0.4157, acc-0.9200, valid loss-0.3874, acc-0.8918, test loss-0.3919, acc-0.8879\n",
      "Iter-74470, train loss-0.5240, acc-0.8000, valid loss-0.3873, acc-0.8918, test loss-0.3918, acc-0.8879\n",
      "Iter-74480, train loss-0.3698, acc-0.9000, valid loss-0.3873, acc-0.8918, test loss-0.3918, acc-0.8879\n",
      "Iter-74490, train loss-0.2748, acc-0.9000, valid loss-0.3873, acc-0.8918, test loss-0.3918, acc-0.8879\n",
      "Iter-74500, train loss-0.3026, acc-0.9400, valid loss-0.3873, acc-0.8918, test loss-0.3918, acc-0.8879\n",
      "Iter-74510, train loss-0.2468, acc-0.9600, valid loss-0.3873, acc-0.8918, test loss-0.3918, acc-0.8879\n",
      "Iter-74520, train loss-0.4736, acc-0.8400, valid loss-0.3872, acc-0.8918, test loss-0.3918, acc-0.8879\n",
      "Iter-74530, train loss-0.5082, acc-0.8400, valid loss-0.3872, acc-0.8918, test loss-0.3917, acc-0.8879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-74540, train loss-0.3804, acc-0.8800, valid loss-0.3872, acc-0.8918, test loss-0.3917, acc-0.8879\n",
      "Iter-74550, train loss-0.3725, acc-0.9000, valid loss-0.3872, acc-0.8920, test loss-0.3917, acc-0.8879\n",
      "Iter-74560, train loss-0.2884, acc-0.9200, valid loss-0.3872, acc-0.8920, test loss-0.3917, acc-0.8879\n",
      "Iter-74570, train loss-0.4500, acc-0.8200, valid loss-0.3871, acc-0.8920, test loss-0.3917, acc-0.8879\n",
      "Iter-74580, train loss-0.2604, acc-0.9400, valid loss-0.3871, acc-0.8920, test loss-0.3916, acc-0.8880\n",
      "Iter-74590, train loss-0.2751, acc-0.9200, valid loss-0.3871, acc-0.8920, test loss-0.3916, acc-0.8879\n",
      "Iter-74600, train loss-0.8817, acc-0.7000, valid loss-0.3871, acc-0.8920, test loss-0.3916, acc-0.8880\n",
      "Iter-74610, train loss-0.5314, acc-0.8400, valid loss-0.3871, acc-0.8920, test loss-0.3916, acc-0.8879\n",
      "Iter-74620, train loss-0.4507, acc-0.8200, valid loss-0.3870, acc-0.8920, test loss-0.3916, acc-0.8879\n",
      "Iter-74630, train loss-0.1732, acc-0.9800, valid loss-0.3870, acc-0.8920, test loss-0.3916, acc-0.8879\n",
      "Iter-74640, train loss-0.4987, acc-0.8400, valid loss-0.3870, acc-0.8918, test loss-0.3915, acc-0.8879\n",
      "Iter-74650, train loss-0.3392, acc-0.9000, valid loss-0.3870, acc-0.8918, test loss-0.3915, acc-0.8879\n",
      "Iter-74660, train loss-0.3193, acc-0.8800, valid loss-0.3870, acc-0.8918, test loss-0.3915, acc-0.8880\n",
      "Iter-74670, train loss-0.3067, acc-0.9200, valid loss-0.3869, acc-0.8918, test loss-0.3915, acc-0.8881\n",
      "Iter-74680, train loss-0.4514, acc-0.8400, valid loss-0.3869, acc-0.8920, test loss-0.3915, acc-0.8879\n",
      "Iter-74690, train loss-0.3859, acc-0.8600, valid loss-0.3869, acc-0.8918, test loss-0.3914, acc-0.8879\n",
      "Iter-74700, train loss-0.3607, acc-0.8600, valid loss-0.3869, acc-0.8918, test loss-0.3914, acc-0.8879\n",
      "Iter-74710, train loss-0.5463, acc-0.8600, valid loss-0.3869, acc-0.8918, test loss-0.3914, acc-0.8879\n",
      "Iter-74720, train loss-0.5298, acc-0.8400, valid loss-0.3869, acc-0.8920, test loss-0.3914, acc-0.8879\n",
      "Iter-74730, train loss-0.3077, acc-0.9400, valid loss-0.3868, acc-0.8920, test loss-0.3914, acc-0.8879\n",
      "Iter-74740, train loss-0.3447, acc-0.8800, valid loss-0.3868, acc-0.8920, test loss-0.3914, acc-0.8879\n",
      "Iter-74750, train loss-0.4373, acc-0.8800, valid loss-0.3868, acc-0.8918, test loss-0.3913, acc-0.8880\n",
      "Iter-74760, train loss-0.4854, acc-0.8600, valid loss-0.3868, acc-0.8918, test loss-0.3913, acc-0.8880\n",
      "Iter-74770, train loss-0.3377, acc-0.9000, valid loss-0.3868, acc-0.8918, test loss-0.3913, acc-0.8879\n",
      "Iter-74780, train loss-0.3707, acc-0.8800, valid loss-0.3867, acc-0.8918, test loss-0.3913, acc-0.8879\n",
      "Iter-74790, train loss-0.3876, acc-0.8800, valid loss-0.3867, acc-0.8918, test loss-0.3913, acc-0.8880\n",
      "Iter-74800, train loss-0.3621, acc-0.9200, valid loss-0.3867, acc-0.8918, test loss-0.3912, acc-0.8879\n",
      "Iter-74810, train loss-0.5031, acc-0.8400, valid loss-0.3867, acc-0.8918, test loss-0.3912, acc-0.8879\n",
      "Iter-74820, train loss-0.3429, acc-0.8800, valid loss-0.3867, acc-0.8918, test loss-0.3912, acc-0.8879\n",
      "Iter-74830, train loss-0.5181, acc-0.8400, valid loss-0.3867, acc-0.8918, test loss-0.3912, acc-0.8880\n",
      "Iter-74840, train loss-0.1606, acc-0.9800, valid loss-0.3866, acc-0.8918, test loss-0.3912, acc-0.8881\n",
      "Iter-74850, train loss-0.3825, acc-0.9200, valid loss-0.3866, acc-0.8918, test loss-0.3911, acc-0.8881\n",
      "Iter-74860, train loss-0.5683, acc-0.8400, valid loss-0.3866, acc-0.8918, test loss-0.3911, acc-0.8880\n",
      "Iter-74870, train loss-0.3901, acc-0.9000, valid loss-0.3866, acc-0.8920, test loss-0.3911, acc-0.8881\n",
      "Iter-74880, train loss-0.3943, acc-0.9000, valid loss-0.3866, acc-0.8920, test loss-0.3911, acc-0.8880\n",
      "Iter-74890, train loss-0.2111, acc-0.9400, valid loss-0.3865, acc-0.8920, test loss-0.3911, acc-0.8881\n",
      "Iter-74900, train loss-0.6284, acc-0.8200, valid loss-0.3865, acc-0.8918, test loss-0.3911, acc-0.8881\n",
      "Iter-74910, train loss-0.4221, acc-0.8800, valid loss-0.3865, acc-0.8918, test loss-0.3910, acc-0.8881\n",
      "Iter-74920, train loss-0.3819, acc-0.9000, valid loss-0.3865, acc-0.8918, test loss-0.3910, acc-0.8881\n",
      "Iter-74930, train loss-0.3556, acc-0.9000, valid loss-0.3865, acc-0.8918, test loss-0.3910, acc-0.8880\n",
      "Iter-74940, train loss-0.5359, acc-0.8400, valid loss-0.3865, acc-0.8918, test loss-0.3910, acc-0.8879\n",
      "Iter-74950, train loss-0.3133, acc-0.9000, valid loss-0.3864, acc-0.8918, test loss-0.3910, acc-0.8881\n",
      "Iter-74960, train loss-0.3213, acc-0.8800, valid loss-0.3864, acc-0.8918, test loss-0.3909, acc-0.8880\n",
      "Iter-74970, train loss-0.3638, acc-0.9000, valid loss-0.3864, acc-0.8918, test loss-0.3909, acc-0.8882\n",
      "Iter-74980, train loss-0.2768, acc-0.9200, valid loss-0.3864, acc-0.8918, test loss-0.3909, acc-0.8880\n",
      "Iter-74990, train loss-0.5725, acc-0.8200, valid loss-0.3863, acc-0.8918, test loss-0.3909, acc-0.8881\n",
      "Iter-75000, train loss-0.4505, acc-0.8800, valid loss-0.3863, acc-0.8916, test loss-0.3909, acc-0.8881\n",
      "Iter-75010, train loss-0.4125, acc-0.8400, valid loss-0.3863, acc-0.8916, test loss-0.3909, acc-0.8879\n",
      "Iter-75020, train loss-0.1536, acc-0.9600, valid loss-0.3863, acc-0.8918, test loss-0.3908, acc-0.8880\n",
      "Iter-75030, train loss-0.4788, acc-0.8400, valid loss-0.3862, acc-0.8918, test loss-0.3908, acc-0.8880\n",
      "Iter-75040, train loss-0.5714, acc-0.8600, valid loss-0.3862, acc-0.8918, test loss-0.3908, acc-0.8880\n",
      "Iter-75050, train loss-0.3921, acc-0.8600, valid loss-0.3862, acc-0.8918, test loss-0.3908, acc-0.8880\n",
      "Iter-75060, train loss-0.3176, acc-0.9200, valid loss-0.3862, acc-0.8918, test loss-0.3907, acc-0.8880\n",
      "Iter-75070, train loss-0.5426, acc-0.8400, valid loss-0.3862, acc-0.8918, test loss-0.3907, acc-0.8881\n",
      "Iter-75080, train loss-0.4356, acc-0.8800, valid loss-0.3862, acc-0.8918, test loss-0.3907, acc-0.8881\n",
      "Iter-75090, train loss-0.3477, acc-0.9000, valid loss-0.3861, acc-0.8918, test loss-0.3907, acc-0.8881\n",
      "Iter-75100, train loss-0.3764, acc-0.8600, valid loss-0.3861, acc-0.8918, test loss-0.3907, acc-0.8881\n",
      "Iter-75110, train loss-0.4138, acc-0.8800, valid loss-0.3861, acc-0.8918, test loss-0.3907, acc-0.8880\n",
      "Iter-75120, train loss-0.3419, acc-0.8800, valid loss-0.3861, acc-0.8918, test loss-0.3906, acc-0.8880\n",
      "Iter-75130, train loss-0.3023, acc-0.9200, valid loss-0.3861, acc-0.8918, test loss-0.3906, acc-0.8880\n",
      "Iter-75140, train loss-0.3997, acc-0.8800, valid loss-0.3860, acc-0.8920, test loss-0.3906, acc-0.8880\n",
      "Iter-75150, train loss-0.3625, acc-0.8800, valid loss-0.3860, acc-0.8920, test loss-0.3906, acc-0.8880\n",
      "Iter-75160, train loss-0.3651, acc-0.9200, valid loss-0.3860, acc-0.8916, test loss-0.3906, acc-0.8880\n",
      "Iter-75170, train loss-0.4391, acc-0.9200, valid loss-0.3860, acc-0.8916, test loss-0.3905, acc-0.8881\n",
      "Iter-75180, train loss-0.3875, acc-0.9400, valid loss-0.3860, acc-0.8916, test loss-0.3905, acc-0.8882\n",
      "Iter-75190, train loss-0.3807, acc-0.9400, valid loss-0.3860, acc-0.8916, test loss-0.3905, acc-0.8882\n",
      "Iter-75200, train loss-0.5815, acc-0.8600, valid loss-0.3860, acc-0.8916, test loss-0.3905, acc-0.8882\n",
      "Iter-75210, train loss-0.3703, acc-0.9000, valid loss-0.3859, acc-0.8916, test loss-0.3905, acc-0.8881\n",
      "Iter-75220, train loss-0.6864, acc-0.7800, valid loss-0.3859, acc-0.8916, test loss-0.3905, acc-0.8881\n",
      "Iter-75230, train loss-0.5540, acc-0.8600, valid loss-0.3859, acc-0.8916, test loss-0.3905, acc-0.8881\n",
      "Iter-75240, train loss-0.4444, acc-0.9000, valid loss-0.3859, acc-0.8916, test loss-0.3904, acc-0.8882\n",
      "Iter-75250, train loss-0.4079, acc-0.8800, valid loss-0.3859, acc-0.8916, test loss-0.3904, acc-0.8881\n",
      "Iter-75260, train loss-0.4155, acc-0.9000, valid loss-0.3858, acc-0.8916, test loss-0.3904, acc-0.8881\n",
      "Iter-75270, train loss-0.4880, acc-0.9400, valid loss-0.3858, acc-0.8916, test loss-0.3904, acc-0.8880\n",
      "Iter-75280, train loss-0.5502, acc-0.8000, valid loss-0.3858, acc-0.8918, test loss-0.3904, acc-0.8880\n",
      "Iter-75290, train loss-0.3583, acc-0.9400, valid loss-0.3858, acc-0.8918, test loss-0.3903, acc-0.8881\n",
      "Iter-75300, train loss-0.4763, acc-0.9000, valid loss-0.3857, acc-0.8918, test loss-0.3903, acc-0.8881\n",
      "Iter-75310, train loss-0.7363, acc-0.8000, valid loss-0.3857, acc-0.8918, test loss-0.3903, acc-0.8881\n",
      "Iter-75320, train loss-0.3737, acc-0.8800, valid loss-0.3857, acc-0.8918, test loss-0.3903, acc-0.8879\n",
      "Iter-75330, train loss-0.5701, acc-0.8200, valid loss-0.3857, acc-0.8918, test loss-0.3903, acc-0.8881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-75340, train loss-0.4375, acc-0.8800, valid loss-0.3857, acc-0.8918, test loss-0.3903, acc-0.8881\n",
      "Iter-75350, train loss-0.3041, acc-0.9000, valid loss-0.3857, acc-0.8918, test loss-0.3902, acc-0.8881\n",
      "Iter-75360, train loss-0.2589, acc-0.9200, valid loss-0.3856, acc-0.8918, test loss-0.3902, acc-0.8881\n",
      "Iter-75370, train loss-0.4956, acc-0.9000, valid loss-0.3856, acc-0.8918, test loss-0.3902, acc-0.8881\n",
      "Iter-75380, train loss-0.2491, acc-0.9600, valid loss-0.3856, acc-0.8918, test loss-0.3902, acc-0.8882\n",
      "Iter-75390, train loss-0.3660, acc-0.9200, valid loss-0.3856, acc-0.8918, test loss-0.3902, acc-0.8882\n",
      "Iter-75400, train loss-0.2273, acc-0.9200, valid loss-0.3855, acc-0.8918, test loss-0.3902, acc-0.8882\n",
      "Iter-75410, train loss-0.4227, acc-0.8800, valid loss-0.3855, acc-0.8918, test loss-0.3901, acc-0.8882\n",
      "Iter-75420, train loss-0.5601, acc-0.9000, valid loss-0.3855, acc-0.8916, test loss-0.3901, acc-0.8882\n",
      "Iter-75430, train loss-0.2608, acc-0.9400, valid loss-0.3855, acc-0.8916, test loss-0.3901, acc-0.8882\n",
      "Iter-75440, train loss-0.3319, acc-0.8600, valid loss-0.3855, acc-0.8916, test loss-0.3901, acc-0.8883\n",
      "Iter-75450, train loss-0.4572, acc-0.9000, valid loss-0.3854, acc-0.8918, test loss-0.3901, acc-0.8883\n",
      "Iter-75460, train loss-0.3240, acc-0.9000, valid loss-0.3854, acc-0.8916, test loss-0.3900, acc-0.8884\n",
      "Iter-75470, train loss-0.2665, acc-0.9200, valid loss-0.3854, acc-0.8918, test loss-0.3900, acc-0.8883\n",
      "Iter-75480, train loss-0.5715, acc-0.8400, valid loss-0.3854, acc-0.8918, test loss-0.3900, acc-0.8883\n",
      "Iter-75490, train loss-0.3714, acc-0.8800, valid loss-0.3854, acc-0.8918, test loss-0.3900, acc-0.8882\n",
      "Iter-75500, train loss-0.4307, acc-0.8800, valid loss-0.3854, acc-0.8918, test loss-0.3900, acc-0.8883\n",
      "Iter-75510, train loss-0.3720, acc-0.8800, valid loss-0.3854, acc-0.8918, test loss-0.3900, acc-0.8883\n",
      "Iter-75520, train loss-0.3874, acc-0.9000, valid loss-0.3853, acc-0.8918, test loss-0.3899, acc-0.8884\n",
      "Iter-75530, train loss-0.3385, acc-0.8800, valid loss-0.3853, acc-0.8918, test loss-0.3899, acc-0.8884\n",
      "Iter-75540, train loss-0.4413, acc-0.8200, valid loss-0.3853, acc-0.8916, test loss-0.3899, acc-0.8884\n",
      "Iter-75550, train loss-0.3594, acc-0.8800, valid loss-0.3853, acc-0.8918, test loss-0.3899, acc-0.8884\n",
      "Iter-75560, train loss-0.5776, acc-0.8600, valid loss-0.3852, acc-0.8918, test loss-0.3899, acc-0.8884\n",
      "Iter-75570, train loss-0.3352, acc-0.9600, valid loss-0.3852, acc-0.8918, test loss-0.3898, acc-0.8884\n",
      "Iter-75580, train loss-0.3263, acc-0.8800, valid loss-0.3852, acc-0.8916, test loss-0.3898, acc-0.8884\n",
      "Iter-75590, train loss-0.3296, acc-0.9200, valid loss-0.3852, acc-0.8918, test loss-0.3898, acc-0.8883\n",
      "Iter-75600, train loss-0.3684, acc-0.9000, valid loss-0.3852, acc-0.8918, test loss-0.3898, acc-0.8883\n",
      "Iter-75610, train loss-0.4484, acc-0.9000, valid loss-0.3852, acc-0.8916, test loss-0.3897, acc-0.8886\n",
      "Iter-75620, train loss-0.4092, acc-0.8800, valid loss-0.3851, acc-0.8916, test loss-0.3897, acc-0.8885\n",
      "Iter-75630, train loss-0.2833, acc-0.9400, valid loss-0.3851, acc-0.8914, test loss-0.3897, acc-0.8886\n",
      "Iter-75640, train loss-0.3845, acc-0.9200, valid loss-0.3851, acc-0.8916, test loss-0.3897, acc-0.8885\n",
      "Iter-75650, train loss-0.6545, acc-0.8600, valid loss-0.3851, acc-0.8914, test loss-0.3896, acc-0.8885\n",
      "Iter-75660, train loss-0.3298, acc-0.9000, valid loss-0.3851, acc-0.8916, test loss-0.3896, acc-0.8885\n",
      "Iter-75670, train loss-0.3665, acc-0.9000, valid loss-0.3850, acc-0.8918, test loss-0.3896, acc-0.8889\n",
      "Iter-75680, train loss-0.2769, acc-0.9400, valid loss-0.3850, acc-0.8918, test loss-0.3896, acc-0.8888\n",
      "Iter-75690, train loss-0.4354, acc-0.8400, valid loss-0.3850, acc-0.8918, test loss-0.3896, acc-0.8888\n",
      "Iter-75700, train loss-0.5973, acc-0.8600, valid loss-0.3850, acc-0.8918, test loss-0.3895, acc-0.8888\n",
      "Iter-75710, train loss-0.4420, acc-0.8800, valid loss-0.3850, acc-0.8920, test loss-0.3895, acc-0.8888\n",
      "Iter-75720, train loss-0.4245, acc-0.8800, valid loss-0.3849, acc-0.8920, test loss-0.3895, acc-0.8888\n",
      "Iter-75730, train loss-0.3636, acc-0.8800, valid loss-0.3849, acc-0.8920, test loss-0.3895, acc-0.8888\n",
      "Iter-75740, train loss-0.6075, acc-0.8600, valid loss-0.3849, acc-0.8920, test loss-0.3895, acc-0.8888\n",
      "Iter-75750, train loss-0.3448, acc-0.9000, valid loss-0.3849, acc-0.8920, test loss-0.3895, acc-0.8887\n",
      "Iter-75760, train loss-0.3125, acc-0.9600, valid loss-0.3849, acc-0.8920, test loss-0.3894, acc-0.8886\n",
      "Iter-75770, train loss-0.4981, acc-0.8400, valid loss-0.3848, acc-0.8920, test loss-0.3894, acc-0.8886\n",
      "Iter-75780, train loss-0.4335, acc-0.8600, valid loss-0.3848, acc-0.8920, test loss-0.3894, acc-0.8887\n",
      "Iter-75790, train loss-0.6143, acc-0.7800, valid loss-0.3848, acc-0.8922, test loss-0.3894, acc-0.8887\n",
      "Iter-75800, train loss-0.2917, acc-0.9200, valid loss-0.3848, acc-0.8920, test loss-0.3894, acc-0.8887\n",
      "Iter-75810, train loss-0.5452, acc-0.8400, valid loss-0.3848, acc-0.8920, test loss-0.3893, acc-0.8888\n",
      "Iter-75820, train loss-0.2503, acc-0.9000, valid loss-0.3848, acc-0.8920, test loss-0.3893, acc-0.8887\n",
      "Iter-75830, train loss-0.2318, acc-0.9200, valid loss-0.3847, acc-0.8920, test loss-0.3893, acc-0.8889\n",
      "Iter-75840, train loss-0.3279, acc-0.9200, valid loss-0.3847, acc-0.8920, test loss-0.3893, acc-0.8888\n",
      "Iter-75850, train loss-0.5133, acc-0.8200, valid loss-0.3847, acc-0.8922, test loss-0.3893, acc-0.8888\n",
      "Iter-75860, train loss-0.6032, acc-0.8000, valid loss-0.3847, acc-0.8920, test loss-0.3892, acc-0.8888\n",
      "Iter-75870, train loss-0.4873, acc-0.8600, valid loss-0.3846, acc-0.8922, test loss-0.3892, acc-0.8888\n",
      "Iter-75880, train loss-0.4085, acc-0.8400, valid loss-0.3846, acc-0.8924, test loss-0.3892, acc-0.8889\n",
      "Iter-75890, train loss-0.5404, acc-0.8600, valid loss-0.3846, acc-0.8924, test loss-0.3892, acc-0.8889\n",
      "Iter-75900, train loss-0.1928, acc-0.9400, valid loss-0.3846, acc-0.8924, test loss-0.3892, acc-0.8889\n",
      "Iter-75910, train loss-0.4760, acc-0.8800, valid loss-0.3845, acc-0.8924, test loss-0.3892, acc-0.8889\n",
      "Iter-75920, train loss-0.5913, acc-0.8400, valid loss-0.3845, acc-0.8924, test loss-0.3891, acc-0.8890\n",
      "Iter-75930, train loss-0.3557, acc-0.9200, valid loss-0.3845, acc-0.8926, test loss-0.3891, acc-0.8890\n",
      "Iter-75940, train loss-0.2116, acc-0.9800, valid loss-0.3845, acc-0.8922, test loss-0.3891, acc-0.8891\n",
      "Iter-75950, train loss-0.5429, acc-0.8800, valid loss-0.3845, acc-0.8922, test loss-0.3891, acc-0.8891\n",
      "Iter-75960, train loss-0.1514, acc-0.9800, valid loss-0.3844, acc-0.8922, test loss-0.3891, acc-0.8890\n",
      "Iter-75970, train loss-0.2706, acc-0.9200, valid loss-0.3844, acc-0.8924, test loss-0.3890, acc-0.8889\n",
      "Iter-75980, train loss-0.2593, acc-0.9400, valid loss-0.3844, acc-0.8924, test loss-0.3890, acc-0.8890\n",
      "Iter-75990, train loss-0.3371, acc-0.9400, valid loss-0.3844, acc-0.8924, test loss-0.3890, acc-0.8890\n",
      "Iter-76000, train loss-0.3362, acc-0.9200, valid loss-0.3844, acc-0.8922, test loss-0.3890, acc-0.8890\n",
      "Iter-76010, train loss-0.2410, acc-0.9200, valid loss-0.3843, acc-0.8924, test loss-0.3890, acc-0.8891\n",
      "Iter-76020, train loss-0.3741, acc-0.9000, valid loss-0.3843, acc-0.8922, test loss-0.3890, acc-0.8891\n",
      "Iter-76030, train loss-0.4226, acc-0.8600, valid loss-0.3843, acc-0.8922, test loss-0.3889, acc-0.8890\n",
      "Iter-76040, train loss-0.4800, acc-0.8000, valid loss-0.3843, acc-0.8922, test loss-0.3889, acc-0.8890\n",
      "Iter-76050, train loss-0.5771, acc-0.8200, valid loss-0.3843, acc-0.8922, test loss-0.3889, acc-0.8892\n",
      "Iter-76060, train loss-0.3118, acc-0.9200, valid loss-0.3842, acc-0.8922, test loss-0.3889, acc-0.8891\n",
      "Iter-76070, train loss-0.6894, acc-0.7600, valid loss-0.3842, acc-0.8924, test loss-0.3889, acc-0.8892\n",
      "Iter-76080, train loss-0.2876, acc-0.9000, valid loss-0.3842, acc-0.8924, test loss-0.3888, acc-0.8891\n",
      "Iter-76090, train loss-0.3355, acc-0.9000, valid loss-0.3842, acc-0.8922, test loss-0.3888, acc-0.8891\n",
      "Iter-76100, train loss-0.7571, acc-0.7800, valid loss-0.3842, acc-0.8922, test loss-0.3888, acc-0.8892\n",
      "Iter-76110, train loss-0.4432, acc-0.9000, valid loss-0.3842, acc-0.8922, test loss-0.3888, acc-0.8892\n",
      "Iter-76120, train loss-0.4609, acc-0.8800, valid loss-0.3842, acc-0.8922, test loss-0.3888, acc-0.8893\n",
      "Iter-76130, train loss-0.4834, acc-0.8800, valid loss-0.3841, acc-0.8922, test loss-0.3888, acc-0.8893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-76140, train loss-0.4605, acc-0.8800, valid loss-0.3841, acc-0.8922, test loss-0.3887, acc-0.8893\n",
      "Iter-76150, train loss-0.3110, acc-0.9400, valid loss-0.3841, acc-0.8920, test loss-0.3887, acc-0.8892\n",
      "Iter-76160, train loss-0.4552, acc-0.8200, valid loss-0.3841, acc-0.8920, test loss-0.3887, acc-0.8893\n",
      "Iter-76170, train loss-0.3108, acc-0.9000, valid loss-0.3841, acc-0.8920, test loss-0.3887, acc-0.8891\n",
      "Iter-76180, train loss-0.5335, acc-0.8200, valid loss-0.3841, acc-0.8920, test loss-0.3887, acc-0.8891\n",
      "Iter-76190, train loss-0.2182, acc-0.9400, valid loss-0.3840, acc-0.8920, test loss-0.3886, acc-0.8891\n",
      "Iter-76200, train loss-0.2493, acc-0.9600, valid loss-0.3840, acc-0.8920, test loss-0.3886, acc-0.8892\n",
      "Iter-76210, train loss-0.2515, acc-0.9200, valid loss-0.3840, acc-0.8920, test loss-0.3886, acc-0.8891\n",
      "Iter-76220, train loss-0.3194, acc-0.8800, valid loss-0.3840, acc-0.8922, test loss-0.3886, acc-0.8889\n",
      "Iter-76230, train loss-0.6172, acc-0.8800, valid loss-0.3840, acc-0.8920, test loss-0.3886, acc-0.8890\n",
      "Iter-76240, train loss-0.6047, acc-0.8600, valid loss-0.3840, acc-0.8920, test loss-0.3886, acc-0.8891\n",
      "Iter-76250, train loss-0.3268, acc-0.8800, valid loss-0.3839, acc-0.8920, test loss-0.3885, acc-0.8891\n",
      "Iter-76260, train loss-0.5349, acc-0.8400, valid loss-0.3839, acc-0.8920, test loss-0.3885, acc-0.8891\n",
      "Iter-76270, train loss-0.5884, acc-0.8000, valid loss-0.3839, acc-0.8920, test loss-0.3885, acc-0.8891\n",
      "Iter-76280, train loss-0.4007, acc-0.9000, valid loss-0.3839, acc-0.8922, test loss-0.3885, acc-0.8890\n",
      "Iter-76290, train loss-0.5260, acc-0.8600, valid loss-0.3838, acc-0.8922, test loss-0.3885, acc-0.8892\n",
      "Iter-76300, train loss-0.4777, acc-0.8200, valid loss-0.3838, acc-0.8922, test loss-0.3884, acc-0.8891\n",
      "Iter-76310, train loss-0.4163, acc-0.8600, valid loss-0.3838, acc-0.8924, test loss-0.3884, acc-0.8893\n",
      "Iter-76320, train loss-0.5023, acc-0.8400, valid loss-0.3838, acc-0.8922, test loss-0.3884, acc-0.8892\n",
      "Iter-76330, train loss-0.3338, acc-0.9200, valid loss-0.3838, acc-0.8922, test loss-0.3884, acc-0.8893\n",
      "Iter-76340, train loss-0.5842, acc-0.8200, valid loss-0.3837, acc-0.8922, test loss-0.3883, acc-0.8893\n",
      "Iter-76350, train loss-0.2640, acc-0.9000, valid loss-0.3837, acc-0.8922, test loss-0.3883, acc-0.8894\n",
      "Iter-76360, train loss-0.5249, acc-0.8200, valid loss-0.3837, acc-0.8922, test loss-0.3883, acc-0.8892\n",
      "Iter-76370, train loss-0.3437, acc-0.8800, valid loss-0.3837, acc-0.8922, test loss-0.3883, acc-0.8895\n",
      "Iter-76380, train loss-0.4377, acc-0.9000, valid loss-0.3837, acc-0.8924, test loss-0.3883, acc-0.8894\n",
      "Iter-76390, train loss-0.2510, acc-0.9600, valid loss-0.3836, acc-0.8924, test loss-0.3882, acc-0.8894\n",
      "Iter-76400, train loss-0.2757, acc-0.9400, valid loss-0.3836, acc-0.8924, test loss-0.3882, acc-0.8894\n",
      "Iter-76410, train loss-0.3069, acc-0.9600, valid loss-0.3836, acc-0.8924, test loss-0.3882, acc-0.8894\n",
      "Iter-76420, train loss-0.4925, acc-0.8800, valid loss-0.3836, acc-0.8924, test loss-0.3882, acc-0.8894\n",
      "Iter-76430, train loss-0.3862, acc-0.9000, valid loss-0.3836, acc-0.8924, test loss-0.3882, acc-0.8894\n",
      "Iter-76440, train loss-0.2058, acc-0.9800, valid loss-0.3835, acc-0.8924, test loss-0.3881, acc-0.8895\n",
      "Iter-76450, train loss-0.7178, acc-0.7600, valid loss-0.3835, acc-0.8924, test loss-0.3881, acc-0.8895\n",
      "Iter-76460, train loss-0.4376, acc-0.8200, valid loss-0.3835, acc-0.8924, test loss-0.3881, acc-0.8895\n",
      "Iter-76470, train loss-0.4031, acc-0.9400, valid loss-0.3835, acc-0.8924, test loss-0.3881, acc-0.8895\n",
      "Iter-76480, train loss-0.4939, acc-0.8200, valid loss-0.3835, acc-0.8924, test loss-0.3881, acc-0.8896\n",
      "Iter-76490, train loss-0.3315, acc-0.9400, valid loss-0.3835, acc-0.8924, test loss-0.3881, acc-0.8896\n",
      "Iter-76500, train loss-0.3238, acc-0.9600, valid loss-0.3835, acc-0.8924, test loss-0.3880, acc-0.8895\n",
      "Iter-76510, train loss-0.3524, acc-0.9000, valid loss-0.3834, acc-0.8924, test loss-0.3880, acc-0.8894\n",
      "Iter-76520, train loss-0.5929, acc-0.8000, valid loss-0.3834, acc-0.8924, test loss-0.3880, acc-0.8893\n",
      "Iter-76530, train loss-0.2430, acc-0.9200, valid loss-0.3834, acc-0.8924, test loss-0.3880, acc-0.8895\n",
      "Iter-76540, train loss-0.4347, acc-0.8200, valid loss-0.3834, acc-0.8924, test loss-0.3880, acc-0.8896\n",
      "Iter-76550, train loss-0.3006, acc-0.9400, valid loss-0.3834, acc-0.8924, test loss-0.3880, acc-0.8896\n",
      "Iter-76560, train loss-0.4810, acc-0.8800, valid loss-0.3833, acc-0.8924, test loss-0.3879, acc-0.8896\n",
      "Iter-76570, train loss-0.5092, acc-0.8200, valid loss-0.3833, acc-0.8924, test loss-0.3879, acc-0.8896\n",
      "Iter-76580, train loss-0.2225, acc-0.9400, valid loss-0.3833, acc-0.8924, test loss-0.3879, acc-0.8896\n",
      "Iter-76590, train loss-0.3407, acc-0.9000, valid loss-0.3833, acc-0.8924, test loss-0.3879, acc-0.8896\n",
      "Iter-76600, train loss-0.2767, acc-0.9200, valid loss-0.3833, acc-0.8924, test loss-0.3879, acc-0.8895\n",
      "Iter-76610, train loss-0.2657, acc-0.9400, valid loss-0.3833, acc-0.8924, test loss-0.3879, acc-0.8895\n",
      "Iter-76620, train loss-0.4313, acc-0.8800, valid loss-0.3833, acc-0.8924, test loss-0.3878, acc-0.8895\n",
      "Iter-76630, train loss-0.3374, acc-0.8800, valid loss-0.3832, acc-0.8924, test loss-0.3878, acc-0.8895\n",
      "Iter-76640, train loss-0.3224, acc-0.9000, valid loss-0.3832, acc-0.8924, test loss-0.3878, acc-0.8896\n",
      "Iter-76650, train loss-0.4528, acc-0.8200, valid loss-0.3832, acc-0.8924, test loss-0.3878, acc-0.8896\n",
      "Iter-76660, train loss-0.2583, acc-0.9600, valid loss-0.3832, acc-0.8924, test loss-0.3878, acc-0.8896\n",
      "Iter-76670, train loss-0.4726, acc-0.8800, valid loss-0.3831, acc-0.8924, test loss-0.3877, acc-0.8896\n",
      "Iter-76680, train loss-0.4703, acc-0.8600, valid loss-0.3831, acc-0.8926, test loss-0.3877, acc-0.8896\n",
      "Iter-76690, train loss-0.5655, acc-0.8600, valid loss-0.3831, acc-0.8926, test loss-0.3877, acc-0.8896\n",
      "Iter-76700, train loss-0.3304, acc-0.9200, valid loss-0.3831, acc-0.8924, test loss-0.3877, acc-0.8896\n",
      "Iter-76710, train loss-0.6516, acc-0.8200, valid loss-0.3830, acc-0.8924, test loss-0.3877, acc-0.8896\n",
      "Iter-76720, train loss-0.2523, acc-0.9200, valid loss-0.3830, acc-0.8924, test loss-0.3877, acc-0.8895\n",
      "Iter-76730, train loss-0.3584, acc-0.9000, valid loss-0.3830, acc-0.8924, test loss-0.3876, acc-0.8895\n",
      "Iter-76740, train loss-0.3822, acc-0.9200, valid loss-0.3830, acc-0.8926, test loss-0.3876, acc-0.8895\n",
      "Iter-76750, train loss-0.6533, acc-0.8800, valid loss-0.3830, acc-0.8924, test loss-0.3876, acc-0.8896\n",
      "Iter-76760, train loss-0.2092, acc-0.9600, valid loss-0.3829, acc-0.8926, test loss-0.3876, acc-0.8896\n",
      "Iter-76770, train loss-0.3810, acc-0.9200, valid loss-0.3829, acc-0.8926, test loss-0.3876, acc-0.8895\n",
      "Iter-76780, train loss-0.2981, acc-0.9400, valid loss-0.3829, acc-0.8924, test loss-0.3875, acc-0.8895\n",
      "Iter-76790, train loss-0.2692, acc-0.9000, valid loss-0.3829, acc-0.8926, test loss-0.3875, acc-0.8895\n",
      "Iter-76800, train loss-0.4685, acc-0.8400, valid loss-0.3828, acc-0.8926, test loss-0.3875, acc-0.8895\n",
      "Iter-76810, train loss-0.4105, acc-0.9000, valid loss-0.3828, acc-0.8924, test loss-0.3875, acc-0.8895\n",
      "Iter-76820, train loss-0.4384, acc-0.8600, valid loss-0.3828, acc-0.8924, test loss-0.3875, acc-0.8895\n",
      "Iter-76830, train loss-0.3281, acc-0.9000, valid loss-0.3828, acc-0.8924, test loss-0.3874, acc-0.8894\n",
      "Iter-76840, train loss-0.3016, acc-0.9000, valid loss-0.3828, acc-0.8924, test loss-0.3874, acc-0.8894\n",
      "Iter-76850, train loss-0.4002, acc-0.8800, valid loss-0.3827, acc-0.8924, test loss-0.3874, acc-0.8894\n",
      "Iter-76860, train loss-0.4289, acc-0.8400, valid loss-0.3827, acc-0.8924, test loss-0.3874, acc-0.8894\n",
      "Iter-76870, train loss-0.3837, acc-0.8200, valid loss-0.3827, acc-0.8922, test loss-0.3874, acc-0.8894\n",
      "Iter-76880, train loss-0.4211, acc-0.9000, valid loss-0.3827, acc-0.8922, test loss-0.3873, acc-0.8894\n",
      "Iter-76890, train loss-0.3509, acc-0.9000, valid loss-0.3827, acc-0.8922, test loss-0.3873, acc-0.8895\n",
      "Iter-76900, train loss-0.3921, acc-0.9000, valid loss-0.3827, acc-0.8922, test loss-0.3873, acc-0.8895\n",
      "Iter-76910, train loss-0.2325, acc-0.9600, valid loss-0.3826, acc-0.8922, test loss-0.3873, acc-0.8895\n",
      "Iter-76920, train loss-0.5821, acc-0.8400, valid loss-0.3826, acc-0.8922, test loss-0.3873, acc-0.8895\n",
      "Iter-76930, train loss-0.2248, acc-0.9800, valid loss-0.3826, acc-0.8922, test loss-0.3873, acc-0.8895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-76940, train loss-0.5799, acc-0.8400, valid loss-0.3826, acc-0.8922, test loss-0.3873, acc-0.8895\n",
      "Iter-76950, train loss-0.2069, acc-0.9600, valid loss-0.3826, acc-0.8922, test loss-0.3872, acc-0.8895\n",
      "Iter-76960, train loss-0.4346, acc-0.8600, valid loss-0.3825, acc-0.8926, test loss-0.3872, acc-0.8895\n",
      "Iter-76970, train loss-0.3506, acc-0.9200, valid loss-0.3825, acc-0.8922, test loss-0.3872, acc-0.8896\n",
      "Iter-76980, train loss-0.3002, acc-0.9200, valid loss-0.3825, acc-0.8922, test loss-0.3872, acc-0.8897\n",
      "Iter-76990, train loss-0.4969, acc-0.8600, valid loss-0.3825, acc-0.8922, test loss-0.3872, acc-0.8898\n",
      "Iter-77000, train loss-0.3097, acc-0.8800, valid loss-0.3825, acc-0.8924, test loss-0.3871, acc-0.8898\n",
      "Iter-77010, train loss-0.1873, acc-0.9800, valid loss-0.3824, acc-0.8924, test loss-0.3871, acc-0.8898\n",
      "Iter-77020, train loss-0.3544, acc-0.9000, valid loss-0.3824, acc-0.8924, test loss-0.3871, acc-0.8898\n",
      "Iter-77030, train loss-0.3318, acc-0.9000, valid loss-0.3824, acc-0.8924, test loss-0.3871, acc-0.8898\n",
      "Iter-77040, train loss-0.4977, acc-0.8600, valid loss-0.3824, acc-0.8924, test loss-0.3871, acc-0.8898\n",
      "Iter-77050, train loss-0.5445, acc-0.8800, valid loss-0.3824, acc-0.8924, test loss-0.3871, acc-0.8898\n",
      "Iter-77060, train loss-0.6740, acc-0.7800, valid loss-0.3824, acc-0.8924, test loss-0.3871, acc-0.8898\n",
      "Iter-77070, train loss-0.4289, acc-0.8600, valid loss-0.3823, acc-0.8924, test loss-0.3870, acc-0.8897\n",
      "Iter-77080, train loss-0.4561, acc-0.8000, valid loss-0.3823, acc-0.8924, test loss-0.3870, acc-0.8896\n",
      "Iter-77090, train loss-0.2960, acc-0.9600, valid loss-0.3823, acc-0.8924, test loss-0.3870, acc-0.8896\n",
      "Iter-77100, train loss-0.3032, acc-0.9200, valid loss-0.3823, acc-0.8924, test loss-0.3870, acc-0.8897\n",
      "Iter-77110, train loss-0.3622, acc-0.9200, valid loss-0.3823, acc-0.8924, test loss-0.3870, acc-0.8897\n",
      "Iter-77120, train loss-0.5871, acc-0.8400, valid loss-0.3822, acc-0.8924, test loss-0.3869, acc-0.8898\n",
      "Iter-77130, train loss-0.2860, acc-0.9200, valid loss-0.3822, acc-0.8924, test loss-0.3869, acc-0.8897\n",
      "Iter-77140, train loss-0.4191, acc-0.9000, valid loss-0.3822, acc-0.8926, test loss-0.3869, acc-0.8897\n",
      "Iter-77150, train loss-0.5891, acc-0.8200, valid loss-0.3822, acc-0.8926, test loss-0.3869, acc-0.8897\n",
      "Iter-77160, train loss-0.4554, acc-0.8600, valid loss-0.3822, acc-0.8926, test loss-0.3869, acc-0.8898\n",
      "Iter-77170, train loss-0.5962, acc-0.8400, valid loss-0.3822, acc-0.8926, test loss-0.3869, acc-0.8898\n",
      "Iter-77180, train loss-0.3189, acc-0.8800, valid loss-0.3822, acc-0.8926, test loss-0.3868, acc-0.8898\n",
      "Iter-77190, train loss-0.3214, acc-0.9200, valid loss-0.3821, acc-0.8924, test loss-0.3868, acc-0.8897\n",
      "Iter-77200, train loss-0.2683, acc-0.9400, valid loss-0.3821, acc-0.8926, test loss-0.3868, acc-0.8898\n",
      "Iter-77210, train loss-0.4805, acc-0.9000, valid loss-0.3821, acc-0.8924, test loss-0.3868, acc-0.8897\n",
      "Iter-77220, train loss-0.3504, acc-0.9000, valid loss-0.3821, acc-0.8924, test loss-0.3868, acc-0.8897\n",
      "Iter-77230, train loss-0.2781, acc-0.9200, valid loss-0.3821, acc-0.8924, test loss-0.3867, acc-0.8897\n",
      "Iter-77240, train loss-0.7028, acc-0.8000, valid loss-0.3820, acc-0.8924, test loss-0.3867, acc-0.8897\n",
      "Iter-77250, train loss-0.4982, acc-0.8000, valid loss-0.3820, acc-0.8924, test loss-0.3867, acc-0.8898\n",
      "Iter-77260, train loss-0.4519, acc-0.8400, valid loss-0.3820, acc-0.8924, test loss-0.3867, acc-0.8898\n",
      "Iter-77270, train loss-0.3702, acc-0.8600, valid loss-0.3820, acc-0.8924, test loss-0.3867, acc-0.8899\n",
      "Iter-77280, train loss-0.4325, acc-0.8200, valid loss-0.3820, acc-0.8924, test loss-0.3866, acc-0.8899\n",
      "Iter-77290, train loss-0.2901, acc-0.9400, valid loss-0.3819, acc-0.8924, test loss-0.3866, acc-0.8900\n",
      "Iter-77300, train loss-0.3632, acc-0.8600, valid loss-0.3819, acc-0.8926, test loss-0.3866, acc-0.8899\n",
      "Iter-77310, train loss-0.3572, acc-0.9200, valid loss-0.3819, acc-0.8926, test loss-0.3866, acc-0.8900\n",
      "Iter-77320, train loss-0.3207, acc-0.9000, valid loss-0.3819, acc-0.8926, test loss-0.3866, acc-0.8900\n",
      "Iter-77330, train loss-0.4041, acc-0.8600, valid loss-0.3818, acc-0.8924, test loss-0.3865, acc-0.8899\n",
      "Iter-77340, train loss-0.5715, acc-0.8800, valid loss-0.3818, acc-0.8926, test loss-0.3865, acc-0.8898\n",
      "Iter-77350, train loss-0.5291, acc-0.8400, valid loss-0.3818, acc-0.8928, test loss-0.3865, acc-0.8898\n",
      "Iter-77360, train loss-0.3582, acc-0.9600, valid loss-0.3818, acc-0.8928, test loss-0.3865, acc-0.8899\n",
      "Iter-77370, train loss-0.4486, acc-0.8600, valid loss-0.3818, acc-0.8928, test loss-0.3865, acc-0.8899\n",
      "Iter-77380, train loss-0.4910, acc-0.8800, valid loss-0.3818, acc-0.8928, test loss-0.3865, acc-0.8899\n",
      "Iter-77390, train loss-0.6491, acc-0.8400, valid loss-0.3818, acc-0.8928, test loss-0.3864, acc-0.8899\n",
      "Iter-77400, train loss-0.3083, acc-0.9400, valid loss-0.3817, acc-0.8928, test loss-0.3864, acc-0.8899\n",
      "Iter-77410, train loss-0.2075, acc-0.9400, valid loss-0.3817, acc-0.8928, test loss-0.3864, acc-0.8899\n",
      "Iter-77420, train loss-0.4115, acc-0.8400, valid loss-0.3817, acc-0.8928, test loss-0.3864, acc-0.8900\n",
      "Iter-77430, train loss-0.4768, acc-0.8200, valid loss-0.3817, acc-0.8930, test loss-0.3864, acc-0.8900\n",
      "Iter-77440, train loss-0.4052, acc-0.8800, valid loss-0.3817, acc-0.8928, test loss-0.3864, acc-0.8900\n",
      "Iter-77450, train loss-0.2997, acc-0.9400, valid loss-0.3817, acc-0.8930, test loss-0.3864, acc-0.8899\n",
      "Iter-77460, train loss-0.2730, acc-0.9400, valid loss-0.3817, acc-0.8930, test loss-0.3863, acc-0.8899\n",
      "Iter-77470, train loss-0.2659, acc-0.9400, valid loss-0.3817, acc-0.8930, test loss-0.3863, acc-0.8899\n",
      "Iter-77480, train loss-0.3446, acc-0.8800, valid loss-0.3816, acc-0.8930, test loss-0.3863, acc-0.8899\n",
      "Iter-77490, train loss-0.2927, acc-0.8800, valid loss-0.3816, acc-0.8928, test loss-0.3863, acc-0.8898\n",
      "Iter-77500, train loss-0.2283, acc-0.9400, valid loss-0.3816, acc-0.8930, test loss-0.3863, acc-0.8900\n",
      "Iter-77510, train loss-0.4445, acc-0.9000, valid loss-0.3816, acc-0.8928, test loss-0.3863, acc-0.8899\n",
      "Iter-77520, train loss-0.4164, acc-0.8800, valid loss-0.3816, acc-0.8928, test loss-0.3862, acc-0.8899\n",
      "Iter-77530, train loss-0.4049, acc-0.9000, valid loss-0.3816, acc-0.8928, test loss-0.3862, acc-0.8899\n",
      "Iter-77540, train loss-0.5046, acc-0.8600, valid loss-0.3815, acc-0.8928, test loss-0.3862, acc-0.8899\n",
      "Iter-77550, train loss-0.3470, acc-0.9000, valid loss-0.3815, acc-0.8928, test loss-0.3862, acc-0.8899\n",
      "Iter-77560, train loss-0.2752, acc-0.9400, valid loss-0.3815, acc-0.8930, test loss-0.3862, acc-0.8899\n",
      "Iter-77570, train loss-0.3731, acc-0.9000, valid loss-0.3815, acc-0.8930, test loss-0.3862, acc-0.8899\n",
      "Iter-77580, train loss-0.4322, acc-0.8400, valid loss-0.3815, acc-0.8930, test loss-0.3861, acc-0.8899\n",
      "Iter-77590, train loss-0.6286, acc-0.8200, valid loss-0.3814, acc-0.8930, test loss-0.3861, acc-0.8899\n",
      "Iter-77600, train loss-0.4979, acc-0.8800, valid loss-0.3814, acc-0.8932, test loss-0.3861, acc-0.8901\n",
      "Iter-77610, train loss-0.5655, acc-0.8000, valid loss-0.3814, acc-0.8932, test loss-0.3861, acc-0.8899\n",
      "Iter-77620, train loss-0.3102, acc-0.9400, valid loss-0.3814, acc-0.8932, test loss-0.3861, acc-0.8901\n",
      "Iter-77630, train loss-0.4122, acc-0.8600, valid loss-0.3814, acc-0.8932, test loss-0.3860, acc-0.8902\n",
      "Iter-77640, train loss-0.4834, acc-0.8200, valid loss-0.3814, acc-0.8932, test loss-0.3860, acc-0.8901\n",
      "Iter-77650, train loss-0.2907, acc-0.8800, valid loss-0.3814, acc-0.8932, test loss-0.3860, acc-0.8902\n",
      "Iter-77660, train loss-0.4396, acc-0.8800, valid loss-0.3814, acc-0.8930, test loss-0.3860, acc-0.8901\n",
      "Iter-77670, train loss-0.4130, acc-0.8600, valid loss-0.3813, acc-0.8930, test loss-0.3860, acc-0.8902\n",
      "Iter-77680, train loss-0.4844, acc-0.9000, valid loss-0.3813, acc-0.8930, test loss-0.3860, acc-0.8902\n",
      "Iter-77690, train loss-0.1940, acc-0.9600, valid loss-0.3813, acc-0.8930, test loss-0.3860, acc-0.8901\n",
      "Iter-77700, train loss-0.3773, acc-0.8600, valid loss-0.3813, acc-0.8930, test loss-0.3859, acc-0.8901\n",
      "Iter-77710, train loss-0.3866, acc-0.8600, valid loss-0.3813, acc-0.8930, test loss-0.3859, acc-0.8900\n",
      "Iter-77720, train loss-0.3873, acc-0.9000, valid loss-0.3813, acc-0.8930, test loss-0.3859, acc-0.8901\n",
      "Iter-77730, train loss-0.4237, acc-0.8800, valid loss-0.3813, acc-0.8930, test loss-0.3859, acc-0.8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-77740, train loss-0.2992, acc-0.9000, valid loss-0.3812, acc-0.8930, test loss-0.3859, acc-0.8903\n",
      "Iter-77750, train loss-0.5019, acc-0.8800, valid loss-0.3812, acc-0.8928, test loss-0.3858, acc-0.8904\n",
      "Iter-77760, train loss-0.6224, acc-0.8600, valid loss-0.3812, acc-0.8930, test loss-0.3858, acc-0.8903\n",
      "Iter-77770, train loss-0.3064, acc-0.9000, valid loss-0.3812, acc-0.8930, test loss-0.3858, acc-0.8903\n",
      "Iter-77780, train loss-0.4075, acc-0.8800, valid loss-0.3812, acc-0.8930, test loss-0.3858, acc-0.8903\n",
      "Iter-77790, train loss-0.3088, acc-0.9400, valid loss-0.3811, acc-0.8930, test loss-0.3857, acc-0.8903\n",
      "Iter-77800, train loss-0.6618, acc-0.8400, valid loss-0.3811, acc-0.8930, test loss-0.3857, acc-0.8902\n",
      "Iter-77810, train loss-0.5223, acc-0.8400, valid loss-0.3811, acc-0.8930, test loss-0.3857, acc-0.8902\n",
      "Iter-77820, train loss-0.4445, acc-0.8800, valid loss-0.3811, acc-0.8930, test loss-0.3857, acc-0.8903\n",
      "Iter-77830, train loss-0.2695, acc-0.9400, valid loss-0.3811, acc-0.8930, test loss-0.3857, acc-0.8902\n",
      "Iter-77840, train loss-0.3341, acc-0.9200, valid loss-0.3811, acc-0.8930, test loss-0.3857, acc-0.8903\n",
      "Iter-77850, train loss-0.3106, acc-0.9200, valid loss-0.3810, acc-0.8930, test loss-0.3857, acc-0.8904\n",
      "Iter-77860, train loss-0.2007, acc-0.9800, valid loss-0.3810, acc-0.8930, test loss-0.3856, acc-0.8904\n",
      "Iter-77870, train loss-0.2875, acc-0.9200, valid loss-0.3810, acc-0.8930, test loss-0.3856, acc-0.8903\n",
      "Iter-77880, train loss-0.4179, acc-0.9200, valid loss-0.3810, acc-0.8930, test loss-0.3856, acc-0.8903\n",
      "Iter-77890, train loss-0.2457, acc-0.9400, valid loss-0.3810, acc-0.8930, test loss-0.3856, acc-0.8902\n",
      "Iter-77900, train loss-0.5403, acc-0.8400, valid loss-0.3809, acc-0.8930, test loss-0.3856, acc-0.8902\n",
      "Iter-77910, train loss-0.3676, acc-0.9000, valid loss-0.3809, acc-0.8930, test loss-0.3855, acc-0.8902\n",
      "Iter-77920, train loss-0.3523, acc-0.8600, valid loss-0.3809, acc-0.8930, test loss-0.3855, acc-0.8901\n",
      "Iter-77930, train loss-0.4484, acc-0.8600, valid loss-0.3809, acc-0.8930, test loss-0.3855, acc-0.8901\n",
      "Iter-77940, train loss-0.4746, acc-0.8800, valid loss-0.3809, acc-0.8930, test loss-0.3855, acc-0.8901\n",
      "Iter-77950, train loss-0.3682, acc-0.8600, valid loss-0.3809, acc-0.8930, test loss-0.3855, acc-0.8902\n",
      "Iter-77960, train loss-0.4667, acc-0.8800, valid loss-0.3808, acc-0.8930, test loss-0.3854, acc-0.8901\n",
      "Iter-77970, train loss-0.5452, acc-0.7400, valid loss-0.3808, acc-0.8930, test loss-0.3854, acc-0.8901\n",
      "Iter-77980, train loss-0.5467, acc-0.8000, valid loss-0.3808, acc-0.8930, test loss-0.3854, acc-0.8902\n",
      "Iter-77990, train loss-0.3660, acc-0.8000, valid loss-0.3808, acc-0.8930, test loss-0.3854, acc-0.8903\n",
      "Iter-78000, train loss-0.3336, acc-0.9000, valid loss-0.3808, acc-0.8930, test loss-0.3854, acc-0.8903\n",
      "Iter-78010, train loss-0.5225, acc-0.9000, valid loss-0.3808, acc-0.8930, test loss-0.3854, acc-0.8903\n",
      "Iter-78020, train loss-0.2753, acc-0.9400, valid loss-0.3808, acc-0.8930, test loss-0.3854, acc-0.8903\n",
      "Iter-78030, train loss-0.3687, acc-0.8800, valid loss-0.3808, acc-0.8930, test loss-0.3853, acc-0.8903\n",
      "Iter-78040, train loss-0.6556, acc-0.8400, valid loss-0.3807, acc-0.8928, test loss-0.3853, acc-0.8903\n",
      "Iter-78050, train loss-0.2423, acc-0.9200, valid loss-0.3807, acc-0.8928, test loss-0.3853, acc-0.8902\n",
      "Iter-78060, train loss-0.3803, acc-0.8600, valid loss-0.3807, acc-0.8928, test loss-0.3853, acc-0.8902\n",
      "Iter-78070, train loss-0.3373, acc-0.9000, valid loss-0.3807, acc-0.8930, test loss-0.3853, acc-0.8902\n",
      "Iter-78080, train loss-0.4587, acc-0.8200, valid loss-0.3807, acc-0.8930, test loss-0.3852, acc-0.8905\n",
      "Iter-78090, train loss-0.2434, acc-0.9400, valid loss-0.3806, acc-0.8928, test loss-0.3852, acc-0.8904\n",
      "Iter-78100, train loss-0.3101, acc-0.9400, valid loss-0.3806, acc-0.8928, test loss-0.3852, acc-0.8905\n",
      "Iter-78110, train loss-0.4666, acc-0.8600, valid loss-0.3806, acc-0.8928, test loss-0.3852, acc-0.8904\n",
      "Iter-78120, train loss-0.1860, acc-0.9800, valid loss-0.3806, acc-0.8930, test loss-0.3852, acc-0.8904\n",
      "Iter-78130, train loss-0.8459, acc-0.7600, valid loss-0.3806, acc-0.8930, test loss-0.3852, acc-0.8903\n",
      "Iter-78140, train loss-0.3369, acc-0.9200, valid loss-0.3806, acc-0.8930, test loss-0.3852, acc-0.8905\n",
      "Iter-78150, train loss-0.2401, acc-0.9600, valid loss-0.3806, acc-0.8928, test loss-0.3851, acc-0.8903\n",
      "Iter-78160, train loss-0.3354, acc-0.9000, valid loss-0.3806, acc-0.8930, test loss-0.3851, acc-0.8903\n",
      "Iter-78170, train loss-0.3649, acc-0.9000, valid loss-0.3805, acc-0.8928, test loss-0.3851, acc-0.8904\n",
      "Iter-78180, train loss-0.2592, acc-0.9200, valid loss-0.3805, acc-0.8928, test loss-0.3851, acc-0.8903\n",
      "Iter-78190, train loss-0.3473, acc-0.8800, valid loss-0.3805, acc-0.8928, test loss-0.3851, acc-0.8904\n",
      "Iter-78200, train loss-0.4022, acc-0.9000, valid loss-0.3805, acc-0.8928, test loss-0.3851, acc-0.8903\n",
      "Iter-78210, train loss-0.3921, acc-0.9000, valid loss-0.3805, acc-0.8926, test loss-0.3850, acc-0.8903\n",
      "Iter-78220, train loss-0.3772, acc-0.8800, valid loss-0.3805, acc-0.8928, test loss-0.3850, acc-0.8903\n",
      "Iter-78230, train loss-0.3202, acc-0.9000, valid loss-0.3804, acc-0.8928, test loss-0.3850, acc-0.8903\n",
      "Iter-78240, train loss-0.3983, acc-0.8800, valid loss-0.3804, acc-0.8928, test loss-0.3850, acc-0.8904\n",
      "Iter-78250, train loss-0.4484, acc-0.8600, valid loss-0.3804, acc-0.8930, test loss-0.3850, acc-0.8904\n",
      "Iter-78260, train loss-0.2563, acc-0.9200, valid loss-0.3804, acc-0.8930, test loss-0.3850, acc-0.8904\n",
      "Iter-78270, train loss-0.5465, acc-0.8400, valid loss-0.3804, acc-0.8930, test loss-0.3849, acc-0.8904\n",
      "Iter-78280, train loss-0.3728, acc-0.9000, valid loss-0.3803, acc-0.8928, test loss-0.3849, acc-0.8904\n",
      "Iter-78290, train loss-0.3661, acc-0.8600, valid loss-0.3803, acc-0.8928, test loss-0.3849, acc-0.8905\n",
      "Iter-78300, train loss-0.3080, acc-0.9200, valid loss-0.3803, acc-0.8928, test loss-0.3849, acc-0.8905\n",
      "Iter-78310, train loss-0.3207, acc-0.8800, valid loss-0.3803, acc-0.8930, test loss-0.3848, acc-0.8904\n",
      "Iter-78320, train loss-0.1718, acc-0.9800, valid loss-0.3803, acc-0.8930, test loss-0.3848, acc-0.8904\n",
      "Iter-78330, train loss-0.3402, acc-0.8800, valid loss-0.3803, acc-0.8930, test loss-0.3848, acc-0.8905\n",
      "Iter-78340, train loss-0.4300, acc-0.9000, valid loss-0.3802, acc-0.8930, test loss-0.3848, acc-0.8904\n",
      "Iter-78350, train loss-0.3825, acc-0.9000, valid loss-0.3802, acc-0.8930, test loss-0.3848, acc-0.8904\n",
      "Iter-78360, train loss-0.3068, acc-0.9200, valid loss-0.3802, acc-0.8930, test loss-0.3848, acc-0.8904\n",
      "Iter-78370, train loss-0.4009, acc-0.8800, valid loss-0.3802, acc-0.8930, test loss-0.3847, acc-0.8904\n",
      "Iter-78380, train loss-0.5050, acc-0.8200, valid loss-0.3802, acc-0.8928, test loss-0.3847, acc-0.8906\n",
      "Iter-78390, train loss-0.6058, acc-0.8600, valid loss-0.3802, acc-0.8928, test loss-0.3847, acc-0.8906\n",
      "Iter-78400, train loss-0.3661, acc-0.9200, valid loss-0.3801, acc-0.8928, test loss-0.3847, acc-0.8907\n",
      "Iter-78410, train loss-0.4808, acc-0.8600, valid loss-0.3801, acc-0.8928, test loss-0.3847, acc-0.8906\n",
      "Iter-78420, train loss-0.5089, acc-0.9000, valid loss-0.3801, acc-0.8928, test loss-0.3847, acc-0.8906\n",
      "Iter-78430, train loss-0.5331, acc-0.8800, valid loss-0.3801, acc-0.8928, test loss-0.3846, acc-0.8907\n",
      "Iter-78440, train loss-0.2675, acc-0.9400, valid loss-0.3801, acc-0.8930, test loss-0.3846, acc-0.8908\n",
      "Iter-78450, train loss-0.5020, acc-0.8800, valid loss-0.3801, acc-0.8932, test loss-0.3846, acc-0.8909\n",
      "Iter-78460, train loss-0.3919, acc-0.8800, valid loss-0.3800, acc-0.8930, test loss-0.3846, acc-0.8908\n",
      "Iter-78470, train loss-0.3551, acc-0.9200, valid loss-0.3800, acc-0.8928, test loss-0.3846, acc-0.8909\n",
      "Iter-78480, train loss-0.5553, acc-0.8400, valid loss-0.3800, acc-0.8928, test loss-0.3846, acc-0.8908\n",
      "Iter-78490, train loss-0.3656, acc-0.8600, valid loss-0.3800, acc-0.8928, test loss-0.3845, acc-0.8909\n",
      "Iter-78500, train loss-0.3286, acc-0.8800, valid loss-0.3799, acc-0.8928, test loss-0.3845, acc-0.8909\n",
      "Iter-78510, train loss-0.3113, acc-0.9000, valid loss-0.3799, acc-0.8930, test loss-0.3845, acc-0.8909\n",
      "Iter-78520, train loss-0.3956, acc-0.9000, valid loss-0.3799, acc-0.8930, test loss-0.3845, acc-0.8909\n",
      "Iter-78530, train loss-0.3593, acc-0.9200, valid loss-0.3799, acc-0.8930, test loss-0.3845, acc-0.8908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-78540, train loss-0.4496, acc-0.8600, valid loss-0.3799, acc-0.8930, test loss-0.3844, acc-0.8910\n",
      "Iter-78550, train loss-0.4418, acc-0.8800, valid loss-0.3799, acc-0.8930, test loss-0.3844, acc-0.8910\n",
      "Iter-78560, train loss-0.3929, acc-0.9000, valid loss-0.3799, acc-0.8930, test loss-0.3844, acc-0.8909\n",
      "Iter-78570, train loss-0.4217, acc-0.8600, valid loss-0.3798, acc-0.8930, test loss-0.3844, acc-0.8908\n",
      "Iter-78580, train loss-0.2327, acc-0.9400, valid loss-0.3798, acc-0.8928, test loss-0.3844, acc-0.8908\n",
      "Iter-78590, train loss-0.3526, acc-0.8800, valid loss-0.3798, acc-0.8928, test loss-0.3843, acc-0.8908\n",
      "Iter-78600, train loss-0.4872, acc-0.8800, valid loss-0.3798, acc-0.8928, test loss-0.3843, acc-0.8908\n",
      "Iter-78610, train loss-0.2860, acc-0.9200, valid loss-0.3798, acc-0.8928, test loss-0.3843, acc-0.8909\n",
      "Iter-78620, train loss-0.2381, acc-0.9400, valid loss-0.3798, acc-0.8932, test loss-0.3843, acc-0.8908\n",
      "Iter-78630, train loss-0.4835, acc-0.8600, valid loss-0.3798, acc-0.8932, test loss-0.3843, acc-0.8907\n",
      "Iter-78640, train loss-0.5833, acc-0.8000, valid loss-0.3797, acc-0.8932, test loss-0.3843, acc-0.8908\n",
      "Iter-78650, train loss-0.4959, acc-0.8600, valid loss-0.3797, acc-0.8930, test loss-0.3842, acc-0.8908\n",
      "Iter-78660, train loss-0.4604, acc-0.8400, valid loss-0.3797, acc-0.8928, test loss-0.3842, acc-0.8907\n",
      "Iter-78670, train loss-0.6373, acc-0.8600, valid loss-0.3797, acc-0.8928, test loss-0.3842, acc-0.8908\n",
      "Iter-78680, train loss-0.2445, acc-0.9400, valid loss-0.3796, acc-0.8932, test loss-0.3842, acc-0.8907\n",
      "Iter-78690, train loss-0.4103, acc-0.8800, valid loss-0.3796, acc-0.8930, test loss-0.3842, acc-0.8907\n",
      "Iter-78700, train loss-0.1699, acc-0.9600, valid loss-0.3796, acc-0.8930, test loss-0.3842, acc-0.8907\n",
      "Iter-78710, train loss-0.3998, acc-0.8800, valid loss-0.3796, acc-0.8930, test loss-0.3842, acc-0.8907\n",
      "Iter-78720, train loss-0.3833, acc-0.9400, valid loss-0.3796, acc-0.8930, test loss-0.3841, acc-0.8907\n",
      "Iter-78730, train loss-0.2820, acc-0.9400, valid loss-0.3796, acc-0.8930, test loss-0.3841, acc-0.8907\n",
      "Iter-78740, train loss-0.4500, acc-0.8600, valid loss-0.3795, acc-0.8930, test loss-0.3841, acc-0.8908\n",
      "Iter-78750, train loss-0.3364, acc-0.9200, valid loss-0.3795, acc-0.8930, test loss-0.3841, acc-0.8907\n",
      "Iter-78760, train loss-0.3201, acc-0.9000, valid loss-0.3795, acc-0.8930, test loss-0.3841, acc-0.8906\n",
      "Iter-78770, train loss-0.4157, acc-0.9000, valid loss-0.3795, acc-0.8930, test loss-0.3840, acc-0.8906\n",
      "Iter-78780, train loss-0.3328, acc-0.9200, valid loss-0.3795, acc-0.8930, test loss-0.3840, acc-0.8907\n",
      "Iter-78790, train loss-0.2639, acc-0.9400, valid loss-0.3795, acc-0.8930, test loss-0.3840, acc-0.8907\n",
      "Iter-78800, train loss-0.2685, acc-0.9400, valid loss-0.3795, acc-0.8932, test loss-0.3840, acc-0.8907\n",
      "Iter-78810, train loss-0.3286, acc-0.9000, valid loss-0.3794, acc-0.8932, test loss-0.3840, acc-0.8907\n",
      "Iter-78820, train loss-0.5312, acc-0.8800, valid loss-0.3794, acc-0.8932, test loss-0.3839, acc-0.8909\n",
      "Iter-78830, train loss-0.4723, acc-0.8800, valid loss-0.3794, acc-0.8932, test loss-0.3839, acc-0.8908\n",
      "Iter-78840, train loss-0.3964, acc-0.9200, valid loss-0.3794, acc-0.8932, test loss-0.3839, acc-0.8907\n",
      "Iter-78850, train loss-0.3495, acc-0.9000, valid loss-0.3794, acc-0.8932, test loss-0.3839, acc-0.8905\n",
      "Iter-78860, train loss-0.3521, acc-0.9000, valid loss-0.3793, acc-0.8930, test loss-0.3839, acc-0.8909\n",
      "Iter-78870, train loss-0.2188, acc-0.9600, valid loss-0.3793, acc-0.8932, test loss-0.3838, acc-0.8908\n",
      "Iter-78880, train loss-0.4260, acc-0.8800, valid loss-0.3793, acc-0.8932, test loss-0.3838, acc-0.8909\n",
      "Iter-78890, train loss-0.4718, acc-0.8800, valid loss-0.3793, acc-0.8930, test loss-0.3838, acc-0.8908\n",
      "Iter-78900, train loss-0.5057, acc-0.8800, valid loss-0.3793, acc-0.8930, test loss-0.3838, acc-0.8909\n",
      "Iter-78910, train loss-0.5406, acc-0.8600, valid loss-0.3792, acc-0.8932, test loss-0.3838, acc-0.8909\n",
      "Iter-78920, train loss-0.4199, acc-0.8600, valid loss-0.3792, acc-0.8930, test loss-0.3838, acc-0.8909\n",
      "Iter-78930, train loss-0.3891, acc-0.8800, valid loss-0.3792, acc-0.8932, test loss-0.3837, acc-0.8909\n",
      "Iter-78940, train loss-0.4272, acc-0.8600, valid loss-0.3792, acc-0.8932, test loss-0.3837, acc-0.8910\n",
      "Iter-78950, train loss-0.3626, acc-0.9000, valid loss-0.3792, acc-0.8930, test loss-0.3837, acc-0.8910\n",
      "Iter-78960, train loss-0.2428, acc-0.9400, valid loss-0.3792, acc-0.8930, test loss-0.3837, acc-0.8911\n",
      "Iter-78970, train loss-0.3580, acc-0.9000, valid loss-0.3791, acc-0.8930, test loss-0.3837, acc-0.8910\n",
      "Iter-78980, train loss-0.4366, acc-0.8400, valid loss-0.3791, acc-0.8930, test loss-0.3837, acc-0.8909\n",
      "Iter-78990, train loss-0.5875, acc-0.8600, valid loss-0.3791, acc-0.8930, test loss-0.3836, acc-0.8911\n",
      "Iter-79000, train loss-0.2311, acc-0.9600, valid loss-0.3791, acc-0.8930, test loss-0.3836, acc-0.8912\n",
      "Iter-79010, train loss-0.4476, acc-0.9000, valid loss-0.3791, acc-0.8930, test loss-0.3836, acc-0.8911\n",
      "Iter-79020, train loss-0.6828, acc-0.7600, valid loss-0.3791, acc-0.8930, test loss-0.3836, acc-0.8912\n",
      "Iter-79030, train loss-0.2212, acc-0.9600, valid loss-0.3790, acc-0.8930, test loss-0.3836, acc-0.8911\n",
      "Iter-79040, train loss-0.2299, acc-0.9200, valid loss-0.3790, acc-0.8930, test loss-0.3836, acc-0.8911\n",
      "Iter-79050, train loss-0.4589, acc-0.8600, valid loss-0.3790, acc-0.8932, test loss-0.3836, acc-0.8910\n",
      "Iter-79060, train loss-0.3137, acc-0.9200, valid loss-0.3790, acc-0.8930, test loss-0.3835, acc-0.8912\n",
      "Iter-79070, train loss-0.3294, acc-0.9200, valid loss-0.3790, acc-0.8932, test loss-0.3835, acc-0.8911\n",
      "Iter-79080, train loss-0.4236, acc-0.9000, valid loss-0.3790, acc-0.8932, test loss-0.3835, acc-0.8911\n",
      "Iter-79090, train loss-0.4735, acc-0.8600, valid loss-0.3790, acc-0.8932, test loss-0.3835, acc-0.8911\n",
      "Iter-79100, train loss-0.3573, acc-0.8600, valid loss-0.3789, acc-0.8932, test loss-0.3835, acc-0.8910\n",
      "Iter-79110, train loss-0.5101, acc-0.8200, valid loss-0.3789, acc-0.8932, test loss-0.3835, acc-0.8911\n",
      "Iter-79120, train loss-0.2231, acc-0.9200, valid loss-0.3789, acc-0.8932, test loss-0.3834, acc-0.8911\n",
      "Iter-79130, train loss-0.4367, acc-0.9200, valid loss-0.3789, acc-0.8932, test loss-0.3834, acc-0.8911\n",
      "Iter-79140, train loss-0.5012, acc-0.8400, valid loss-0.3788, acc-0.8930, test loss-0.3834, acc-0.8911\n",
      "Iter-79150, train loss-0.4348, acc-0.8400, valid loss-0.3788, acc-0.8932, test loss-0.3834, acc-0.8911\n",
      "Iter-79160, train loss-0.4584, acc-0.9000, valid loss-0.3788, acc-0.8932, test loss-0.3834, acc-0.8911\n",
      "Iter-79170, train loss-0.3307, acc-0.9200, valid loss-0.3788, acc-0.8932, test loss-0.3834, acc-0.8911\n",
      "Iter-79180, train loss-0.2990, acc-0.9200, valid loss-0.3787, acc-0.8932, test loss-0.3833, acc-0.8912\n",
      "Iter-79190, train loss-0.5381, acc-0.8400, valid loss-0.3787, acc-0.8934, test loss-0.3833, acc-0.8911\n",
      "Iter-79200, train loss-0.3242, acc-0.8800, valid loss-0.3787, acc-0.8932, test loss-0.3833, acc-0.8911\n",
      "Iter-79210, train loss-0.3882, acc-0.9200, valid loss-0.3787, acc-0.8934, test loss-0.3833, acc-0.8911\n",
      "Iter-79220, train loss-0.3311, acc-0.8600, valid loss-0.3787, acc-0.8938, test loss-0.3833, acc-0.8911\n",
      "Iter-79230, train loss-0.4410, acc-0.9000, valid loss-0.3787, acc-0.8936, test loss-0.3832, acc-0.8911\n",
      "Iter-79240, train loss-0.2844, acc-0.9000, valid loss-0.3787, acc-0.8936, test loss-0.3832, acc-0.8911\n",
      "Iter-79250, train loss-0.5085, acc-0.8600, valid loss-0.3787, acc-0.8936, test loss-0.3832, acc-0.8911\n",
      "Iter-79260, train loss-0.4427, acc-0.9000, valid loss-0.3786, acc-0.8936, test loss-0.3832, acc-0.8912\n",
      "Iter-79270, train loss-0.4004, acc-0.8800, valid loss-0.3786, acc-0.8936, test loss-0.3832, acc-0.8912\n",
      "Iter-79280, train loss-0.3673, acc-0.9000, valid loss-0.3786, acc-0.8934, test loss-0.3831, acc-0.8912\n",
      "Iter-79290, train loss-0.4615, acc-0.8600, valid loss-0.3786, acc-0.8936, test loss-0.3831, acc-0.8912\n",
      "Iter-79300, train loss-0.4790, acc-0.9200, valid loss-0.3785, acc-0.8936, test loss-0.3831, acc-0.8912\n",
      "Iter-79310, train loss-0.5080, acc-0.8000, valid loss-0.3785, acc-0.8936, test loss-0.3831, acc-0.8913\n",
      "Iter-79320, train loss-0.6371, acc-0.8200, valid loss-0.3785, acc-0.8936, test loss-0.3831, acc-0.8913\n",
      "Iter-79330, train loss-0.3286, acc-0.9000, valid loss-0.3785, acc-0.8936, test loss-0.3831, acc-0.8913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79340, train loss-0.3131, acc-0.9400, valid loss-0.3785, acc-0.8936, test loss-0.3830, acc-0.8913\n",
      "Iter-79350, train loss-0.4232, acc-0.8600, valid loss-0.3785, acc-0.8934, test loss-0.3830, acc-0.8913\n",
      "Iter-79360, train loss-0.4792, acc-0.8600, valid loss-0.3784, acc-0.8934, test loss-0.3830, acc-0.8912\n",
      "Iter-79370, train loss-0.6708, acc-0.8000, valid loss-0.3784, acc-0.8938, test loss-0.3830, acc-0.8912\n",
      "Iter-79380, train loss-0.3837, acc-0.9000, valid loss-0.3784, acc-0.8938, test loss-0.3830, acc-0.8912\n",
      "Iter-79390, train loss-0.3401, acc-0.9000, valid loss-0.3784, acc-0.8938, test loss-0.3830, acc-0.8912\n",
      "Iter-79400, train loss-0.4723, acc-0.9000, valid loss-0.3784, acc-0.8938, test loss-0.3830, acc-0.8912\n",
      "Iter-79410, train loss-0.4963, acc-0.8200, valid loss-0.3783, acc-0.8938, test loss-0.3829, acc-0.8912\n",
      "Iter-79420, train loss-0.4776, acc-0.8600, valid loss-0.3783, acc-0.8938, test loss-0.3829, acc-0.8912\n",
      "Iter-79430, train loss-0.4729, acc-0.9200, valid loss-0.3783, acc-0.8938, test loss-0.3829, acc-0.8911\n",
      "Iter-79440, train loss-0.4203, acc-0.9000, valid loss-0.3783, acc-0.8938, test loss-0.3829, acc-0.8910\n",
      "Iter-79450, train loss-0.5552, acc-0.8600, valid loss-0.3783, acc-0.8938, test loss-0.3829, acc-0.8910\n",
      "Iter-79460, train loss-0.5720, acc-0.8600, valid loss-0.3783, acc-0.8938, test loss-0.3829, acc-0.8910\n",
      "Iter-79470, train loss-0.4800, acc-0.8600, valid loss-0.3783, acc-0.8938, test loss-0.3829, acc-0.8910\n",
      "Iter-79480, train loss-0.3685, acc-0.8200, valid loss-0.3782, acc-0.8936, test loss-0.3828, acc-0.8910\n",
      "Iter-79490, train loss-0.4866, acc-0.8600, valid loss-0.3782, acc-0.8936, test loss-0.3828, acc-0.8910\n",
      "Iter-79500, train loss-0.4475, acc-0.8400, valid loss-0.3782, acc-0.8938, test loss-0.3828, acc-0.8910\n",
      "Iter-79510, train loss-0.2537, acc-0.9600, valid loss-0.3782, acc-0.8934, test loss-0.3828, acc-0.8910\n",
      "Iter-79520, train loss-0.3606, acc-0.9000, valid loss-0.3781, acc-0.8934, test loss-0.3828, acc-0.8910\n",
      "Iter-79530, train loss-0.4014, acc-0.8400, valid loss-0.3781, acc-0.8934, test loss-0.3828, acc-0.8910\n",
      "Iter-79540, train loss-0.3870, acc-0.8800, valid loss-0.3781, acc-0.8934, test loss-0.3827, acc-0.8911\n",
      "Iter-79550, train loss-0.3839, acc-0.9400, valid loss-0.3781, acc-0.8936, test loss-0.3827, acc-0.8911\n",
      "Iter-79560, train loss-0.5051, acc-0.8400, valid loss-0.3781, acc-0.8938, test loss-0.3827, acc-0.8912\n",
      "Iter-79570, train loss-0.5944, acc-0.8400, valid loss-0.3781, acc-0.8938, test loss-0.3827, acc-0.8912\n",
      "Iter-79580, train loss-0.4474, acc-0.8800, valid loss-0.3781, acc-0.8938, test loss-0.3827, acc-0.8911\n",
      "Iter-79590, train loss-0.3292, acc-0.9000, valid loss-0.3781, acc-0.8936, test loss-0.3827, acc-0.8911\n",
      "Iter-79600, train loss-0.3702, acc-0.9000, valid loss-0.3780, acc-0.8934, test loss-0.3827, acc-0.8911\n",
      "Iter-79610, train loss-0.2462, acc-0.9200, valid loss-0.3780, acc-0.8938, test loss-0.3826, acc-0.8911\n",
      "Iter-79620, train loss-0.3523, acc-0.8800, valid loss-0.3780, acc-0.8936, test loss-0.3826, acc-0.8911\n",
      "Iter-79630, train loss-0.3143, acc-0.9200, valid loss-0.3780, acc-0.8936, test loss-0.3826, acc-0.8912\n",
      "Iter-79640, train loss-0.2990, acc-0.9200, valid loss-0.3780, acc-0.8934, test loss-0.3826, acc-0.8911\n",
      "Iter-79650, train loss-0.2362, acc-0.9600, valid loss-0.3779, acc-0.8934, test loss-0.3826, acc-0.8913\n",
      "Iter-79660, train loss-0.4109, acc-0.8600, valid loss-0.3779, acc-0.8934, test loss-0.3826, acc-0.8914\n",
      "Iter-79670, train loss-0.3872, acc-0.8600, valid loss-0.3779, acc-0.8934, test loss-0.3825, acc-0.8911\n",
      "Iter-79680, train loss-0.6013, acc-0.8000, valid loss-0.3779, acc-0.8934, test loss-0.3825, acc-0.8912\n",
      "Iter-79690, train loss-0.4066, acc-0.9000, valid loss-0.3778, acc-0.8934, test loss-0.3825, acc-0.8913\n",
      "Iter-79700, train loss-0.3840, acc-0.8800, valid loss-0.3778, acc-0.8934, test loss-0.3825, acc-0.8913\n",
      "Iter-79710, train loss-0.6395, acc-0.8200, valid loss-0.3778, acc-0.8934, test loss-0.3825, acc-0.8914\n",
      "Iter-79720, train loss-0.3183, acc-0.9400, valid loss-0.3778, acc-0.8934, test loss-0.3825, acc-0.8914\n",
      "Iter-79730, train loss-0.3790, acc-0.9000, valid loss-0.3778, acc-0.8934, test loss-0.3825, acc-0.8913\n",
      "Iter-79740, train loss-0.5733, acc-0.8400, valid loss-0.3778, acc-0.8934, test loss-0.3824, acc-0.8914\n",
      "Iter-79750, train loss-0.3932, acc-0.9000, valid loss-0.3777, acc-0.8934, test loss-0.3824, acc-0.8914\n",
      "Iter-79760, train loss-0.3185, acc-0.9000, valid loss-0.3777, acc-0.8934, test loss-0.3824, acc-0.8913\n",
      "Iter-79770, train loss-0.4232, acc-0.8400, valid loss-0.3777, acc-0.8932, test loss-0.3824, acc-0.8914\n",
      "Iter-79780, train loss-0.3857, acc-0.8800, valid loss-0.3777, acc-0.8932, test loss-0.3824, acc-0.8914\n",
      "Iter-79790, train loss-0.3321, acc-0.9200, valid loss-0.3777, acc-0.8932, test loss-0.3824, acc-0.8913\n",
      "Iter-79800, train loss-0.4625, acc-0.8400, valid loss-0.3777, acc-0.8932, test loss-0.3823, acc-0.8913\n",
      "Iter-79810, train loss-0.3268, acc-0.8800, valid loss-0.3776, acc-0.8932, test loss-0.3823, acc-0.8913\n",
      "Iter-79820, train loss-0.5565, acc-0.8800, valid loss-0.3776, acc-0.8932, test loss-0.3823, acc-0.8913\n",
      "Iter-79830, train loss-0.3864, acc-0.9000, valid loss-0.3776, acc-0.8932, test loss-0.3823, acc-0.8913\n",
      "Iter-79840, train loss-0.4234, acc-0.8000, valid loss-0.3776, acc-0.8932, test loss-0.3823, acc-0.8914\n",
      "Iter-79850, train loss-0.2803, acc-0.9800, valid loss-0.3776, acc-0.8932, test loss-0.3822, acc-0.8914\n",
      "Iter-79860, train loss-0.5983, acc-0.8600, valid loss-0.3775, acc-0.8932, test loss-0.3822, acc-0.8914\n",
      "Iter-79870, train loss-0.1289, acc-0.9800, valid loss-0.3775, acc-0.8932, test loss-0.3822, acc-0.8914\n",
      "Iter-79880, train loss-0.6349, acc-0.8200, valid loss-0.3775, acc-0.8932, test loss-0.3822, acc-0.8914\n",
      "Iter-79890, train loss-0.2367, acc-0.9600, valid loss-0.3775, acc-0.8932, test loss-0.3822, acc-0.8914\n",
      "Iter-79900, train loss-0.4247, acc-0.9000, valid loss-0.3775, acc-0.8932, test loss-0.3822, acc-0.8914\n",
      "Iter-79910, train loss-0.3498, acc-0.9000, valid loss-0.3775, acc-0.8932, test loss-0.3822, acc-0.8914\n",
      "Iter-79920, train loss-0.3540, acc-0.9000, valid loss-0.3775, acc-0.8932, test loss-0.3821, acc-0.8914\n",
      "Iter-79930, train loss-0.4280, acc-0.9000, valid loss-0.3774, acc-0.8932, test loss-0.3821, acc-0.8914\n",
      "Iter-79940, train loss-0.3436, acc-0.9200, valid loss-0.3774, acc-0.8932, test loss-0.3821, acc-0.8914\n",
      "Iter-79950, train loss-0.3843, acc-0.8600, valid loss-0.3774, acc-0.8932, test loss-0.3821, acc-0.8914\n",
      "Iter-79960, train loss-0.2952, acc-0.9000, valid loss-0.3774, acc-0.8934, test loss-0.3821, acc-0.8914\n",
      "Iter-79970, train loss-0.3773, acc-0.8800, valid loss-0.3774, acc-0.8934, test loss-0.3820, acc-0.8913\n",
      "Iter-79980, train loss-0.4194, acc-0.8600, valid loss-0.3773, acc-0.8934, test loss-0.3820, acc-0.8914\n",
      "Iter-79990, train loss-0.2602, acc-0.9400, valid loss-0.3773, acc-0.8934, test loss-0.3820, acc-0.8914\n",
      "Iter-80000, train loss-0.3531, acc-0.9000, valid loss-0.3773, acc-0.8934, test loss-0.3820, acc-0.8913\n",
      "Iter-80010, train loss-0.3624, acc-0.9400, valid loss-0.3773, acc-0.8934, test loss-0.3820, acc-0.8914\n",
      "Iter-80020, train loss-0.2325, acc-0.9600, valid loss-0.3773, acc-0.8934, test loss-0.3820, acc-0.8914\n",
      "Iter-80030, train loss-0.4362, acc-0.8800, valid loss-0.3773, acc-0.8934, test loss-0.3819, acc-0.8914\n",
      "Iter-80040, train loss-0.3690, acc-0.8800, valid loss-0.3772, acc-0.8932, test loss-0.3819, acc-0.8915\n",
      "Iter-80050, train loss-0.4097, acc-0.8800, valid loss-0.3772, acc-0.8932, test loss-0.3819, acc-0.8915\n",
      "Iter-80060, train loss-0.2020, acc-0.9400, valid loss-0.3772, acc-0.8934, test loss-0.3819, acc-0.8915\n",
      "Iter-80070, train loss-0.5147, acc-0.9000, valid loss-0.3772, acc-0.8934, test loss-0.3819, acc-0.8916\n",
      "Iter-80080, train loss-0.5790, acc-0.8400, valid loss-0.3772, acc-0.8932, test loss-0.3819, acc-0.8915\n",
      "Iter-80090, train loss-0.3847, acc-0.9000, valid loss-0.3772, acc-0.8934, test loss-0.3818, acc-0.8916\n",
      "Iter-80100, train loss-0.2665, acc-0.9400, valid loss-0.3771, acc-0.8934, test loss-0.3818, acc-0.8915\n",
      "Iter-80110, train loss-0.3067, acc-0.9200, valid loss-0.3771, acc-0.8932, test loss-0.3818, acc-0.8915\n",
      "Iter-80120, train loss-0.4435, acc-0.8400, valid loss-0.3771, acc-0.8932, test loss-0.3818, acc-0.8915\n",
      "Iter-80130, train loss-0.4220, acc-0.9200, valid loss-0.3771, acc-0.8932, test loss-0.3818, acc-0.8915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-80140, train loss-0.2637, acc-0.9400, valid loss-0.3771, acc-0.8932, test loss-0.3818, acc-0.8915\n",
      "Iter-80150, train loss-0.2832, acc-0.9400, valid loss-0.3771, acc-0.8932, test loss-0.3818, acc-0.8916\n",
      "Iter-80160, train loss-0.3145, acc-0.9400, valid loss-0.3771, acc-0.8932, test loss-0.3817, acc-0.8916\n",
      "Iter-80170, train loss-0.3922, acc-0.9000, valid loss-0.3770, acc-0.8932, test loss-0.3817, acc-0.8915\n",
      "Iter-80180, train loss-0.3091, acc-0.8800, valid loss-0.3770, acc-0.8932, test loss-0.3817, acc-0.8915\n",
      "Iter-80190, train loss-0.3581, acc-0.8600, valid loss-0.3770, acc-0.8932, test loss-0.3817, acc-0.8915\n",
      "Iter-80200, train loss-0.3357, acc-0.9000, valid loss-0.3770, acc-0.8932, test loss-0.3817, acc-0.8916\n",
      "Iter-80210, train loss-0.3545, acc-0.9200, valid loss-0.3770, acc-0.8932, test loss-0.3817, acc-0.8916\n",
      "Iter-80220, train loss-0.5532, acc-0.8200, valid loss-0.3770, acc-0.8934, test loss-0.3817, acc-0.8916\n",
      "Iter-80230, train loss-0.3856, acc-0.8800, valid loss-0.3769, acc-0.8934, test loss-0.3816, acc-0.8916\n",
      "Iter-80240, train loss-0.5068, acc-0.8800, valid loss-0.3769, acc-0.8934, test loss-0.3816, acc-0.8915\n",
      "Iter-80250, train loss-0.3929, acc-0.8600, valid loss-0.3769, acc-0.8934, test loss-0.3816, acc-0.8915\n",
      "Iter-80260, train loss-0.5699, acc-0.8400, valid loss-0.3769, acc-0.8934, test loss-0.3816, acc-0.8916\n",
      "Iter-80270, train loss-0.5285, acc-0.9000, valid loss-0.3769, acc-0.8934, test loss-0.3816, acc-0.8916\n",
      "Iter-80280, train loss-0.1808, acc-0.9600, valid loss-0.3768, acc-0.8932, test loss-0.3816, acc-0.8916\n",
      "Iter-80290, train loss-0.4414, acc-0.8200, valid loss-0.3768, acc-0.8932, test loss-0.3815, acc-0.8916\n",
      "Iter-80300, train loss-0.3912, acc-0.8800, valid loss-0.3768, acc-0.8932, test loss-0.3815, acc-0.8917\n",
      "Iter-80310, train loss-0.7363, acc-0.7600, valid loss-0.3768, acc-0.8932, test loss-0.3815, acc-0.8917\n",
      "Iter-80320, train loss-0.4173, acc-0.8800, valid loss-0.3768, acc-0.8932, test loss-0.3815, acc-0.8915\n",
      "Iter-80330, train loss-0.5732, acc-0.8200, valid loss-0.3767, acc-0.8932, test loss-0.3815, acc-0.8915\n",
      "Iter-80340, train loss-0.2378, acc-0.9600, valid loss-0.3767, acc-0.8932, test loss-0.3815, acc-0.8916\n",
      "Iter-80350, train loss-0.5933, acc-0.8600, valid loss-0.3767, acc-0.8934, test loss-0.3814, acc-0.8916\n",
      "Iter-80360, train loss-0.4586, acc-0.8200, valid loss-0.3767, acc-0.8936, test loss-0.3814, acc-0.8917\n",
      "Iter-80370, train loss-0.6336, acc-0.8200, valid loss-0.3767, acc-0.8936, test loss-0.3814, acc-0.8916\n",
      "Iter-80380, train loss-0.3343, acc-0.9000, valid loss-0.3767, acc-0.8936, test loss-0.3814, acc-0.8916\n",
      "Iter-80390, train loss-0.2275, acc-0.9600, valid loss-0.3766, acc-0.8936, test loss-0.3814, acc-0.8916\n",
      "Iter-80400, train loss-0.2367, acc-0.9000, valid loss-0.3766, acc-0.8936, test loss-0.3814, acc-0.8917\n",
      "Iter-80410, train loss-0.3991, acc-0.8800, valid loss-0.3766, acc-0.8934, test loss-0.3813, acc-0.8915\n",
      "Iter-80420, train loss-0.2607, acc-0.9000, valid loss-0.3766, acc-0.8934, test loss-0.3813, acc-0.8915\n",
      "Iter-80430, train loss-0.3243, acc-0.9000, valid loss-0.3766, acc-0.8934, test loss-0.3813, acc-0.8916\n",
      "Iter-80440, train loss-0.5476, acc-0.8400, valid loss-0.3766, acc-0.8934, test loss-0.3813, acc-0.8917\n",
      "Iter-80450, train loss-0.3061, acc-0.9200, valid loss-0.3765, acc-0.8934, test loss-0.3813, acc-0.8916\n",
      "Iter-80460, train loss-0.4481, acc-0.8600, valid loss-0.3765, acc-0.8934, test loss-0.3813, acc-0.8917\n",
      "Iter-80470, train loss-0.3184, acc-0.9200, valid loss-0.3765, acc-0.8934, test loss-0.3813, acc-0.8918\n",
      "Iter-80480, train loss-0.3368, acc-0.9200, valid loss-0.3765, acc-0.8934, test loss-0.3813, acc-0.8918\n",
      "Iter-80490, train loss-0.2801, acc-0.9400, valid loss-0.3764, acc-0.8934, test loss-0.3812, acc-0.8917\n",
      "Iter-80500, train loss-0.3178, acc-0.9200, valid loss-0.3764, acc-0.8934, test loss-0.3812, acc-0.8917\n",
      "Iter-80510, train loss-0.4731, acc-0.8400, valid loss-0.3764, acc-0.8934, test loss-0.3812, acc-0.8919\n",
      "Iter-80520, train loss-0.4316, acc-0.8600, valid loss-0.3764, acc-0.8934, test loss-0.3812, acc-0.8919\n",
      "Iter-80530, train loss-0.4420, acc-0.8200, valid loss-0.3764, acc-0.8934, test loss-0.3812, acc-0.8919\n",
      "Iter-80540, train loss-0.2288, acc-0.9600, valid loss-0.3764, acc-0.8934, test loss-0.3812, acc-0.8919\n",
      "Iter-80550, train loss-0.4004, acc-0.9200, valid loss-0.3764, acc-0.8934, test loss-0.3812, acc-0.8919\n",
      "Iter-80560, train loss-0.6407, acc-0.7800, valid loss-0.3763, acc-0.8934, test loss-0.3811, acc-0.8919\n",
      "Iter-80570, train loss-0.3511, acc-0.8800, valid loss-0.3763, acc-0.8934, test loss-0.3811, acc-0.8918\n",
      "Iter-80580, train loss-0.2173, acc-0.9600, valid loss-0.3763, acc-0.8934, test loss-0.3811, acc-0.8918\n",
      "Iter-80590, train loss-0.3975, acc-0.9000, valid loss-0.3763, acc-0.8934, test loss-0.3811, acc-0.8919\n",
      "Iter-80600, train loss-0.3557, acc-0.9000, valid loss-0.3763, acc-0.8934, test loss-0.3811, acc-0.8920\n",
      "Iter-80610, train loss-0.3684, acc-0.9000, valid loss-0.3763, acc-0.8934, test loss-0.3811, acc-0.8920\n",
      "Iter-80620, train loss-0.4619, acc-0.8600, valid loss-0.3762, acc-0.8934, test loss-0.3811, acc-0.8919\n",
      "Iter-80630, train loss-0.5813, acc-0.8200, valid loss-0.3762, acc-0.8934, test loss-0.3810, acc-0.8920\n",
      "Iter-80640, train loss-0.3310, acc-0.8400, valid loss-0.3762, acc-0.8934, test loss-0.3810, acc-0.8920\n",
      "Iter-80650, train loss-0.3048, acc-0.9600, valid loss-0.3762, acc-0.8934, test loss-0.3810, acc-0.8921\n",
      "Iter-80660, train loss-0.3219, acc-0.9000, valid loss-0.3762, acc-0.8934, test loss-0.3810, acc-0.8920\n",
      "Iter-80670, train loss-0.2764, acc-0.9400, valid loss-0.3762, acc-0.8934, test loss-0.3810, acc-0.8920\n",
      "Iter-80680, train loss-0.4954, acc-0.8600, valid loss-0.3762, acc-0.8932, test loss-0.3810, acc-0.8920\n",
      "Iter-80690, train loss-0.4628, acc-0.9000, valid loss-0.3761, acc-0.8934, test loss-0.3810, acc-0.8920\n",
      "Iter-80700, train loss-0.3432, acc-0.9000, valid loss-0.3761, acc-0.8934, test loss-0.3810, acc-0.8920\n",
      "Iter-80710, train loss-0.4799, acc-0.8400, valid loss-0.3761, acc-0.8934, test loss-0.3809, acc-0.8920\n",
      "Iter-80720, train loss-0.4863, acc-0.8800, valid loss-0.3761, acc-0.8936, test loss-0.3809, acc-0.8920\n",
      "Iter-80730, train loss-0.3082, acc-0.9400, valid loss-0.3761, acc-0.8934, test loss-0.3809, acc-0.8920\n",
      "Iter-80740, train loss-0.2779, acc-0.9200, valid loss-0.3761, acc-0.8932, test loss-0.3809, acc-0.8920\n",
      "Iter-80750, train loss-0.4892, acc-0.8800, valid loss-0.3761, acc-0.8934, test loss-0.3809, acc-0.8920\n",
      "Iter-80760, train loss-0.2378, acc-0.9800, valid loss-0.3760, acc-0.8932, test loss-0.3809, acc-0.8921\n",
      "Iter-80770, train loss-0.5942, acc-0.8600, valid loss-0.3760, acc-0.8932, test loss-0.3809, acc-0.8920\n",
      "Iter-80780, train loss-0.2336, acc-0.9400, valid loss-0.3760, acc-0.8932, test loss-0.3809, acc-0.8921\n",
      "Iter-80790, train loss-0.4580, acc-0.8600, valid loss-0.3760, acc-0.8932, test loss-0.3808, acc-0.8921\n",
      "Iter-80800, train loss-0.5393, acc-0.8400, valid loss-0.3760, acc-0.8932, test loss-0.3808, acc-0.8921\n",
      "Iter-80810, train loss-0.6102, acc-0.8200, valid loss-0.3759, acc-0.8932, test loss-0.3808, acc-0.8921\n",
      "Iter-80820, train loss-0.3746, acc-0.8600, valid loss-0.3759, acc-0.8932, test loss-0.3808, acc-0.8921\n",
      "Iter-80830, train loss-0.3976, acc-0.8800, valid loss-0.3759, acc-0.8932, test loss-0.3808, acc-0.8921\n",
      "Iter-80840, train loss-0.5032, acc-0.8600, valid loss-0.3759, acc-0.8932, test loss-0.3807, acc-0.8922\n",
      "Iter-80850, train loss-0.6279, acc-0.8200, valid loss-0.3759, acc-0.8932, test loss-0.3807, acc-0.8922\n",
      "Iter-80860, train loss-0.4547, acc-0.8800, valid loss-0.3759, acc-0.8932, test loss-0.3807, acc-0.8922\n",
      "Iter-80870, train loss-0.3972, acc-0.8600, valid loss-0.3759, acc-0.8932, test loss-0.3807, acc-0.8921\n",
      "Iter-80880, train loss-0.2985, acc-0.9600, valid loss-0.3758, acc-0.8932, test loss-0.3807, acc-0.8922\n",
      "Iter-80890, train loss-0.4740, acc-0.8400, valid loss-0.3758, acc-0.8932, test loss-0.3807, acc-0.8921\n",
      "Iter-80900, train loss-0.5128, acc-0.8400, valid loss-0.3758, acc-0.8932, test loss-0.3806, acc-0.8921\n",
      "Iter-80910, train loss-0.2312, acc-0.9200, valid loss-0.3758, acc-0.8934, test loss-0.3806, acc-0.8921\n",
      "Iter-80920, train loss-0.3713, acc-0.9200, valid loss-0.3758, acc-0.8936, test loss-0.3806, acc-0.8921\n",
      "Iter-80930, train loss-0.2497, acc-0.9400, valid loss-0.3757, acc-0.8936, test loss-0.3806, acc-0.8921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-80940, train loss-0.2625, acc-0.9400, valid loss-0.3757, acc-0.8934, test loss-0.3806, acc-0.8921\n",
      "Iter-80950, train loss-0.3117, acc-0.9600, valid loss-0.3757, acc-0.8934, test loss-0.3806, acc-0.8920\n",
      "Iter-80960, train loss-0.5258, acc-0.8400, valid loss-0.3757, acc-0.8934, test loss-0.3806, acc-0.8920\n",
      "Iter-80970, train loss-0.4725, acc-0.8200, valid loss-0.3757, acc-0.8938, test loss-0.3805, acc-0.8922\n",
      "Iter-80980, train loss-0.4973, acc-0.8600, valid loss-0.3756, acc-0.8940, test loss-0.3805, acc-0.8922\n",
      "Iter-80990, train loss-0.2794, acc-0.9000, valid loss-0.3756, acc-0.8936, test loss-0.3805, acc-0.8922\n",
      "Iter-81000, train loss-0.3246, acc-0.9200, valid loss-0.3756, acc-0.8940, test loss-0.3805, acc-0.8921\n",
      "Iter-81010, train loss-0.4313, acc-0.9000, valid loss-0.3756, acc-0.8940, test loss-0.3805, acc-0.8921\n",
      "Iter-81020, train loss-0.2147, acc-0.9400, valid loss-0.3756, acc-0.8938, test loss-0.3804, acc-0.8921\n",
      "Iter-81030, train loss-0.1828, acc-0.9800, valid loss-0.3756, acc-0.8936, test loss-0.3804, acc-0.8922\n",
      "Iter-81040, train loss-0.2104, acc-0.9600, valid loss-0.3756, acc-0.8938, test loss-0.3804, acc-0.8920\n",
      "Iter-81050, train loss-0.3282, acc-0.9000, valid loss-0.3756, acc-0.8938, test loss-0.3804, acc-0.8920\n",
      "Iter-81060, train loss-0.4036, acc-0.9000, valid loss-0.3755, acc-0.8940, test loss-0.3804, acc-0.8921\n",
      "Iter-81070, train loss-0.4084, acc-0.9000, valid loss-0.3755, acc-0.8938, test loss-0.3804, acc-0.8920\n",
      "Iter-81080, train loss-0.2688, acc-0.9400, valid loss-0.3755, acc-0.8938, test loss-0.3804, acc-0.8920\n",
      "Iter-81090, train loss-0.5202, acc-0.8800, valid loss-0.3755, acc-0.8940, test loss-0.3803, acc-0.8921\n",
      "Iter-81100, train loss-0.4232, acc-0.9200, valid loss-0.3755, acc-0.8940, test loss-0.3803, acc-0.8921\n",
      "Iter-81110, train loss-0.5081, acc-0.8400, valid loss-0.3755, acc-0.8940, test loss-0.3803, acc-0.8920\n",
      "Iter-81120, train loss-0.3568, acc-0.8800, valid loss-0.3755, acc-0.8940, test loss-0.3803, acc-0.8921\n",
      "Iter-81130, train loss-0.5134, acc-0.8400, valid loss-0.3754, acc-0.8942, test loss-0.3803, acc-0.8921\n",
      "Iter-81140, train loss-0.4656, acc-0.8400, valid loss-0.3754, acc-0.8940, test loss-0.3803, acc-0.8922\n",
      "Iter-81150, train loss-0.3735, acc-0.9000, valid loss-0.3754, acc-0.8940, test loss-0.3803, acc-0.8922\n",
      "Iter-81160, train loss-0.3567, acc-0.8600, valid loss-0.3754, acc-0.8940, test loss-0.3803, acc-0.8923\n",
      "Iter-81170, train loss-0.3975, acc-0.8600, valid loss-0.3754, acc-0.8940, test loss-0.3802, acc-0.8923\n",
      "Iter-81180, train loss-0.1893, acc-0.9400, valid loss-0.3754, acc-0.8940, test loss-0.3802, acc-0.8924\n",
      "Iter-81190, train loss-0.4569, acc-0.9000, valid loss-0.3753, acc-0.8940, test loss-0.3802, acc-0.8923\n",
      "Iter-81200, train loss-0.3348, acc-0.8800, valid loss-0.3753, acc-0.8938, test loss-0.3802, acc-0.8923\n",
      "Iter-81210, train loss-0.3143, acc-0.8600, valid loss-0.3753, acc-0.8938, test loss-0.3802, acc-0.8923\n",
      "Iter-81220, train loss-0.2417, acc-0.9600, valid loss-0.3753, acc-0.8938, test loss-0.3801, acc-0.8923\n",
      "Iter-81230, train loss-0.3887, acc-0.8800, valid loss-0.3753, acc-0.8938, test loss-0.3801, acc-0.8924\n",
      "Iter-81240, train loss-0.4901, acc-0.8200, valid loss-0.3752, acc-0.8938, test loss-0.3801, acc-0.8923\n",
      "Iter-81250, train loss-0.1975, acc-0.9800, valid loss-0.3752, acc-0.8938, test loss-0.3801, acc-0.8923\n",
      "Iter-81260, train loss-0.3320, acc-0.9000, valid loss-0.3752, acc-0.8940, test loss-0.3801, acc-0.8923\n",
      "Iter-81270, train loss-0.5197, acc-0.8000, valid loss-0.3752, acc-0.8936, test loss-0.3801, acc-0.8924\n",
      "Iter-81280, train loss-0.3399, acc-0.9000, valid loss-0.3752, acc-0.8938, test loss-0.3801, acc-0.8922\n",
      "Iter-81290, train loss-0.4276, acc-0.8800, valid loss-0.3752, acc-0.8938, test loss-0.3801, acc-0.8922\n",
      "Iter-81300, train loss-0.4582, acc-0.8600, valid loss-0.3751, acc-0.8938, test loss-0.3800, acc-0.8922\n",
      "Iter-81310, train loss-0.4491, acc-0.8800, valid loss-0.3751, acc-0.8938, test loss-0.3800, acc-0.8923\n",
      "Iter-81320, train loss-0.2219, acc-0.9600, valid loss-0.3751, acc-0.8938, test loss-0.3800, acc-0.8924\n",
      "Iter-81330, train loss-0.3598, acc-0.9400, valid loss-0.3751, acc-0.8938, test loss-0.3800, acc-0.8925\n",
      "Iter-81340, train loss-0.6678, acc-0.8400, valid loss-0.3751, acc-0.8938, test loss-0.3800, acc-0.8925\n",
      "Iter-81350, train loss-0.2858, acc-0.9200, valid loss-0.3751, acc-0.8938, test loss-0.3800, acc-0.8924\n",
      "Iter-81360, train loss-0.4024, acc-0.9000, valid loss-0.3751, acc-0.8938, test loss-0.3800, acc-0.8924\n",
      "Iter-81370, train loss-0.4352, acc-0.8400, valid loss-0.3751, acc-0.8938, test loss-0.3799, acc-0.8924\n",
      "Iter-81380, train loss-0.4639, acc-0.8200, valid loss-0.3751, acc-0.8938, test loss-0.3799, acc-0.8923\n",
      "Iter-81390, train loss-0.5273, acc-0.8400, valid loss-0.3751, acc-0.8938, test loss-0.3799, acc-0.8923\n",
      "Iter-81400, train loss-0.2737, acc-0.9400, valid loss-0.3750, acc-0.8938, test loss-0.3799, acc-0.8923\n",
      "Iter-81410, train loss-0.4910, acc-0.9000, valid loss-0.3750, acc-0.8938, test loss-0.3799, acc-0.8923\n",
      "Iter-81420, train loss-0.3050, acc-0.9400, valid loss-0.3750, acc-0.8938, test loss-0.3799, acc-0.8922\n",
      "Iter-81430, train loss-0.3177, acc-0.9400, valid loss-0.3750, acc-0.8938, test loss-0.3799, acc-0.8922\n",
      "Iter-81440, train loss-0.4068, acc-0.8800, valid loss-0.3750, acc-0.8938, test loss-0.3798, acc-0.8922\n",
      "Iter-81450, train loss-0.6331, acc-0.8400, valid loss-0.3749, acc-0.8938, test loss-0.3798, acc-0.8922\n",
      "Iter-81460, train loss-0.4705, acc-0.8400, valid loss-0.3749, acc-0.8938, test loss-0.3798, acc-0.8923\n",
      "Iter-81470, train loss-0.5805, acc-0.9000, valid loss-0.3749, acc-0.8938, test loss-0.3798, acc-0.8923\n",
      "Iter-81480, train loss-0.4953, acc-0.8600, valid loss-0.3749, acc-0.8938, test loss-0.3798, acc-0.8923\n",
      "Iter-81490, train loss-0.4403, acc-0.9000, valid loss-0.3749, acc-0.8938, test loss-0.3798, acc-0.8924\n",
      "Iter-81500, train loss-0.3632, acc-0.9000, valid loss-0.3749, acc-0.8938, test loss-0.3798, acc-0.8924\n",
      "Iter-81510, train loss-0.2471, acc-0.9200, valid loss-0.3748, acc-0.8938, test loss-0.3797, acc-0.8924\n",
      "Iter-81520, train loss-0.2419, acc-0.9200, valid loss-0.3748, acc-0.8938, test loss-0.3797, acc-0.8924\n",
      "Iter-81530, train loss-0.3344, acc-0.9200, valid loss-0.3748, acc-0.8936, test loss-0.3797, acc-0.8924\n",
      "Iter-81540, train loss-0.5590, acc-0.8600, valid loss-0.3748, acc-0.8936, test loss-0.3797, acc-0.8924\n",
      "Iter-81550, train loss-0.4526, acc-0.8400, valid loss-0.3748, acc-0.8934, test loss-0.3797, acc-0.8924\n",
      "Iter-81560, train loss-0.4545, acc-0.9000, valid loss-0.3747, acc-0.8936, test loss-0.3797, acc-0.8924\n",
      "Iter-81570, train loss-0.4774, acc-0.8400, valid loss-0.3747, acc-0.8936, test loss-0.3796, acc-0.8924\n",
      "Iter-81580, train loss-0.3052, acc-0.9200, valid loss-0.3747, acc-0.8936, test loss-0.3796, acc-0.8924\n",
      "Iter-81590, train loss-0.2451, acc-0.9400, valid loss-0.3747, acc-0.8936, test loss-0.3796, acc-0.8924\n",
      "Iter-81600, train loss-0.5791, acc-0.8800, valid loss-0.3747, acc-0.8936, test loss-0.3796, acc-0.8924\n",
      "Iter-81610, train loss-0.8530, acc-0.7600, valid loss-0.3747, acc-0.8938, test loss-0.3796, acc-0.8924\n",
      "Iter-81620, train loss-0.3126, acc-0.8800, valid loss-0.3746, acc-0.8938, test loss-0.3795, acc-0.8924\n",
      "Iter-81630, train loss-0.4464, acc-0.8800, valid loss-0.3746, acc-0.8938, test loss-0.3795, acc-0.8924\n",
      "Iter-81640, train loss-0.3589, acc-0.9200, valid loss-0.3746, acc-0.8938, test loss-0.3795, acc-0.8923\n",
      "Iter-81650, train loss-0.4547, acc-0.8600, valid loss-0.3746, acc-0.8940, test loss-0.3795, acc-0.8923\n",
      "Iter-81660, train loss-0.2985, acc-0.9400, valid loss-0.3746, acc-0.8940, test loss-0.3795, acc-0.8923\n",
      "Iter-81670, train loss-0.5736, acc-0.8200, valid loss-0.3745, acc-0.8938, test loss-0.3795, acc-0.8923\n",
      "Iter-81680, train loss-0.2710, acc-0.9200, valid loss-0.3745, acc-0.8940, test loss-0.3795, acc-0.8923\n",
      "Iter-81690, train loss-0.4131, acc-0.8800, valid loss-0.3745, acc-0.8940, test loss-0.3794, acc-0.8923\n",
      "Iter-81700, train loss-0.2433, acc-0.9400, valid loss-0.3745, acc-0.8940, test loss-0.3794, acc-0.8923\n",
      "Iter-81710, train loss-0.4418, acc-0.8600, valid loss-0.3745, acc-0.8942, test loss-0.3794, acc-0.8923\n",
      "Iter-81720, train loss-0.3104, acc-0.9200, valid loss-0.3744, acc-0.8940, test loss-0.3794, acc-0.8924\n",
      "Iter-81730, train loss-0.3858, acc-0.9000, valid loss-0.3744, acc-0.8942, test loss-0.3794, acc-0.8923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-81740, train loss-0.3070, acc-0.9600, valid loss-0.3744, acc-0.8942, test loss-0.3794, acc-0.8923\n",
      "Iter-81750, train loss-0.2957, acc-0.9200, valid loss-0.3744, acc-0.8942, test loss-0.3793, acc-0.8923\n",
      "Iter-81760, train loss-0.2960, acc-0.9400, valid loss-0.3744, acc-0.8942, test loss-0.3793, acc-0.8923\n",
      "Iter-81770, train loss-0.4650, acc-0.8400, valid loss-0.3744, acc-0.8942, test loss-0.3793, acc-0.8923\n",
      "Iter-81780, train loss-0.3899, acc-0.8400, valid loss-0.3744, acc-0.8942, test loss-0.3793, acc-0.8923\n",
      "Iter-81790, train loss-0.4028, acc-0.8800, valid loss-0.3744, acc-0.8940, test loss-0.3793, acc-0.8924\n",
      "Iter-81800, train loss-0.4326, acc-0.9200, valid loss-0.3743, acc-0.8940, test loss-0.3793, acc-0.8924\n",
      "Iter-81810, train loss-0.6128, acc-0.8400, valid loss-0.3743, acc-0.8940, test loss-0.3793, acc-0.8924\n",
      "Iter-81820, train loss-0.3808, acc-0.8800, valid loss-0.3743, acc-0.8940, test loss-0.3792, acc-0.8925\n",
      "Iter-81830, train loss-0.3748, acc-0.8600, valid loss-0.3743, acc-0.8940, test loss-0.3792, acc-0.8925\n",
      "Iter-81840, train loss-0.3880, acc-0.9000, valid loss-0.3743, acc-0.8942, test loss-0.3792, acc-0.8925\n",
      "Iter-81850, train loss-0.5334, acc-0.8600, valid loss-0.3743, acc-0.8940, test loss-0.3792, acc-0.8924\n",
      "Iter-81860, train loss-0.3144, acc-0.9000, valid loss-0.3742, acc-0.8940, test loss-0.3792, acc-0.8925\n",
      "Iter-81870, train loss-0.3959, acc-0.9200, valid loss-0.3742, acc-0.8940, test loss-0.3792, acc-0.8925\n",
      "Iter-81880, train loss-0.2449, acc-0.9600, valid loss-0.3742, acc-0.8940, test loss-0.3791, acc-0.8926\n",
      "Iter-81890, train loss-0.3254, acc-0.9200, valid loss-0.3742, acc-0.8940, test loss-0.3791, acc-0.8926\n",
      "Iter-81900, train loss-0.3602, acc-0.9000, valid loss-0.3742, acc-0.8940, test loss-0.3791, acc-0.8926\n",
      "Iter-81910, train loss-0.4295, acc-0.8200, valid loss-0.3742, acc-0.8940, test loss-0.3791, acc-0.8926\n",
      "Iter-81920, train loss-0.3893, acc-0.8600, valid loss-0.3742, acc-0.8938, test loss-0.3791, acc-0.8926\n",
      "Iter-81930, train loss-0.4396, acc-0.8800, valid loss-0.3742, acc-0.8938, test loss-0.3791, acc-0.8926\n",
      "Iter-81940, train loss-0.3215, acc-0.9000, valid loss-0.3741, acc-0.8938, test loss-0.3791, acc-0.8926\n",
      "Iter-81950, train loss-0.4006, acc-0.8800, valid loss-0.3741, acc-0.8938, test loss-0.3790, acc-0.8926\n",
      "Iter-81960, train loss-0.2636, acc-0.9200, valid loss-0.3741, acc-0.8938, test loss-0.3790, acc-0.8926\n",
      "Iter-81970, train loss-0.4185, acc-0.8600, valid loss-0.3741, acc-0.8938, test loss-0.3790, acc-0.8926\n",
      "Iter-81980, train loss-0.2562, acc-0.9600, valid loss-0.3741, acc-0.8938, test loss-0.3790, acc-0.8925\n",
      "Iter-81990, train loss-0.4904, acc-0.8800, valid loss-0.3740, acc-0.8938, test loss-0.3790, acc-0.8925\n",
      "Iter-82000, train loss-0.4729, acc-0.8400, valid loss-0.3740, acc-0.8938, test loss-0.3790, acc-0.8925\n",
      "Iter-82010, train loss-0.5430, acc-0.8400, valid loss-0.3740, acc-0.8938, test loss-0.3790, acc-0.8925\n",
      "Iter-82020, train loss-0.2803, acc-0.9000, valid loss-0.3740, acc-0.8936, test loss-0.3790, acc-0.8925\n",
      "Iter-82030, train loss-0.4348, acc-0.8800, valid loss-0.3740, acc-0.8934, test loss-0.3789, acc-0.8925\n",
      "Iter-82040, train loss-0.5106, acc-0.8000, valid loss-0.3740, acc-0.8938, test loss-0.3789, acc-0.8925\n",
      "Iter-82050, train loss-0.2650, acc-0.9400, valid loss-0.3740, acc-0.8940, test loss-0.3789, acc-0.8925\n",
      "Iter-82060, train loss-0.2839, acc-0.9600, valid loss-0.3739, acc-0.8942, test loss-0.3789, acc-0.8925\n",
      "Iter-82070, train loss-0.4675, acc-0.8600, valid loss-0.3739, acc-0.8942, test loss-0.3789, acc-0.8925\n",
      "Iter-82080, train loss-0.4313, acc-0.8200, valid loss-0.3739, acc-0.8942, test loss-0.3789, acc-0.8925\n",
      "Iter-82090, train loss-0.4293, acc-0.9400, valid loss-0.3739, acc-0.8940, test loss-0.3789, acc-0.8925\n",
      "Iter-82100, train loss-0.6741, acc-0.8200, valid loss-0.3739, acc-0.8940, test loss-0.3788, acc-0.8926\n",
      "Iter-82110, train loss-0.1863, acc-0.9800, valid loss-0.3738, acc-0.8940, test loss-0.3788, acc-0.8926\n",
      "Iter-82120, train loss-0.2887, acc-0.9000, valid loss-0.3738, acc-0.8942, test loss-0.3788, acc-0.8926\n",
      "Iter-82130, train loss-0.2628, acc-0.9000, valid loss-0.3738, acc-0.8940, test loss-0.3788, acc-0.8927\n",
      "Iter-82140, train loss-0.3085, acc-0.9200, valid loss-0.3738, acc-0.8938, test loss-0.3788, acc-0.8927\n",
      "Iter-82150, train loss-0.5550, acc-0.8800, valid loss-0.3738, acc-0.8942, test loss-0.3788, acc-0.8926\n",
      "Iter-82160, train loss-0.2499, acc-0.9200, valid loss-0.3738, acc-0.8940, test loss-0.3787, acc-0.8926\n",
      "Iter-82170, train loss-0.4800, acc-0.8400, valid loss-0.3737, acc-0.8940, test loss-0.3787, acc-0.8926\n",
      "Iter-82180, train loss-0.4338, acc-0.8600, valid loss-0.3737, acc-0.8946, test loss-0.3787, acc-0.8926\n",
      "Iter-82190, train loss-0.4581, acc-0.8400, valid loss-0.3737, acc-0.8946, test loss-0.3787, acc-0.8926\n",
      "Iter-82200, train loss-0.3784, acc-0.8800, valid loss-0.3737, acc-0.8940, test loss-0.3787, acc-0.8926\n",
      "Iter-82210, train loss-0.3241, acc-0.9200, valid loss-0.3737, acc-0.8942, test loss-0.3786, acc-0.8926\n",
      "Iter-82220, train loss-0.4761, acc-0.8400, valid loss-0.3736, acc-0.8942, test loss-0.3786, acc-0.8926\n",
      "Iter-82230, train loss-0.4939, acc-0.9000, valid loss-0.3736, acc-0.8942, test loss-0.3786, acc-0.8925\n",
      "Iter-82240, train loss-0.4672, acc-0.8600, valid loss-0.3736, acc-0.8942, test loss-0.3786, acc-0.8925\n",
      "Iter-82250, train loss-0.4867, acc-0.8400, valid loss-0.3736, acc-0.8940, test loss-0.3786, acc-0.8925\n",
      "Iter-82260, train loss-0.2496, acc-0.9400, valid loss-0.3736, acc-0.8946, test loss-0.3786, acc-0.8926\n",
      "Iter-82270, train loss-0.4921, acc-0.8400, valid loss-0.3736, acc-0.8942, test loss-0.3785, acc-0.8925\n",
      "Iter-82280, train loss-0.3990, acc-0.8800, valid loss-0.3735, acc-0.8942, test loss-0.3785, acc-0.8926\n",
      "Iter-82290, train loss-0.2877, acc-0.9200, valid loss-0.3735, acc-0.8942, test loss-0.3785, acc-0.8926\n",
      "Iter-82300, train loss-0.3798, acc-0.9000, valid loss-0.3735, acc-0.8944, test loss-0.3785, acc-0.8926\n",
      "Iter-82310, train loss-0.2603, acc-0.9400, valid loss-0.3735, acc-0.8944, test loss-0.3785, acc-0.8926\n",
      "Iter-82320, train loss-0.5561, acc-0.8200, valid loss-0.3735, acc-0.8944, test loss-0.3785, acc-0.8926\n",
      "Iter-82330, train loss-0.4117, acc-0.8800, valid loss-0.3735, acc-0.8942, test loss-0.3785, acc-0.8926\n",
      "Iter-82340, train loss-0.3115, acc-0.9400, valid loss-0.3735, acc-0.8942, test loss-0.3784, acc-0.8927\n",
      "Iter-82350, train loss-0.3004, acc-0.9200, valid loss-0.3734, acc-0.8948, test loss-0.3784, acc-0.8927\n",
      "Iter-82360, train loss-0.4584, acc-0.8400, valid loss-0.3734, acc-0.8948, test loss-0.3784, acc-0.8927\n",
      "Iter-82370, train loss-0.4015, acc-0.8800, valid loss-0.3734, acc-0.8948, test loss-0.3784, acc-0.8927\n",
      "Iter-82380, train loss-0.5055, acc-0.8600, valid loss-0.3734, acc-0.8946, test loss-0.3784, acc-0.8927\n",
      "Iter-82390, train loss-0.2052, acc-0.9400, valid loss-0.3734, acc-0.8946, test loss-0.3783, acc-0.8927\n",
      "Iter-82400, train loss-0.4813, acc-0.8400, valid loss-0.3733, acc-0.8946, test loss-0.3783, acc-0.8927\n",
      "Iter-82410, train loss-0.4004, acc-0.8600, valid loss-0.3733, acc-0.8944, test loss-0.3783, acc-0.8927\n",
      "Iter-82420, train loss-0.3770, acc-0.8600, valid loss-0.3733, acc-0.8944, test loss-0.3783, acc-0.8927\n",
      "Iter-82430, train loss-0.4033, acc-0.8600, valid loss-0.3733, acc-0.8946, test loss-0.3783, acc-0.8928\n",
      "Iter-82440, train loss-0.3548, acc-0.8800, valid loss-0.3733, acc-0.8944, test loss-0.3783, acc-0.8928\n",
      "Iter-82450, train loss-0.2908, acc-0.9000, valid loss-0.3733, acc-0.8944, test loss-0.3783, acc-0.8928\n",
      "Iter-82460, train loss-0.3780, acc-0.8800, valid loss-0.3733, acc-0.8946, test loss-0.3782, acc-0.8928\n",
      "Iter-82470, train loss-0.2291, acc-0.9800, valid loss-0.3732, acc-0.8946, test loss-0.3782, acc-0.8928\n",
      "Iter-82480, train loss-0.5580, acc-0.8400, valid loss-0.3732, acc-0.8948, test loss-0.3782, acc-0.8928\n",
      "Iter-82490, train loss-0.3086, acc-0.9200, valid loss-0.3732, acc-0.8946, test loss-0.3782, acc-0.8928\n",
      "Iter-82500, train loss-0.4156, acc-0.9000, valid loss-0.3732, acc-0.8946, test loss-0.3782, acc-0.8928\n",
      "Iter-82510, train loss-0.3349, acc-0.9000, valid loss-0.3732, acc-0.8946, test loss-0.3782, acc-0.8928\n",
      "Iter-82520, train loss-0.2457, acc-0.9200, valid loss-0.3732, acc-0.8946, test loss-0.3781, acc-0.8928\n",
      "Iter-82530, train loss-0.4580, acc-0.8400, valid loss-0.3732, acc-0.8948, test loss-0.3781, acc-0.8928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-82540, train loss-0.5345, acc-0.9000, valid loss-0.3731, acc-0.8948, test loss-0.3781, acc-0.8928\n",
      "Iter-82550, train loss-0.2177, acc-0.9200, valid loss-0.3731, acc-0.8946, test loss-0.3781, acc-0.8928\n",
      "Iter-82560, train loss-0.4008, acc-0.9000, valid loss-0.3731, acc-0.8950, test loss-0.3781, acc-0.8928\n",
      "Iter-82570, train loss-0.2177, acc-0.9800, valid loss-0.3731, acc-0.8946, test loss-0.3781, acc-0.8927\n",
      "Iter-82580, train loss-0.4206, acc-0.8600, valid loss-0.3731, acc-0.8946, test loss-0.3781, acc-0.8927\n",
      "Iter-82590, train loss-0.3465, acc-0.9000, valid loss-0.3731, acc-0.8948, test loss-0.3781, acc-0.8927\n",
      "Iter-82600, train loss-0.4433, acc-0.9000, valid loss-0.3731, acc-0.8948, test loss-0.3780, acc-0.8927\n",
      "Iter-82610, train loss-0.4528, acc-0.8000, valid loss-0.3731, acc-0.8948, test loss-0.3780, acc-0.8927\n",
      "Iter-82620, train loss-0.2744, acc-0.9200, valid loss-0.3730, acc-0.8948, test loss-0.3780, acc-0.8927\n",
      "Iter-82630, train loss-0.2932, acc-0.9000, valid loss-0.3730, acc-0.8948, test loss-0.3780, acc-0.8927\n",
      "Iter-82640, train loss-0.6028, acc-0.7800, valid loss-0.3730, acc-0.8946, test loss-0.3780, acc-0.8927\n",
      "Iter-82650, train loss-0.4688, acc-0.8200, valid loss-0.3730, acc-0.8946, test loss-0.3780, acc-0.8927\n",
      "Iter-82660, train loss-0.4097, acc-0.9000, valid loss-0.3730, acc-0.8946, test loss-0.3780, acc-0.8927\n",
      "Iter-82670, train loss-0.2699, acc-0.9400, valid loss-0.3730, acc-0.8946, test loss-0.3779, acc-0.8927\n",
      "Iter-82680, train loss-0.4375, acc-0.8800, valid loss-0.3730, acc-0.8944, test loss-0.3779, acc-0.8928\n",
      "Iter-82690, train loss-0.4558, acc-0.8200, valid loss-0.3729, acc-0.8944, test loss-0.3779, acc-0.8928\n",
      "Iter-82700, train loss-0.5926, acc-0.8400, valid loss-0.3729, acc-0.8944, test loss-0.3779, acc-0.8928\n",
      "Iter-82710, train loss-0.4791, acc-0.8200, valid loss-0.3729, acc-0.8944, test loss-0.3779, acc-0.8928\n",
      "Iter-82720, train loss-0.2694, acc-0.8800, valid loss-0.3729, acc-0.8946, test loss-0.3779, acc-0.8928\n",
      "Iter-82730, train loss-0.5240, acc-0.8800, valid loss-0.3729, acc-0.8948, test loss-0.3779, acc-0.8928\n",
      "Iter-82740, train loss-0.4705, acc-0.8600, valid loss-0.3729, acc-0.8952, test loss-0.3778, acc-0.8928\n",
      "Iter-82750, train loss-0.4375, acc-0.8400, valid loss-0.3729, acc-0.8952, test loss-0.3778, acc-0.8928\n",
      "Iter-82760, train loss-0.3038, acc-0.8800, valid loss-0.3728, acc-0.8952, test loss-0.3778, acc-0.8928\n",
      "Iter-82770, train loss-0.3113, acc-0.9000, valid loss-0.3728, acc-0.8952, test loss-0.3778, acc-0.8928\n",
      "Iter-82780, train loss-0.3782, acc-0.9200, valid loss-0.3728, acc-0.8952, test loss-0.3778, acc-0.8928\n",
      "Iter-82790, train loss-0.5016, acc-0.8800, valid loss-0.3728, acc-0.8952, test loss-0.3778, acc-0.8928\n",
      "Iter-82800, train loss-0.1897, acc-0.9800, valid loss-0.3728, acc-0.8946, test loss-0.3777, acc-0.8928\n",
      "Iter-82810, train loss-0.3424, acc-0.8800, valid loss-0.3728, acc-0.8950, test loss-0.3777, acc-0.8928\n",
      "Iter-82820, train loss-0.3489, acc-0.9200, valid loss-0.3728, acc-0.8948, test loss-0.3777, acc-0.8928\n",
      "Iter-82830, train loss-0.3296, acc-0.9000, valid loss-0.3727, acc-0.8948, test loss-0.3777, acc-0.8928\n",
      "Iter-82840, train loss-0.5125, acc-0.8200, valid loss-0.3727, acc-0.8950, test loss-0.3777, acc-0.8928\n",
      "Iter-82850, train loss-0.3794, acc-0.9200, valid loss-0.3727, acc-0.8950, test loss-0.3777, acc-0.8928\n",
      "Iter-82860, train loss-0.2427, acc-0.9400, valid loss-0.3727, acc-0.8948, test loss-0.3776, acc-0.8928\n",
      "Iter-82870, train loss-0.4872, acc-0.8600, valid loss-0.3727, acc-0.8948, test loss-0.3776, acc-0.8928\n",
      "Iter-82880, train loss-0.4643, acc-0.8600, valid loss-0.3726, acc-0.8948, test loss-0.3776, acc-0.8928\n",
      "Iter-82890, train loss-0.5065, acc-0.8200, valid loss-0.3726, acc-0.8950, test loss-0.3776, acc-0.8928\n",
      "Iter-82900, train loss-0.3773, acc-0.8800, valid loss-0.3726, acc-0.8948, test loss-0.3776, acc-0.8928\n",
      "Iter-82910, train loss-0.2060, acc-0.9600, valid loss-0.3726, acc-0.8948, test loss-0.3776, acc-0.8928\n",
      "Iter-82920, train loss-0.4652, acc-0.9000, valid loss-0.3726, acc-0.8948, test loss-0.3775, acc-0.8928\n",
      "Iter-82930, train loss-0.4891, acc-0.8600, valid loss-0.3725, acc-0.8948, test loss-0.3775, acc-0.8928\n",
      "Iter-82940, train loss-0.3794, acc-0.8800, valid loss-0.3725, acc-0.8948, test loss-0.3775, acc-0.8929\n",
      "Iter-82950, train loss-0.3595, acc-0.9000, valid loss-0.3725, acc-0.8948, test loss-0.3775, acc-0.8928\n",
      "Iter-82960, train loss-0.3765, acc-0.9200, valid loss-0.3725, acc-0.8948, test loss-0.3775, acc-0.8928\n",
      "Iter-82970, train loss-0.5467, acc-0.8400, valid loss-0.3725, acc-0.8948, test loss-0.3775, acc-0.8928\n",
      "Iter-82980, train loss-0.3550, acc-0.9000, valid loss-0.3724, acc-0.8952, test loss-0.3774, acc-0.8928\n",
      "Iter-82990, train loss-0.3524, acc-0.9200, valid loss-0.3724, acc-0.8956, test loss-0.3774, acc-0.8928\n",
      "Iter-83000, train loss-0.5062, acc-0.8400, valid loss-0.3724, acc-0.8952, test loss-0.3774, acc-0.8928\n",
      "Iter-83010, train loss-0.3628, acc-0.8800, valid loss-0.3724, acc-0.8952, test loss-0.3774, acc-0.8928\n",
      "Iter-83020, train loss-0.1773, acc-0.9600, valid loss-0.3724, acc-0.8948, test loss-0.3774, acc-0.8928\n",
      "Iter-83030, train loss-0.4097, acc-0.9200, valid loss-0.3724, acc-0.8948, test loss-0.3774, acc-0.8928\n",
      "Iter-83040, train loss-0.5262, acc-0.8800, valid loss-0.3723, acc-0.8950, test loss-0.3774, acc-0.8928\n",
      "Iter-83050, train loss-0.3590, acc-0.9000, valid loss-0.3723, acc-0.8950, test loss-0.3774, acc-0.8928\n",
      "Iter-83060, train loss-0.3177, acc-0.9200, valid loss-0.3723, acc-0.8950, test loss-0.3773, acc-0.8928\n",
      "Iter-83070, train loss-0.4357, acc-0.9000, valid loss-0.3723, acc-0.8948, test loss-0.3773, acc-0.8928\n",
      "Iter-83080, train loss-0.4536, acc-0.8800, valid loss-0.3723, acc-0.8948, test loss-0.3773, acc-0.8928\n",
      "Iter-83090, train loss-0.3498, acc-0.9000, valid loss-0.3723, acc-0.8948, test loss-0.3773, acc-0.8928\n",
      "Iter-83100, train loss-0.3903, acc-0.8800, valid loss-0.3722, acc-0.8950, test loss-0.3773, acc-0.8928\n",
      "Iter-83110, train loss-0.4806, acc-0.8200, valid loss-0.3722, acc-0.8950, test loss-0.3772, acc-0.8928\n",
      "Iter-83120, train loss-0.3597, acc-0.8600, valid loss-0.3722, acc-0.8950, test loss-0.3772, acc-0.8928\n",
      "Iter-83130, train loss-0.3354, acc-0.9400, valid loss-0.3722, acc-0.8950, test loss-0.3772, acc-0.8928\n",
      "Iter-83140, train loss-0.1924, acc-0.9400, valid loss-0.3722, acc-0.8950, test loss-0.3772, acc-0.8928\n",
      "Iter-83150, train loss-0.5798, acc-0.8200, valid loss-0.3722, acc-0.8950, test loss-0.3772, acc-0.8928\n",
      "Iter-83160, train loss-0.3071, acc-0.8800, valid loss-0.3721, acc-0.8950, test loss-0.3772, acc-0.8928\n",
      "Iter-83170, train loss-0.2617, acc-0.9400, valid loss-0.3721, acc-0.8952, test loss-0.3772, acc-0.8928\n",
      "Iter-83180, train loss-0.3375, acc-0.8800, valid loss-0.3721, acc-0.8950, test loss-0.3771, acc-0.8928\n",
      "Iter-83190, train loss-0.4923, acc-0.8400, valid loss-0.3721, acc-0.8948, test loss-0.3771, acc-0.8928\n",
      "Iter-83200, train loss-0.3156, acc-0.9200, valid loss-0.3721, acc-0.8948, test loss-0.3771, acc-0.8927\n",
      "Iter-83210, train loss-0.4180, acc-0.8400, valid loss-0.3721, acc-0.8948, test loss-0.3771, acc-0.8927\n",
      "Iter-83220, train loss-0.3588, acc-0.9400, valid loss-0.3720, acc-0.8948, test loss-0.3771, acc-0.8928\n",
      "Iter-83230, train loss-0.4228, acc-0.8800, valid loss-0.3720, acc-0.8950, test loss-0.3771, acc-0.8927\n",
      "Iter-83240, train loss-0.2469, acc-0.9600, valid loss-0.3720, acc-0.8950, test loss-0.3770, acc-0.8927\n",
      "Iter-83250, train loss-0.4686, acc-0.9200, valid loss-0.3720, acc-0.8948, test loss-0.3770, acc-0.8927\n",
      "Iter-83260, train loss-0.2628, acc-0.9200, valid loss-0.3720, acc-0.8948, test loss-0.3770, acc-0.8927\n",
      "Iter-83270, train loss-0.4119, acc-0.8800, valid loss-0.3720, acc-0.8950, test loss-0.3770, acc-0.8927\n",
      "Iter-83280, train loss-0.5142, acc-0.8600, valid loss-0.3720, acc-0.8950, test loss-0.3770, acc-0.8928\n",
      "Iter-83290, train loss-0.3390, acc-0.8600, valid loss-0.3719, acc-0.8950, test loss-0.3770, acc-0.8928\n",
      "Iter-83300, train loss-0.5559, acc-0.8400, valid loss-0.3719, acc-0.8952, test loss-0.3770, acc-0.8929\n",
      "Iter-83310, train loss-0.3789, acc-0.8800, valid loss-0.3719, acc-0.8952, test loss-0.3770, acc-0.8929\n",
      "Iter-83320, train loss-0.5188, acc-0.8800, valid loss-0.3719, acc-0.8952, test loss-0.3769, acc-0.8929\n",
      "Iter-83330, train loss-0.3931, acc-0.8400, valid loss-0.3719, acc-0.8950, test loss-0.3769, acc-0.8929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-83340, train loss-0.3798, acc-0.9600, valid loss-0.3719, acc-0.8948, test loss-0.3769, acc-0.8929\n",
      "Iter-83350, train loss-0.5800, acc-0.8400, valid loss-0.3719, acc-0.8948, test loss-0.3769, acc-0.8929\n",
      "Iter-83360, train loss-0.3621, acc-0.9000, valid loss-0.3719, acc-0.8946, test loss-0.3769, acc-0.8930\n",
      "Iter-83370, train loss-0.3357, acc-0.9400, valid loss-0.3718, acc-0.8948, test loss-0.3769, acc-0.8929\n",
      "Iter-83380, train loss-0.4584, acc-0.8800, valid loss-0.3718, acc-0.8948, test loss-0.3768, acc-0.8928\n",
      "Iter-83390, train loss-0.3315, acc-0.9400, valid loss-0.3718, acc-0.8950, test loss-0.3768, acc-0.8927\n",
      "Iter-83400, train loss-0.2534, acc-0.9200, valid loss-0.3718, acc-0.8952, test loss-0.3768, acc-0.8928\n",
      "Iter-83410, train loss-0.2338, acc-0.9600, valid loss-0.3718, acc-0.8950, test loss-0.3768, acc-0.8928\n",
      "Iter-83420, train loss-0.2234, acc-0.9000, valid loss-0.3718, acc-0.8948, test loss-0.3768, acc-0.8928\n",
      "Iter-83430, train loss-0.5087, acc-0.8800, valid loss-0.3717, acc-0.8950, test loss-0.3768, acc-0.8930\n",
      "Iter-83440, train loss-0.3643, acc-0.8800, valid loss-0.3717, acc-0.8952, test loss-0.3767, acc-0.8929\n",
      "Iter-83450, train loss-0.3301, acc-0.9200, valid loss-0.3717, acc-0.8948, test loss-0.3767, acc-0.8929\n",
      "Iter-83460, train loss-0.3124, acc-0.9000, valid loss-0.3717, acc-0.8950, test loss-0.3767, acc-0.8929\n",
      "Iter-83470, train loss-0.2894, acc-0.9200, valid loss-0.3717, acc-0.8950, test loss-0.3767, acc-0.8929\n",
      "Iter-83480, train loss-0.4339, acc-0.8400, valid loss-0.3716, acc-0.8950, test loss-0.3767, acc-0.8929\n",
      "Iter-83490, train loss-0.2783, acc-0.9200, valid loss-0.3716, acc-0.8950, test loss-0.3767, acc-0.8930\n",
      "Iter-83500, train loss-0.5220, acc-0.8400, valid loss-0.3716, acc-0.8950, test loss-0.3767, acc-0.8929\n",
      "Iter-83510, train loss-0.3586, acc-0.9000, valid loss-0.3716, acc-0.8952, test loss-0.3767, acc-0.8931\n",
      "Iter-83520, train loss-0.3597, acc-0.9000, valid loss-0.3716, acc-0.8950, test loss-0.3766, acc-0.8930\n",
      "Iter-83530, train loss-0.2010, acc-0.9400, valid loss-0.3716, acc-0.8948, test loss-0.3766, acc-0.8929\n",
      "Iter-83540, train loss-0.4456, acc-0.9200, valid loss-0.3715, acc-0.8950, test loss-0.3766, acc-0.8930\n",
      "Iter-83550, train loss-0.3625, acc-0.9200, valid loss-0.3715, acc-0.8950, test loss-0.3766, acc-0.8930\n",
      "Iter-83560, train loss-0.7035, acc-0.7800, valid loss-0.3715, acc-0.8950, test loss-0.3766, acc-0.8929\n",
      "Iter-83570, train loss-0.4747, acc-0.8800, valid loss-0.3715, acc-0.8952, test loss-0.3766, acc-0.8931\n",
      "Iter-83580, train loss-0.3031, acc-0.9400, valid loss-0.3715, acc-0.8952, test loss-0.3766, acc-0.8930\n",
      "Iter-83590, train loss-0.4146, acc-0.8800, valid loss-0.3715, acc-0.8952, test loss-0.3766, acc-0.8929\n",
      "Iter-83600, train loss-0.4206, acc-0.8400, valid loss-0.3714, acc-0.8952, test loss-0.3765, acc-0.8931\n",
      "Iter-83610, train loss-0.4917, acc-0.8800, valid loss-0.3714, acc-0.8952, test loss-0.3765, acc-0.8931\n",
      "Iter-83620, train loss-0.5619, acc-0.8200, valid loss-0.3714, acc-0.8952, test loss-0.3765, acc-0.8931\n",
      "Iter-83630, train loss-0.2257, acc-0.9600, valid loss-0.3714, acc-0.8952, test loss-0.3765, acc-0.8931\n",
      "Iter-83640, train loss-0.4234, acc-0.8800, valid loss-0.3714, acc-0.8952, test loss-0.3765, acc-0.8930\n",
      "Iter-83650, train loss-0.4855, acc-0.8200, valid loss-0.3714, acc-0.8954, test loss-0.3765, acc-0.8929\n",
      "Iter-83660, train loss-0.3164, acc-0.9200, valid loss-0.3713, acc-0.8954, test loss-0.3765, acc-0.8929\n",
      "Iter-83670, train loss-0.4217, acc-0.9000, valid loss-0.3713, acc-0.8954, test loss-0.3765, acc-0.8929\n",
      "Iter-83680, train loss-0.5580, acc-0.9200, valid loss-0.3713, acc-0.8954, test loss-0.3764, acc-0.8930\n",
      "Iter-83690, train loss-0.1438, acc-0.9800, valid loss-0.3713, acc-0.8954, test loss-0.3764, acc-0.8930\n",
      "Iter-83700, train loss-0.3166, acc-0.9200, valid loss-0.3713, acc-0.8954, test loss-0.3764, acc-0.8930\n",
      "Iter-83710, train loss-0.2731, acc-0.9400, valid loss-0.3712, acc-0.8952, test loss-0.3764, acc-0.8930\n",
      "Iter-83720, train loss-0.2733, acc-0.9400, valid loss-0.3712, acc-0.8952, test loss-0.3764, acc-0.8930\n",
      "Iter-83730, train loss-0.5476, acc-0.8600, valid loss-0.3712, acc-0.8952, test loss-0.3764, acc-0.8931\n",
      "Iter-83740, train loss-0.4646, acc-0.8600, valid loss-0.3712, acc-0.8952, test loss-0.3764, acc-0.8931\n",
      "Iter-83750, train loss-0.4859, acc-0.9000, valid loss-0.3712, acc-0.8952, test loss-0.3763, acc-0.8931\n",
      "Iter-83760, train loss-0.1683, acc-0.9600, valid loss-0.3711, acc-0.8952, test loss-0.3763, acc-0.8931\n",
      "Iter-83770, train loss-0.4086, acc-0.8800, valid loss-0.3711, acc-0.8952, test loss-0.3763, acc-0.8931\n",
      "Iter-83780, train loss-0.2775, acc-0.9200, valid loss-0.3711, acc-0.8952, test loss-0.3763, acc-0.8931\n",
      "Iter-83790, train loss-0.3606, acc-0.9400, valid loss-0.3711, acc-0.8952, test loss-0.3762, acc-0.8931\n",
      "Iter-83800, train loss-0.3076, acc-0.9000, valid loss-0.3711, acc-0.8952, test loss-0.3762, acc-0.8931\n",
      "Iter-83810, train loss-0.3490, acc-0.9200, valid loss-0.3711, acc-0.8952, test loss-0.3762, acc-0.8931\n",
      "Iter-83820, train loss-0.6294, acc-0.8200, valid loss-0.3710, acc-0.8954, test loss-0.3762, acc-0.8931\n",
      "Iter-83830, train loss-0.6410, acc-0.7800, valid loss-0.3710, acc-0.8954, test loss-0.3762, acc-0.8931\n",
      "Iter-83840, train loss-0.3918, acc-0.8600, valid loss-0.3710, acc-0.8952, test loss-0.3762, acc-0.8931\n",
      "Iter-83850, train loss-0.3929, acc-0.9200, valid loss-0.3710, acc-0.8954, test loss-0.3762, acc-0.8931\n",
      "Iter-83860, train loss-0.4318, acc-0.8600, valid loss-0.3710, acc-0.8954, test loss-0.3762, acc-0.8931\n",
      "Iter-83870, train loss-0.3248, acc-0.8800, valid loss-0.3710, acc-0.8952, test loss-0.3761, acc-0.8931\n",
      "Iter-83880, train loss-0.3147, acc-0.9200, valid loss-0.3709, acc-0.8954, test loss-0.3761, acc-0.8931\n",
      "Iter-83890, train loss-0.4336, acc-0.8600, valid loss-0.3709, acc-0.8954, test loss-0.3761, acc-0.8931\n",
      "Iter-83900, train loss-0.3097, acc-0.8800, valid loss-0.3709, acc-0.8954, test loss-0.3761, acc-0.8931\n",
      "Iter-83910, train loss-0.3701, acc-0.9000, valid loss-0.3709, acc-0.8954, test loss-0.3761, acc-0.8931\n",
      "Iter-83920, train loss-0.2770, acc-0.9400, valid loss-0.3709, acc-0.8954, test loss-0.3760, acc-0.8931\n",
      "Iter-83930, train loss-0.3708, acc-0.8800, valid loss-0.3708, acc-0.8954, test loss-0.3760, acc-0.8932\n",
      "Iter-83940, train loss-0.4158, acc-0.8600, valid loss-0.3708, acc-0.8952, test loss-0.3760, acc-0.8932\n",
      "Iter-83950, train loss-0.4827, acc-0.9200, valid loss-0.3708, acc-0.8952, test loss-0.3760, acc-0.8932\n",
      "Iter-83960, train loss-0.3673, acc-0.8800, valid loss-0.3708, acc-0.8954, test loss-0.3760, acc-0.8932\n",
      "Iter-83970, train loss-0.3935, acc-0.9000, valid loss-0.3708, acc-0.8954, test loss-0.3759, acc-0.8932\n",
      "Iter-83980, train loss-0.3595, acc-0.9400, valid loss-0.3707, acc-0.8952, test loss-0.3759, acc-0.8932\n",
      "Iter-83990, train loss-0.4057, acc-0.9200, valid loss-0.3707, acc-0.8952, test loss-0.3759, acc-0.8932\n",
      "Iter-84000, train loss-0.3005, acc-0.9400, valid loss-0.3707, acc-0.8954, test loss-0.3759, acc-0.8932\n",
      "Iter-84010, train loss-0.3617, acc-0.8800, valid loss-0.3707, acc-0.8954, test loss-0.3759, acc-0.8932\n",
      "Iter-84020, train loss-0.4310, acc-0.9000, valid loss-0.3707, acc-0.8954, test loss-0.3759, acc-0.8932\n",
      "Iter-84030, train loss-0.5608, acc-0.9000, valid loss-0.3707, acc-0.8954, test loss-0.3758, acc-0.8932\n",
      "Iter-84040, train loss-0.2919, acc-0.9200, valid loss-0.3707, acc-0.8954, test loss-0.3758, acc-0.8932\n",
      "Iter-84050, train loss-0.4822, acc-0.9200, valid loss-0.3706, acc-0.8954, test loss-0.3758, acc-0.8931\n",
      "Iter-84060, train loss-0.2888, acc-0.9200, valid loss-0.3706, acc-0.8954, test loss-0.3758, acc-0.8931\n",
      "Iter-84070, train loss-0.5797, acc-0.8400, valid loss-0.3706, acc-0.8954, test loss-0.3758, acc-0.8931\n",
      "Iter-84080, train loss-0.3048, acc-0.9200, valid loss-0.3706, acc-0.8954, test loss-0.3758, acc-0.8932\n",
      "Iter-84090, train loss-0.2792, acc-0.9400, valid loss-0.3706, acc-0.8954, test loss-0.3758, acc-0.8932\n",
      "Iter-84100, train loss-0.4861, acc-0.8400, valid loss-0.3706, acc-0.8954, test loss-0.3757, acc-0.8932\n",
      "Iter-84110, train loss-0.2886, acc-0.9600, valid loss-0.3706, acc-0.8954, test loss-0.3757, acc-0.8930\n",
      "Iter-84120, train loss-0.5522, acc-0.8600, valid loss-0.3706, acc-0.8954, test loss-0.3757, acc-0.8931\n",
      "Iter-84130, train loss-0.4338, acc-0.9000, valid loss-0.3705, acc-0.8954, test loss-0.3757, acc-0.8931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-84140, train loss-0.4111, acc-0.8800, valid loss-0.3705, acc-0.8954, test loss-0.3757, acc-0.8931\n",
      "Iter-84150, train loss-0.4153, acc-0.9000, valid loss-0.3705, acc-0.8954, test loss-0.3757, acc-0.8931\n",
      "Iter-84160, train loss-0.2587, acc-0.9600, valid loss-0.3705, acc-0.8956, test loss-0.3756, acc-0.8932\n",
      "Iter-84170, train loss-0.4044, acc-0.8400, valid loss-0.3705, acc-0.8956, test loss-0.3756, acc-0.8932\n",
      "Iter-84180, train loss-0.3830, acc-0.8600, valid loss-0.3705, acc-0.8956, test loss-0.3756, acc-0.8932\n",
      "Iter-84190, train loss-0.4727, acc-0.8600, valid loss-0.3704, acc-0.8956, test loss-0.3756, acc-0.8932\n",
      "Iter-84200, train loss-0.6397, acc-0.8200, valid loss-0.3704, acc-0.8956, test loss-0.3756, acc-0.8932\n",
      "Iter-84210, train loss-0.6462, acc-0.8200, valid loss-0.3704, acc-0.8956, test loss-0.3756, acc-0.8932\n",
      "Iter-84220, train loss-0.3214, acc-0.9200, valid loss-0.3704, acc-0.8956, test loss-0.3755, acc-0.8932\n",
      "Iter-84230, train loss-0.5214, acc-0.8000, valid loss-0.3704, acc-0.8956, test loss-0.3755, acc-0.8932\n",
      "Iter-84240, train loss-0.5631, acc-0.8400, valid loss-0.3704, acc-0.8956, test loss-0.3755, acc-0.8932\n",
      "Iter-84250, train loss-0.2296, acc-0.9600, valid loss-0.3703, acc-0.8956, test loss-0.3755, acc-0.8932\n",
      "Iter-84260, train loss-0.3206, acc-0.9200, valid loss-0.3703, acc-0.8956, test loss-0.3755, acc-0.8932\n",
      "Iter-84270, train loss-0.3340, acc-0.9000, valid loss-0.3703, acc-0.8956, test loss-0.3755, acc-0.8932\n",
      "Iter-84280, train loss-0.5345, acc-0.8000, valid loss-0.3703, acc-0.8956, test loss-0.3754, acc-0.8932\n",
      "Iter-84290, train loss-0.2375, acc-0.9200, valid loss-0.3703, acc-0.8956, test loss-0.3754, acc-0.8932\n",
      "Iter-84300, train loss-0.3501, acc-0.9200, valid loss-0.3703, acc-0.8956, test loss-0.3754, acc-0.8932\n",
      "Iter-84310, train loss-0.4489, acc-0.9000, valid loss-0.3703, acc-0.8956, test loss-0.3754, acc-0.8932\n",
      "Iter-84320, train loss-0.5309, acc-0.8200, valid loss-0.3703, acc-0.8956, test loss-0.3754, acc-0.8932\n",
      "Iter-84330, train loss-0.2565, acc-0.9200, valid loss-0.3702, acc-0.8956, test loss-0.3754, acc-0.8932\n",
      "Iter-84340, train loss-0.2618, acc-0.9200, valid loss-0.3702, acc-0.8956, test loss-0.3754, acc-0.8932\n",
      "Iter-84350, train loss-0.3399, acc-0.9000, valid loss-0.3702, acc-0.8956, test loss-0.3754, acc-0.8932\n",
      "Iter-84360, train loss-0.3890, acc-0.8800, valid loss-0.3702, acc-0.8956, test loss-0.3754, acc-0.8932\n",
      "Iter-84370, train loss-0.3607, acc-0.9000, valid loss-0.3702, acc-0.8956, test loss-0.3753, acc-0.8933\n",
      "Iter-84380, train loss-0.4717, acc-0.9000, valid loss-0.3702, acc-0.8956, test loss-0.3753, acc-0.8933\n",
      "Iter-84390, train loss-0.2727, acc-0.9600, valid loss-0.3702, acc-0.8956, test loss-0.3753, acc-0.8933\n",
      "Iter-84400, train loss-0.3562, acc-0.8800, valid loss-0.3701, acc-0.8956, test loss-0.3753, acc-0.8931\n",
      "Iter-84410, train loss-0.3751, acc-0.8800, valid loss-0.3701, acc-0.8956, test loss-0.3753, acc-0.8933\n",
      "Iter-84420, train loss-0.2113, acc-0.9600, valid loss-0.3701, acc-0.8956, test loss-0.3753, acc-0.8932\n",
      "Iter-84430, train loss-0.2795, acc-0.9200, valid loss-0.3701, acc-0.8956, test loss-0.3752, acc-0.8933\n",
      "Iter-84440, train loss-0.2014, acc-0.9600, valid loss-0.3701, acc-0.8956, test loss-0.3752, acc-0.8933\n",
      "Iter-84450, train loss-0.5365, acc-0.8600, valid loss-0.3701, acc-0.8956, test loss-0.3752, acc-0.8933\n",
      "Iter-84460, train loss-0.3517, acc-0.9200, valid loss-0.3701, acc-0.8956, test loss-0.3752, acc-0.8934\n",
      "Iter-84470, train loss-0.5150, acc-0.9000, valid loss-0.3700, acc-0.8956, test loss-0.3752, acc-0.8933\n",
      "Iter-84480, train loss-0.5123, acc-0.8600, valid loss-0.3700, acc-0.8956, test loss-0.3752, acc-0.8931\n",
      "Iter-84490, train loss-0.6336, acc-0.8400, valid loss-0.3700, acc-0.8956, test loss-0.3751, acc-0.8932\n",
      "Iter-84500, train loss-0.5805, acc-0.8200, valid loss-0.3700, acc-0.8956, test loss-0.3751, acc-0.8931\n",
      "Iter-84510, train loss-0.3380, acc-0.8800, valid loss-0.3700, acc-0.8956, test loss-0.3751, acc-0.8931\n",
      "Iter-84520, train loss-0.5177, acc-0.8600, valid loss-0.3700, acc-0.8956, test loss-0.3751, acc-0.8931\n",
      "Iter-84530, train loss-0.3636, acc-0.9000, valid loss-0.3699, acc-0.8958, test loss-0.3751, acc-0.8932\n",
      "Iter-84540, train loss-0.4136, acc-0.8600, valid loss-0.3699, acc-0.8956, test loss-0.3751, acc-0.8930\n",
      "Iter-84550, train loss-0.3088, acc-0.9600, valid loss-0.3699, acc-0.8960, test loss-0.3750, acc-0.8931\n",
      "Iter-84560, train loss-0.3570, acc-0.8600, valid loss-0.3699, acc-0.8960, test loss-0.3750, acc-0.8930\n",
      "Iter-84570, train loss-0.5963, acc-0.8600, valid loss-0.3699, acc-0.8958, test loss-0.3750, acc-0.8930\n",
      "Iter-84580, train loss-0.2420, acc-0.9400, valid loss-0.3699, acc-0.8960, test loss-0.3750, acc-0.8929\n",
      "Iter-84590, train loss-0.3385, acc-0.9200, valid loss-0.3699, acc-0.8960, test loss-0.3750, acc-0.8929\n",
      "Iter-84600, train loss-0.3858, acc-0.8200, valid loss-0.3699, acc-0.8960, test loss-0.3750, acc-0.8930\n",
      "Iter-84610, train loss-0.6326, acc-0.8400, valid loss-0.3698, acc-0.8958, test loss-0.3749, acc-0.8929\n",
      "Iter-84620, train loss-0.4116, acc-0.8600, valid loss-0.3698, acc-0.8960, test loss-0.3749, acc-0.8929\n",
      "Iter-84630, train loss-0.3379, acc-0.8800, valid loss-0.3698, acc-0.8960, test loss-0.3749, acc-0.8928\n",
      "Iter-84640, train loss-0.7305, acc-0.7600, valid loss-0.3698, acc-0.8960, test loss-0.3749, acc-0.8929\n",
      "Iter-84650, train loss-0.4119, acc-0.8800, valid loss-0.3698, acc-0.8960, test loss-0.3749, acc-0.8928\n",
      "Iter-84660, train loss-0.2931, acc-0.9200, valid loss-0.3697, acc-0.8960, test loss-0.3749, acc-0.8929\n",
      "Iter-84670, train loss-0.3166, acc-0.9000, valid loss-0.3697, acc-0.8960, test loss-0.3748, acc-0.8929\n",
      "Iter-84680, train loss-0.6165, acc-0.8400, valid loss-0.3697, acc-0.8960, test loss-0.3748, acc-0.8930\n",
      "Iter-84690, train loss-0.3195, acc-0.9200, valid loss-0.3697, acc-0.8960, test loss-0.3748, acc-0.8930\n",
      "Iter-84700, train loss-0.3657, acc-0.8800, valid loss-0.3697, acc-0.8960, test loss-0.3748, acc-0.8929\n",
      "Iter-84710, train loss-0.3170, acc-0.9000, valid loss-0.3697, acc-0.8960, test loss-0.3748, acc-0.8929\n",
      "Iter-84720, train loss-0.3810, acc-0.8600, valid loss-0.3696, acc-0.8960, test loss-0.3748, acc-0.8929\n",
      "Iter-84730, train loss-0.5716, acc-0.8400, valid loss-0.3696, acc-0.8960, test loss-0.3748, acc-0.8929\n",
      "Iter-84740, train loss-0.4086, acc-0.9000, valid loss-0.3696, acc-0.8960, test loss-0.3747, acc-0.8929\n",
      "Iter-84750, train loss-0.6138, acc-0.7800, valid loss-0.3696, acc-0.8960, test loss-0.3747, acc-0.8929\n",
      "Iter-84760, train loss-0.2417, acc-0.9400, valid loss-0.3696, acc-0.8960, test loss-0.3747, acc-0.8929\n",
      "Iter-84770, train loss-0.5401, acc-0.8800, valid loss-0.3696, acc-0.8960, test loss-0.3747, acc-0.8929\n",
      "Iter-84780, train loss-0.4993, acc-0.8400, valid loss-0.3696, acc-0.8960, test loss-0.3747, acc-0.8929\n",
      "Iter-84790, train loss-0.2818, acc-0.9400, valid loss-0.3695, acc-0.8960, test loss-0.3747, acc-0.8929\n",
      "Iter-84800, train loss-0.4582, acc-0.8400, valid loss-0.3695, acc-0.8960, test loss-0.3747, acc-0.8929\n",
      "Iter-84810, train loss-0.6144, acc-0.8400, valid loss-0.3695, acc-0.8960, test loss-0.3746, acc-0.8929\n",
      "Iter-84820, train loss-0.4367, acc-0.8800, valid loss-0.3695, acc-0.8960, test loss-0.3746, acc-0.8929\n",
      "Iter-84830, train loss-0.2145, acc-0.9200, valid loss-0.3695, acc-0.8960, test loss-0.3746, acc-0.8929\n",
      "Iter-84840, train loss-0.3327, acc-0.9000, valid loss-0.3695, acc-0.8960, test loss-0.3746, acc-0.8929\n",
      "Iter-84850, train loss-0.5915, acc-0.8600, valid loss-0.3694, acc-0.8962, test loss-0.3746, acc-0.8929\n",
      "Iter-84860, train loss-0.3764, acc-0.9000, valid loss-0.3694, acc-0.8960, test loss-0.3746, acc-0.8929\n",
      "Iter-84870, train loss-0.2521, acc-0.9200, valid loss-0.3694, acc-0.8960, test loss-0.3745, acc-0.8929\n",
      "Iter-84880, train loss-0.5445, acc-0.8000, valid loss-0.3694, acc-0.8962, test loss-0.3745, acc-0.8929\n",
      "Iter-84890, train loss-0.6859, acc-0.8000, valid loss-0.3694, acc-0.8962, test loss-0.3745, acc-0.8929\n",
      "Iter-84900, train loss-0.3272, acc-0.9000, valid loss-0.3694, acc-0.8962, test loss-0.3745, acc-0.8929\n",
      "Iter-84910, train loss-0.3118, acc-0.9000, valid loss-0.3694, acc-0.8962, test loss-0.3745, acc-0.8929\n",
      "Iter-84920, train loss-0.4403, acc-0.8800, valid loss-0.3693, acc-0.8962, test loss-0.3745, acc-0.8929\n",
      "Iter-84930, train loss-0.1529, acc-0.9600, valid loss-0.3693, acc-0.8962, test loss-0.3745, acc-0.8929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-84940, train loss-0.5512, acc-0.8600, valid loss-0.3693, acc-0.8962, test loss-0.3744, acc-0.8929\n",
      "Iter-84950, train loss-0.5036, acc-0.8600, valid loss-0.3693, acc-0.8962, test loss-0.3744, acc-0.8929\n",
      "Iter-84960, train loss-0.4686, acc-0.8800, valid loss-0.3693, acc-0.8962, test loss-0.3744, acc-0.8929\n",
      "Iter-84970, train loss-0.3744, acc-0.9200, valid loss-0.3692, acc-0.8962, test loss-0.3744, acc-0.8929\n",
      "Iter-84980, train loss-0.3925, acc-0.8800, valid loss-0.3692, acc-0.8962, test loss-0.3744, acc-0.8929\n",
      "Iter-84990, train loss-0.6799, acc-0.7800, valid loss-0.3692, acc-0.8962, test loss-0.3744, acc-0.8929\n",
      "Iter-85000, train loss-0.3106, acc-0.8800, valid loss-0.3692, acc-0.8962, test loss-0.3744, acc-0.8929\n",
      "Iter-85010, train loss-0.2532, acc-0.9400, valid loss-0.3692, acc-0.8962, test loss-0.3744, acc-0.8929\n",
      "Iter-85020, train loss-0.5545, acc-0.8400, valid loss-0.3692, acc-0.8962, test loss-0.3743, acc-0.8929\n",
      "Iter-85030, train loss-0.3796, acc-0.9200, valid loss-0.3692, acc-0.8960, test loss-0.3743, acc-0.8929\n",
      "Iter-85040, train loss-0.3328, acc-0.8800, valid loss-0.3691, acc-0.8960, test loss-0.3743, acc-0.8929\n",
      "Iter-85050, train loss-0.3223, acc-0.9000, valid loss-0.3691, acc-0.8958, test loss-0.3743, acc-0.8929\n",
      "Iter-85060, train loss-0.5348, acc-0.8000, valid loss-0.3691, acc-0.8958, test loss-0.3743, acc-0.8929\n",
      "Iter-85070, train loss-0.2493, acc-0.9400, valid loss-0.3691, acc-0.8958, test loss-0.3743, acc-0.8929\n",
      "Iter-85080, train loss-0.4044, acc-0.8600, valid loss-0.3691, acc-0.8960, test loss-0.3743, acc-0.8929\n",
      "Iter-85090, train loss-0.5871, acc-0.8200, valid loss-0.3691, acc-0.8960, test loss-0.3742, acc-0.8929\n",
      "Iter-85100, train loss-0.3660, acc-0.8800, valid loss-0.3691, acc-0.8960, test loss-0.3742, acc-0.8929\n",
      "Iter-85110, train loss-0.3628, acc-0.9000, valid loss-0.3690, acc-0.8960, test loss-0.3742, acc-0.8928\n",
      "Iter-85120, train loss-0.2286, acc-0.9600, valid loss-0.3690, acc-0.8960, test loss-0.3742, acc-0.8929\n",
      "Iter-85130, train loss-0.3136, acc-0.9400, valid loss-0.3690, acc-0.8962, test loss-0.3742, acc-0.8929\n",
      "Iter-85140, train loss-0.2281, acc-0.9600, valid loss-0.3690, acc-0.8962, test loss-0.3742, acc-0.8929\n",
      "Iter-85150, train loss-0.4328, acc-0.8400, valid loss-0.3690, acc-0.8960, test loss-0.3741, acc-0.8929\n",
      "Iter-85160, train loss-0.2931, acc-0.9400, valid loss-0.3689, acc-0.8962, test loss-0.3741, acc-0.8929\n",
      "Iter-85170, train loss-0.4799, acc-0.8000, valid loss-0.3689, acc-0.8962, test loss-0.3741, acc-0.8928\n",
      "Iter-85180, train loss-0.4732, acc-0.9000, valid loss-0.3689, acc-0.8960, test loss-0.3741, acc-0.8928\n",
      "Iter-85190, train loss-0.3647, acc-0.8800, valid loss-0.3689, acc-0.8960, test loss-0.3741, acc-0.8929\n",
      "Iter-85200, train loss-0.3467, acc-0.9200, valid loss-0.3689, acc-0.8960, test loss-0.3741, acc-0.8929\n",
      "Iter-85210, train loss-0.3101, acc-0.9200, valid loss-0.3689, acc-0.8960, test loss-0.3741, acc-0.8928\n",
      "Iter-85220, train loss-0.4097, acc-0.8400, valid loss-0.3688, acc-0.8960, test loss-0.3741, acc-0.8927\n",
      "Iter-85230, train loss-0.3327, acc-0.9200, valid loss-0.3688, acc-0.8960, test loss-0.3741, acc-0.8927\n",
      "Iter-85240, train loss-0.3197, acc-0.9600, valid loss-0.3688, acc-0.8960, test loss-0.3740, acc-0.8928\n",
      "Iter-85250, train loss-0.5250, acc-0.9000, valid loss-0.3688, acc-0.8962, test loss-0.3740, acc-0.8928\n",
      "Iter-85260, train loss-0.3122, acc-0.9200, valid loss-0.3688, acc-0.8960, test loss-0.3740, acc-0.8928\n",
      "Iter-85270, train loss-0.2809, acc-0.9200, valid loss-0.3687, acc-0.8960, test loss-0.3740, acc-0.8928\n",
      "Iter-85280, train loss-0.4891, acc-0.8000, valid loss-0.3687, acc-0.8958, test loss-0.3740, acc-0.8929\n",
      "Iter-85290, train loss-0.6460, acc-0.8200, valid loss-0.3687, acc-0.8960, test loss-0.3740, acc-0.8929\n",
      "Iter-85300, train loss-0.4004, acc-0.8400, valid loss-0.3687, acc-0.8958, test loss-0.3739, acc-0.8929\n",
      "Iter-85310, train loss-0.3405, acc-0.8800, valid loss-0.3687, acc-0.8960, test loss-0.3739, acc-0.8929\n",
      "Iter-85320, train loss-0.5064, acc-0.9000, valid loss-0.3687, acc-0.8960, test loss-0.3739, acc-0.8929\n",
      "Iter-85330, train loss-0.4181, acc-0.8400, valid loss-0.3687, acc-0.8958, test loss-0.3739, acc-0.8929\n",
      "Iter-85340, train loss-0.3412, acc-0.8800, valid loss-0.3686, acc-0.8958, test loss-0.3739, acc-0.8929\n",
      "Iter-85350, train loss-0.4101, acc-0.9200, valid loss-0.3686, acc-0.8958, test loss-0.3738, acc-0.8929\n",
      "Iter-85360, train loss-0.5924, acc-0.9000, valid loss-0.3686, acc-0.8958, test loss-0.3738, acc-0.8929\n",
      "Iter-85370, train loss-0.2796, acc-0.9200, valid loss-0.3686, acc-0.8958, test loss-0.3738, acc-0.8929\n",
      "Iter-85380, train loss-0.4786, acc-0.8600, valid loss-0.3686, acc-0.8958, test loss-0.3738, acc-0.8929\n",
      "Iter-85390, train loss-0.4297, acc-0.8400, valid loss-0.3686, acc-0.8958, test loss-0.3738, acc-0.8929\n",
      "Iter-85400, train loss-0.3561, acc-0.8600, valid loss-0.3686, acc-0.8958, test loss-0.3738, acc-0.8929\n",
      "Iter-85410, train loss-0.6184, acc-0.8400, valid loss-0.3685, acc-0.8958, test loss-0.3738, acc-0.8929\n",
      "Iter-85420, train loss-0.5044, acc-0.8600, valid loss-0.3685, acc-0.8956, test loss-0.3737, acc-0.8929\n",
      "Iter-85430, train loss-0.6277, acc-0.8400, valid loss-0.3685, acc-0.8956, test loss-0.3737, acc-0.8929\n",
      "Iter-85440, train loss-0.5665, acc-0.8800, valid loss-0.3685, acc-0.8956, test loss-0.3737, acc-0.8929\n",
      "Iter-85450, train loss-0.3322, acc-0.9200, valid loss-0.3685, acc-0.8958, test loss-0.3737, acc-0.8929\n",
      "Iter-85460, train loss-0.2612, acc-0.9400, valid loss-0.3685, acc-0.8956, test loss-0.3737, acc-0.8929\n",
      "Iter-85470, train loss-0.3207, acc-0.9400, valid loss-0.3685, acc-0.8956, test loss-0.3737, acc-0.8929\n",
      "Iter-85480, train loss-0.5855, acc-0.8600, valid loss-0.3685, acc-0.8958, test loss-0.3736, acc-0.8928\n",
      "Iter-85490, train loss-0.3322, acc-0.9400, valid loss-0.3685, acc-0.8958, test loss-0.3736, acc-0.8929\n",
      "Iter-85500, train loss-0.2359, acc-0.9400, valid loss-0.3684, acc-0.8958, test loss-0.3736, acc-0.8929\n",
      "Iter-85510, train loss-0.3385, acc-0.9200, valid loss-0.3684, acc-0.8958, test loss-0.3736, acc-0.8929\n",
      "Iter-85520, train loss-0.3622, acc-0.9200, valid loss-0.3684, acc-0.8958, test loss-0.3736, acc-0.8928\n",
      "Iter-85530, train loss-0.2306, acc-0.9400, valid loss-0.3684, acc-0.8956, test loss-0.3736, acc-0.8927\n",
      "Iter-85540, train loss-0.3275, acc-0.9000, valid loss-0.3684, acc-0.8956, test loss-0.3736, acc-0.8928\n",
      "Iter-85550, train loss-0.3450, acc-0.9200, valid loss-0.3684, acc-0.8956, test loss-0.3735, acc-0.8928\n",
      "Iter-85560, train loss-0.3938, acc-0.8400, valid loss-0.3683, acc-0.8956, test loss-0.3735, acc-0.8928\n",
      "Iter-85570, train loss-0.4180, acc-0.8600, valid loss-0.3683, acc-0.8956, test loss-0.3735, acc-0.8929\n",
      "Iter-85580, train loss-0.6069, acc-0.8400, valid loss-0.3683, acc-0.8958, test loss-0.3735, acc-0.8928\n",
      "Iter-85590, train loss-0.3123, acc-0.9000, valid loss-0.3683, acc-0.8960, test loss-0.3735, acc-0.8929\n",
      "Iter-85600, train loss-0.2293, acc-0.9600, valid loss-0.3683, acc-0.8958, test loss-0.3735, acc-0.8930\n",
      "Iter-85610, train loss-0.3093, acc-0.9000, valid loss-0.3682, acc-0.8960, test loss-0.3735, acc-0.8928\n",
      "Iter-85620, train loss-0.6888, acc-0.8000, valid loss-0.3682, acc-0.8962, test loss-0.3735, acc-0.8930\n",
      "Iter-85630, train loss-0.3854, acc-0.9000, valid loss-0.3682, acc-0.8960, test loss-0.3735, acc-0.8930\n",
      "Iter-85640, train loss-0.2812, acc-0.9400, valid loss-0.3682, acc-0.8962, test loss-0.3734, acc-0.8930\n",
      "Iter-85650, train loss-0.4556, acc-0.8200, valid loss-0.3682, acc-0.8962, test loss-0.3734, acc-0.8929\n",
      "Iter-85660, train loss-0.3690, acc-0.9000, valid loss-0.3682, acc-0.8962, test loss-0.3734, acc-0.8930\n",
      "Iter-85670, train loss-0.2945, acc-0.9600, valid loss-0.3682, acc-0.8962, test loss-0.3734, acc-0.8930\n",
      "Iter-85680, train loss-0.4097, acc-0.8600, valid loss-0.3682, acc-0.8960, test loss-0.3734, acc-0.8929\n",
      "Iter-85690, train loss-0.4116, acc-0.9200, valid loss-0.3681, acc-0.8960, test loss-0.3734, acc-0.8928\n",
      "Iter-85700, train loss-0.3616, acc-0.9400, valid loss-0.3681, acc-0.8958, test loss-0.3733, acc-0.8928\n",
      "Iter-85710, train loss-0.4503, acc-0.8800, valid loss-0.3681, acc-0.8960, test loss-0.3733, acc-0.8929\n",
      "Iter-85720, train loss-0.7583, acc-0.8600, valid loss-0.3681, acc-0.8960, test loss-0.3733, acc-0.8929\n",
      "Iter-85730, train loss-0.4204, acc-0.8800, valid loss-0.3681, acc-0.8960, test loss-0.3733, acc-0.8929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-85740, train loss-0.3825, acc-0.9000, valid loss-0.3681, acc-0.8960, test loss-0.3733, acc-0.8929\n",
      "Iter-85750, train loss-0.3459, acc-0.9200, valid loss-0.3681, acc-0.8960, test loss-0.3733, acc-0.8930\n",
      "Iter-85760, train loss-0.6408, acc-0.8200, valid loss-0.3680, acc-0.8960, test loss-0.3733, acc-0.8929\n",
      "Iter-85770, train loss-0.5759, acc-0.8200, valid loss-0.3680, acc-0.8960, test loss-0.3733, acc-0.8928\n",
      "Iter-85780, train loss-0.8734, acc-0.7600, valid loss-0.3680, acc-0.8960, test loss-0.3733, acc-0.8930\n",
      "Iter-85790, train loss-0.4069, acc-0.8800, valid loss-0.3680, acc-0.8960, test loss-0.3733, acc-0.8930\n",
      "Iter-85800, train loss-0.3324, acc-0.9000, valid loss-0.3680, acc-0.8960, test loss-0.3732, acc-0.8930\n",
      "Iter-85810, train loss-0.2256, acc-0.9600, valid loss-0.3680, acc-0.8960, test loss-0.3732, acc-0.8929\n",
      "Iter-85820, train loss-0.3100, acc-0.8800, valid loss-0.3680, acc-0.8960, test loss-0.3732, acc-0.8930\n",
      "Iter-85830, train loss-0.3477, acc-0.9000, valid loss-0.3680, acc-0.8960, test loss-0.3732, acc-0.8929\n",
      "Iter-85840, train loss-0.4814, acc-0.8400, valid loss-0.3680, acc-0.8960, test loss-0.3732, acc-0.8930\n",
      "Iter-85850, train loss-0.3275, acc-0.9600, valid loss-0.3679, acc-0.8958, test loss-0.3732, acc-0.8928\n",
      "Iter-85860, train loss-0.3432, acc-0.9000, valid loss-0.3679, acc-0.8958, test loss-0.3731, acc-0.8930\n",
      "Iter-85870, train loss-0.3000, acc-0.8800, valid loss-0.3679, acc-0.8958, test loss-0.3731, acc-0.8930\n",
      "Iter-85880, train loss-0.3815, acc-0.8600, valid loss-0.3679, acc-0.8958, test loss-0.3731, acc-0.8930\n",
      "Iter-85890, train loss-0.4513, acc-0.8600, valid loss-0.3678, acc-0.8958, test loss-0.3731, acc-0.8930\n",
      "Iter-85900, train loss-0.6407, acc-0.7600, valid loss-0.3678, acc-0.8958, test loss-0.3731, acc-0.8931\n",
      "Iter-85910, train loss-0.4690, acc-0.8600, valid loss-0.3678, acc-0.8958, test loss-0.3731, acc-0.8930\n",
      "Iter-85920, train loss-0.4027, acc-0.9000, valid loss-0.3678, acc-0.8958, test loss-0.3731, acc-0.8930\n",
      "Iter-85930, train loss-0.2238, acc-0.9400, valid loss-0.3678, acc-0.8958, test loss-0.3731, acc-0.8929\n",
      "Iter-85940, train loss-0.2820, acc-0.9600, valid loss-0.3678, acc-0.8958, test loss-0.3730, acc-0.8930\n",
      "Iter-85950, train loss-0.2763, acc-0.9000, valid loss-0.3677, acc-0.8958, test loss-0.3730, acc-0.8931\n",
      "Iter-85960, train loss-0.3078, acc-0.9000, valid loss-0.3677, acc-0.8958, test loss-0.3730, acc-0.8930\n",
      "Iter-85970, train loss-0.3165, acc-0.9200, valid loss-0.3677, acc-0.8958, test loss-0.3730, acc-0.8930\n",
      "Iter-85980, train loss-0.2878, acc-0.9200, valid loss-0.3677, acc-0.8960, test loss-0.3730, acc-0.8930\n",
      "Iter-85990, train loss-0.3819, acc-0.8600, valid loss-0.3677, acc-0.8960, test loss-0.3730, acc-0.8931\n",
      "Iter-86000, train loss-0.3737, acc-0.8600, valid loss-0.3677, acc-0.8960, test loss-0.3729, acc-0.8931\n",
      "Iter-86010, train loss-0.2231, acc-0.9200, valid loss-0.3676, acc-0.8960, test loss-0.3729, acc-0.8931\n",
      "Iter-86020, train loss-0.3917, acc-0.8800, valid loss-0.3676, acc-0.8960, test loss-0.3729, acc-0.8931\n",
      "Iter-86030, train loss-0.5426, acc-0.8800, valid loss-0.3676, acc-0.8960, test loss-0.3729, acc-0.8931\n",
      "Iter-86040, train loss-0.5126, acc-0.8600, valid loss-0.3676, acc-0.8960, test loss-0.3729, acc-0.8931\n",
      "Iter-86050, train loss-0.3595, acc-0.8800, valid loss-0.3676, acc-0.8960, test loss-0.3729, acc-0.8931\n",
      "Iter-86060, train loss-0.4930, acc-0.8800, valid loss-0.3676, acc-0.8960, test loss-0.3729, acc-0.8931\n",
      "Iter-86070, train loss-0.3247, acc-0.9200, valid loss-0.3675, acc-0.8960, test loss-0.3728, acc-0.8931\n",
      "Iter-86080, train loss-0.4312, acc-0.9000, valid loss-0.3675, acc-0.8960, test loss-0.3728, acc-0.8931\n",
      "Iter-86090, train loss-0.4152, acc-0.8600, valid loss-0.3675, acc-0.8960, test loss-0.3728, acc-0.8931\n",
      "Iter-86100, train loss-0.3357, acc-0.9200, valid loss-0.3675, acc-0.8960, test loss-0.3728, acc-0.8931\n",
      "Iter-86110, train loss-0.2658, acc-0.9200, valid loss-0.3675, acc-0.8962, test loss-0.3728, acc-0.8931\n",
      "Iter-86120, train loss-0.4722, acc-0.8800, valid loss-0.3675, acc-0.8960, test loss-0.3728, acc-0.8931\n",
      "Iter-86130, train loss-0.3637, acc-0.9000, valid loss-0.3675, acc-0.8960, test loss-0.3728, acc-0.8931\n",
      "Iter-86140, train loss-0.1728, acc-0.9600, valid loss-0.3674, acc-0.8962, test loss-0.3728, acc-0.8931\n",
      "Iter-86150, train loss-0.3239, acc-0.9200, valid loss-0.3674, acc-0.8960, test loss-0.3727, acc-0.8931\n",
      "Iter-86160, train loss-0.3265, acc-0.9200, valid loss-0.3674, acc-0.8960, test loss-0.3727, acc-0.8931\n",
      "Iter-86170, train loss-0.3767, acc-0.9200, valid loss-0.3674, acc-0.8962, test loss-0.3727, acc-0.8931\n",
      "Iter-86180, train loss-0.2469, acc-0.9200, valid loss-0.3674, acc-0.8960, test loss-0.3727, acc-0.8931\n",
      "Iter-86190, train loss-0.3088, acc-0.9000, valid loss-0.3674, acc-0.8960, test loss-0.3727, acc-0.8931\n",
      "Iter-86200, train loss-0.3881, acc-0.8800, valid loss-0.3673, acc-0.8962, test loss-0.3727, acc-0.8931\n",
      "Iter-86210, train loss-0.3732, acc-0.9400, valid loss-0.3673, acc-0.8962, test loss-0.3727, acc-0.8931\n",
      "Iter-86220, train loss-0.4216, acc-0.8600, valid loss-0.3673, acc-0.8962, test loss-0.3727, acc-0.8931\n",
      "Iter-86230, train loss-0.4019, acc-0.8600, valid loss-0.3673, acc-0.8962, test loss-0.3726, acc-0.8931\n",
      "Iter-86240, train loss-0.3550, acc-0.9200, valid loss-0.3673, acc-0.8962, test loss-0.3726, acc-0.8931\n",
      "Iter-86250, train loss-0.2964, acc-0.9400, valid loss-0.3673, acc-0.8962, test loss-0.3726, acc-0.8931\n",
      "Iter-86260, train loss-0.3049, acc-0.9400, valid loss-0.3673, acc-0.8962, test loss-0.3726, acc-0.8931\n",
      "Iter-86270, train loss-0.4099, acc-0.8600, valid loss-0.3672, acc-0.8960, test loss-0.3726, acc-0.8932\n",
      "Iter-86280, train loss-0.3594, acc-0.9200, valid loss-0.3672, acc-0.8962, test loss-0.3726, acc-0.8932\n",
      "Iter-86290, train loss-0.3895, acc-0.8800, valid loss-0.3672, acc-0.8962, test loss-0.3726, acc-0.8932\n",
      "Iter-86300, train loss-0.2941, acc-0.9000, valid loss-0.3672, acc-0.8962, test loss-0.3725, acc-0.8932\n",
      "Iter-86310, train loss-0.5111, acc-0.8800, valid loss-0.3672, acc-0.8962, test loss-0.3725, acc-0.8932\n",
      "Iter-86320, train loss-0.3807, acc-0.8800, valid loss-0.3671, acc-0.8964, test loss-0.3725, acc-0.8932\n",
      "Iter-86330, train loss-0.6284, acc-0.7800, valid loss-0.3671, acc-0.8964, test loss-0.3725, acc-0.8932\n",
      "Iter-86340, train loss-0.3492, acc-0.8800, valid loss-0.3671, acc-0.8962, test loss-0.3725, acc-0.8932\n",
      "Iter-86350, train loss-0.2716, acc-0.9200, valid loss-0.3671, acc-0.8966, test loss-0.3725, acc-0.8931\n",
      "Iter-86360, train loss-0.5128, acc-0.8600, valid loss-0.3671, acc-0.8964, test loss-0.3725, acc-0.8932\n",
      "Iter-86370, train loss-0.4558, acc-0.8600, valid loss-0.3671, acc-0.8966, test loss-0.3724, acc-0.8933\n",
      "Iter-86380, train loss-0.4446, acc-0.8800, valid loss-0.3670, acc-0.8966, test loss-0.3724, acc-0.8932\n",
      "Iter-86390, train loss-0.2839, acc-0.9000, valid loss-0.3670, acc-0.8964, test loss-0.3724, acc-0.8932\n",
      "Iter-86400, train loss-0.3679, acc-0.8800, valid loss-0.3670, acc-0.8964, test loss-0.3724, acc-0.8932\n",
      "Iter-86410, train loss-0.4847, acc-0.8800, valid loss-0.3670, acc-0.8964, test loss-0.3724, acc-0.8932\n",
      "Iter-86420, train loss-0.3339, acc-0.8800, valid loss-0.3670, acc-0.8964, test loss-0.3724, acc-0.8931\n",
      "Iter-86430, train loss-0.5426, acc-0.8200, valid loss-0.3670, acc-0.8964, test loss-0.3724, acc-0.8932\n",
      "Iter-86440, train loss-0.6327, acc-0.8400, valid loss-0.3670, acc-0.8964, test loss-0.3723, acc-0.8932\n",
      "Iter-86450, train loss-0.4441, acc-0.8400, valid loss-0.3669, acc-0.8964, test loss-0.3723, acc-0.8932\n",
      "Iter-86460, train loss-0.1952, acc-0.9600, valid loss-0.3669, acc-0.8964, test loss-0.3723, acc-0.8932\n",
      "Iter-86470, train loss-0.5381, acc-0.8600, valid loss-0.3669, acc-0.8964, test loss-0.3723, acc-0.8932\n",
      "Iter-86480, train loss-0.2945, acc-0.9200, valid loss-0.3669, acc-0.8964, test loss-0.3723, acc-0.8932\n",
      "Iter-86490, train loss-0.3914, acc-0.8400, valid loss-0.3669, acc-0.8964, test loss-0.3723, acc-0.8933\n",
      "Iter-86500, train loss-0.5009, acc-0.8600, valid loss-0.3669, acc-0.8964, test loss-0.3723, acc-0.8933\n",
      "Iter-86510, train loss-0.4570, acc-0.8400, valid loss-0.3669, acc-0.8964, test loss-0.3723, acc-0.8933\n",
      "Iter-86520, train loss-0.3923, acc-0.8400, valid loss-0.3669, acc-0.8964, test loss-0.3722, acc-0.8933\n",
      "Iter-86530, train loss-0.2467, acc-0.9200, valid loss-0.3668, acc-0.8964, test loss-0.3722, acc-0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-86540, train loss-0.3827, acc-0.8600, valid loss-0.3668, acc-0.8966, test loss-0.3722, acc-0.8933\n",
      "Iter-86550, train loss-0.5308, acc-0.8400, valid loss-0.3668, acc-0.8966, test loss-0.3722, acc-0.8933\n",
      "Iter-86560, train loss-0.3679, acc-0.8800, valid loss-0.3668, acc-0.8966, test loss-0.3722, acc-0.8934\n",
      "Iter-86570, train loss-0.1303, acc-0.9800, valid loss-0.3668, acc-0.8966, test loss-0.3722, acc-0.8934\n",
      "Iter-86580, train loss-0.4830, acc-0.8800, valid loss-0.3668, acc-0.8966, test loss-0.3722, acc-0.8934\n",
      "Iter-86590, train loss-0.3251, acc-0.8800, valid loss-0.3667, acc-0.8964, test loss-0.3721, acc-0.8934\n",
      "Iter-86600, train loss-0.4943, acc-0.8600, valid loss-0.3667, acc-0.8964, test loss-0.3721, acc-0.8934\n",
      "Iter-86610, train loss-0.3390, acc-0.9200, valid loss-0.3667, acc-0.8964, test loss-0.3721, acc-0.8934\n",
      "Iter-86620, train loss-0.3832, acc-0.9200, valid loss-0.3667, acc-0.8966, test loss-0.3721, acc-0.8934\n",
      "Iter-86630, train loss-0.5972, acc-0.8200, valid loss-0.3667, acc-0.8966, test loss-0.3721, acc-0.8934\n",
      "Iter-86640, train loss-0.3652, acc-0.8400, valid loss-0.3667, acc-0.8966, test loss-0.3721, acc-0.8934\n",
      "Iter-86650, train loss-0.3054, acc-0.9000, valid loss-0.3667, acc-0.8966, test loss-0.3721, acc-0.8934\n",
      "Iter-86660, train loss-0.3047, acc-0.8800, valid loss-0.3667, acc-0.8968, test loss-0.3720, acc-0.8935\n",
      "Iter-86670, train loss-0.4367, acc-0.8400, valid loss-0.3667, acc-0.8970, test loss-0.3720, acc-0.8935\n",
      "Iter-86680, train loss-0.2204, acc-0.9400, valid loss-0.3666, acc-0.8970, test loss-0.3720, acc-0.8935\n",
      "Iter-86690, train loss-0.3459, acc-0.9200, valid loss-0.3666, acc-0.8968, test loss-0.3720, acc-0.8935\n",
      "Iter-86700, train loss-0.3508, acc-0.9000, valid loss-0.3666, acc-0.8970, test loss-0.3720, acc-0.8935\n",
      "Iter-86710, train loss-0.4022, acc-0.8600, valid loss-0.3666, acc-0.8968, test loss-0.3720, acc-0.8935\n",
      "Iter-86720, train loss-0.4375, acc-0.9000, valid loss-0.3666, acc-0.8966, test loss-0.3719, acc-0.8935\n",
      "Iter-86730, train loss-0.5141, acc-0.7800, valid loss-0.3666, acc-0.8966, test loss-0.3719, acc-0.8936\n",
      "Iter-86740, train loss-0.3463, acc-0.8800, valid loss-0.3666, acc-0.8968, test loss-0.3719, acc-0.8936\n",
      "Iter-86750, train loss-0.4089, acc-0.8800, valid loss-0.3665, acc-0.8964, test loss-0.3719, acc-0.8936\n",
      "Iter-86760, train loss-0.4997, acc-0.8600, valid loss-0.3665, acc-0.8966, test loss-0.3719, acc-0.8935\n",
      "Iter-86770, train loss-0.2222, acc-0.9200, valid loss-0.3665, acc-0.8964, test loss-0.3719, acc-0.8935\n",
      "Iter-86780, train loss-0.2916, acc-0.9400, valid loss-0.3665, acc-0.8962, test loss-0.3719, acc-0.8936\n",
      "Iter-86790, train loss-0.2020, acc-0.9400, valid loss-0.3665, acc-0.8962, test loss-0.3719, acc-0.8935\n",
      "Iter-86800, train loss-0.4699, acc-0.7800, valid loss-0.3665, acc-0.8962, test loss-0.3718, acc-0.8935\n",
      "Iter-86810, train loss-0.1496, acc-1.0000, valid loss-0.3665, acc-0.8964, test loss-0.3718, acc-0.8935\n",
      "Iter-86820, train loss-0.4394, acc-0.8400, valid loss-0.3665, acc-0.8962, test loss-0.3718, acc-0.8935\n",
      "Iter-86830, train loss-0.3604, acc-0.9000, valid loss-0.3665, acc-0.8962, test loss-0.3718, acc-0.8935\n",
      "Iter-86840, train loss-0.1996, acc-0.9600, valid loss-0.3665, acc-0.8964, test loss-0.3718, acc-0.8935\n",
      "Iter-86850, train loss-0.2668, acc-0.9600, valid loss-0.3664, acc-0.8964, test loss-0.3718, acc-0.8935\n",
      "Iter-86860, train loss-0.2391, acc-0.9400, valid loss-0.3664, acc-0.8964, test loss-0.3717, acc-0.8935\n",
      "Iter-86870, train loss-0.4204, acc-0.9000, valid loss-0.3664, acc-0.8964, test loss-0.3717, acc-0.8935\n",
      "Iter-86880, train loss-0.3045, acc-0.9400, valid loss-0.3664, acc-0.8964, test loss-0.3717, acc-0.8935\n",
      "Iter-86890, train loss-0.4651, acc-0.8800, valid loss-0.3664, acc-0.8964, test loss-0.3717, acc-0.8934\n",
      "Iter-86900, train loss-0.3995, acc-0.9200, valid loss-0.3664, acc-0.8962, test loss-0.3717, acc-0.8933\n",
      "Iter-86910, train loss-0.4127, acc-0.8800, valid loss-0.3663, acc-0.8964, test loss-0.3717, acc-0.8934\n",
      "Iter-86920, train loss-0.4367, acc-0.9200, valid loss-0.3663, acc-0.8962, test loss-0.3716, acc-0.8933\n",
      "Iter-86930, train loss-0.3883, acc-0.9000, valid loss-0.3663, acc-0.8964, test loss-0.3716, acc-0.8935\n",
      "Iter-86940, train loss-0.3984, acc-0.8800, valid loss-0.3663, acc-0.8960, test loss-0.3716, acc-0.8936\n",
      "Iter-86950, train loss-0.5761, acc-0.8000, valid loss-0.3663, acc-0.8964, test loss-0.3716, acc-0.8935\n",
      "Iter-86960, train loss-0.3849, acc-0.8800, valid loss-0.3663, acc-0.8964, test loss-0.3716, acc-0.8935\n",
      "Iter-86970, train loss-0.4278, acc-0.8800, valid loss-0.3663, acc-0.8964, test loss-0.3716, acc-0.8936\n",
      "Iter-86980, train loss-0.5026, acc-0.8400, valid loss-0.3663, acc-0.8964, test loss-0.3715, acc-0.8936\n",
      "Iter-86990, train loss-0.1813, acc-0.9600, valid loss-0.3662, acc-0.8966, test loss-0.3715, acc-0.8936\n",
      "Iter-87000, train loss-0.4966, acc-0.8600, valid loss-0.3662, acc-0.8966, test loss-0.3715, acc-0.8935\n",
      "Iter-87010, train loss-0.4837, acc-0.9000, valid loss-0.3662, acc-0.8966, test loss-0.3715, acc-0.8936\n",
      "Iter-87020, train loss-0.3109, acc-0.8600, valid loss-0.3662, acc-0.8966, test loss-0.3715, acc-0.8936\n",
      "Iter-87030, train loss-0.2765, acc-0.9000, valid loss-0.3662, acc-0.8966, test loss-0.3715, acc-0.8935\n",
      "Iter-87040, train loss-0.4556, acc-0.8800, valid loss-0.3662, acc-0.8966, test loss-0.3715, acc-0.8936\n",
      "Iter-87050, train loss-0.4100, acc-0.9200, valid loss-0.3661, acc-0.8964, test loss-0.3714, acc-0.8935\n",
      "Iter-87060, train loss-0.1310, acc-0.9800, valid loss-0.3661, acc-0.8964, test loss-0.3714, acc-0.8935\n",
      "Iter-87070, train loss-0.2022, acc-0.9400, valid loss-0.3661, acc-0.8964, test loss-0.3714, acc-0.8934\n",
      "Iter-87080, train loss-0.3196, acc-0.9200, valid loss-0.3661, acc-0.8962, test loss-0.3714, acc-0.8934\n",
      "Iter-87090, train loss-0.3708, acc-0.9000, valid loss-0.3661, acc-0.8962, test loss-0.3714, acc-0.8935\n",
      "Iter-87100, train loss-0.2208, acc-0.9400, valid loss-0.3661, acc-0.8962, test loss-0.3714, acc-0.8936\n",
      "Iter-87110, train loss-0.1663, acc-0.9800, valid loss-0.3661, acc-0.8962, test loss-0.3714, acc-0.8935\n",
      "Iter-87120, train loss-0.2820, acc-0.9400, valid loss-0.3660, acc-0.8964, test loss-0.3714, acc-0.8936\n",
      "Iter-87130, train loss-0.4739, acc-0.8400, valid loss-0.3660, acc-0.8962, test loss-0.3713, acc-0.8936\n",
      "Iter-87140, train loss-0.3589, acc-0.8800, valid loss-0.3660, acc-0.8962, test loss-0.3713, acc-0.8936\n",
      "Iter-87150, train loss-0.3241, acc-0.9000, valid loss-0.3660, acc-0.8962, test loss-0.3713, acc-0.8936\n",
      "Iter-87160, train loss-0.5265, acc-0.8000, valid loss-0.3660, acc-0.8960, test loss-0.3713, acc-0.8936\n",
      "Iter-87170, train loss-0.4484, acc-0.8800, valid loss-0.3660, acc-0.8962, test loss-0.3713, acc-0.8935\n",
      "Iter-87180, train loss-0.2206, acc-0.9400, valid loss-0.3660, acc-0.8962, test loss-0.3713, acc-0.8937\n",
      "Iter-87190, train loss-0.5494, acc-0.8400, valid loss-0.3659, acc-0.8962, test loss-0.3712, acc-0.8937\n",
      "Iter-87200, train loss-0.4723, acc-0.8800, valid loss-0.3659, acc-0.8962, test loss-0.3712, acc-0.8938\n",
      "Iter-87210, train loss-0.2453, acc-0.9600, valid loss-0.3659, acc-0.8962, test loss-0.3712, acc-0.8936\n",
      "Iter-87220, train loss-0.3626, acc-0.9000, valid loss-0.3659, acc-0.8964, test loss-0.3712, acc-0.8936\n",
      "Iter-87230, train loss-0.4506, acc-0.8400, valid loss-0.3659, acc-0.8962, test loss-0.3712, acc-0.8936\n",
      "Iter-87240, train loss-0.4887, acc-0.8600, valid loss-0.3659, acc-0.8962, test loss-0.3712, acc-0.8936\n",
      "Iter-87250, train loss-0.5130, acc-0.8400, valid loss-0.3659, acc-0.8962, test loss-0.3712, acc-0.8936\n",
      "Iter-87260, train loss-0.4236, acc-0.9000, valid loss-0.3659, acc-0.8964, test loss-0.3712, acc-0.8936\n",
      "Iter-87270, train loss-0.2577, acc-0.9600, valid loss-0.3658, acc-0.8962, test loss-0.3711, acc-0.8937\n",
      "Iter-87280, train loss-0.3492, acc-0.8400, valid loss-0.3658, acc-0.8962, test loss-0.3711, acc-0.8938\n",
      "Iter-87290, train loss-0.4076, acc-0.8600, valid loss-0.3658, acc-0.8964, test loss-0.3711, acc-0.8938\n",
      "Iter-87300, train loss-0.3168, acc-0.9400, valid loss-0.3658, acc-0.8964, test loss-0.3711, acc-0.8938\n",
      "Iter-87310, train loss-0.2542, acc-0.9400, valid loss-0.3658, acc-0.8966, test loss-0.3711, acc-0.8936\n",
      "Iter-87320, train loss-0.3961, acc-0.8800, valid loss-0.3658, acc-0.8966, test loss-0.3711, acc-0.8938\n",
      "Iter-87330, train loss-0.5288, acc-0.8000, valid loss-0.3658, acc-0.8964, test loss-0.3711, acc-0.8937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-87340, train loss-0.5892, acc-0.8400, valid loss-0.3658, acc-0.8964, test loss-0.3711, acc-0.8936\n",
      "Iter-87350, train loss-0.4251, acc-0.9000, valid loss-0.3658, acc-0.8964, test loss-0.3711, acc-0.8936\n",
      "Iter-87360, train loss-0.3353, acc-0.8600, valid loss-0.3657, acc-0.8964, test loss-0.3710, acc-0.8937\n",
      "Iter-87370, train loss-0.4017, acc-0.8400, valid loss-0.3657, acc-0.8964, test loss-0.3710, acc-0.8937\n",
      "Iter-87380, train loss-0.2309, acc-0.9200, valid loss-0.3657, acc-0.8964, test loss-0.3710, acc-0.8936\n",
      "Iter-87390, train loss-0.5614, acc-0.8200, valid loss-0.3657, acc-0.8966, test loss-0.3710, acc-0.8937\n",
      "Iter-87400, train loss-0.4045, acc-0.8600, valid loss-0.3657, acc-0.8964, test loss-0.3710, acc-0.8937\n",
      "Iter-87410, train loss-0.3450, acc-0.8600, valid loss-0.3657, acc-0.8964, test loss-0.3710, acc-0.8937\n",
      "Iter-87420, train loss-0.5819, acc-0.8400, valid loss-0.3657, acc-0.8968, test loss-0.3710, acc-0.8938\n",
      "Iter-87430, train loss-0.3156, acc-0.9600, valid loss-0.3656, acc-0.8968, test loss-0.3709, acc-0.8938\n",
      "Iter-87440, train loss-0.6946, acc-0.8200, valid loss-0.3656, acc-0.8966, test loss-0.3709, acc-0.8937\n",
      "Iter-87450, train loss-0.4715, acc-0.8600, valid loss-0.3656, acc-0.8966, test loss-0.3709, acc-0.8937\n",
      "Iter-87460, train loss-0.2729, acc-0.9400, valid loss-0.3656, acc-0.8966, test loss-0.3709, acc-0.8938\n",
      "Iter-87470, train loss-0.3647, acc-0.8600, valid loss-0.3656, acc-0.8966, test loss-0.3709, acc-0.8937\n",
      "Iter-87480, train loss-0.2714, acc-0.9200, valid loss-0.3656, acc-0.8966, test loss-0.3709, acc-0.8937\n",
      "Iter-87490, train loss-0.2166, acc-0.9800, valid loss-0.3656, acc-0.8966, test loss-0.3709, acc-0.8936\n",
      "Iter-87500, train loss-0.6351, acc-0.8200, valid loss-0.3656, acc-0.8968, test loss-0.3709, acc-0.8937\n",
      "Iter-87510, train loss-0.6001, acc-0.7600, valid loss-0.3656, acc-0.8968, test loss-0.3708, acc-0.8937\n",
      "Iter-87520, train loss-0.3131, acc-0.9200, valid loss-0.3655, acc-0.8968, test loss-0.3708, acc-0.8937\n",
      "Iter-87530, train loss-0.3452, acc-0.9400, valid loss-0.3655, acc-0.8968, test loss-0.3708, acc-0.8938\n",
      "Iter-87540, train loss-0.6234, acc-0.8200, valid loss-0.3655, acc-0.8966, test loss-0.3708, acc-0.8937\n",
      "Iter-87550, train loss-0.4127, acc-0.8800, valid loss-0.3655, acc-0.8966, test loss-0.3708, acc-0.8937\n",
      "Iter-87560, train loss-0.4011, acc-0.9200, valid loss-0.3655, acc-0.8966, test loss-0.3708, acc-0.8936\n",
      "Iter-87570, train loss-0.3380, acc-0.8800, valid loss-0.3655, acc-0.8966, test loss-0.3707, acc-0.8936\n",
      "Iter-87580, train loss-0.4256, acc-0.8600, valid loss-0.3654, acc-0.8968, test loss-0.3707, acc-0.8937\n",
      "Iter-87590, train loss-0.1546, acc-1.0000, valid loss-0.3654, acc-0.8968, test loss-0.3707, acc-0.8937\n",
      "Iter-87600, train loss-0.3640, acc-0.8800, valid loss-0.3654, acc-0.8970, test loss-0.3707, acc-0.8937\n",
      "Iter-87610, train loss-0.2470, acc-0.9200, valid loss-0.3654, acc-0.8970, test loss-0.3707, acc-0.8937\n",
      "Iter-87620, train loss-0.3901, acc-0.9200, valid loss-0.3654, acc-0.8968, test loss-0.3707, acc-0.8937\n",
      "Iter-87630, train loss-0.3801, acc-0.9000, valid loss-0.3654, acc-0.8970, test loss-0.3707, acc-0.8937\n",
      "Iter-87640, train loss-0.2258, acc-0.9600, valid loss-0.3653, acc-0.8968, test loss-0.3706, acc-0.8937\n",
      "Iter-87650, train loss-0.3585, acc-0.8800, valid loss-0.3653, acc-0.8968, test loss-0.3706, acc-0.8937\n",
      "Iter-87660, train loss-0.4142, acc-0.8600, valid loss-0.3653, acc-0.8970, test loss-0.3706, acc-0.8937\n",
      "Iter-87670, train loss-0.4524, acc-0.8800, valid loss-0.3653, acc-0.8970, test loss-0.3706, acc-0.8936\n",
      "Iter-87680, train loss-0.5943, acc-0.8400, valid loss-0.3653, acc-0.8970, test loss-0.3706, acc-0.8937\n",
      "Iter-87690, train loss-0.2653, acc-0.9200, valid loss-0.3653, acc-0.8968, test loss-0.3706, acc-0.8939\n",
      "Iter-87700, train loss-0.3340, acc-0.9200, valid loss-0.3653, acc-0.8966, test loss-0.3705, acc-0.8939\n",
      "Iter-87710, train loss-0.2904, acc-0.8800, valid loss-0.3652, acc-0.8966, test loss-0.3705, acc-0.8939\n",
      "Iter-87720, train loss-0.3073, acc-0.9200, valid loss-0.3652, acc-0.8968, test loss-0.3705, acc-0.8938\n",
      "Iter-87730, train loss-0.3820, acc-0.9000, valid loss-0.3652, acc-0.8968, test loss-0.3705, acc-0.8938\n",
      "Iter-87740, train loss-0.3490, acc-0.9200, valid loss-0.3652, acc-0.8966, test loss-0.3705, acc-0.8938\n",
      "Iter-87750, train loss-0.4026, acc-0.9000, valid loss-0.3652, acc-0.8966, test loss-0.3705, acc-0.8937\n",
      "Iter-87760, train loss-0.2948, acc-0.9600, valid loss-0.3651, acc-0.8966, test loss-0.3705, acc-0.8937\n",
      "Iter-87770, train loss-0.5654, acc-0.8400, valid loss-0.3651, acc-0.8968, test loss-0.3704, acc-0.8937\n",
      "Iter-87780, train loss-0.2887, acc-0.9000, valid loss-0.3651, acc-0.8968, test loss-0.3704, acc-0.8938\n",
      "Iter-87790, train loss-0.4093, acc-0.9200, valid loss-0.3651, acc-0.8966, test loss-0.3704, acc-0.8938\n",
      "Iter-87800, train loss-0.1414, acc-0.9800, valid loss-0.3651, acc-0.8966, test loss-0.3704, acc-0.8938\n",
      "Iter-87810, train loss-0.3745, acc-0.9200, valid loss-0.3651, acc-0.8966, test loss-0.3703, acc-0.8937\n",
      "Iter-87820, train loss-0.5488, acc-0.7600, valid loss-0.3650, acc-0.8966, test loss-0.3703, acc-0.8937\n",
      "Iter-87830, train loss-0.3136, acc-0.9000, valid loss-0.3650, acc-0.8966, test loss-0.3703, acc-0.8937\n",
      "Iter-87840, train loss-0.4762, acc-0.8600, valid loss-0.3650, acc-0.8968, test loss-0.3703, acc-0.8938\n",
      "Iter-87850, train loss-0.2309, acc-0.9600, valid loss-0.3650, acc-0.8968, test loss-0.3703, acc-0.8938\n",
      "Iter-87860, train loss-0.5259, acc-0.8200, valid loss-0.3650, acc-0.8968, test loss-0.3703, acc-0.8938\n",
      "Iter-87870, train loss-0.2713, acc-0.9200, valid loss-0.3650, acc-0.8968, test loss-0.3702, acc-0.8937\n",
      "Iter-87880, train loss-0.4711, acc-0.9000, valid loss-0.3650, acc-0.8968, test loss-0.3702, acc-0.8938\n",
      "Iter-87890, train loss-0.2233, acc-0.9400, valid loss-0.3650, acc-0.8970, test loss-0.3702, acc-0.8937\n",
      "Iter-87900, train loss-0.4451, acc-0.8400, valid loss-0.3649, acc-0.8970, test loss-0.3702, acc-0.8936\n",
      "Iter-87910, train loss-0.4041, acc-0.8800, valid loss-0.3649, acc-0.8968, test loss-0.3702, acc-0.8936\n",
      "Iter-87920, train loss-0.3552, acc-0.8400, valid loss-0.3649, acc-0.8966, test loss-0.3702, acc-0.8936\n",
      "Iter-87930, train loss-0.1902, acc-0.9800, valid loss-0.3649, acc-0.8968, test loss-0.3702, acc-0.8936\n",
      "Iter-87940, train loss-0.2709, acc-0.9400, valid loss-0.3649, acc-0.8968, test loss-0.3701, acc-0.8936\n",
      "Iter-87950, train loss-0.2833, acc-0.9000, valid loss-0.3649, acc-0.8970, test loss-0.3701, acc-0.8937\n",
      "Iter-87960, train loss-0.3815, acc-0.9000, valid loss-0.3648, acc-0.8968, test loss-0.3701, acc-0.8936\n",
      "Iter-87970, train loss-0.3610, acc-0.8800, valid loss-0.3648, acc-0.8968, test loss-0.3701, acc-0.8937\n",
      "Iter-87980, train loss-0.3461, acc-0.9400, valid loss-0.3648, acc-0.8970, test loss-0.3701, acc-0.8937\n",
      "Iter-87990, train loss-0.2079, acc-0.9600, valid loss-0.3648, acc-0.8970, test loss-0.3701, acc-0.8937\n",
      "Iter-88000, train loss-0.3616, acc-0.8600, valid loss-0.3648, acc-0.8968, test loss-0.3701, acc-0.8937\n",
      "Iter-88010, train loss-0.3063, acc-0.8600, valid loss-0.3648, acc-0.8970, test loss-0.3701, acc-0.8937\n",
      "Iter-88020, train loss-0.2548, acc-0.9000, valid loss-0.3648, acc-0.8970, test loss-0.3701, acc-0.8935\n",
      "Iter-88030, train loss-0.4001, acc-0.8400, valid loss-0.3648, acc-0.8972, test loss-0.3700, acc-0.8935\n",
      "Iter-88040, train loss-0.3731, acc-0.9000, valid loss-0.3647, acc-0.8974, test loss-0.3700, acc-0.8935\n",
      "Iter-88050, train loss-0.2829, acc-0.8800, valid loss-0.3647, acc-0.8974, test loss-0.3700, acc-0.8936\n",
      "Iter-88060, train loss-0.4732, acc-0.8800, valid loss-0.3647, acc-0.8976, test loss-0.3700, acc-0.8935\n",
      "Iter-88070, train loss-0.3268, acc-0.9000, valid loss-0.3647, acc-0.8976, test loss-0.3700, acc-0.8935\n",
      "Iter-88080, train loss-0.5944, acc-0.8600, valid loss-0.3647, acc-0.8976, test loss-0.3700, acc-0.8935\n",
      "Iter-88090, train loss-0.3108, acc-0.9200, valid loss-0.3647, acc-0.8976, test loss-0.3699, acc-0.8936\n",
      "Iter-88100, train loss-0.4888, acc-0.8400, valid loss-0.3647, acc-0.8976, test loss-0.3699, acc-0.8936\n",
      "Iter-88110, train loss-0.4317, acc-0.8600, valid loss-0.3646, acc-0.8976, test loss-0.3699, acc-0.8937\n",
      "Iter-88120, train loss-0.2595, acc-0.9600, valid loss-0.3646, acc-0.8976, test loss-0.3699, acc-0.8937\n",
      "Iter-88130, train loss-0.2025, acc-0.9600, valid loss-0.3646, acc-0.8976, test loss-0.3699, acc-0.8937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-88140, train loss-0.2127, acc-0.9800, valid loss-0.3646, acc-0.8974, test loss-0.3699, acc-0.8937\n",
      "Iter-88150, train loss-0.3536, acc-0.8800, valid loss-0.3646, acc-0.8974, test loss-0.3699, acc-0.8937\n",
      "Iter-88160, train loss-0.5052, acc-0.7800, valid loss-0.3646, acc-0.8974, test loss-0.3698, acc-0.8936\n",
      "Iter-88170, train loss-0.5386, acc-0.8400, valid loss-0.3646, acc-0.8974, test loss-0.3698, acc-0.8936\n",
      "Iter-88180, train loss-0.3130, acc-0.9200, valid loss-0.3645, acc-0.8974, test loss-0.3698, acc-0.8936\n",
      "Iter-88190, train loss-0.1810, acc-0.9800, valid loss-0.3645, acc-0.8974, test loss-0.3698, acc-0.8936\n",
      "Iter-88200, train loss-0.3681, acc-0.8800, valid loss-0.3645, acc-0.8974, test loss-0.3698, acc-0.8936\n",
      "Iter-88210, train loss-0.7211, acc-0.7600, valid loss-0.3645, acc-0.8974, test loss-0.3698, acc-0.8936\n",
      "Iter-88220, train loss-0.3935, acc-0.8400, valid loss-0.3645, acc-0.8974, test loss-0.3698, acc-0.8937\n",
      "Iter-88230, train loss-0.4859, acc-0.9000, valid loss-0.3645, acc-0.8974, test loss-0.3698, acc-0.8937\n",
      "Iter-88240, train loss-0.4911, acc-0.8600, valid loss-0.3645, acc-0.8974, test loss-0.3697, acc-0.8938\n",
      "Iter-88250, train loss-0.1934, acc-0.9800, valid loss-0.3645, acc-0.8974, test loss-0.3697, acc-0.8936\n",
      "Iter-88260, train loss-0.4715, acc-0.8600, valid loss-0.3644, acc-0.8974, test loss-0.3697, acc-0.8938\n",
      "Iter-88270, train loss-0.2601, acc-0.9400, valid loss-0.3644, acc-0.8974, test loss-0.3697, acc-0.8937\n",
      "Iter-88280, train loss-0.4200, acc-0.9200, valid loss-0.3644, acc-0.8974, test loss-0.3697, acc-0.8938\n",
      "Iter-88290, train loss-0.5843, acc-0.8000, valid loss-0.3644, acc-0.8974, test loss-0.3697, acc-0.8938\n",
      "Iter-88300, train loss-0.5329, acc-0.8200, valid loss-0.3644, acc-0.8974, test loss-0.3696, acc-0.8938\n",
      "Iter-88310, train loss-0.2135, acc-0.9800, valid loss-0.3644, acc-0.8974, test loss-0.3696, acc-0.8938\n",
      "Iter-88320, train loss-0.2060, acc-0.9400, valid loss-0.3644, acc-0.8974, test loss-0.3696, acc-0.8938\n",
      "Iter-88330, train loss-0.2728, acc-0.9400, valid loss-0.3644, acc-0.8976, test loss-0.3696, acc-0.8938\n",
      "Iter-88340, train loss-0.2565, acc-0.9000, valid loss-0.3644, acc-0.8974, test loss-0.3696, acc-0.8938\n",
      "Iter-88350, train loss-0.5285, acc-0.8000, valid loss-0.3643, acc-0.8976, test loss-0.3696, acc-0.8938\n",
      "Iter-88360, train loss-0.3659, acc-0.8800, valid loss-0.3643, acc-0.8978, test loss-0.3696, acc-0.8937\n",
      "Iter-88370, train loss-0.3639, acc-0.9400, valid loss-0.3643, acc-0.8978, test loss-0.3696, acc-0.8937\n",
      "Iter-88380, train loss-0.3978, acc-0.8800, valid loss-0.3643, acc-0.8978, test loss-0.3695, acc-0.8937\n",
      "Iter-88390, train loss-0.5918, acc-0.8800, valid loss-0.3643, acc-0.8976, test loss-0.3695, acc-0.8936\n",
      "Iter-88400, train loss-0.4785, acc-0.8600, valid loss-0.3643, acc-0.8976, test loss-0.3695, acc-0.8936\n",
      "Iter-88410, train loss-0.3010, acc-0.9200, valid loss-0.3642, acc-0.8978, test loss-0.3695, acc-0.8937\n",
      "Iter-88420, train loss-0.3535, acc-0.8400, valid loss-0.3643, acc-0.8978, test loss-0.3695, acc-0.8936\n",
      "Iter-88430, train loss-0.6133, acc-0.9000, valid loss-0.3642, acc-0.8976, test loss-0.3695, acc-0.8937\n",
      "Iter-88440, train loss-0.2530, acc-0.9200, valid loss-0.3642, acc-0.8978, test loss-0.3695, acc-0.8936\n",
      "Iter-88450, train loss-0.2207, acc-0.9400, valid loss-0.3642, acc-0.8978, test loss-0.3694, acc-0.8937\n",
      "Iter-88460, train loss-0.4732, acc-0.8800, valid loss-0.3642, acc-0.8976, test loss-0.3694, acc-0.8937\n",
      "Iter-88470, train loss-0.5199, acc-0.8600, valid loss-0.3642, acc-0.8978, test loss-0.3694, acc-0.8937\n",
      "Iter-88480, train loss-0.2030, acc-0.9400, valid loss-0.3642, acc-0.8976, test loss-0.3694, acc-0.8937\n",
      "Iter-88490, train loss-0.3927, acc-0.9200, valid loss-0.3641, acc-0.8976, test loss-0.3694, acc-0.8937\n",
      "Iter-88500, train loss-0.3554, acc-0.9000, valid loss-0.3641, acc-0.8976, test loss-0.3694, acc-0.8938\n",
      "Iter-88510, train loss-0.4400, acc-0.9000, valid loss-0.3641, acc-0.8978, test loss-0.3694, acc-0.8939\n",
      "Iter-88520, train loss-0.3632, acc-0.9000, valid loss-0.3641, acc-0.8976, test loss-0.3694, acc-0.8939\n",
      "Iter-88530, train loss-0.4022, acc-0.8800, valid loss-0.3641, acc-0.8976, test loss-0.3693, acc-0.8938\n",
      "Iter-88540, train loss-0.5098, acc-0.7800, valid loss-0.3640, acc-0.8978, test loss-0.3693, acc-0.8938\n",
      "Iter-88550, train loss-0.5460, acc-0.8400, valid loss-0.3640, acc-0.8976, test loss-0.3693, acc-0.8938\n",
      "Iter-88560, train loss-0.3428, acc-0.9200, valid loss-0.3640, acc-0.8976, test loss-0.3693, acc-0.8938\n",
      "Iter-88570, train loss-0.3917, acc-0.9000, valid loss-0.3640, acc-0.8976, test loss-0.3693, acc-0.8938\n",
      "Iter-88580, train loss-0.4414, acc-0.8000, valid loss-0.3640, acc-0.8978, test loss-0.3693, acc-0.8938\n",
      "Iter-88590, train loss-0.3786, acc-0.8400, valid loss-0.3640, acc-0.8978, test loss-0.3693, acc-0.8939\n",
      "Iter-88600, train loss-0.5440, acc-0.8200, valid loss-0.3640, acc-0.8974, test loss-0.3693, acc-0.8938\n",
      "Iter-88610, train loss-0.4410, acc-0.8800, valid loss-0.3639, acc-0.8976, test loss-0.3692, acc-0.8938\n",
      "Iter-88620, train loss-0.3130, acc-0.9000, valid loss-0.3639, acc-0.8976, test loss-0.3692, acc-0.8939\n",
      "Iter-88630, train loss-0.5149, acc-0.8800, valid loss-0.3639, acc-0.8976, test loss-0.3692, acc-0.8938\n",
      "Iter-88640, train loss-0.5820, acc-0.8400, valid loss-0.3639, acc-0.8978, test loss-0.3692, acc-0.8938\n",
      "Iter-88650, train loss-0.3242, acc-0.9400, valid loss-0.3639, acc-0.8976, test loss-0.3692, acc-0.8939\n",
      "Iter-88660, train loss-0.3981, acc-0.8600, valid loss-0.3639, acc-0.8976, test loss-0.3692, acc-0.8939\n",
      "Iter-88670, train loss-0.3982, acc-0.8800, valid loss-0.3639, acc-0.8976, test loss-0.3692, acc-0.8940\n",
      "Iter-88680, train loss-0.2441, acc-0.9800, valid loss-0.3638, acc-0.8976, test loss-0.3692, acc-0.8941\n",
      "Iter-88690, train loss-0.1928, acc-0.9600, valid loss-0.3638, acc-0.8974, test loss-0.3691, acc-0.8941\n",
      "Iter-88700, train loss-0.3528, acc-0.9000, valid loss-0.3638, acc-0.8974, test loss-0.3691, acc-0.8941\n",
      "Iter-88710, train loss-0.5123, acc-0.8800, valid loss-0.3638, acc-0.8974, test loss-0.3691, acc-0.8941\n",
      "Iter-88720, train loss-0.4560, acc-0.8800, valid loss-0.3638, acc-0.8976, test loss-0.3691, acc-0.8941\n",
      "Iter-88730, train loss-0.3828, acc-0.8800, valid loss-0.3638, acc-0.8974, test loss-0.3691, acc-0.8941\n",
      "Iter-88740, train loss-0.4077, acc-0.8800, valid loss-0.3638, acc-0.8978, test loss-0.3691, acc-0.8941\n",
      "Iter-88750, train loss-0.3583, acc-0.8800, valid loss-0.3637, acc-0.8976, test loss-0.3691, acc-0.8941\n",
      "Iter-88760, train loss-0.5391, acc-0.8800, valid loss-0.3637, acc-0.8978, test loss-0.3690, acc-0.8942\n",
      "Iter-88770, train loss-0.2868, acc-0.9000, valid loss-0.3637, acc-0.8976, test loss-0.3690, acc-0.8942\n",
      "Iter-88780, train loss-0.5256, acc-0.8200, valid loss-0.3637, acc-0.8978, test loss-0.3690, acc-0.8941\n",
      "Iter-88790, train loss-0.2962, acc-0.9200, valid loss-0.3637, acc-0.8976, test loss-0.3690, acc-0.8941\n",
      "Iter-88800, train loss-0.4583, acc-0.9200, valid loss-0.3637, acc-0.8978, test loss-0.3690, acc-0.8942\n",
      "Iter-88810, train loss-0.3314, acc-0.8800, valid loss-0.3637, acc-0.8976, test loss-0.3690, acc-0.8941\n",
      "Iter-88820, train loss-0.3215, acc-0.9400, valid loss-0.3637, acc-0.8976, test loss-0.3690, acc-0.8941\n",
      "Iter-88830, train loss-0.2679, acc-0.9200, valid loss-0.3636, acc-0.8976, test loss-0.3690, acc-0.8942\n",
      "Iter-88840, train loss-0.4450, acc-0.8400, valid loss-0.3636, acc-0.8976, test loss-0.3690, acc-0.8942\n",
      "Iter-88850, train loss-0.4239, acc-0.8600, valid loss-0.3636, acc-0.8976, test loss-0.3689, acc-0.8942\n",
      "Iter-88860, train loss-0.4688, acc-0.8400, valid loss-0.3636, acc-0.8976, test loss-0.3689, acc-0.8942\n",
      "Iter-88870, train loss-0.2925, acc-0.9000, valid loss-0.3636, acc-0.8976, test loss-0.3689, acc-0.8941\n",
      "Iter-88880, train loss-0.2470, acc-0.9400, valid loss-0.3636, acc-0.8976, test loss-0.3689, acc-0.8941\n",
      "Iter-88890, train loss-0.3364, acc-0.9200, valid loss-0.3636, acc-0.8976, test loss-0.3689, acc-0.8941\n",
      "Iter-88900, train loss-0.4202, acc-0.8400, valid loss-0.3636, acc-0.8972, test loss-0.3689, acc-0.8941\n",
      "Iter-88910, train loss-0.4940, acc-0.8600, valid loss-0.3635, acc-0.8972, test loss-0.3689, acc-0.8940\n",
      "Iter-88920, train loss-0.3763, acc-0.8800, valid loss-0.3635, acc-0.8970, test loss-0.3689, acc-0.8941\n",
      "Iter-88930, train loss-0.3598, acc-0.9200, valid loss-0.3635, acc-0.8968, test loss-0.3689, acc-0.8941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-88940, train loss-0.4957, acc-0.8800, valid loss-0.3635, acc-0.8970, test loss-0.3688, acc-0.8941\n",
      "Iter-88950, train loss-0.4680, acc-0.8400, valid loss-0.3635, acc-0.8970, test loss-0.3688, acc-0.8941\n",
      "Iter-88960, train loss-0.2548, acc-0.9400, valid loss-0.3635, acc-0.8970, test loss-0.3688, acc-0.8941\n",
      "Iter-88970, train loss-0.2233, acc-0.9400, valid loss-0.3634, acc-0.8970, test loss-0.3688, acc-0.8941\n",
      "Iter-88980, train loss-0.3374, acc-0.8800, valid loss-0.3634, acc-0.8970, test loss-0.3688, acc-0.8940\n",
      "Iter-88990, train loss-0.1784, acc-0.9400, valid loss-0.3634, acc-0.8970, test loss-0.3688, acc-0.8940\n",
      "Iter-89000, train loss-0.2460, acc-0.9200, valid loss-0.3634, acc-0.8974, test loss-0.3688, acc-0.8941\n",
      "Iter-89010, train loss-0.4154, acc-0.8800, valid loss-0.3634, acc-0.8974, test loss-0.3688, acc-0.8941\n",
      "Iter-89020, train loss-0.6016, acc-0.8000, valid loss-0.3633, acc-0.8972, test loss-0.3688, acc-0.8941\n",
      "Iter-89030, train loss-0.3533, acc-0.8800, valid loss-0.3633, acc-0.8974, test loss-0.3687, acc-0.8942\n",
      "Iter-89040, train loss-0.2966, acc-0.9000, valid loss-0.3633, acc-0.8974, test loss-0.3687, acc-0.8943\n",
      "Iter-89050, train loss-0.2989, acc-0.9400, valid loss-0.3633, acc-0.8972, test loss-0.3687, acc-0.8944\n",
      "Iter-89060, train loss-0.4548, acc-0.9000, valid loss-0.3633, acc-0.8972, test loss-0.3687, acc-0.8942\n",
      "Iter-89070, train loss-0.3107, acc-0.9200, valid loss-0.3632, acc-0.8976, test loss-0.3687, acc-0.8942\n",
      "Iter-89080, train loss-0.4987, acc-0.8600, valid loss-0.3632, acc-0.8980, test loss-0.3687, acc-0.8942\n",
      "Iter-89090, train loss-0.3573, acc-0.8400, valid loss-0.3632, acc-0.8980, test loss-0.3686, acc-0.8942\n",
      "Iter-89100, train loss-0.2516, acc-0.9200, valid loss-0.3632, acc-0.8980, test loss-0.3686, acc-0.8943\n",
      "Iter-89110, train loss-0.3458, acc-0.9200, valid loss-0.3632, acc-0.8978, test loss-0.3686, acc-0.8943\n",
      "Iter-89120, train loss-0.2829, acc-0.9200, valid loss-0.3632, acc-0.8982, test loss-0.3686, acc-0.8943\n",
      "Iter-89130, train loss-0.4368, acc-0.8400, valid loss-0.3632, acc-0.8976, test loss-0.3686, acc-0.8943\n",
      "Iter-89140, train loss-0.2963, acc-0.9600, valid loss-0.3631, acc-0.8978, test loss-0.3686, acc-0.8942\n",
      "Iter-89150, train loss-0.4986, acc-0.8200, valid loss-0.3631, acc-0.8978, test loss-0.3686, acc-0.8942\n",
      "Iter-89160, train loss-0.2586, acc-0.9200, valid loss-0.3631, acc-0.8978, test loss-0.3685, acc-0.8942\n",
      "Iter-89170, train loss-0.2809, acc-0.9400, valid loss-0.3631, acc-0.8980, test loss-0.3685, acc-0.8942\n",
      "Iter-89180, train loss-0.4679, acc-0.8600, valid loss-0.3631, acc-0.8980, test loss-0.3685, acc-0.8942\n",
      "Iter-89190, train loss-0.3488, acc-0.8600, valid loss-0.3631, acc-0.8978, test loss-0.3685, acc-0.8942\n",
      "Iter-89200, train loss-0.3929, acc-0.8800, valid loss-0.3631, acc-0.8974, test loss-0.3685, acc-0.8943\n",
      "Iter-89210, train loss-0.4152, acc-0.9000, valid loss-0.3630, acc-0.8972, test loss-0.3685, acc-0.8943\n",
      "Iter-89220, train loss-0.3209, acc-0.9400, valid loss-0.3630, acc-0.8974, test loss-0.3685, acc-0.8943\n",
      "Iter-89230, train loss-0.3659, acc-0.9200, valid loss-0.3630, acc-0.8974, test loss-0.3685, acc-0.8942\n",
      "Iter-89240, train loss-0.4693, acc-0.8000, valid loss-0.3630, acc-0.8974, test loss-0.3685, acc-0.8943\n",
      "Iter-89250, train loss-0.5774, acc-0.8600, valid loss-0.3630, acc-0.8976, test loss-0.3685, acc-0.8943\n",
      "Iter-89260, train loss-0.2405, acc-0.9200, valid loss-0.3630, acc-0.8974, test loss-0.3684, acc-0.8942\n",
      "Iter-89270, train loss-0.4224, acc-0.8600, valid loss-0.3630, acc-0.8972, test loss-0.3684, acc-0.8942\n",
      "Iter-89280, train loss-0.3983, acc-0.8600, valid loss-0.3629, acc-0.8970, test loss-0.3684, acc-0.8942\n",
      "Iter-89290, train loss-0.5052, acc-0.9000, valid loss-0.3629, acc-0.8970, test loss-0.3684, acc-0.8942\n",
      "Iter-89300, train loss-0.1885, acc-0.9600, valid loss-0.3629, acc-0.8972, test loss-0.3684, acc-0.8942\n",
      "Iter-89310, train loss-0.3244, acc-0.9200, valid loss-0.3629, acc-0.8972, test loss-0.3684, acc-0.8943\n",
      "Iter-89320, train loss-0.3205, acc-0.8600, valid loss-0.3629, acc-0.8974, test loss-0.3684, acc-0.8943\n",
      "Iter-89330, train loss-0.2861, acc-0.9000, valid loss-0.3629, acc-0.8976, test loss-0.3684, acc-0.8943\n",
      "Iter-89340, train loss-0.5290, acc-0.8400, valid loss-0.3629, acc-0.8976, test loss-0.3684, acc-0.8942\n",
      "Iter-89350, train loss-0.4507, acc-0.8000, valid loss-0.3629, acc-0.8976, test loss-0.3683, acc-0.8942\n",
      "Iter-89360, train loss-0.2488, acc-0.9400, valid loss-0.3629, acc-0.8978, test loss-0.3683, acc-0.8941\n",
      "Iter-89370, train loss-0.3159, acc-0.9000, valid loss-0.3628, acc-0.8976, test loss-0.3683, acc-0.8942\n",
      "Iter-89380, train loss-0.4963, acc-0.8400, valid loss-0.3628, acc-0.8976, test loss-0.3683, acc-0.8942\n",
      "Iter-89390, train loss-0.2479, acc-0.9400, valid loss-0.3628, acc-0.8976, test loss-0.3683, acc-0.8943\n",
      "Iter-89400, train loss-0.4218, acc-0.8800, valid loss-0.3628, acc-0.8976, test loss-0.3683, acc-0.8943\n",
      "Iter-89410, train loss-0.4965, acc-0.8400, valid loss-0.3628, acc-0.8976, test loss-0.3683, acc-0.8941\n",
      "Iter-89420, train loss-0.3116, acc-0.9000, valid loss-0.3628, acc-0.8974, test loss-0.3683, acc-0.8942\n",
      "Iter-89430, train loss-0.5532, acc-0.8800, valid loss-0.3628, acc-0.8972, test loss-0.3682, acc-0.8942\n",
      "Iter-89440, train loss-0.4250, acc-0.8600, valid loss-0.3627, acc-0.8976, test loss-0.3682, acc-0.8942\n",
      "Iter-89450, train loss-0.3845, acc-0.9000, valid loss-0.3627, acc-0.8972, test loss-0.3682, acc-0.8942\n",
      "Iter-89460, train loss-0.2511, acc-0.9400, valid loss-0.3627, acc-0.8972, test loss-0.3682, acc-0.8942\n",
      "Iter-89470, train loss-0.3388, acc-0.9000, valid loss-0.3627, acc-0.8972, test loss-0.3682, acc-0.8942\n",
      "Iter-89480, train loss-0.5366, acc-0.8400, valid loss-0.3627, acc-0.8972, test loss-0.3682, acc-0.8942\n",
      "Iter-89490, train loss-0.3705, acc-0.9000, valid loss-0.3627, acc-0.8972, test loss-0.3682, acc-0.8942\n",
      "Iter-89500, train loss-0.4330, acc-0.8600, valid loss-0.3626, acc-0.8972, test loss-0.3682, acc-0.8942\n",
      "Iter-89510, train loss-0.4426, acc-0.9000, valid loss-0.3626, acc-0.8972, test loss-0.3682, acc-0.8943\n",
      "Iter-89520, train loss-0.6120, acc-0.8800, valid loss-0.3626, acc-0.8972, test loss-0.3681, acc-0.8943\n",
      "Iter-89530, train loss-0.2815, acc-0.9400, valid loss-0.3626, acc-0.8972, test loss-0.3681, acc-0.8944\n",
      "Iter-89540, train loss-0.4745, acc-0.8800, valid loss-0.3626, acc-0.8972, test loss-0.3681, acc-0.8943\n",
      "Iter-89550, train loss-0.3820, acc-0.9200, valid loss-0.3626, acc-0.8972, test loss-0.3681, acc-0.8943\n",
      "Iter-89560, train loss-0.3022, acc-0.9200, valid loss-0.3626, acc-0.8972, test loss-0.3681, acc-0.8943\n",
      "Iter-89570, train loss-0.3252, acc-0.9400, valid loss-0.3625, acc-0.8972, test loss-0.3681, acc-0.8943\n",
      "Iter-89580, train loss-0.3287, acc-0.9400, valid loss-0.3625, acc-0.8972, test loss-0.3681, acc-0.8944\n",
      "Iter-89590, train loss-0.3780, acc-0.9200, valid loss-0.3625, acc-0.8974, test loss-0.3681, acc-0.8944\n",
      "Iter-89600, train loss-0.4686, acc-0.8400, valid loss-0.3625, acc-0.8972, test loss-0.3680, acc-0.8944\n",
      "Iter-89610, train loss-0.4004, acc-0.8800, valid loss-0.3625, acc-0.8972, test loss-0.3680, acc-0.8944\n",
      "Iter-89620, train loss-0.5160, acc-0.8600, valid loss-0.3624, acc-0.8972, test loss-0.3680, acc-0.8943\n",
      "Iter-89630, train loss-0.5364, acc-0.8400, valid loss-0.3624, acc-0.8972, test loss-0.3680, acc-0.8944\n",
      "Iter-89640, train loss-0.3355, acc-0.9200, valid loss-0.3624, acc-0.8972, test loss-0.3679, acc-0.8942\n",
      "Iter-89650, train loss-0.3479, acc-0.8600, valid loss-0.3624, acc-0.8972, test loss-0.3679, acc-0.8943\n",
      "Iter-89660, train loss-0.1502, acc-0.9600, valid loss-0.3624, acc-0.8972, test loss-0.3679, acc-0.8943\n",
      "Iter-89670, train loss-0.3784, acc-0.8800, valid loss-0.3624, acc-0.8970, test loss-0.3679, acc-0.8943\n",
      "Iter-89680, train loss-0.3428, acc-0.8600, valid loss-0.3623, acc-0.8970, test loss-0.3679, acc-0.8943\n",
      "Iter-89690, train loss-0.4654, acc-0.8200, valid loss-0.3623, acc-0.8972, test loss-0.3679, acc-0.8943\n",
      "Iter-89700, train loss-0.4348, acc-0.8800, valid loss-0.3623, acc-0.8972, test loss-0.3679, acc-0.8941\n",
      "Iter-89710, train loss-0.5920, acc-0.9000, valid loss-0.3623, acc-0.8974, test loss-0.3679, acc-0.8941\n",
      "Iter-89720, train loss-0.3336, acc-0.9200, valid loss-0.3623, acc-0.8974, test loss-0.3679, acc-0.8941\n",
      "Iter-89730, train loss-0.4043, acc-0.9000, valid loss-0.3623, acc-0.8974, test loss-0.3679, acc-0.8941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-89740, train loss-0.5525, acc-0.8000, valid loss-0.3623, acc-0.8974, test loss-0.3679, acc-0.8941\n",
      "Iter-89750, train loss-0.5896, acc-0.8200, valid loss-0.3622, acc-0.8970, test loss-0.3678, acc-0.8941\n",
      "Iter-89760, train loss-0.4448, acc-0.8600, valid loss-0.3622, acc-0.8970, test loss-0.3678, acc-0.8942\n",
      "Iter-89770, train loss-0.4034, acc-0.9000, valid loss-0.3622, acc-0.8970, test loss-0.3678, acc-0.8943\n",
      "Iter-89780, train loss-0.4334, acc-0.9200, valid loss-0.3622, acc-0.8970, test loss-0.3678, acc-0.8943\n",
      "Iter-89790, train loss-0.4039, acc-0.8800, valid loss-0.3622, acc-0.8970, test loss-0.3678, acc-0.8943\n",
      "Iter-89800, train loss-0.3364, acc-0.9000, valid loss-0.3622, acc-0.8970, test loss-0.3678, acc-0.8943\n",
      "Iter-89810, train loss-0.6099, acc-0.8000, valid loss-0.3622, acc-0.8970, test loss-0.3677, acc-0.8941\n",
      "Iter-89820, train loss-0.5105, acc-0.8400, valid loss-0.3622, acc-0.8970, test loss-0.3677, acc-0.8941\n",
      "Iter-89830, train loss-0.4950, acc-0.8400, valid loss-0.3621, acc-0.8968, test loss-0.3677, acc-0.8942\n",
      "Iter-89840, train loss-0.3602, acc-0.8800, valid loss-0.3621, acc-0.8968, test loss-0.3677, acc-0.8943\n",
      "Iter-89850, train loss-0.3323, acc-0.9000, valid loss-0.3621, acc-0.8972, test loss-0.3677, acc-0.8941\n",
      "Iter-89860, train loss-0.2402, acc-0.9000, valid loss-0.3621, acc-0.8972, test loss-0.3677, acc-0.8943\n",
      "Iter-89870, train loss-0.4734, acc-0.8400, valid loss-0.3621, acc-0.8970, test loss-0.3677, acc-0.8942\n",
      "Iter-89880, train loss-0.3777, acc-0.8400, valid loss-0.3621, acc-0.8972, test loss-0.3677, acc-0.8942\n",
      "Iter-89890, train loss-0.4246, acc-0.8600, valid loss-0.3621, acc-0.8970, test loss-0.3676, acc-0.8942\n",
      "Iter-89900, train loss-0.4169, acc-0.9000, valid loss-0.3621, acc-0.8970, test loss-0.3676, acc-0.8942\n",
      "Iter-89910, train loss-0.4385, acc-0.8800, valid loss-0.3621, acc-0.8970, test loss-0.3676, acc-0.8940\n",
      "Iter-89920, train loss-0.4273, acc-0.8600, valid loss-0.3620, acc-0.8972, test loss-0.3676, acc-0.8941\n",
      "Iter-89930, train loss-0.4178, acc-0.9000, valid loss-0.3620, acc-0.8972, test loss-0.3676, acc-0.8940\n",
      "Iter-89940, train loss-0.2862, acc-0.9200, valid loss-0.3620, acc-0.8974, test loss-0.3676, acc-0.8942\n",
      "Iter-89950, train loss-0.3586, acc-0.9200, valid loss-0.3620, acc-0.8970, test loss-0.3676, acc-0.8942\n",
      "Iter-89960, train loss-0.6079, acc-0.7600, valid loss-0.3620, acc-0.8972, test loss-0.3675, acc-0.8942\n",
      "Iter-89970, train loss-0.2934, acc-0.9000, valid loss-0.3620, acc-0.8974, test loss-0.3675, acc-0.8942\n",
      "Iter-89980, train loss-0.3546, acc-0.9400, valid loss-0.3620, acc-0.8976, test loss-0.3675, acc-0.8942\n",
      "Iter-89990, train loss-0.2749, acc-0.9200, valid loss-0.3619, acc-0.8976, test loss-0.3675, acc-0.8942\n",
      "Iter-90000, train loss-0.4547, acc-0.8400, valid loss-0.3619, acc-0.8976, test loss-0.3675, acc-0.8942\n",
      "Iter-90010, train loss-0.3996, acc-0.8600, valid loss-0.3619, acc-0.8976, test loss-0.3675, acc-0.8941\n",
      "Iter-90020, train loss-0.3616, acc-0.9000, valid loss-0.3619, acc-0.8974, test loss-0.3674, acc-0.8942\n",
      "Iter-90030, train loss-0.4525, acc-0.8800, valid loss-0.3619, acc-0.8974, test loss-0.3674, acc-0.8943\n",
      "Iter-90040, train loss-0.3866, acc-0.8800, valid loss-0.3619, acc-0.8974, test loss-0.3674, acc-0.8944\n",
      "Iter-90050, train loss-0.2919, acc-0.9000, valid loss-0.3619, acc-0.8974, test loss-0.3674, acc-0.8943\n",
      "Iter-90060, train loss-0.5963, acc-0.8600, valid loss-0.3619, acc-0.8976, test loss-0.3674, acc-0.8943\n",
      "Iter-90070, train loss-0.4255, acc-0.9000, valid loss-0.3619, acc-0.8976, test loss-0.3674, acc-0.8943\n",
      "Iter-90080, train loss-0.2311, acc-0.9200, valid loss-0.3618, acc-0.8976, test loss-0.3673, acc-0.8943\n",
      "Iter-90090, train loss-0.5716, acc-0.9200, valid loss-0.3618, acc-0.8976, test loss-0.3673, acc-0.8942\n",
      "Iter-90100, train loss-0.3032, acc-0.9000, valid loss-0.3618, acc-0.8976, test loss-0.3673, acc-0.8942\n",
      "Iter-90110, train loss-0.2907, acc-0.9400, valid loss-0.3618, acc-0.8976, test loss-0.3673, acc-0.8942\n",
      "Iter-90120, train loss-0.4430, acc-0.8600, valid loss-0.3618, acc-0.8976, test loss-0.3673, acc-0.8943\n",
      "Iter-90130, train loss-0.4369, acc-0.8800, valid loss-0.3618, acc-0.8976, test loss-0.3673, acc-0.8942\n",
      "Iter-90140, train loss-0.5091, acc-0.7800, valid loss-0.3618, acc-0.8976, test loss-0.3673, acc-0.8942\n",
      "Iter-90150, train loss-0.3474, acc-0.9000, valid loss-0.3618, acc-0.8974, test loss-0.3673, acc-0.8943\n",
      "Iter-90160, train loss-0.3538, acc-0.9200, valid loss-0.3617, acc-0.8974, test loss-0.3672, acc-0.8942\n",
      "Iter-90170, train loss-0.3851, acc-0.8600, valid loss-0.3617, acc-0.8974, test loss-0.3672, acc-0.8943\n",
      "Iter-90180, train loss-0.3394, acc-0.8800, valid loss-0.3617, acc-0.8976, test loss-0.3672, acc-0.8943\n",
      "Iter-90190, train loss-0.2795, acc-0.9200, valid loss-0.3617, acc-0.8976, test loss-0.3672, acc-0.8943\n",
      "Iter-90200, train loss-0.3329, acc-0.9000, valid loss-0.3617, acc-0.8976, test loss-0.3672, acc-0.8943\n",
      "Iter-90210, train loss-0.3847, acc-0.8600, valid loss-0.3616, acc-0.8976, test loss-0.3672, acc-0.8942\n",
      "Iter-90220, train loss-0.3153, acc-0.9400, valid loss-0.3616, acc-0.8976, test loss-0.3672, acc-0.8941\n",
      "Iter-90230, train loss-0.5597, acc-0.8200, valid loss-0.3616, acc-0.8978, test loss-0.3671, acc-0.8941\n",
      "Iter-90240, train loss-0.3981, acc-0.8600, valid loss-0.3616, acc-0.8978, test loss-0.3671, acc-0.8940\n",
      "Iter-90250, train loss-0.2925, acc-0.9400, valid loss-0.3616, acc-0.8978, test loss-0.3671, acc-0.8940\n",
      "Iter-90260, train loss-0.4842, acc-0.8800, valid loss-0.3616, acc-0.8980, test loss-0.3671, acc-0.8940\n",
      "Iter-90270, train loss-0.4988, acc-0.9000, valid loss-0.3616, acc-0.8980, test loss-0.3671, acc-0.8938\n",
      "Iter-90280, train loss-0.4144, acc-0.9000, valid loss-0.3615, acc-0.8978, test loss-0.3671, acc-0.8938\n",
      "Iter-90290, train loss-0.3969, acc-0.8600, valid loss-0.3615, acc-0.8978, test loss-0.3671, acc-0.8941\n",
      "Iter-90300, train loss-0.2472, acc-0.9400, valid loss-0.3615, acc-0.8978, test loss-0.3671, acc-0.8942\n",
      "Iter-90310, train loss-0.3824, acc-0.8800, valid loss-0.3615, acc-0.8976, test loss-0.3670, acc-0.8942\n",
      "Iter-90320, train loss-0.2164, acc-0.9400, valid loss-0.3615, acc-0.8976, test loss-0.3670, acc-0.8943\n",
      "Iter-90330, train loss-0.2197, acc-0.9400, valid loss-0.3615, acc-0.8976, test loss-0.3670, acc-0.8941\n",
      "Iter-90340, train loss-0.4620, acc-0.8600, valid loss-0.3615, acc-0.8976, test loss-0.3670, acc-0.8942\n",
      "Iter-90350, train loss-0.4170, acc-0.8800, valid loss-0.3615, acc-0.8976, test loss-0.3670, acc-0.8942\n",
      "Iter-90360, train loss-0.5694, acc-0.8800, valid loss-0.3614, acc-0.8976, test loss-0.3670, acc-0.8941\n",
      "Iter-90370, train loss-0.3521, acc-0.9000, valid loss-0.3614, acc-0.8974, test loss-0.3670, acc-0.8941\n",
      "Iter-90380, train loss-0.5457, acc-0.9000, valid loss-0.3614, acc-0.8972, test loss-0.3670, acc-0.8940\n",
      "Iter-90390, train loss-0.3206, acc-0.9000, valid loss-0.3614, acc-0.8972, test loss-0.3670, acc-0.8941\n",
      "Iter-90400, train loss-0.1903, acc-0.9800, valid loss-0.3614, acc-0.8974, test loss-0.3669, acc-0.8938\n",
      "Iter-90410, train loss-0.2858, acc-0.9000, valid loss-0.3614, acc-0.8976, test loss-0.3669, acc-0.8939\n",
      "Iter-90420, train loss-0.2944, acc-0.9000, valid loss-0.3614, acc-0.8974, test loss-0.3669, acc-0.8939\n",
      "Iter-90430, train loss-0.3882, acc-0.9000, valid loss-0.3614, acc-0.8974, test loss-0.3669, acc-0.8938\n",
      "Iter-90440, train loss-0.4507, acc-0.8800, valid loss-0.3613, acc-0.8976, test loss-0.3669, acc-0.8939\n",
      "Iter-90450, train loss-0.2822, acc-0.9600, valid loss-0.3613, acc-0.8976, test loss-0.3668, acc-0.8942\n",
      "Iter-90460, train loss-0.4139, acc-0.9000, valid loss-0.3613, acc-0.8976, test loss-0.3668, acc-0.8941\n",
      "Iter-90470, train loss-0.3255, acc-0.9200, valid loss-0.3613, acc-0.8976, test loss-0.3668, acc-0.8941\n",
      "Iter-90480, train loss-0.2966, acc-0.9200, valid loss-0.3613, acc-0.8976, test loss-0.3668, acc-0.8942\n",
      "Iter-90490, train loss-0.6330, acc-0.8000, valid loss-0.3613, acc-0.8978, test loss-0.3668, acc-0.8942\n",
      "Iter-90500, train loss-0.3855, acc-0.8800, valid loss-0.3613, acc-0.8978, test loss-0.3668, acc-0.8942\n",
      "Iter-90510, train loss-0.4527, acc-0.8400, valid loss-0.3612, acc-0.8976, test loss-0.3668, acc-0.8941\n",
      "Iter-90520, train loss-0.3121, acc-0.9400, valid loss-0.3612, acc-0.8978, test loss-0.3668, acc-0.8941\n",
      "Iter-90530, train loss-0.2715, acc-0.9000, valid loss-0.3612, acc-0.8980, test loss-0.3668, acc-0.8941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90540, train loss-0.5032, acc-0.8600, valid loss-0.3612, acc-0.8980, test loss-0.3667, acc-0.8942\n",
      "Iter-90550, train loss-0.4590, acc-0.8800, valid loss-0.3612, acc-0.8980, test loss-0.3667, acc-0.8942\n",
      "Iter-90560, train loss-0.4978, acc-0.9000, valid loss-0.3612, acc-0.8980, test loss-0.3667, acc-0.8943\n",
      "Iter-90570, train loss-0.7095, acc-0.7600, valid loss-0.3612, acc-0.8980, test loss-0.3667, acc-0.8943\n",
      "Iter-90580, train loss-0.5573, acc-0.8400, valid loss-0.3611, acc-0.8980, test loss-0.3667, acc-0.8943\n",
      "Iter-90590, train loss-0.4393, acc-0.8800, valid loss-0.3611, acc-0.8980, test loss-0.3667, acc-0.8943\n",
      "Iter-90600, train loss-0.2692, acc-0.9400, valid loss-0.3611, acc-0.8982, test loss-0.3667, acc-0.8942\n",
      "Iter-90610, train loss-0.3090, acc-0.9200, valid loss-0.3611, acc-0.8984, test loss-0.3666, acc-0.8942\n",
      "Iter-90620, train loss-0.5939, acc-0.9000, valid loss-0.3611, acc-0.8984, test loss-0.3666, acc-0.8942\n",
      "Iter-90630, train loss-0.1529, acc-0.9600, valid loss-0.3611, acc-0.8984, test loss-0.3666, acc-0.8942\n",
      "Iter-90640, train loss-0.3775, acc-0.9000, valid loss-0.3611, acc-0.8984, test loss-0.3666, acc-0.8942\n",
      "Iter-90650, train loss-0.2744, acc-0.9200, valid loss-0.3611, acc-0.8982, test loss-0.3666, acc-0.8942\n",
      "Iter-90660, train loss-0.2328, acc-0.9400, valid loss-0.3611, acc-0.8980, test loss-0.3666, acc-0.8942\n",
      "Iter-90670, train loss-0.4090, acc-0.8400, valid loss-0.3610, acc-0.8984, test loss-0.3666, acc-0.8942\n",
      "Iter-90680, train loss-0.2036, acc-0.9800, valid loss-0.3610, acc-0.8982, test loss-0.3665, acc-0.8942\n",
      "Iter-90690, train loss-0.7760, acc-0.7400, valid loss-0.3610, acc-0.8980, test loss-0.3665, acc-0.8942\n",
      "Iter-90700, train loss-0.3675, acc-0.8800, valid loss-0.3610, acc-0.8980, test loss-0.3665, acc-0.8943\n",
      "Iter-90710, train loss-0.2534, acc-0.9400, valid loss-0.3610, acc-0.8980, test loss-0.3665, acc-0.8942\n",
      "Iter-90720, train loss-0.3206, acc-0.9600, valid loss-0.3610, acc-0.8980, test loss-0.3665, acc-0.8942\n",
      "Iter-90730, train loss-0.3564, acc-0.9400, valid loss-0.3610, acc-0.8980, test loss-0.3665, acc-0.8942\n",
      "Iter-90740, train loss-0.3465, acc-0.9200, valid loss-0.3610, acc-0.8980, test loss-0.3665, acc-0.8942\n",
      "Iter-90750, train loss-0.2139, acc-0.9600, valid loss-0.3610, acc-0.8980, test loss-0.3665, acc-0.8942\n",
      "Iter-90760, train loss-0.3426, acc-0.9200, valid loss-0.3609, acc-0.8980, test loss-0.3665, acc-0.8942\n",
      "Iter-90770, train loss-0.3488, acc-0.8400, valid loss-0.3609, acc-0.8976, test loss-0.3664, acc-0.8942\n",
      "Iter-90780, train loss-0.3417, acc-0.9400, valid loss-0.3609, acc-0.8976, test loss-0.3664, acc-0.8943\n",
      "Iter-90790, train loss-0.4537, acc-0.8800, valid loss-0.3609, acc-0.8976, test loss-0.3664, acc-0.8943\n",
      "Iter-90800, train loss-0.4052, acc-0.8800, valid loss-0.3609, acc-0.8976, test loss-0.3664, acc-0.8943\n",
      "Iter-90810, train loss-0.3152, acc-0.9200, valid loss-0.3609, acc-0.8976, test loss-0.3664, acc-0.8942\n",
      "Iter-90820, train loss-0.4726, acc-0.8800, valid loss-0.3609, acc-0.8978, test loss-0.3664, acc-0.8941\n",
      "Iter-90830, train loss-0.2137, acc-0.9600, valid loss-0.3608, acc-0.8976, test loss-0.3664, acc-0.8942\n",
      "Iter-90840, train loss-0.4204, acc-0.8800, valid loss-0.3608, acc-0.8978, test loss-0.3663, acc-0.8942\n",
      "Iter-90850, train loss-0.3230, acc-0.9000, valid loss-0.3608, acc-0.8976, test loss-0.3663, acc-0.8941\n",
      "Iter-90860, train loss-0.3815, acc-0.8800, valid loss-0.3608, acc-0.8978, test loss-0.3663, acc-0.8942\n",
      "Iter-90870, train loss-0.4789, acc-0.8400, valid loss-0.3608, acc-0.8978, test loss-0.3663, acc-0.8943\n",
      "Iter-90880, train loss-0.3616, acc-0.9200, valid loss-0.3608, acc-0.8976, test loss-0.3663, acc-0.8943\n",
      "Iter-90890, train loss-0.3547, acc-0.8600, valid loss-0.3607, acc-0.8976, test loss-0.3663, acc-0.8943\n",
      "Iter-90900, train loss-0.3981, acc-0.9000, valid loss-0.3607, acc-0.8976, test loss-0.3663, acc-0.8944\n",
      "Iter-90910, train loss-0.4780, acc-0.8800, valid loss-0.3607, acc-0.8976, test loss-0.3663, acc-0.8944\n",
      "Iter-90920, train loss-0.5246, acc-0.8600, valid loss-0.3607, acc-0.8978, test loss-0.3663, acc-0.8944\n",
      "Iter-90930, train loss-0.3985, acc-0.9000, valid loss-0.3607, acc-0.8978, test loss-0.3663, acc-0.8943\n",
      "Iter-90940, train loss-0.3372, acc-0.8800, valid loss-0.3607, acc-0.8978, test loss-0.3663, acc-0.8943\n",
      "Iter-90950, train loss-0.2906, acc-0.9000, valid loss-0.3607, acc-0.8978, test loss-0.3662, acc-0.8942\n",
      "Iter-90960, train loss-0.2298, acc-0.9600, valid loss-0.3607, acc-0.8976, test loss-0.3662, acc-0.8942\n",
      "Iter-90970, train loss-0.4207, acc-0.8800, valid loss-0.3607, acc-0.8976, test loss-0.3662, acc-0.8942\n",
      "Iter-90980, train loss-0.6580, acc-0.8200, valid loss-0.3607, acc-0.8978, test loss-0.3662, acc-0.8942\n",
      "Iter-90990, train loss-0.2047, acc-0.9200, valid loss-0.3607, acc-0.8978, test loss-0.3662, acc-0.8942\n",
      "Iter-91000, train loss-0.3252, acc-0.9000, valid loss-0.3607, acc-0.8982, test loss-0.3662, acc-0.8942\n",
      "Iter-91010, train loss-0.4404, acc-0.8400, valid loss-0.3606, acc-0.8980, test loss-0.3662, acc-0.8942\n",
      "Iter-91020, train loss-0.2405, acc-0.9400, valid loss-0.3606, acc-0.8978, test loss-0.3662, acc-0.8942\n",
      "Iter-91030, train loss-0.2919, acc-0.9000, valid loss-0.3606, acc-0.8980, test loss-0.3661, acc-0.8942\n",
      "Iter-91040, train loss-0.5399, acc-0.8000, valid loss-0.3606, acc-0.8976, test loss-0.3661, acc-0.8942\n",
      "Iter-91050, train loss-0.3586, acc-0.9400, valid loss-0.3606, acc-0.8978, test loss-0.3661, acc-0.8942\n",
      "Iter-91060, train loss-0.3870, acc-0.9400, valid loss-0.3606, acc-0.8976, test loss-0.3661, acc-0.8943\n",
      "Iter-91070, train loss-0.3079, acc-0.9400, valid loss-0.3606, acc-0.8976, test loss-0.3661, acc-0.8942\n",
      "Iter-91080, train loss-0.5828, acc-0.8800, valid loss-0.3606, acc-0.8976, test loss-0.3661, acc-0.8941\n",
      "Iter-91090, train loss-0.3686, acc-0.9400, valid loss-0.3606, acc-0.8976, test loss-0.3661, acc-0.8941\n",
      "Iter-91100, train loss-0.4421, acc-0.8200, valid loss-0.3605, acc-0.8978, test loss-0.3661, acc-0.8941\n",
      "Iter-91110, train loss-0.3504, acc-0.9400, valid loss-0.3605, acc-0.8978, test loss-0.3661, acc-0.8942\n",
      "Iter-91120, train loss-0.2901, acc-0.9600, valid loss-0.3605, acc-0.8978, test loss-0.3661, acc-0.8942\n",
      "Iter-91130, train loss-0.2135, acc-0.9000, valid loss-0.3605, acc-0.8978, test loss-0.3661, acc-0.8942\n",
      "Iter-91140, train loss-0.2863, acc-0.9600, valid loss-0.3604, acc-0.8978, test loss-0.3660, acc-0.8942\n",
      "Iter-91150, train loss-0.4341, acc-0.8600, valid loss-0.3604, acc-0.8978, test loss-0.3660, acc-0.8943\n",
      "Iter-91160, train loss-0.3593, acc-0.9400, valid loss-0.3604, acc-0.8978, test loss-0.3660, acc-0.8941\n",
      "Iter-91170, train loss-0.3533, acc-0.9200, valid loss-0.3604, acc-0.8978, test loss-0.3660, acc-0.8942\n",
      "Iter-91180, train loss-0.2787, acc-0.9000, valid loss-0.3604, acc-0.8978, test loss-0.3660, acc-0.8942\n",
      "Iter-91190, train loss-0.6335, acc-0.8400, valid loss-0.3604, acc-0.8978, test loss-0.3659, acc-0.8942\n",
      "Iter-91200, train loss-0.1586, acc-0.9800, valid loss-0.3604, acc-0.8978, test loss-0.3659, acc-0.8944\n",
      "Iter-91210, train loss-0.3430, acc-0.9000, valid loss-0.3603, acc-0.8978, test loss-0.3659, acc-0.8944\n",
      "Iter-91220, train loss-0.4118, acc-0.9000, valid loss-0.3603, acc-0.8978, test loss-0.3659, acc-0.8944\n",
      "Iter-91230, train loss-0.2982, acc-0.9000, valid loss-0.3603, acc-0.8978, test loss-0.3659, acc-0.8943\n",
      "Iter-91240, train loss-0.3555, acc-0.9200, valid loss-0.3603, acc-0.8978, test loss-0.3659, acc-0.8944\n",
      "Iter-91250, train loss-0.7149, acc-0.8000, valid loss-0.3603, acc-0.8978, test loss-0.3659, acc-0.8943\n",
      "Iter-91260, train loss-0.3709, acc-0.8800, valid loss-0.3602, acc-0.8978, test loss-0.3658, acc-0.8946\n",
      "Iter-91270, train loss-0.6612, acc-0.8200, valid loss-0.3602, acc-0.8980, test loss-0.3658, acc-0.8946\n",
      "Iter-91280, train loss-0.3554, acc-0.9000, valid loss-0.3602, acc-0.8980, test loss-0.3658, acc-0.8947\n",
      "Iter-91290, train loss-0.3338, acc-0.8800, valid loss-0.3602, acc-0.8978, test loss-0.3658, acc-0.8945\n",
      "Iter-91300, train loss-0.1415, acc-0.9800, valid loss-0.3602, acc-0.8978, test loss-0.3658, acc-0.8945\n",
      "Iter-91310, train loss-0.4129, acc-0.8600, valid loss-0.3602, acc-0.8980, test loss-0.3658, acc-0.8946\n",
      "Iter-91320, train loss-0.4446, acc-0.8200, valid loss-0.3602, acc-0.8980, test loss-0.3658, acc-0.8946\n",
      "Iter-91330, train loss-0.3838, acc-0.8600, valid loss-0.3602, acc-0.8978, test loss-0.3658, acc-0.8946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-91340, train loss-0.3697, acc-0.8800, valid loss-0.3601, acc-0.8978, test loss-0.3657, acc-0.8945\n",
      "Iter-91350, train loss-0.3939, acc-0.9000, valid loss-0.3601, acc-0.8978, test loss-0.3657, acc-0.8945\n",
      "Iter-91360, train loss-0.3353, acc-0.9000, valid loss-0.3601, acc-0.8978, test loss-0.3657, acc-0.8948\n",
      "Iter-91370, train loss-0.4018, acc-0.8400, valid loss-0.3601, acc-0.8978, test loss-0.3657, acc-0.8947\n",
      "Iter-91380, train loss-0.3619, acc-0.9000, valid loss-0.3601, acc-0.8980, test loss-0.3657, acc-0.8947\n",
      "Iter-91390, train loss-0.3240, acc-0.9400, valid loss-0.3601, acc-0.8982, test loss-0.3657, acc-0.8947\n",
      "Iter-91400, train loss-0.4843, acc-0.8400, valid loss-0.3600, acc-0.8982, test loss-0.3657, acc-0.8946\n",
      "Iter-91410, train loss-0.3214, acc-0.9000, valid loss-0.3600, acc-0.8980, test loss-0.3657, acc-0.8946\n",
      "Iter-91420, train loss-0.3959, acc-0.8800, valid loss-0.3600, acc-0.8980, test loss-0.3656, acc-0.8947\n",
      "Iter-91430, train loss-0.3023, acc-0.8800, valid loss-0.3600, acc-0.8978, test loss-0.3656, acc-0.8948\n",
      "Iter-91440, train loss-0.4524, acc-0.9000, valid loss-0.3600, acc-0.8978, test loss-0.3656, acc-0.8947\n",
      "Iter-91450, train loss-0.4072, acc-0.8800, valid loss-0.3600, acc-0.8980, test loss-0.3656, acc-0.8946\n",
      "Iter-91460, train loss-0.5683, acc-0.8600, valid loss-0.3600, acc-0.8978, test loss-0.3656, acc-0.8947\n",
      "Iter-91470, train loss-0.2953, acc-0.9200, valid loss-0.3600, acc-0.8978, test loss-0.3656, acc-0.8945\n",
      "Iter-91480, train loss-0.3381, acc-0.8600, valid loss-0.3599, acc-0.8978, test loss-0.3656, acc-0.8946\n",
      "Iter-91490, train loss-0.2643, acc-0.9400, valid loss-0.3599, acc-0.8978, test loss-0.3656, acc-0.8946\n",
      "Iter-91500, train loss-0.2873, acc-0.9200, valid loss-0.3599, acc-0.8978, test loss-0.3655, acc-0.8947\n",
      "Iter-91510, train loss-0.2841, acc-0.9600, valid loss-0.3599, acc-0.8980, test loss-0.3655, acc-0.8946\n",
      "Iter-91520, train loss-0.4260, acc-0.9200, valid loss-0.3599, acc-0.8978, test loss-0.3655, acc-0.8947\n",
      "Iter-91530, train loss-0.4049, acc-0.8800, valid loss-0.3599, acc-0.8980, test loss-0.3655, acc-0.8947\n",
      "Iter-91540, train loss-0.4364, acc-0.8800, valid loss-0.3599, acc-0.8980, test loss-0.3655, acc-0.8946\n",
      "Iter-91550, train loss-0.6215, acc-0.8400, valid loss-0.3599, acc-0.8980, test loss-0.3655, acc-0.8946\n",
      "Iter-91560, train loss-0.3044, acc-0.9200, valid loss-0.3598, acc-0.8980, test loss-0.3655, acc-0.8946\n",
      "Iter-91570, train loss-0.4088, acc-0.9000, valid loss-0.3598, acc-0.8980, test loss-0.3654, acc-0.8944\n",
      "Iter-91580, train loss-0.3250, acc-0.9200, valid loss-0.3598, acc-0.8980, test loss-0.3654, acc-0.8945\n",
      "Iter-91590, train loss-0.3434, acc-0.9200, valid loss-0.3598, acc-0.8980, test loss-0.3654, acc-0.8945\n",
      "Iter-91600, train loss-0.4154, acc-0.9200, valid loss-0.3598, acc-0.8982, test loss-0.3654, acc-0.8946\n",
      "Iter-91610, train loss-0.3194, acc-0.9400, valid loss-0.3598, acc-0.8982, test loss-0.3654, acc-0.8945\n",
      "Iter-91620, train loss-0.4443, acc-0.8800, valid loss-0.3598, acc-0.8982, test loss-0.3654, acc-0.8945\n",
      "Iter-91630, train loss-0.5112, acc-0.8000, valid loss-0.3597, acc-0.8980, test loss-0.3654, acc-0.8947\n",
      "Iter-91640, train loss-0.4729, acc-0.8200, valid loss-0.3597, acc-0.8980, test loss-0.3654, acc-0.8945\n",
      "Iter-91650, train loss-0.1818, acc-0.9800, valid loss-0.3597, acc-0.8978, test loss-0.3653, acc-0.8946\n",
      "Iter-91660, train loss-0.3698, acc-0.9200, valid loss-0.3597, acc-0.8978, test loss-0.3653, acc-0.8946\n",
      "Iter-91670, train loss-0.4730, acc-0.8000, valid loss-0.3597, acc-0.8978, test loss-0.3653, acc-0.8945\n",
      "Iter-91680, train loss-0.4970, acc-0.8200, valid loss-0.3597, acc-0.8978, test loss-0.3653, acc-0.8945\n",
      "Iter-91690, train loss-0.3210, acc-0.9200, valid loss-0.3597, acc-0.8978, test loss-0.3653, acc-0.8945\n",
      "Iter-91700, train loss-0.1964, acc-0.9600, valid loss-0.3597, acc-0.8978, test loss-0.3653, acc-0.8945\n",
      "Iter-91710, train loss-0.3411, acc-0.8800, valid loss-0.3597, acc-0.8978, test loss-0.3653, acc-0.8945\n",
      "Iter-91720, train loss-0.4975, acc-0.8200, valid loss-0.3596, acc-0.8978, test loss-0.3653, acc-0.8945\n",
      "Iter-91730, train loss-0.4417, acc-0.8600, valid loss-0.3596, acc-0.8978, test loss-0.3653, acc-0.8946\n",
      "Iter-91740, train loss-0.5000, acc-0.8800, valid loss-0.3596, acc-0.8980, test loss-0.3652, acc-0.8945\n",
      "Iter-91750, train loss-0.3471, acc-0.9200, valid loss-0.3596, acc-0.8980, test loss-0.3652, acc-0.8946\n",
      "Iter-91760, train loss-0.1811, acc-0.9800, valid loss-0.3596, acc-0.8980, test loss-0.3652, acc-0.8945\n",
      "Iter-91770, train loss-0.3925, acc-0.9200, valid loss-0.3596, acc-0.8978, test loss-0.3652, acc-0.8945\n",
      "Iter-91780, train loss-0.4697, acc-0.8600, valid loss-0.3596, acc-0.8978, test loss-0.3652, acc-0.8947\n",
      "Iter-91790, train loss-0.3496, acc-0.9000, valid loss-0.3595, acc-0.8978, test loss-0.3652, acc-0.8946\n",
      "Iter-91800, train loss-0.4655, acc-0.8600, valid loss-0.3595, acc-0.8980, test loss-0.3652, acc-0.8946\n",
      "Iter-91810, train loss-0.2339, acc-0.9600, valid loss-0.3595, acc-0.8980, test loss-0.3651, acc-0.8946\n",
      "Iter-91820, train loss-0.4674, acc-0.8200, valid loss-0.3595, acc-0.8982, test loss-0.3651, acc-0.8946\n",
      "Iter-91830, train loss-0.3216, acc-0.9400, valid loss-0.3595, acc-0.8980, test loss-0.3651, acc-0.8946\n",
      "Iter-91840, train loss-0.5148, acc-0.8600, valid loss-0.3595, acc-0.8980, test loss-0.3651, acc-0.8946\n",
      "Iter-91850, train loss-0.3075, acc-0.8400, valid loss-0.3595, acc-0.8980, test loss-0.3651, acc-0.8945\n",
      "Iter-91860, train loss-0.2577, acc-0.9200, valid loss-0.3595, acc-0.8980, test loss-0.3651, acc-0.8945\n",
      "Iter-91870, train loss-0.3375, acc-0.9000, valid loss-0.3595, acc-0.8980, test loss-0.3651, acc-0.8946\n",
      "Iter-91880, train loss-0.3210, acc-0.9200, valid loss-0.3594, acc-0.8980, test loss-0.3651, acc-0.8945\n",
      "Iter-91890, train loss-0.4187, acc-0.8800, valid loss-0.3594, acc-0.8980, test loss-0.3651, acc-0.8946\n",
      "Iter-91900, train loss-0.2243, acc-0.9600, valid loss-0.3594, acc-0.8978, test loss-0.3650, acc-0.8947\n",
      "Iter-91910, train loss-0.3836, acc-0.9200, valid loss-0.3594, acc-0.8978, test loss-0.3650, acc-0.8947\n",
      "Iter-91920, train loss-0.4339, acc-0.8400, valid loss-0.3594, acc-0.8980, test loss-0.3650, acc-0.8947\n",
      "Iter-91930, train loss-0.3003, acc-0.9200, valid loss-0.3594, acc-0.8980, test loss-0.3650, acc-0.8946\n",
      "Iter-91940, train loss-0.3516, acc-0.9200, valid loss-0.3594, acc-0.8980, test loss-0.3650, acc-0.8945\n",
      "Iter-91950, train loss-0.5286, acc-0.8800, valid loss-0.3594, acc-0.8980, test loss-0.3650, acc-0.8946\n",
      "Iter-91960, train loss-0.4251, acc-0.8200, valid loss-0.3593, acc-0.8980, test loss-0.3650, acc-0.8946\n",
      "Iter-91970, train loss-0.3764, acc-0.9000, valid loss-0.3593, acc-0.8982, test loss-0.3650, acc-0.8947\n",
      "Iter-91980, train loss-0.3437, acc-0.9000, valid loss-0.3593, acc-0.8982, test loss-0.3649, acc-0.8947\n",
      "Iter-91990, train loss-0.7801, acc-0.7200, valid loss-0.3593, acc-0.8984, test loss-0.3649, acc-0.8948\n",
      "Iter-92000, train loss-0.3578, acc-0.9000, valid loss-0.3593, acc-0.8982, test loss-0.3649, acc-0.8947\n",
      "Iter-92010, train loss-0.5375, acc-0.8800, valid loss-0.3593, acc-0.8982, test loss-0.3649, acc-0.8947\n",
      "Iter-92020, train loss-0.3248, acc-0.9400, valid loss-0.3593, acc-0.8982, test loss-0.3649, acc-0.8948\n",
      "Iter-92030, train loss-0.4925, acc-0.8800, valid loss-0.3593, acc-0.8982, test loss-0.3649, acc-0.8947\n",
      "Iter-92040, train loss-0.3830, acc-0.8600, valid loss-0.3592, acc-0.8982, test loss-0.3649, acc-0.8948\n",
      "Iter-92050, train loss-0.2526, acc-0.9400, valid loss-0.3592, acc-0.8982, test loss-0.3649, acc-0.8948\n",
      "Iter-92060, train loss-0.3296, acc-0.9200, valid loss-0.3592, acc-0.8982, test loss-0.3648, acc-0.8948\n",
      "Iter-92070, train loss-0.4317, acc-0.9000, valid loss-0.3592, acc-0.8980, test loss-0.3648, acc-0.8948\n",
      "Iter-92080, train loss-0.5135, acc-0.8600, valid loss-0.3592, acc-0.8980, test loss-0.3648, acc-0.8948\n",
      "Iter-92090, train loss-0.3833, acc-0.9000, valid loss-0.3592, acc-0.8980, test loss-0.3648, acc-0.8949\n",
      "Iter-92100, train loss-0.4310, acc-0.8200, valid loss-0.3592, acc-0.8980, test loss-0.3648, acc-0.8950\n",
      "Iter-92110, train loss-0.2589, acc-0.9600, valid loss-0.3591, acc-0.8980, test loss-0.3648, acc-0.8950\n",
      "Iter-92120, train loss-0.4856, acc-0.8200, valid loss-0.3591, acc-0.8980, test loss-0.3648, acc-0.8948\n",
      "Iter-92130, train loss-0.2481, acc-0.9200, valid loss-0.3591, acc-0.8982, test loss-0.3648, acc-0.8947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-92140, train loss-0.3841, acc-0.9000, valid loss-0.3591, acc-0.8980, test loss-0.3648, acc-0.8947\n",
      "Iter-92150, train loss-0.2286, acc-0.9600, valid loss-0.3591, acc-0.8980, test loss-0.3648, acc-0.8949\n",
      "Iter-92160, train loss-0.5656, acc-0.8200, valid loss-0.3591, acc-0.8978, test loss-0.3647, acc-0.8949\n",
      "Iter-92170, train loss-0.2905, acc-0.9200, valid loss-0.3591, acc-0.8978, test loss-0.3647, acc-0.8949\n",
      "Iter-92180, train loss-0.2658, acc-0.9200, valid loss-0.3591, acc-0.8978, test loss-0.3647, acc-0.8949\n",
      "Iter-92190, train loss-0.4546, acc-0.8600, valid loss-0.3591, acc-0.8978, test loss-0.3647, acc-0.8949\n",
      "Iter-92200, train loss-0.3464, acc-0.9000, valid loss-0.3590, acc-0.8978, test loss-0.3647, acc-0.8949\n",
      "Iter-92210, train loss-0.2979, acc-0.9000, valid loss-0.3590, acc-0.8978, test loss-0.3647, acc-0.8949\n",
      "Iter-92220, train loss-0.6034, acc-0.8400, valid loss-0.3590, acc-0.8978, test loss-0.3647, acc-0.8949\n",
      "Iter-92230, train loss-0.3319, acc-0.9200, valid loss-0.3590, acc-0.8978, test loss-0.3647, acc-0.8949\n",
      "Iter-92240, train loss-0.3640, acc-0.8800, valid loss-0.3590, acc-0.8978, test loss-0.3646, acc-0.8949\n",
      "Iter-92250, train loss-0.2947, acc-0.9000, valid loss-0.3590, acc-0.8976, test loss-0.3646, acc-0.8949\n",
      "Iter-92260, train loss-0.2154, acc-0.9400, valid loss-0.3590, acc-0.8976, test loss-0.3646, acc-0.8950\n",
      "Iter-92270, train loss-0.3362, acc-0.8600, valid loss-0.3590, acc-0.8976, test loss-0.3646, acc-0.8950\n",
      "Iter-92280, train loss-0.5427, acc-0.8800, valid loss-0.3589, acc-0.8976, test loss-0.3646, acc-0.8950\n",
      "Iter-92290, train loss-0.3132, acc-0.9000, valid loss-0.3589, acc-0.8980, test loss-0.3645, acc-0.8951\n",
      "Iter-92300, train loss-0.3427, acc-0.9200, valid loss-0.3589, acc-0.8980, test loss-0.3645, acc-0.8951\n",
      "Iter-92310, train loss-0.5209, acc-0.8600, valid loss-0.3589, acc-0.8980, test loss-0.3645, acc-0.8951\n",
      "Iter-92320, train loss-0.2908, acc-0.9200, valid loss-0.3589, acc-0.8980, test loss-0.3645, acc-0.8951\n",
      "Iter-92330, train loss-0.4436, acc-0.8400, valid loss-0.3589, acc-0.8980, test loss-0.3645, acc-0.8951\n",
      "Iter-92340, train loss-0.6651, acc-0.8200, valid loss-0.3588, acc-0.8980, test loss-0.3645, acc-0.8950\n",
      "Iter-92350, train loss-0.7284, acc-0.8000, valid loss-0.3588, acc-0.8980, test loss-0.3645, acc-0.8951\n",
      "Iter-92360, train loss-0.3797, acc-0.9000, valid loss-0.3588, acc-0.8980, test loss-0.3645, acc-0.8951\n",
      "Iter-92370, train loss-0.5947, acc-0.7800, valid loss-0.3588, acc-0.8978, test loss-0.3645, acc-0.8951\n",
      "Iter-92380, train loss-0.3486, acc-0.8800, valid loss-0.3588, acc-0.8980, test loss-0.3645, acc-0.8951\n",
      "Iter-92390, train loss-0.3135, acc-0.9400, valid loss-0.3588, acc-0.8980, test loss-0.3645, acc-0.8951\n",
      "Iter-92400, train loss-0.3245, acc-0.9000, valid loss-0.3588, acc-0.8980, test loss-0.3645, acc-0.8950\n",
      "Iter-92410, train loss-0.3631, acc-0.9000, valid loss-0.3588, acc-0.8982, test loss-0.3644, acc-0.8950\n",
      "Iter-92420, train loss-0.3620, acc-0.8600, valid loss-0.3588, acc-0.8982, test loss-0.3644, acc-0.8951\n",
      "Iter-92430, train loss-0.2773, acc-0.9200, valid loss-0.3587, acc-0.8980, test loss-0.3644, acc-0.8951\n",
      "Iter-92440, train loss-0.1856, acc-0.9600, valid loss-0.3587, acc-0.8982, test loss-0.3644, acc-0.8951\n",
      "Iter-92450, train loss-0.4491, acc-0.8600, valid loss-0.3587, acc-0.8980, test loss-0.3644, acc-0.8951\n",
      "Iter-92460, train loss-0.3423, acc-0.8800, valid loss-0.3587, acc-0.8980, test loss-0.3644, acc-0.8950\n",
      "Iter-92470, train loss-0.5029, acc-0.8800, valid loss-0.3587, acc-0.8980, test loss-0.3644, acc-0.8950\n",
      "Iter-92480, train loss-0.1889, acc-0.9400, valid loss-0.3587, acc-0.8980, test loss-0.3644, acc-0.8950\n",
      "Iter-92490, train loss-0.6707, acc-0.7800, valid loss-0.3586, acc-0.8982, test loss-0.3643, acc-0.8950\n",
      "Iter-92500, train loss-0.5232, acc-0.8800, valid loss-0.3586, acc-0.8982, test loss-0.3643, acc-0.8950\n",
      "Iter-92510, train loss-0.3011, acc-0.8800, valid loss-0.3586, acc-0.8980, test loss-0.3643, acc-0.8951\n",
      "Iter-92520, train loss-0.2857, acc-0.9000, valid loss-0.3586, acc-0.8980, test loss-0.3643, acc-0.8952\n",
      "Iter-92530, train loss-0.4946, acc-0.8800, valid loss-0.3586, acc-0.8978, test loss-0.3643, acc-0.8952\n",
      "Iter-92540, train loss-0.3736, acc-0.9400, valid loss-0.3586, acc-0.8982, test loss-0.3643, acc-0.8951\n",
      "Iter-92550, train loss-0.4982, acc-0.8000, valid loss-0.3586, acc-0.8978, test loss-0.3642, acc-0.8952\n",
      "Iter-92560, train loss-0.6997, acc-0.8200, valid loss-0.3586, acc-0.8980, test loss-0.3642, acc-0.8951\n",
      "Iter-92570, train loss-0.5707, acc-0.8200, valid loss-0.3585, acc-0.8980, test loss-0.3642, acc-0.8951\n",
      "Iter-92580, train loss-0.5062, acc-0.8800, valid loss-0.3585, acc-0.8980, test loss-0.3642, acc-0.8952\n",
      "Iter-92590, train loss-0.5773, acc-0.8400, valid loss-0.3585, acc-0.8980, test loss-0.3642, acc-0.8952\n",
      "Iter-92600, train loss-0.2333, acc-0.9800, valid loss-0.3585, acc-0.8982, test loss-0.3642, acc-0.8952\n",
      "Iter-92610, train loss-0.4063, acc-0.8800, valid loss-0.3585, acc-0.8980, test loss-0.3642, acc-0.8952\n",
      "Iter-92620, train loss-0.2788, acc-0.9000, valid loss-0.3585, acc-0.8982, test loss-0.3641, acc-0.8952\n",
      "Iter-92630, train loss-0.4631, acc-0.8600, valid loss-0.3585, acc-0.8982, test loss-0.3641, acc-0.8952\n",
      "Iter-92640, train loss-0.4074, acc-0.8800, valid loss-0.3584, acc-0.8982, test loss-0.3641, acc-0.8952\n",
      "Iter-92650, train loss-0.2969, acc-0.9400, valid loss-0.3584, acc-0.8982, test loss-0.3641, acc-0.8951\n",
      "Iter-92660, train loss-0.3892, acc-0.8800, valid loss-0.3584, acc-0.8984, test loss-0.3641, acc-0.8951\n",
      "Iter-92670, train loss-0.2600, acc-0.9400, valid loss-0.3584, acc-0.8982, test loss-0.3641, acc-0.8952\n",
      "Iter-92680, train loss-0.3524, acc-0.9000, valid loss-0.3584, acc-0.8980, test loss-0.3641, acc-0.8953\n",
      "Iter-92690, train loss-0.3889, acc-0.9000, valid loss-0.3584, acc-0.8980, test loss-0.3640, acc-0.8952\n",
      "Iter-92700, train loss-0.3419, acc-0.9000, valid loss-0.3583, acc-0.8980, test loss-0.3640, acc-0.8953\n",
      "Iter-92710, train loss-0.6133, acc-0.8600, valid loss-0.3583, acc-0.8982, test loss-0.3640, acc-0.8953\n",
      "Iter-92720, train loss-0.2223, acc-0.9400, valid loss-0.3583, acc-0.8982, test loss-0.3640, acc-0.8952\n",
      "Iter-92730, train loss-0.5281, acc-0.8800, valid loss-0.3583, acc-0.8980, test loss-0.3640, acc-0.8952\n",
      "Iter-92740, train loss-0.4025, acc-0.8000, valid loss-0.3583, acc-0.8980, test loss-0.3640, acc-0.8951\n",
      "Iter-92750, train loss-0.3009, acc-0.9200, valid loss-0.3583, acc-0.8980, test loss-0.3640, acc-0.8951\n",
      "Iter-92760, train loss-0.3729, acc-0.9000, valid loss-0.3583, acc-0.8982, test loss-0.3640, acc-0.8952\n",
      "Iter-92770, train loss-0.2298, acc-0.9400, valid loss-0.3583, acc-0.8984, test loss-0.3640, acc-0.8953\n",
      "Iter-92780, train loss-0.4518, acc-0.8400, valid loss-0.3583, acc-0.8984, test loss-0.3639, acc-0.8953\n",
      "Iter-92790, train loss-0.2993, acc-0.9200, valid loss-0.3582, acc-0.8984, test loss-0.3639, acc-0.8953\n",
      "Iter-92800, train loss-0.3965, acc-0.8600, valid loss-0.3582, acc-0.8984, test loss-0.3639, acc-0.8953\n",
      "Iter-92810, train loss-0.2715, acc-0.8800, valid loss-0.3582, acc-0.8984, test loss-0.3639, acc-0.8953\n",
      "Iter-92820, train loss-0.4687, acc-0.8600, valid loss-0.3582, acc-0.8984, test loss-0.3639, acc-0.8952\n",
      "Iter-92830, train loss-0.3362, acc-0.9000, valid loss-0.3582, acc-0.8984, test loss-0.3639, acc-0.8951\n",
      "Iter-92840, train loss-0.4639, acc-0.8800, valid loss-0.3581, acc-0.8984, test loss-0.3639, acc-0.8951\n",
      "Iter-92850, train loss-0.3999, acc-0.9000, valid loss-0.3581, acc-0.8984, test loss-0.3638, acc-0.8951\n",
      "Iter-92860, train loss-0.3561, acc-0.9000, valid loss-0.3581, acc-0.8986, test loss-0.3638, acc-0.8951\n",
      "Iter-92870, train loss-0.2562, acc-0.9400, valid loss-0.3581, acc-0.8986, test loss-0.3638, acc-0.8951\n",
      "Iter-92880, train loss-0.5138, acc-0.8600, valid loss-0.3581, acc-0.8986, test loss-0.3638, acc-0.8950\n",
      "Iter-92890, train loss-0.4340, acc-0.8400, valid loss-0.3581, acc-0.8984, test loss-0.3638, acc-0.8952\n",
      "Iter-92900, train loss-0.3280, acc-0.9000, valid loss-0.3581, acc-0.8986, test loss-0.3638, acc-0.8951\n",
      "Iter-92910, train loss-0.3242, acc-0.8800, valid loss-0.3581, acc-0.8984, test loss-0.3638, acc-0.8952\n",
      "Iter-92920, train loss-0.5708, acc-0.8400, valid loss-0.3580, acc-0.8984, test loss-0.3638, acc-0.8952\n",
      "Iter-92930, train loss-0.4767, acc-0.8800, valid loss-0.3580, acc-0.8984, test loss-0.3637, acc-0.8952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-92940, train loss-0.3680, acc-0.8800, valid loss-0.3580, acc-0.8984, test loss-0.3637, acc-0.8951\n",
      "Iter-92950, train loss-0.6030, acc-0.8200, valid loss-0.3580, acc-0.8984, test loss-0.3637, acc-0.8952\n",
      "Iter-92960, train loss-0.4300, acc-0.8600, valid loss-0.3580, acc-0.8984, test loss-0.3637, acc-0.8952\n",
      "Iter-92970, train loss-0.3504, acc-0.9000, valid loss-0.3580, acc-0.8984, test loss-0.3637, acc-0.8950\n",
      "Iter-92980, train loss-0.3109, acc-0.9000, valid loss-0.3579, acc-0.8984, test loss-0.3637, acc-0.8952\n",
      "Iter-92990, train loss-0.6092, acc-0.8400, valid loss-0.3579, acc-0.8984, test loss-0.3637, acc-0.8951\n",
      "Iter-93000, train loss-0.4655, acc-0.8600, valid loss-0.3579, acc-0.8984, test loss-0.3637, acc-0.8952\n",
      "Iter-93010, train loss-0.5699, acc-0.8200, valid loss-0.3579, acc-0.8984, test loss-0.3637, acc-0.8952\n",
      "Iter-93020, train loss-0.2555, acc-0.9200, valid loss-0.3579, acc-0.8984, test loss-0.3637, acc-0.8951\n",
      "Iter-93030, train loss-0.4886, acc-0.9200, valid loss-0.3579, acc-0.8984, test loss-0.3637, acc-0.8952\n",
      "Iter-93040, train loss-0.6275, acc-0.8200, valid loss-0.3579, acc-0.8984, test loss-0.3637, acc-0.8952\n",
      "Iter-93050, train loss-0.3014, acc-0.9000, valid loss-0.3579, acc-0.8986, test loss-0.3636, acc-0.8952\n",
      "Iter-93060, train loss-0.4256, acc-0.9400, valid loss-0.3579, acc-0.8984, test loss-0.3636, acc-0.8950\n",
      "Iter-93070, train loss-0.4719, acc-0.8400, valid loss-0.3579, acc-0.8982, test loss-0.3636, acc-0.8949\n",
      "Iter-93080, train loss-0.4575, acc-0.8400, valid loss-0.3579, acc-0.8984, test loss-0.3636, acc-0.8948\n",
      "Iter-93090, train loss-0.3285, acc-0.9000, valid loss-0.3578, acc-0.8984, test loss-0.3636, acc-0.8948\n",
      "Iter-93100, train loss-0.2429, acc-0.9200, valid loss-0.3578, acc-0.8984, test loss-0.3636, acc-0.8948\n",
      "Iter-93110, train loss-0.3258, acc-0.9000, valid loss-0.3578, acc-0.8984, test loss-0.3636, acc-0.8948\n",
      "Iter-93120, train loss-0.4472, acc-0.8400, valid loss-0.3578, acc-0.8984, test loss-0.3636, acc-0.8948\n",
      "Iter-93130, train loss-0.2642, acc-0.9000, valid loss-0.3578, acc-0.8984, test loss-0.3636, acc-0.8948\n",
      "Iter-93140, train loss-0.3147, acc-0.9000, valid loss-0.3578, acc-0.8984, test loss-0.3635, acc-0.8948\n",
      "Iter-93150, train loss-0.4755, acc-0.9000, valid loss-0.3578, acc-0.8986, test loss-0.3635, acc-0.8948\n",
      "Iter-93160, train loss-0.4076, acc-0.8600, valid loss-0.3578, acc-0.8986, test loss-0.3635, acc-0.8948\n",
      "Iter-93170, train loss-0.3707, acc-0.9000, valid loss-0.3577, acc-0.8984, test loss-0.3635, acc-0.8948\n",
      "Iter-93180, train loss-0.5506, acc-0.8600, valid loss-0.3577, acc-0.8986, test loss-0.3635, acc-0.8948\n",
      "Iter-93190, train loss-0.3799, acc-0.8600, valid loss-0.3577, acc-0.8986, test loss-0.3635, acc-0.8948\n",
      "Iter-93200, train loss-0.4471, acc-0.8600, valid loss-0.3577, acc-0.8986, test loss-0.3634, acc-0.8948\n",
      "Iter-93210, train loss-0.6353, acc-0.7800, valid loss-0.3577, acc-0.8986, test loss-0.3634, acc-0.8948\n",
      "Iter-93220, train loss-0.3595, acc-0.9200, valid loss-0.3577, acc-0.8986, test loss-0.3634, acc-0.8948\n",
      "Iter-93230, train loss-0.2769, acc-0.9200, valid loss-0.3577, acc-0.8986, test loss-0.3634, acc-0.8948\n",
      "Iter-93240, train loss-0.4739, acc-0.8800, valid loss-0.3576, acc-0.8986, test loss-0.3634, acc-0.8949\n",
      "Iter-93250, train loss-0.3764, acc-0.9000, valid loss-0.3576, acc-0.8986, test loss-0.3634, acc-0.8948\n",
      "Iter-93260, train loss-0.5164, acc-0.8200, valid loss-0.3576, acc-0.8986, test loss-0.3634, acc-0.8948\n",
      "Iter-93270, train loss-0.4879, acc-0.8600, valid loss-0.3576, acc-0.8986, test loss-0.3634, acc-0.8950\n",
      "Iter-93280, train loss-0.3913, acc-0.9000, valid loss-0.3576, acc-0.8986, test loss-0.3634, acc-0.8949\n",
      "Iter-93290, train loss-0.4813, acc-0.9200, valid loss-0.3576, acc-0.8986, test loss-0.3633, acc-0.8949\n",
      "Iter-93300, train loss-0.3545, acc-0.9000, valid loss-0.3576, acc-0.8986, test loss-0.3633, acc-0.8949\n",
      "Iter-93310, train loss-0.4248, acc-0.9000, valid loss-0.3576, acc-0.8986, test loss-0.3633, acc-0.8948\n",
      "Iter-93320, train loss-0.5713, acc-0.8600, valid loss-0.3575, acc-0.8986, test loss-0.3633, acc-0.8950\n",
      "Iter-93330, train loss-0.4891, acc-0.9000, valid loss-0.3575, acc-0.8986, test loss-0.3633, acc-0.8948\n",
      "Iter-93340, train loss-0.4766, acc-0.9200, valid loss-0.3575, acc-0.8986, test loss-0.3633, acc-0.8948\n",
      "Iter-93350, train loss-0.2898, acc-0.9400, valid loss-0.3575, acc-0.8986, test loss-0.3633, acc-0.8947\n",
      "Iter-93360, train loss-0.2346, acc-0.9400, valid loss-0.3575, acc-0.8986, test loss-0.3633, acc-0.8949\n",
      "Iter-93370, train loss-0.4359, acc-0.9000, valid loss-0.3575, acc-0.8986, test loss-0.3632, acc-0.8948\n",
      "Iter-93380, train loss-0.3084, acc-0.9400, valid loss-0.3575, acc-0.8986, test loss-0.3632, acc-0.8948\n",
      "Iter-93390, train loss-0.2694, acc-0.9200, valid loss-0.3574, acc-0.8988, test loss-0.3632, acc-0.8951\n",
      "Iter-93400, train loss-0.4283, acc-0.8600, valid loss-0.3574, acc-0.8988, test loss-0.3632, acc-0.8951\n",
      "Iter-93410, train loss-0.5205, acc-0.8800, valid loss-0.3574, acc-0.8988, test loss-0.3632, acc-0.8952\n",
      "Iter-93420, train loss-0.3917, acc-0.9000, valid loss-0.3574, acc-0.8988, test loss-0.3632, acc-0.8952\n",
      "Iter-93430, train loss-0.4672, acc-0.8600, valid loss-0.3574, acc-0.8988, test loss-0.3632, acc-0.8952\n",
      "Iter-93440, train loss-0.2589, acc-0.9400, valid loss-0.3574, acc-0.8986, test loss-0.3631, acc-0.8952\n",
      "Iter-93450, train loss-0.2177, acc-0.9600, valid loss-0.3574, acc-0.8988, test loss-0.3631, acc-0.8952\n",
      "Iter-93460, train loss-0.4736, acc-0.8800, valid loss-0.3573, acc-0.8988, test loss-0.3631, acc-0.8953\n",
      "Iter-93470, train loss-0.3150, acc-0.8800, valid loss-0.3573, acc-0.8988, test loss-0.3631, acc-0.8953\n",
      "Iter-93480, train loss-0.4229, acc-0.8600, valid loss-0.3573, acc-0.8986, test loss-0.3631, acc-0.8953\n",
      "Iter-93490, train loss-0.3314, acc-0.8800, valid loss-0.3573, acc-0.8986, test loss-0.3631, acc-0.8952\n",
      "Iter-93500, train loss-0.2967, acc-0.9600, valid loss-0.3573, acc-0.8986, test loss-0.3631, acc-0.8953\n",
      "Iter-93510, train loss-0.2495, acc-0.9600, valid loss-0.3573, acc-0.8986, test loss-0.3630, acc-0.8953\n",
      "Iter-93520, train loss-0.3027, acc-0.9400, valid loss-0.3573, acc-0.8988, test loss-0.3630, acc-0.8952\n",
      "Iter-93530, train loss-0.2507, acc-0.9600, valid loss-0.3573, acc-0.8986, test loss-0.3630, acc-0.8952\n",
      "Iter-93540, train loss-0.4121, acc-0.9000, valid loss-0.3572, acc-0.8988, test loss-0.3630, acc-0.8952\n",
      "Iter-93550, train loss-0.3550, acc-0.9000, valid loss-0.3572, acc-0.8988, test loss-0.3630, acc-0.8952\n",
      "Iter-93560, train loss-0.3296, acc-0.8600, valid loss-0.3572, acc-0.8986, test loss-0.3630, acc-0.8952\n",
      "Iter-93570, train loss-0.2079, acc-0.9400, valid loss-0.3572, acc-0.8988, test loss-0.3630, acc-0.8953\n",
      "Iter-93580, train loss-0.2015, acc-0.9400, valid loss-0.3572, acc-0.8984, test loss-0.3629, acc-0.8953\n",
      "Iter-93590, train loss-0.4702, acc-0.8600, valid loss-0.3572, acc-0.8984, test loss-0.3629, acc-0.8953\n",
      "Iter-93600, train loss-0.2162, acc-0.9200, valid loss-0.3572, acc-0.8986, test loss-0.3629, acc-0.8953\n",
      "Iter-93610, train loss-0.5606, acc-0.8000, valid loss-0.3572, acc-0.8986, test loss-0.3629, acc-0.8954\n",
      "Iter-93620, train loss-0.3688, acc-0.9000, valid loss-0.3572, acc-0.8986, test loss-0.3629, acc-0.8954\n",
      "Iter-93630, train loss-0.4740, acc-0.8600, valid loss-0.3571, acc-0.8986, test loss-0.3629, acc-0.8954\n",
      "Iter-93640, train loss-0.3212, acc-0.9200, valid loss-0.3571, acc-0.8986, test loss-0.3629, acc-0.8953\n",
      "Iter-93650, train loss-0.2475, acc-0.9400, valid loss-0.3571, acc-0.8986, test loss-0.3629, acc-0.8953\n",
      "Iter-93660, train loss-0.2980, acc-0.9000, valid loss-0.3571, acc-0.8984, test loss-0.3628, acc-0.8953\n",
      "Iter-93670, train loss-0.6767, acc-0.8400, valid loss-0.3571, acc-0.8984, test loss-0.3628, acc-0.8953\n",
      "Iter-93680, train loss-0.3458, acc-0.9200, valid loss-0.3571, acc-0.8986, test loss-0.3628, acc-0.8951\n",
      "Iter-93690, train loss-0.3413, acc-0.9600, valid loss-0.3571, acc-0.8984, test loss-0.3628, acc-0.8953\n",
      "Iter-93700, train loss-0.2915, acc-0.9200, valid loss-0.3571, acc-0.8984, test loss-0.3628, acc-0.8952\n",
      "Iter-93710, train loss-0.3319, acc-0.9200, valid loss-0.3571, acc-0.8986, test loss-0.3628, acc-0.8952\n",
      "Iter-93720, train loss-0.3571, acc-0.8800, valid loss-0.3570, acc-0.8988, test loss-0.3628, acc-0.8952\n",
      "Iter-93730, train loss-0.5338, acc-0.9000, valid loss-0.3570, acc-0.8984, test loss-0.3628, acc-0.8951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-93740, train loss-0.4410, acc-0.8800, valid loss-0.3570, acc-0.8984, test loss-0.3627, acc-0.8951\n",
      "Iter-93750, train loss-0.3137, acc-0.8600, valid loss-0.3570, acc-0.8988, test loss-0.3627, acc-0.8951\n",
      "Iter-93760, train loss-0.3416, acc-0.9400, valid loss-0.3570, acc-0.8986, test loss-0.3627, acc-0.8953\n",
      "Iter-93770, train loss-0.2168, acc-0.9600, valid loss-0.3570, acc-0.8986, test loss-0.3627, acc-0.8952\n",
      "Iter-93780, train loss-0.5261, acc-0.8800, valid loss-0.3570, acc-0.8986, test loss-0.3627, acc-0.8952\n",
      "Iter-93790, train loss-0.4784, acc-0.9000, valid loss-0.3570, acc-0.8986, test loss-0.3627, acc-0.8952\n",
      "Iter-93800, train loss-0.2340, acc-0.9200, valid loss-0.3570, acc-0.8986, test loss-0.3627, acc-0.8953\n",
      "Iter-93810, train loss-0.5367, acc-0.8600, valid loss-0.3569, acc-0.8986, test loss-0.3627, acc-0.8953\n",
      "Iter-93820, train loss-0.3979, acc-0.9000, valid loss-0.3569, acc-0.8986, test loss-0.3627, acc-0.8954\n",
      "Iter-93830, train loss-0.2503, acc-0.9200, valid loss-0.3569, acc-0.8986, test loss-0.3626, acc-0.8954\n",
      "Iter-93840, train loss-0.3117, acc-0.9400, valid loss-0.3569, acc-0.8986, test loss-0.3626, acc-0.8954\n",
      "Iter-93850, train loss-0.4407, acc-0.8600, valid loss-0.3569, acc-0.8986, test loss-0.3626, acc-0.8954\n",
      "Iter-93860, train loss-0.5797, acc-0.8000, valid loss-0.3569, acc-0.8986, test loss-0.3626, acc-0.8953\n",
      "Iter-93870, train loss-0.3045, acc-0.9400, valid loss-0.3569, acc-0.8986, test loss-0.3626, acc-0.8953\n",
      "Iter-93880, train loss-0.4229, acc-0.8600, valid loss-0.3568, acc-0.8986, test loss-0.3626, acc-0.8955\n",
      "Iter-93890, train loss-0.3733, acc-0.9000, valid loss-0.3568, acc-0.8986, test loss-0.3626, acc-0.8953\n",
      "Iter-93900, train loss-0.4033, acc-0.8600, valid loss-0.3568, acc-0.8988, test loss-0.3625, acc-0.8954\n",
      "Iter-93910, train loss-0.3080, acc-0.9400, valid loss-0.3568, acc-0.8986, test loss-0.3625, acc-0.8955\n",
      "Iter-93920, train loss-0.3800, acc-0.9000, valid loss-0.3568, acc-0.8986, test loss-0.3625, acc-0.8954\n",
      "Iter-93930, train loss-0.4553, acc-0.8600, valid loss-0.3568, acc-0.8986, test loss-0.3625, acc-0.8954\n",
      "Iter-93940, train loss-0.4162, acc-0.9000, valid loss-0.3568, acc-0.8986, test loss-0.3625, acc-0.8954\n",
      "Iter-93950, train loss-0.2663, acc-0.9200, valid loss-0.3567, acc-0.8986, test loss-0.3625, acc-0.8955\n",
      "Iter-93960, train loss-0.4510, acc-0.8200, valid loss-0.3567, acc-0.8986, test loss-0.3625, acc-0.8955\n",
      "Iter-93970, train loss-0.2414, acc-0.9400, valid loss-0.3567, acc-0.8986, test loss-0.3625, acc-0.8955\n",
      "Iter-93980, train loss-0.4455, acc-0.8800, valid loss-0.3567, acc-0.8986, test loss-0.3624, acc-0.8955\n",
      "Iter-93990, train loss-0.4368, acc-0.9000, valid loss-0.3567, acc-0.8986, test loss-0.3624, acc-0.8955\n",
      "Iter-94000, train loss-0.4595, acc-0.8800, valid loss-0.3567, acc-0.8986, test loss-0.3624, acc-0.8955\n",
      "Iter-94010, train loss-0.2776, acc-0.9200, valid loss-0.3567, acc-0.8986, test loss-0.3624, acc-0.8955\n",
      "Iter-94020, train loss-0.3568, acc-0.9200, valid loss-0.3566, acc-0.8986, test loss-0.3624, acc-0.8955\n",
      "Iter-94030, train loss-0.3005, acc-0.9200, valid loss-0.3566, acc-0.8988, test loss-0.3624, acc-0.8955\n",
      "Iter-94040, train loss-0.3251, acc-0.8800, valid loss-0.3566, acc-0.8988, test loss-0.3623, acc-0.8955\n",
      "Iter-94050, train loss-0.2748, acc-0.9400, valid loss-0.3566, acc-0.8988, test loss-0.3623, acc-0.8955\n",
      "Iter-94060, train loss-0.6719, acc-0.7800, valid loss-0.3566, acc-0.8988, test loss-0.3623, acc-0.8955\n",
      "Iter-94070, train loss-0.2664, acc-0.9200, valid loss-0.3566, acc-0.8988, test loss-0.3623, acc-0.8955\n",
      "Iter-94080, train loss-0.3606, acc-0.8800, valid loss-0.3565, acc-0.8986, test loss-0.3623, acc-0.8955\n",
      "Iter-94090, train loss-0.2817, acc-0.9000, valid loss-0.3565, acc-0.8986, test loss-0.3623, acc-0.8956\n",
      "Iter-94100, train loss-0.4125, acc-0.8600, valid loss-0.3565, acc-0.8988, test loss-0.3623, acc-0.8956\n",
      "Iter-94110, train loss-0.5832, acc-0.8400, valid loss-0.3565, acc-0.8988, test loss-0.3622, acc-0.8957\n",
      "Iter-94120, train loss-0.2381, acc-0.9800, valid loss-0.3565, acc-0.8988, test loss-0.3622, acc-0.8958\n",
      "Iter-94130, train loss-0.3559, acc-0.9200, valid loss-0.3565, acc-0.8988, test loss-0.3622, acc-0.8958\n",
      "Iter-94140, train loss-0.6425, acc-0.7800, valid loss-0.3565, acc-0.8986, test loss-0.3622, acc-0.8958\n",
      "Iter-94150, train loss-0.2795, acc-0.9000, valid loss-0.3564, acc-0.8986, test loss-0.3622, acc-0.8959\n",
      "Iter-94160, train loss-0.3791, acc-0.8600, valid loss-0.3564, acc-0.8986, test loss-0.3622, acc-0.8959\n",
      "Iter-94170, train loss-0.5751, acc-0.8400, valid loss-0.3564, acc-0.8986, test loss-0.3622, acc-0.8959\n",
      "Iter-94180, train loss-0.3413, acc-0.9200, valid loss-0.3564, acc-0.8986, test loss-0.3622, acc-0.8959\n",
      "Iter-94190, train loss-0.4228, acc-0.8800, valid loss-0.3564, acc-0.8986, test loss-0.3621, acc-0.8959\n",
      "Iter-94200, train loss-0.3811, acc-0.9600, valid loss-0.3564, acc-0.8986, test loss-0.3621, acc-0.8959\n",
      "Iter-94210, train loss-0.4583, acc-0.8800, valid loss-0.3563, acc-0.8986, test loss-0.3621, acc-0.8959\n",
      "Iter-94220, train loss-0.5104, acc-0.8600, valid loss-0.3563, acc-0.8986, test loss-0.3621, acc-0.8958\n",
      "Iter-94230, train loss-0.5199, acc-0.8600, valid loss-0.3563, acc-0.8988, test loss-0.3621, acc-0.8959\n",
      "Iter-94240, train loss-0.8714, acc-0.7800, valid loss-0.3563, acc-0.8986, test loss-0.3621, acc-0.8959\n",
      "Iter-94250, train loss-0.5160, acc-0.8600, valid loss-0.3563, acc-0.8986, test loss-0.3621, acc-0.8959\n",
      "Iter-94260, train loss-0.5579, acc-0.7800, valid loss-0.3563, acc-0.8986, test loss-0.3620, acc-0.8959\n",
      "Iter-94270, train loss-0.1617, acc-0.9800, valid loss-0.3563, acc-0.8986, test loss-0.3621, acc-0.8959\n",
      "Iter-94280, train loss-0.2828, acc-0.9400, valid loss-0.3563, acc-0.8986, test loss-0.3620, acc-0.8959\n",
      "Iter-94290, train loss-0.5091, acc-0.8600, valid loss-0.3562, acc-0.8986, test loss-0.3620, acc-0.8958\n",
      "Iter-94300, train loss-0.5173, acc-0.8800, valid loss-0.3562, acc-0.8986, test loss-0.3620, acc-0.8958\n",
      "Iter-94310, train loss-0.3539, acc-0.9000, valid loss-0.3562, acc-0.8986, test loss-0.3620, acc-0.8958\n",
      "Iter-94320, train loss-0.2248, acc-0.9000, valid loss-0.3562, acc-0.8988, test loss-0.3620, acc-0.8958\n",
      "Iter-94330, train loss-0.3210, acc-0.9000, valid loss-0.3562, acc-0.8988, test loss-0.3620, acc-0.8958\n",
      "Iter-94340, train loss-0.5490, acc-0.8000, valid loss-0.3562, acc-0.8988, test loss-0.3619, acc-0.8958\n",
      "Iter-94350, train loss-0.4202, acc-0.8600, valid loss-0.3562, acc-0.8988, test loss-0.3619, acc-0.8958\n",
      "Iter-94360, train loss-0.2075, acc-0.9600, valid loss-0.3562, acc-0.8988, test loss-0.3619, acc-0.8958\n",
      "Iter-94370, train loss-0.4138, acc-0.8600, valid loss-0.3561, acc-0.8988, test loss-0.3619, acc-0.8958\n",
      "Iter-94380, train loss-0.3255, acc-0.9000, valid loss-0.3561, acc-0.8988, test loss-0.3619, acc-0.8958\n",
      "Iter-94390, train loss-0.4005, acc-0.8800, valid loss-0.3561, acc-0.8988, test loss-0.3619, acc-0.8958\n",
      "Iter-94400, train loss-0.2682, acc-0.9400, valid loss-0.3561, acc-0.8988, test loss-0.3619, acc-0.8958\n",
      "Iter-94410, train loss-0.1773, acc-0.9600, valid loss-0.3561, acc-0.8988, test loss-0.3619, acc-0.8959\n",
      "Iter-94420, train loss-0.5786, acc-0.8400, valid loss-0.3561, acc-0.8986, test loss-0.3618, acc-0.8959\n",
      "Iter-94430, train loss-0.3720, acc-0.8800, valid loss-0.3561, acc-0.8988, test loss-0.3618, acc-0.8958\n",
      "Iter-94440, train loss-0.4903, acc-0.8400, valid loss-0.3560, acc-0.8988, test loss-0.3618, acc-0.8958\n",
      "Iter-94450, train loss-0.3192, acc-0.9400, valid loss-0.3560, acc-0.8988, test loss-0.3618, acc-0.8958\n",
      "Iter-94460, train loss-0.4247, acc-0.9600, valid loss-0.3560, acc-0.8988, test loss-0.3618, acc-0.8958\n",
      "Iter-94470, train loss-0.3263, acc-0.9000, valid loss-0.3560, acc-0.8990, test loss-0.3618, acc-0.8958\n",
      "Iter-94480, train loss-0.3673, acc-0.9200, valid loss-0.3560, acc-0.8986, test loss-0.3618, acc-0.8958\n",
      "Iter-94490, train loss-0.3476, acc-0.9400, valid loss-0.3560, acc-0.8986, test loss-0.3618, acc-0.8957\n",
      "Iter-94500, train loss-0.4643, acc-0.9000, valid loss-0.3560, acc-0.8986, test loss-0.3618, acc-0.8958\n",
      "Iter-94510, train loss-0.4002, acc-0.8600, valid loss-0.3560, acc-0.8986, test loss-0.3617, acc-0.8958\n",
      "Iter-94520, train loss-0.5368, acc-0.8200, valid loss-0.3560, acc-0.8986, test loss-0.3617, acc-0.8958\n",
      "Iter-94530, train loss-0.3753, acc-0.9200, valid loss-0.3559, acc-0.8988, test loss-0.3617, acc-0.8958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-94540, train loss-0.3928, acc-0.8800, valid loss-0.3559, acc-0.8986, test loss-0.3617, acc-0.8958\n",
      "Iter-94550, train loss-0.3613, acc-0.8600, valid loss-0.3559, acc-0.8986, test loss-0.3617, acc-0.8958\n",
      "Iter-94560, train loss-0.3899, acc-0.9000, valid loss-0.3559, acc-0.8988, test loss-0.3617, acc-0.8958\n",
      "Iter-94570, train loss-0.2515, acc-0.9400, valid loss-0.3559, acc-0.8988, test loss-0.3617, acc-0.8960\n",
      "Iter-94580, train loss-0.2779, acc-0.9000, valid loss-0.3559, acc-0.8988, test loss-0.3617, acc-0.8959\n",
      "Iter-94590, train loss-0.2692, acc-0.9200, valid loss-0.3559, acc-0.8988, test loss-0.3616, acc-0.8961\n",
      "Iter-94600, train loss-0.2782, acc-0.9800, valid loss-0.3558, acc-0.8988, test loss-0.3616, acc-0.8961\n",
      "Iter-94610, train loss-0.4115, acc-0.8600, valid loss-0.3558, acc-0.8988, test loss-0.3616, acc-0.8961\n",
      "Iter-94620, train loss-0.2442, acc-0.9400, valid loss-0.3558, acc-0.8988, test loss-0.3616, acc-0.8960\n",
      "Iter-94630, train loss-0.2736, acc-0.9200, valid loss-0.3558, acc-0.8988, test loss-0.3616, acc-0.8962\n",
      "Iter-94640, train loss-0.5498, acc-0.8200, valid loss-0.3558, acc-0.8988, test loss-0.3616, acc-0.8962\n",
      "Iter-94650, train loss-0.3371, acc-0.9400, valid loss-0.3558, acc-0.8988, test loss-0.3616, acc-0.8961\n",
      "Iter-94660, train loss-0.2676, acc-0.9200, valid loss-0.3558, acc-0.8988, test loss-0.3616, acc-0.8963\n",
      "Iter-94670, train loss-0.5207, acc-0.8200, valid loss-0.3558, acc-0.8988, test loss-0.3616, acc-0.8962\n",
      "Iter-94680, train loss-0.6372, acc-0.8400, valid loss-0.3558, acc-0.8988, test loss-0.3616, acc-0.8962\n",
      "Iter-94690, train loss-0.2126, acc-0.9200, valid loss-0.3557, acc-0.8988, test loss-0.3615, acc-0.8962\n",
      "Iter-94700, train loss-0.2109, acc-0.9800, valid loss-0.3557, acc-0.8988, test loss-0.3615, acc-0.8962\n",
      "Iter-94710, train loss-0.3276, acc-0.9200, valid loss-0.3557, acc-0.8988, test loss-0.3615, acc-0.8963\n",
      "Iter-94720, train loss-0.2215, acc-0.9400, valid loss-0.3557, acc-0.8988, test loss-0.3615, acc-0.8963\n",
      "Iter-94730, train loss-0.6065, acc-0.7800, valid loss-0.3557, acc-0.8988, test loss-0.3615, acc-0.8961\n",
      "Iter-94740, train loss-0.2678, acc-0.9200, valid loss-0.3557, acc-0.8988, test loss-0.3615, acc-0.8961\n",
      "Iter-94750, train loss-0.2915, acc-0.9000, valid loss-0.3557, acc-0.8988, test loss-0.3615, acc-0.8961\n",
      "Iter-94760, train loss-0.2377, acc-0.9600, valid loss-0.3557, acc-0.8988, test loss-0.3614, acc-0.8962\n",
      "Iter-94770, train loss-0.4329, acc-0.8200, valid loss-0.3557, acc-0.8988, test loss-0.3614, acc-0.8963\n",
      "Iter-94780, train loss-0.2370, acc-0.9000, valid loss-0.3556, acc-0.8988, test loss-0.3614, acc-0.8962\n",
      "Iter-94790, train loss-0.2641, acc-0.9000, valid loss-0.3556, acc-0.8988, test loss-0.3614, acc-0.8962\n",
      "Iter-94800, train loss-0.4474, acc-0.8800, valid loss-0.3556, acc-0.8988, test loss-0.3614, acc-0.8961\n",
      "Iter-94810, train loss-0.4081, acc-0.9400, valid loss-0.3556, acc-0.8988, test loss-0.3614, acc-0.8961\n",
      "Iter-94820, train loss-0.4179, acc-0.8800, valid loss-0.3556, acc-0.8988, test loss-0.3614, acc-0.8962\n",
      "Iter-94830, train loss-0.5536, acc-0.8600, valid loss-0.3556, acc-0.8988, test loss-0.3613, acc-0.8962\n",
      "Iter-94840, train loss-0.3851, acc-0.9200, valid loss-0.3556, acc-0.8988, test loss-0.3613, acc-0.8962\n",
      "Iter-94850, train loss-0.4057, acc-0.8600, valid loss-0.3556, acc-0.8988, test loss-0.3613, acc-0.8962\n",
      "Iter-94860, train loss-0.4358, acc-0.8600, valid loss-0.3555, acc-0.8988, test loss-0.3613, acc-0.8962\n",
      "Iter-94870, train loss-0.2472, acc-0.9200, valid loss-0.3555, acc-0.8990, test loss-0.3613, acc-0.8962\n",
      "Iter-94880, train loss-0.3626, acc-0.9400, valid loss-0.3555, acc-0.8990, test loss-0.3613, acc-0.8962\n",
      "Iter-94890, train loss-0.2431, acc-0.9200, valid loss-0.3555, acc-0.8990, test loss-0.3613, acc-0.8962\n",
      "Iter-94900, train loss-0.1548, acc-0.9800, valid loss-0.3555, acc-0.8990, test loss-0.3613, acc-0.8962\n",
      "Iter-94910, train loss-0.2784, acc-0.9200, valid loss-0.3555, acc-0.8990, test loss-0.3613, acc-0.8962\n",
      "Iter-94920, train loss-0.3433, acc-0.9000, valid loss-0.3555, acc-0.8990, test loss-0.3612, acc-0.8962\n",
      "Iter-94930, train loss-0.2944, acc-0.9200, valid loss-0.3555, acc-0.8990, test loss-0.3612, acc-0.8961\n",
      "Iter-94940, train loss-0.6623, acc-0.8000, valid loss-0.3555, acc-0.8990, test loss-0.3612, acc-0.8961\n",
      "Iter-94950, train loss-0.2186, acc-0.9600, valid loss-0.3555, acc-0.8990, test loss-0.3612, acc-0.8961\n",
      "Iter-94960, train loss-0.4927, acc-0.8000, valid loss-0.3555, acc-0.8990, test loss-0.3612, acc-0.8962\n",
      "Iter-94970, train loss-0.4542, acc-0.8400, valid loss-0.3554, acc-0.8990, test loss-0.3612, acc-0.8962\n",
      "Iter-94980, train loss-0.2722, acc-0.9200, valid loss-0.3554, acc-0.8990, test loss-0.3612, acc-0.8961\n",
      "Iter-94990, train loss-0.3007, acc-0.9000, valid loss-0.3554, acc-0.8990, test loss-0.3612, acc-0.8961\n",
      "Iter-95000, train loss-0.2461, acc-0.9200, valid loss-0.3554, acc-0.8990, test loss-0.3612, acc-0.8961\n",
      "Iter-95010, train loss-0.2927, acc-0.9000, valid loss-0.3554, acc-0.8990, test loss-0.3612, acc-0.8961\n",
      "Iter-95020, train loss-0.2831, acc-0.9200, valid loss-0.3554, acc-0.8992, test loss-0.3612, acc-0.8961\n",
      "Iter-95030, train loss-0.5095, acc-0.8000, valid loss-0.3554, acc-0.8992, test loss-0.3611, acc-0.8962\n",
      "Iter-95040, train loss-0.3560, acc-0.9000, valid loss-0.3554, acc-0.8994, test loss-0.3611, acc-0.8961\n",
      "Iter-95050, train loss-0.3484, acc-0.8800, valid loss-0.3554, acc-0.8994, test loss-0.3611, acc-0.8962\n",
      "Iter-95060, train loss-0.2927, acc-0.9200, valid loss-0.3554, acc-0.8994, test loss-0.3611, acc-0.8962\n",
      "Iter-95070, train loss-0.1685, acc-0.9600, valid loss-0.3554, acc-0.8992, test loss-0.3611, acc-0.8962\n",
      "Iter-95080, train loss-0.4288, acc-0.8600, valid loss-0.3554, acc-0.8994, test loss-0.3611, acc-0.8962\n",
      "Iter-95090, train loss-0.2697, acc-0.9400, valid loss-0.3553, acc-0.8994, test loss-0.3611, acc-0.8961\n",
      "Iter-95100, train loss-0.5613, acc-0.8400, valid loss-0.3553, acc-0.8994, test loss-0.3611, acc-0.8961\n",
      "Iter-95110, train loss-0.3584, acc-0.9200, valid loss-0.3553, acc-0.8994, test loss-0.3611, acc-0.8962\n",
      "Iter-95120, train loss-0.3151, acc-0.9000, valid loss-0.3553, acc-0.8992, test loss-0.3611, acc-0.8961\n",
      "Iter-95130, train loss-0.2621, acc-0.9000, valid loss-0.3553, acc-0.8992, test loss-0.3610, acc-0.8962\n",
      "Iter-95140, train loss-0.3030, acc-0.9200, valid loss-0.3553, acc-0.8994, test loss-0.3610, acc-0.8962\n",
      "Iter-95150, train loss-0.3975, acc-0.8400, valid loss-0.3553, acc-0.8992, test loss-0.3610, acc-0.8962\n",
      "Iter-95160, train loss-0.4327, acc-0.8800, valid loss-0.3553, acc-0.8992, test loss-0.3610, acc-0.8961\n",
      "Iter-95170, train loss-0.2257, acc-0.9600, valid loss-0.3553, acc-0.8992, test loss-0.3610, acc-0.8962\n",
      "Iter-95180, train loss-0.2045, acc-0.9800, valid loss-0.3553, acc-0.8992, test loss-0.3610, acc-0.8962\n",
      "Iter-95190, train loss-0.4189, acc-0.8200, valid loss-0.3553, acc-0.8992, test loss-0.3610, acc-0.8962\n",
      "Iter-95200, train loss-0.5126, acc-0.8200, valid loss-0.3553, acc-0.8990, test loss-0.3610, acc-0.8962\n",
      "Iter-95210, train loss-0.5190, acc-0.8400, valid loss-0.3552, acc-0.8990, test loss-0.3609, acc-0.8962\n",
      "Iter-95220, train loss-0.3812, acc-0.9000, valid loss-0.3552, acc-0.8990, test loss-0.3609, acc-0.8962\n",
      "Iter-95230, train loss-0.4113, acc-0.8800, valid loss-0.3552, acc-0.8990, test loss-0.3609, acc-0.8963\n",
      "Iter-95240, train loss-0.4844, acc-0.8600, valid loss-0.3552, acc-0.8990, test loss-0.3609, acc-0.8963\n",
      "Iter-95250, train loss-0.2827, acc-0.9200, valid loss-0.3552, acc-0.8990, test loss-0.3609, acc-0.8963\n",
      "Iter-95260, train loss-0.3152, acc-0.9600, valid loss-0.3552, acc-0.8990, test loss-0.3609, acc-0.8964\n",
      "Iter-95270, train loss-0.3506, acc-0.9000, valid loss-0.3552, acc-0.8990, test loss-0.3609, acc-0.8964\n",
      "Iter-95280, train loss-0.2901, acc-0.9200, valid loss-0.3552, acc-0.8990, test loss-0.3609, acc-0.8964\n",
      "Iter-95290, train loss-0.2788, acc-0.9400, valid loss-0.3551, acc-0.8990, test loss-0.3609, acc-0.8964\n",
      "Iter-95300, train loss-0.3215, acc-0.9200, valid loss-0.3551, acc-0.8988, test loss-0.3608, acc-0.8964\n",
      "Iter-95310, train loss-0.3381, acc-0.9000, valid loss-0.3551, acc-0.8990, test loss-0.3608, acc-0.8964\n",
      "Iter-95320, train loss-0.3896, acc-0.9000, valid loss-0.3551, acc-0.8990, test loss-0.3608, acc-0.8964\n",
      "Iter-95330, train loss-0.2755, acc-0.9200, valid loss-0.3551, acc-0.8990, test loss-0.3608, acc-0.8964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-95340, train loss-0.2483, acc-0.9600, valid loss-0.3551, acc-0.8990, test loss-0.3608, acc-0.8964\n",
      "Iter-95350, train loss-0.3259, acc-0.9000, valid loss-0.3550, acc-0.8990, test loss-0.3608, acc-0.8964\n",
      "Iter-95360, train loss-0.5836, acc-0.8800, valid loss-0.3550, acc-0.8990, test loss-0.3608, acc-0.8964\n",
      "Iter-95370, train loss-0.5117, acc-0.8200, valid loss-0.3550, acc-0.8990, test loss-0.3608, acc-0.8965\n",
      "Iter-95380, train loss-0.4270, acc-0.9200, valid loss-0.3550, acc-0.8990, test loss-0.3608, acc-0.8964\n",
      "Iter-95390, train loss-0.2621, acc-0.9600, valid loss-0.3550, acc-0.8988, test loss-0.3607, acc-0.8965\n",
      "Iter-95400, train loss-0.3508, acc-0.9200, valid loss-0.3550, acc-0.8988, test loss-0.3607, acc-0.8965\n",
      "Iter-95410, train loss-0.3693, acc-0.9200, valid loss-0.3550, acc-0.8988, test loss-0.3607, acc-0.8964\n",
      "Iter-95420, train loss-0.3970, acc-0.8000, valid loss-0.3550, acc-0.8988, test loss-0.3607, acc-0.8964\n",
      "Iter-95430, train loss-0.4088, acc-0.8600, valid loss-0.3550, acc-0.8988, test loss-0.3607, acc-0.8963\n",
      "Iter-95440, train loss-0.4147, acc-0.9400, valid loss-0.3550, acc-0.8988, test loss-0.3607, acc-0.8964\n",
      "Iter-95450, train loss-0.2826, acc-0.9200, valid loss-0.3550, acc-0.8988, test loss-0.3607, acc-0.8962\n",
      "Iter-95460, train loss-0.2236, acc-0.9200, valid loss-0.3550, acc-0.8988, test loss-0.3606, acc-0.8961\n",
      "Iter-95470, train loss-0.3793, acc-0.8800, valid loss-0.3550, acc-0.8988, test loss-0.3606, acc-0.8961\n",
      "Iter-95480, train loss-0.4549, acc-0.9400, valid loss-0.3549, acc-0.8988, test loss-0.3606, acc-0.8962\n",
      "Iter-95490, train loss-0.2694, acc-0.8800, valid loss-0.3549, acc-0.8988, test loss-0.3606, acc-0.8962\n",
      "Iter-95500, train loss-0.4097, acc-0.8600, valid loss-0.3549, acc-0.8988, test loss-0.3606, acc-0.8960\n",
      "Iter-95510, train loss-0.4509, acc-0.8000, valid loss-0.3549, acc-0.8988, test loss-0.3606, acc-0.8961\n",
      "Iter-95520, train loss-0.4344, acc-0.9000, valid loss-0.3549, acc-0.8988, test loss-0.3606, acc-0.8961\n",
      "Iter-95530, train loss-0.2746, acc-0.9600, valid loss-0.3548, acc-0.8988, test loss-0.3606, acc-0.8963\n",
      "Iter-95540, train loss-0.3987, acc-0.8800, valid loss-0.3548, acc-0.8988, test loss-0.3605, acc-0.8961\n",
      "Iter-95550, train loss-0.3271, acc-0.9000, valid loss-0.3548, acc-0.8988, test loss-0.3605, acc-0.8961\n",
      "Iter-95560, train loss-0.4392, acc-0.7800, valid loss-0.3548, acc-0.8988, test loss-0.3605, acc-0.8962\n",
      "Iter-95570, train loss-0.4687, acc-0.9000, valid loss-0.3548, acc-0.8988, test loss-0.3605, acc-0.8963\n",
      "Iter-95580, train loss-0.2659, acc-0.9200, valid loss-0.3548, acc-0.8988, test loss-0.3605, acc-0.8964\n",
      "Iter-95590, train loss-0.4014, acc-0.8600, valid loss-0.3548, acc-0.8988, test loss-0.3605, acc-0.8963\n",
      "Iter-95600, train loss-0.4431, acc-0.9000, valid loss-0.3547, acc-0.8988, test loss-0.3605, acc-0.8963\n",
      "Iter-95610, train loss-0.6076, acc-0.8000, valid loss-0.3547, acc-0.8988, test loss-0.3604, acc-0.8964\n",
      "Iter-95620, train loss-0.3458, acc-0.9000, valid loss-0.3547, acc-0.8990, test loss-0.3604, acc-0.8963\n",
      "Iter-95630, train loss-0.4527, acc-0.8400, valid loss-0.3547, acc-0.8988, test loss-0.3604, acc-0.8963\n",
      "Iter-95640, train loss-0.3734, acc-0.9000, valid loss-0.3547, acc-0.8988, test loss-0.3604, acc-0.8963\n",
      "Iter-95650, train loss-0.4160, acc-0.9000, valid loss-0.3547, acc-0.8988, test loss-0.3604, acc-0.8961\n",
      "Iter-95660, train loss-0.5546, acc-0.8600, valid loss-0.3547, acc-0.8990, test loss-0.3604, acc-0.8962\n",
      "Iter-95670, train loss-0.3637, acc-0.9000, valid loss-0.3547, acc-0.8988, test loss-0.3604, acc-0.8962\n",
      "Iter-95680, train loss-0.3502, acc-0.8800, valid loss-0.3547, acc-0.8988, test loss-0.3604, acc-0.8963\n",
      "Iter-95690, train loss-0.3747, acc-0.9000, valid loss-0.3546, acc-0.8988, test loss-0.3604, acc-0.8963\n",
      "Iter-95700, train loss-0.4649, acc-0.8400, valid loss-0.3546, acc-0.8988, test loss-0.3603, acc-0.8963\n",
      "Iter-95710, train loss-0.4425, acc-0.9000, valid loss-0.3546, acc-0.8988, test loss-0.3603, acc-0.8963\n",
      "Iter-95720, train loss-0.2281, acc-0.9400, valid loss-0.3546, acc-0.8988, test loss-0.3603, acc-0.8963\n",
      "Iter-95730, train loss-0.4200, acc-0.8400, valid loss-0.3546, acc-0.8988, test loss-0.3603, acc-0.8964\n",
      "Iter-95740, train loss-0.3583, acc-0.9200, valid loss-0.3546, acc-0.8988, test loss-0.3603, acc-0.8964\n",
      "Iter-95750, train loss-0.5304, acc-0.8600, valid loss-0.3545, acc-0.8988, test loss-0.3603, acc-0.8964\n",
      "Iter-95760, train loss-0.3711, acc-0.9200, valid loss-0.3545, acc-0.8988, test loss-0.3603, acc-0.8963\n",
      "Iter-95770, train loss-0.3701, acc-0.8800, valid loss-0.3545, acc-0.8988, test loss-0.3603, acc-0.8963\n",
      "Iter-95780, train loss-0.4980, acc-0.9000, valid loss-0.3545, acc-0.8990, test loss-0.3602, acc-0.8963\n",
      "Iter-95790, train loss-0.2730, acc-0.8600, valid loss-0.3545, acc-0.8990, test loss-0.3602, acc-0.8964\n",
      "Iter-95800, train loss-0.4359, acc-0.8800, valid loss-0.3545, acc-0.8992, test loss-0.3602, acc-0.8963\n",
      "Iter-95810, train loss-0.3276, acc-0.9000, valid loss-0.3545, acc-0.8992, test loss-0.3602, acc-0.8963\n",
      "Iter-95820, train loss-0.4496, acc-0.8800, valid loss-0.3545, acc-0.8992, test loss-0.3602, acc-0.8963\n",
      "Iter-95830, train loss-0.3761, acc-0.8800, valid loss-0.3544, acc-0.8992, test loss-0.3602, acc-0.8962\n",
      "Iter-95840, train loss-0.4252, acc-0.9000, valid loss-0.3544, acc-0.8990, test loss-0.3602, acc-0.8962\n",
      "Iter-95850, train loss-0.5139, acc-0.8800, valid loss-0.3544, acc-0.8992, test loss-0.3602, acc-0.8961\n",
      "Iter-95860, train loss-0.5338, acc-0.8600, valid loss-0.3544, acc-0.8992, test loss-0.3602, acc-0.8960\n",
      "Iter-95870, train loss-0.5510, acc-0.9000, valid loss-0.3544, acc-0.8992, test loss-0.3601, acc-0.8960\n",
      "Iter-95880, train loss-0.5169, acc-0.8600, valid loss-0.3544, acc-0.8990, test loss-0.3601, acc-0.8959\n",
      "Iter-95890, train loss-0.2862, acc-0.9200, valid loss-0.3544, acc-0.8990, test loss-0.3601, acc-0.8961\n",
      "Iter-95900, train loss-0.2933, acc-0.9200, valid loss-0.3544, acc-0.8990, test loss-0.3601, acc-0.8961\n",
      "Iter-95910, train loss-0.3140, acc-0.8800, valid loss-0.3544, acc-0.8990, test loss-0.3601, acc-0.8960\n",
      "Iter-95920, train loss-0.4226, acc-0.8800, valid loss-0.3543, acc-0.8990, test loss-0.3601, acc-0.8961\n",
      "Iter-95930, train loss-0.4255, acc-0.9200, valid loss-0.3543, acc-0.8990, test loss-0.3601, acc-0.8960\n",
      "Iter-95940, train loss-0.3921, acc-0.9000, valid loss-0.3543, acc-0.8990, test loss-0.3601, acc-0.8961\n",
      "Iter-95950, train loss-0.6528, acc-0.8000, valid loss-0.3543, acc-0.8990, test loss-0.3600, acc-0.8960\n",
      "Iter-95960, train loss-0.2096, acc-0.9600, valid loss-0.3543, acc-0.8990, test loss-0.3600, acc-0.8961\n",
      "Iter-95970, train loss-0.2924, acc-0.9200, valid loss-0.3543, acc-0.8990, test loss-0.3600, acc-0.8960\n",
      "Iter-95980, train loss-0.4532, acc-0.8200, valid loss-0.3543, acc-0.8990, test loss-0.3600, acc-0.8960\n",
      "Iter-95990, train loss-0.3360, acc-0.9000, valid loss-0.3542, acc-0.8990, test loss-0.3600, acc-0.8959\n",
      "Iter-96000, train loss-0.3807, acc-0.9000, valid loss-0.3542, acc-0.8990, test loss-0.3600, acc-0.8960\n",
      "Iter-96010, train loss-0.3792, acc-0.9000, valid loss-0.3542, acc-0.8990, test loss-0.3600, acc-0.8958\n",
      "Iter-96020, train loss-0.2237, acc-0.9200, valid loss-0.3542, acc-0.8992, test loss-0.3600, acc-0.8959\n",
      "Iter-96030, train loss-0.3776, acc-0.8800, valid loss-0.3542, acc-0.8994, test loss-0.3599, acc-0.8959\n",
      "Iter-96040, train loss-0.1579, acc-0.9600, valid loss-0.3542, acc-0.8992, test loss-0.3599, acc-0.8959\n",
      "Iter-96050, train loss-0.2414, acc-0.9800, valid loss-0.3542, acc-0.8994, test loss-0.3599, acc-0.8959\n",
      "Iter-96060, train loss-0.5395, acc-0.8600, valid loss-0.3542, acc-0.8996, test loss-0.3599, acc-0.8959\n",
      "Iter-96070, train loss-0.5915, acc-0.8200, valid loss-0.3542, acc-0.8994, test loss-0.3599, acc-0.8959\n",
      "Iter-96080, train loss-0.4039, acc-0.8600, valid loss-0.3541, acc-0.8994, test loss-0.3599, acc-0.8959\n",
      "Iter-96090, train loss-0.2559, acc-0.9200, valid loss-0.3541, acc-0.8994, test loss-0.3599, acc-0.8959\n",
      "Iter-96100, train loss-0.6276, acc-0.8200, valid loss-0.3541, acc-0.8994, test loss-0.3599, acc-0.8959\n",
      "Iter-96110, train loss-0.5877, acc-0.8200, valid loss-0.3541, acc-0.8994, test loss-0.3599, acc-0.8959\n",
      "Iter-96120, train loss-0.3714, acc-0.8800, valid loss-0.3541, acc-0.8994, test loss-0.3598, acc-0.8959\n",
      "Iter-96130, train loss-0.3341, acc-0.9000, valid loss-0.3541, acc-0.8994, test loss-0.3598, acc-0.8959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-96140, train loss-0.2133, acc-0.9800, valid loss-0.3541, acc-0.8996, test loss-0.3598, acc-0.8960\n",
      "Iter-96150, train loss-0.2602, acc-0.9400, valid loss-0.3541, acc-0.8996, test loss-0.3598, acc-0.8960\n",
      "Iter-96160, train loss-0.5639, acc-0.8200, valid loss-0.3540, acc-0.8994, test loss-0.3598, acc-0.8960\n",
      "Iter-96170, train loss-0.2963, acc-0.9200, valid loss-0.3540, acc-0.8996, test loss-0.3598, acc-0.8960\n",
      "Iter-96180, train loss-0.6143, acc-0.8200, valid loss-0.3540, acc-0.8996, test loss-0.3598, acc-0.8959\n",
      "Iter-96190, train loss-0.6818, acc-0.7400, valid loss-0.3540, acc-0.8996, test loss-0.3598, acc-0.8958\n",
      "Iter-96200, train loss-0.3409, acc-0.8800, valid loss-0.3540, acc-0.8998, test loss-0.3598, acc-0.8959\n",
      "Iter-96210, train loss-0.6692, acc-0.8400, valid loss-0.3540, acc-0.8994, test loss-0.3597, acc-0.8959\n",
      "Iter-96220, train loss-0.4156, acc-0.9000, valid loss-0.3539, acc-0.8992, test loss-0.3597, acc-0.8959\n",
      "Iter-96230, train loss-0.2887, acc-0.9200, valid loss-0.3540, acc-0.8992, test loss-0.3597, acc-0.8958\n",
      "Iter-96240, train loss-0.3259, acc-0.8600, valid loss-0.3539, acc-0.8992, test loss-0.3597, acc-0.8960\n",
      "Iter-96250, train loss-0.2991, acc-0.9400, valid loss-0.3539, acc-0.8994, test loss-0.3597, acc-0.8958\n",
      "Iter-96260, train loss-0.1760, acc-0.9600, valid loss-0.3539, acc-0.8994, test loss-0.3597, acc-0.8961\n",
      "Iter-96270, train loss-0.4934, acc-0.8200, valid loss-0.3539, acc-0.8994, test loss-0.3597, acc-0.8962\n",
      "Iter-96280, train loss-0.4031, acc-0.8800, valid loss-0.3539, acc-0.8992, test loss-0.3597, acc-0.8959\n",
      "Iter-96290, train loss-0.4272, acc-0.8800, valid loss-0.3539, acc-0.8992, test loss-0.3596, acc-0.8961\n",
      "Iter-96300, train loss-0.3920, acc-0.8600, valid loss-0.3539, acc-0.8996, test loss-0.3596, acc-0.8959\n",
      "Iter-96310, train loss-0.3662, acc-0.9200, valid loss-0.3538, acc-0.8996, test loss-0.3596, acc-0.8958\n",
      "Iter-96320, train loss-0.3603, acc-0.8600, valid loss-0.3538, acc-0.8996, test loss-0.3596, acc-0.8958\n",
      "Iter-96330, train loss-0.1562, acc-0.9800, valid loss-0.3538, acc-0.8994, test loss-0.3596, acc-0.8958\n",
      "Iter-96340, train loss-0.3559, acc-0.9400, valid loss-0.3538, acc-0.8994, test loss-0.3596, acc-0.8958\n",
      "Iter-96350, train loss-0.5940, acc-0.8600, valid loss-0.3538, acc-0.8994, test loss-0.3596, acc-0.8958\n",
      "Iter-96360, train loss-0.2507, acc-0.9400, valid loss-0.3538, acc-0.8992, test loss-0.3596, acc-0.8959\n",
      "Iter-96370, train loss-0.3881, acc-0.9200, valid loss-0.3538, acc-0.8992, test loss-0.3595, acc-0.8960\n",
      "Iter-96380, train loss-0.5896, acc-0.8200, valid loss-0.3538, acc-0.8992, test loss-0.3595, acc-0.8960\n",
      "Iter-96390, train loss-0.3348, acc-0.9000, valid loss-0.3537, acc-0.8992, test loss-0.3595, acc-0.8960\n",
      "Iter-96400, train loss-0.3602, acc-0.9000, valid loss-0.3537, acc-0.8992, test loss-0.3595, acc-0.8959\n",
      "Iter-96410, train loss-0.3935, acc-0.9200, valid loss-0.3537, acc-0.8994, test loss-0.3595, acc-0.8962\n",
      "Iter-96420, train loss-0.5486, acc-0.8400, valid loss-0.3537, acc-0.8994, test loss-0.3595, acc-0.8961\n",
      "Iter-96430, train loss-0.4443, acc-0.8400, valid loss-0.3537, acc-0.8996, test loss-0.3595, acc-0.8962\n",
      "Iter-96440, train loss-0.2614, acc-0.9000, valid loss-0.3537, acc-0.8996, test loss-0.3595, acc-0.8961\n",
      "Iter-96450, train loss-0.2679, acc-0.8600, valid loss-0.3537, acc-0.8996, test loss-0.3595, acc-0.8961\n",
      "Iter-96460, train loss-0.3548, acc-0.9200, valid loss-0.3537, acc-0.8996, test loss-0.3595, acc-0.8961\n",
      "Iter-96470, train loss-0.2688, acc-0.9200, valid loss-0.3536, acc-0.8994, test loss-0.3594, acc-0.8961\n",
      "Iter-96480, train loss-0.2847, acc-0.9000, valid loss-0.3536, acc-0.8996, test loss-0.3594, acc-0.8961\n",
      "Iter-96490, train loss-0.4581, acc-0.8800, valid loss-0.3536, acc-0.8996, test loss-0.3594, acc-0.8962\n",
      "Iter-96500, train loss-0.6441, acc-0.8000, valid loss-0.3536, acc-0.8994, test loss-0.3594, acc-0.8961\n",
      "Iter-96510, train loss-0.3945, acc-0.9000, valid loss-0.3536, acc-0.8994, test loss-0.3594, acc-0.8960\n",
      "Iter-96520, train loss-0.4662, acc-0.9000, valid loss-0.3536, acc-0.8996, test loss-0.3594, acc-0.8960\n",
      "Iter-96530, train loss-0.3787, acc-0.8800, valid loss-0.3536, acc-0.8996, test loss-0.3594, acc-0.8961\n",
      "Iter-96540, train loss-0.2692, acc-0.9400, valid loss-0.3536, acc-0.8994, test loss-0.3593, acc-0.8962\n",
      "Iter-96550, train loss-0.4186, acc-0.9200, valid loss-0.3536, acc-0.8996, test loss-0.3593, acc-0.8961\n",
      "Iter-96560, train loss-0.3645, acc-0.8800, valid loss-0.3535, acc-0.8996, test loss-0.3593, acc-0.8961\n",
      "Iter-96570, train loss-0.1631, acc-0.9600, valid loss-0.3535, acc-0.8996, test loss-0.3593, acc-0.8962\n",
      "Iter-96580, train loss-0.5348, acc-0.8200, valid loss-0.3535, acc-0.8996, test loss-0.3593, acc-0.8962\n",
      "Iter-96590, train loss-0.3637, acc-0.9000, valid loss-0.3535, acc-0.8996, test loss-0.3593, acc-0.8962\n",
      "Iter-96600, train loss-0.2997, acc-0.9200, valid loss-0.3535, acc-0.8996, test loss-0.3593, acc-0.8962\n",
      "Iter-96610, train loss-0.4968, acc-0.8600, valid loss-0.3535, acc-0.8996, test loss-0.3593, acc-0.8962\n",
      "Iter-96620, train loss-0.5673, acc-0.9000, valid loss-0.3535, acc-0.8996, test loss-0.3593, acc-0.8962\n",
      "Iter-96630, train loss-0.1490, acc-0.9400, valid loss-0.3535, acc-0.8996, test loss-0.3592, acc-0.8962\n",
      "Iter-96640, train loss-0.3271, acc-0.9000, valid loss-0.3535, acc-0.8996, test loss-0.3592, acc-0.8962\n",
      "Iter-96650, train loss-0.2200, acc-0.9400, valid loss-0.3535, acc-0.8996, test loss-0.3592, acc-0.8962\n",
      "Iter-96660, train loss-0.4378, acc-0.8800, valid loss-0.3535, acc-0.8996, test loss-0.3592, acc-0.8962\n",
      "Iter-96670, train loss-0.4374, acc-0.8400, valid loss-0.3535, acc-0.8996, test loss-0.3592, acc-0.8962\n",
      "Iter-96680, train loss-0.6583, acc-0.8600, valid loss-0.3534, acc-0.8996, test loss-0.3592, acc-0.8962\n",
      "Iter-96690, train loss-0.5166, acc-0.9200, valid loss-0.3534, acc-0.8996, test loss-0.3592, acc-0.8962\n",
      "Iter-96700, train loss-0.4996, acc-0.8600, valid loss-0.3534, acc-0.8996, test loss-0.3592, acc-0.8962\n",
      "Iter-96710, train loss-0.2405, acc-0.9600, valid loss-0.3534, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96720, train loss-0.3615, acc-0.8600, valid loss-0.3534, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96730, train loss-0.4972, acc-0.8600, valid loss-0.3534, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96740, train loss-0.3785, acc-0.8800, valid loss-0.3534, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96750, train loss-0.3775, acc-0.8600, valid loss-0.3534, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96760, train loss-0.3374, acc-0.8800, valid loss-0.3534, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96770, train loss-0.5913, acc-0.8000, valid loss-0.3533, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96780, train loss-0.2449, acc-0.9400, valid loss-0.3533, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96790, train loss-0.2219, acc-0.9200, valid loss-0.3533, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96800, train loss-0.2751, acc-0.8800, valid loss-0.3533, acc-0.8996, test loss-0.3591, acc-0.8963\n",
      "Iter-96810, train loss-0.5264, acc-0.8600, valid loss-0.3533, acc-0.8998, test loss-0.3591, acc-0.8963\n",
      "Iter-96820, train loss-0.4075, acc-0.9000, valid loss-0.3533, acc-0.8998, test loss-0.3591, acc-0.8963\n",
      "Iter-96830, train loss-0.3493, acc-0.9400, valid loss-0.3533, acc-0.8998, test loss-0.3590, acc-0.8963\n",
      "Iter-96840, train loss-0.3539, acc-0.9000, valid loss-0.3533, acc-0.8998, test loss-0.3590, acc-0.8963\n",
      "Iter-96850, train loss-0.4904, acc-0.8800, valid loss-0.3533, acc-0.8998, test loss-0.3590, acc-0.8963\n",
      "Iter-96860, train loss-0.2490, acc-0.9600, valid loss-0.3533, acc-0.8998, test loss-0.3590, acc-0.8963\n",
      "Iter-96870, train loss-0.3083, acc-0.9200, valid loss-0.3533, acc-0.8998, test loss-0.3590, acc-0.8963\n",
      "Iter-96880, train loss-0.4623, acc-0.9000, valid loss-0.3533, acc-0.9000, test loss-0.3590, acc-0.8963\n",
      "Iter-96890, train loss-0.4565, acc-0.8600, valid loss-0.3532, acc-0.8998, test loss-0.3590, acc-0.8964\n",
      "Iter-96900, train loss-0.3550, acc-0.8800, valid loss-0.3532, acc-0.8998, test loss-0.3590, acc-0.8964\n",
      "Iter-96910, train loss-0.2253, acc-0.9400, valid loss-0.3532, acc-0.9000, test loss-0.3590, acc-0.8963\n",
      "Iter-96920, train loss-0.2674, acc-0.9200, valid loss-0.3532, acc-0.9000, test loss-0.3590, acc-0.8963\n",
      "Iter-96930, train loss-0.3241, acc-0.9200, valid loss-0.3532, acc-0.9002, test loss-0.3589, acc-0.8964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-96940, train loss-0.5724, acc-0.8400, valid loss-0.3532, acc-0.9002, test loss-0.3589, acc-0.8963\n",
      "Iter-96950, train loss-0.2929, acc-0.9400, valid loss-0.3532, acc-0.9002, test loss-0.3589, acc-0.8963\n",
      "Iter-96960, train loss-0.3932, acc-0.9400, valid loss-0.3531, acc-0.9002, test loss-0.3589, acc-0.8963\n",
      "Iter-96970, train loss-0.1667, acc-0.9800, valid loss-0.3531, acc-0.9002, test loss-0.3589, acc-0.8964\n",
      "Iter-96980, train loss-0.3576, acc-0.9200, valid loss-0.3531, acc-0.9002, test loss-0.3589, acc-0.8964\n",
      "Iter-96990, train loss-0.1873, acc-0.9800, valid loss-0.3531, acc-0.9002, test loss-0.3589, acc-0.8963\n",
      "Iter-97000, train loss-0.2107, acc-0.9400, valid loss-0.3531, acc-0.9002, test loss-0.3589, acc-0.8964\n",
      "Iter-97010, train loss-0.2287, acc-0.9400, valid loss-0.3531, acc-0.9002, test loss-0.3589, acc-0.8965\n",
      "Iter-97020, train loss-0.4711, acc-0.8400, valid loss-0.3531, acc-0.9002, test loss-0.3588, acc-0.8964\n",
      "Iter-97030, train loss-0.4120, acc-0.9000, valid loss-0.3531, acc-0.9002, test loss-0.3588, acc-0.8964\n",
      "Iter-97040, train loss-0.2397, acc-0.9400, valid loss-0.3531, acc-0.9002, test loss-0.3588, acc-0.8964\n",
      "Iter-97050, train loss-0.2613, acc-0.9000, valid loss-0.3531, acc-0.9002, test loss-0.3588, acc-0.8963\n",
      "Iter-97060, train loss-0.3549, acc-0.8800, valid loss-0.3530, acc-0.9000, test loss-0.3588, acc-0.8963\n",
      "Iter-97070, train loss-0.3427, acc-0.8800, valid loss-0.3530, acc-0.9000, test loss-0.3588, acc-0.8963\n",
      "Iter-97080, train loss-0.3354, acc-0.9400, valid loss-0.3530, acc-0.9000, test loss-0.3588, acc-0.8964\n",
      "Iter-97090, train loss-0.3301, acc-0.9000, valid loss-0.3530, acc-0.8998, test loss-0.3588, acc-0.8963\n",
      "Iter-97100, train loss-0.4011, acc-0.9000, valid loss-0.3530, acc-0.8998, test loss-0.3588, acc-0.8963\n",
      "Iter-97110, train loss-0.5453, acc-0.8600, valid loss-0.3530, acc-0.9000, test loss-0.3588, acc-0.8963\n",
      "Iter-97120, train loss-0.3920, acc-0.8600, valid loss-0.3530, acc-0.8996, test loss-0.3587, acc-0.8963\n",
      "Iter-97130, train loss-0.4395, acc-0.9200, valid loss-0.3530, acc-0.8998, test loss-0.3587, acc-0.8963\n",
      "Iter-97140, train loss-0.4028, acc-0.8400, valid loss-0.3530, acc-0.8998, test loss-0.3587, acc-0.8963\n",
      "Iter-97150, train loss-0.3595, acc-0.8800, valid loss-0.3530, acc-0.8998, test loss-0.3587, acc-0.8963\n",
      "Iter-97160, train loss-0.5347, acc-0.8600, valid loss-0.3530, acc-0.9000, test loss-0.3587, acc-0.8963\n",
      "Iter-97170, train loss-0.2261, acc-0.9600, valid loss-0.3529, acc-0.9000, test loss-0.3587, acc-0.8963\n",
      "Iter-97180, train loss-0.3892, acc-0.9200, valid loss-0.3529, acc-0.8998, test loss-0.3587, acc-0.8963\n",
      "Iter-97190, train loss-0.2518, acc-0.9400, valid loss-0.3529, acc-0.8996, test loss-0.3587, acc-0.8963\n",
      "Iter-97200, train loss-0.4659, acc-0.8600, valid loss-0.3529, acc-0.8998, test loss-0.3587, acc-0.8963\n",
      "Iter-97210, train loss-0.2294, acc-0.9400, valid loss-0.3529, acc-0.8998, test loss-0.3586, acc-0.8963\n",
      "Iter-97220, train loss-0.2511, acc-0.9600, valid loss-0.3529, acc-0.8998, test loss-0.3586, acc-0.8963\n",
      "Iter-97230, train loss-0.6898, acc-0.8000, valid loss-0.3529, acc-0.8998, test loss-0.3586, acc-0.8963\n",
      "Iter-97240, train loss-0.2876, acc-0.9000, valid loss-0.3529, acc-0.9000, test loss-0.3586, acc-0.8963\n",
      "Iter-97250, train loss-0.2316, acc-0.9200, valid loss-0.3528, acc-0.9000, test loss-0.3586, acc-0.8963\n",
      "Iter-97260, train loss-0.2921, acc-0.9400, valid loss-0.3528, acc-0.9000, test loss-0.3586, acc-0.8963\n",
      "Iter-97270, train loss-0.5621, acc-0.8400, valid loss-0.3528, acc-0.9000, test loss-0.3586, acc-0.8962\n",
      "Iter-97280, train loss-0.5677, acc-0.8200, valid loss-0.3528, acc-0.9000, test loss-0.3586, acc-0.8962\n",
      "Iter-97290, train loss-0.3122, acc-0.8400, valid loss-0.3528, acc-0.9002, test loss-0.3586, acc-0.8962\n",
      "Iter-97300, train loss-0.3891, acc-0.9000, valid loss-0.3528, acc-0.9000, test loss-0.3585, acc-0.8963\n",
      "Iter-97310, train loss-0.3886, acc-0.8800, valid loss-0.3528, acc-0.9000, test loss-0.3585, acc-0.8963\n",
      "Iter-97320, train loss-0.4459, acc-0.8800, valid loss-0.3528, acc-0.9000, test loss-0.3585, acc-0.8963\n",
      "Iter-97330, train loss-0.3560, acc-0.9000, valid loss-0.3528, acc-0.8998, test loss-0.3585, acc-0.8963\n",
      "Iter-97340, train loss-0.5615, acc-0.8200, valid loss-0.3527, acc-0.9000, test loss-0.3585, acc-0.8963\n",
      "Iter-97350, train loss-0.3447, acc-0.8600, valid loss-0.3527, acc-0.9000, test loss-0.3585, acc-0.8962\n",
      "Iter-97360, train loss-0.3517, acc-0.9400, valid loss-0.3527, acc-0.9000, test loss-0.3585, acc-0.8961\n",
      "Iter-97370, train loss-0.4953, acc-0.8800, valid loss-0.3527, acc-0.9000, test loss-0.3585, acc-0.8961\n",
      "Iter-97380, train loss-0.6968, acc-0.7600, valid loss-0.3527, acc-0.9000, test loss-0.3584, acc-0.8961\n",
      "Iter-97390, train loss-0.3149, acc-0.9200, valid loss-0.3527, acc-0.9000, test loss-0.3584, acc-0.8962\n",
      "Iter-97400, train loss-0.3520, acc-0.9200, valid loss-0.3527, acc-0.9002, test loss-0.3584, acc-0.8962\n",
      "Iter-97410, train loss-0.3449, acc-0.9200, valid loss-0.3527, acc-0.9002, test loss-0.3584, acc-0.8962\n",
      "Iter-97420, train loss-0.2012, acc-0.9200, valid loss-0.3527, acc-0.9000, test loss-0.3584, acc-0.8963\n",
      "Iter-97430, train loss-0.3837, acc-0.9200, valid loss-0.3526, acc-0.9000, test loss-0.3584, acc-0.8962\n",
      "Iter-97440, train loss-0.4651, acc-0.8200, valid loss-0.3526, acc-0.9000, test loss-0.3584, acc-0.8962\n",
      "Iter-97450, train loss-0.6770, acc-0.8000, valid loss-0.3526, acc-0.9000, test loss-0.3584, acc-0.8962\n",
      "Iter-97460, train loss-0.2231, acc-0.9200, valid loss-0.3526, acc-0.9000, test loss-0.3584, acc-0.8962\n",
      "Iter-97470, train loss-0.2957, acc-0.9200, valid loss-0.3526, acc-0.9000, test loss-0.3584, acc-0.8962\n",
      "Iter-97480, train loss-0.4468, acc-0.8800, valid loss-0.3526, acc-0.8998, test loss-0.3583, acc-0.8962\n",
      "Iter-97490, train loss-0.5084, acc-0.8800, valid loss-0.3526, acc-0.8998, test loss-0.3583, acc-0.8962\n",
      "Iter-97500, train loss-0.5085, acc-0.8600, valid loss-0.3526, acc-0.8998, test loss-0.3583, acc-0.8963\n",
      "Iter-97510, train loss-0.3177, acc-0.8800, valid loss-0.3525, acc-0.9000, test loss-0.3583, acc-0.8963\n",
      "Iter-97520, train loss-0.5634, acc-0.7800, valid loss-0.3525, acc-0.9000, test loss-0.3583, acc-0.8963\n",
      "Iter-97530, train loss-0.3006, acc-0.8800, valid loss-0.3525, acc-0.9000, test loss-0.3583, acc-0.8963\n",
      "Iter-97540, train loss-0.4056, acc-0.8400, valid loss-0.3525, acc-0.9000, test loss-0.3583, acc-0.8963\n",
      "Iter-97550, train loss-0.3213, acc-0.9400, valid loss-0.3525, acc-0.9000, test loss-0.3583, acc-0.8963\n",
      "Iter-97560, train loss-0.2839, acc-0.9400, valid loss-0.3525, acc-0.9000, test loss-0.3582, acc-0.8963\n",
      "Iter-97570, train loss-0.2455, acc-0.9600, valid loss-0.3525, acc-0.9002, test loss-0.3582, acc-0.8964\n",
      "Iter-97580, train loss-0.4963, acc-0.8800, valid loss-0.3524, acc-0.9002, test loss-0.3582, acc-0.8963\n",
      "Iter-97590, train loss-0.3821, acc-0.8800, valid loss-0.3524, acc-0.9000, test loss-0.3582, acc-0.8963\n",
      "Iter-97600, train loss-0.4217, acc-0.8800, valid loss-0.3524, acc-0.9002, test loss-0.3582, acc-0.8964\n",
      "Iter-97610, train loss-0.3561, acc-0.8600, valid loss-0.3524, acc-0.9002, test loss-0.3582, acc-0.8964\n",
      "Iter-97620, train loss-0.4101, acc-0.8600, valid loss-0.3524, acc-0.9000, test loss-0.3582, acc-0.8963\n",
      "Iter-97630, train loss-0.3453, acc-0.9200, valid loss-0.3524, acc-0.9000, test loss-0.3582, acc-0.8964\n",
      "Iter-97640, train loss-0.4319, acc-0.9000, valid loss-0.3524, acc-0.8998, test loss-0.3581, acc-0.8964\n",
      "Iter-97650, train loss-0.3045, acc-0.9400, valid loss-0.3524, acc-0.8996, test loss-0.3581, acc-0.8964\n",
      "Iter-97660, train loss-0.5303, acc-0.8200, valid loss-0.3524, acc-0.9000, test loss-0.3581, acc-0.8964\n",
      "Iter-97670, train loss-0.3283, acc-0.8800, valid loss-0.3524, acc-0.9000, test loss-0.3581, acc-0.8963\n",
      "Iter-97680, train loss-0.4035, acc-0.9200, valid loss-0.3524, acc-0.8998, test loss-0.3581, acc-0.8963\n",
      "Iter-97690, train loss-0.3441, acc-0.8800, valid loss-0.3523, acc-0.8998, test loss-0.3581, acc-0.8963\n",
      "Iter-97700, train loss-0.4552, acc-0.9000, valid loss-0.3523, acc-0.8996, test loss-0.3581, acc-0.8963\n",
      "Iter-97710, train loss-0.1881, acc-0.9600, valid loss-0.3523, acc-0.8998, test loss-0.3581, acc-0.8965\n",
      "Iter-97720, train loss-0.1964, acc-0.9800, valid loss-0.3523, acc-0.8998, test loss-0.3581, acc-0.8965\n",
      "Iter-97730, train loss-0.2816, acc-0.8600, valid loss-0.3523, acc-0.8996, test loss-0.3580, acc-0.8964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-97740, train loss-0.5303, acc-0.8600, valid loss-0.3523, acc-0.8996, test loss-0.3580, acc-0.8963\n",
      "Iter-97750, train loss-0.1919, acc-0.9600, valid loss-0.3523, acc-0.8996, test loss-0.3580, acc-0.8962\n",
      "Iter-97760, train loss-0.2253, acc-0.9400, valid loss-0.3523, acc-0.8996, test loss-0.3580, acc-0.8963\n",
      "Iter-97770, train loss-0.1746, acc-0.9400, valid loss-0.3522, acc-0.8996, test loss-0.3580, acc-0.8963\n",
      "Iter-97780, train loss-0.4141, acc-0.8400, valid loss-0.3522, acc-0.8998, test loss-0.3580, acc-0.8963\n",
      "Iter-97790, train loss-0.2197, acc-0.9600, valid loss-0.3522, acc-0.8998, test loss-0.3580, acc-0.8964\n",
      "Iter-97800, train loss-0.1936, acc-1.0000, valid loss-0.3522, acc-0.8998, test loss-0.3580, acc-0.8965\n",
      "Iter-97810, train loss-0.3312, acc-0.9000, valid loss-0.3522, acc-0.8998, test loss-0.3580, acc-0.8965\n",
      "Iter-97820, train loss-0.4368, acc-0.8800, valid loss-0.3522, acc-0.9000, test loss-0.3580, acc-0.8964\n",
      "Iter-97830, train loss-0.2294, acc-0.9200, valid loss-0.3522, acc-0.9002, test loss-0.3579, acc-0.8964\n",
      "Iter-97840, train loss-0.3814, acc-0.8600, valid loss-0.3522, acc-0.9002, test loss-0.3579, acc-0.8964\n",
      "Iter-97850, train loss-0.3006, acc-0.9000, valid loss-0.3521, acc-0.9002, test loss-0.3579, acc-0.8964\n",
      "Iter-97860, train loss-0.4090, acc-0.8600, valid loss-0.3521, acc-0.9000, test loss-0.3579, acc-0.8964\n",
      "Iter-97870, train loss-0.3224, acc-0.9600, valid loss-0.3521, acc-0.9000, test loss-0.3579, acc-0.8963\n",
      "Iter-97880, train loss-0.3292, acc-0.9000, valid loss-0.3521, acc-0.9000, test loss-0.3579, acc-0.8964\n",
      "Iter-97890, train loss-0.5803, acc-0.8200, valid loss-0.3521, acc-0.9000, test loss-0.3579, acc-0.8964\n",
      "Iter-97900, train loss-0.4702, acc-0.8600, valid loss-0.3521, acc-0.8998, test loss-0.3579, acc-0.8964\n",
      "Iter-97910, train loss-0.4718, acc-0.8600, valid loss-0.3521, acc-0.8998, test loss-0.3579, acc-0.8963\n",
      "Iter-97920, train loss-0.1888, acc-0.9800, valid loss-0.3521, acc-0.9000, test loss-0.3579, acc-0.8963\n",
      "Iter-97930, train loss-0.3368, acc-0.8800, valid loss-0.3521, acc-0.9000, test loss-0.3579, acc-0.8964\n",
      "Iter-97940, train loss-0.2552, acc-0.9600, valid loss-0.3520, acc-0.9000, test loss-0.3578, acc-0.8964\n",
      "Iter-97950, train loss-0.4250, acc-0.8600, valid loss-0.3520, acc-0.9000, test loss-0.3578, acc-0.8965\n",
      "Iter-97960, train loss-0.3528, acc-0.9000, valid loss-0.3520, acc-0.8998, test loss-0.3578, acc-0.8965\n",
      "Iter-97970, train loss-0.3135, acc-0.8800, valid loss-0.3520, acc-0.8998, test loss-0.3578, acc-0.8964\n",
      "Iter-97980, train loss-0.4203, acc-0.8800, valid loss-0.3520, acc-0.8998, test loss-0.3578, acc-0.8965\n",
      "Iter-97990, train loss-0.2749, acc-0.9400, valid loss-0.3520, acc-0.9000, test loss-0.3578, acc-0.8965\n",
      "Iter-98000, train loss-0.3458, acc-0.9200, valid loss-0.3520, acc-0.8998, test loss-0.3578, acc-0.8965\n",
      "Iter-98010, train loss-0.3568, acc-0.9200, valid loss-0.3520, acc-0.8998, test loss-0.3578, acc-0.8965\n",
      "Iter-98020, train loss-0.2482, acc-0.9400, valid loss-0.3519, acc-0.9000, test loss-0.3578, acc-0.8964\n",
      "Iter-98030, train loss-0.2559, acc-0.9000, valid loss-0.3519, acc-0.9000, test loss-0.3577, acc-0.8964\n",
      "Iter-98040, train loss-0.2836, acc-0.9000, valid loss-0.3519, acc-0.9000, test loss-0.3577, acc-0.8965\n",
      "Iter-98050, train loss-0.4653, acc-0.9000, valid loss-0.3519, acc-0.9000, test loss-0.3577, acc-0.8965\n",
      "Iter-98060, train loss-0.3368, acc-0.9400, valid loss-0.3519, acc-0.8998, test loss-0.3577, acc-0.8964\n",
      "Iter-98070, train loss-0.4542, acc-0.9000, valid loss-0.3519, acc-0.8998, test loss-0.3577, acc-0.8964\n",
      "Iter-98080, train loss-0.3888, acc-0.8600, valid loss-0.3519, acc-0.8998, test loss-0.3577, acc-0.8965\n",
      "Iter-98090, train loss-0.3261, acc-0.9400, valid loss-0.3518, acc-0.8998, test loss-0.3577, acc-0.8965\n",
      "Iter-98100, train loss-0.2114, acc-0.9400, valid loss-0.3518, acc-0.9000, test loss-0.3577, acc-0.8965\n",
      "Iter-98110, train loss-0.3122, acc-0.9000, valid loss-0.3518, acc-0.9000, test loss-0.3576, acc-0.8964\n",
      "Iter-98120, train loss-0.4168, acc-0.9000, valid loss-0.3518, acc-0.9000, test loss-0.3576, acc-0.8965\n",
      "Iter-98130, train loss-0.3802, acc-0.9200, valid loss-0.3518, acc-0.9002, test loss-0.3576, acc-0.8963\n",
      "Iter-98140, train loss-0.3707, acc-0.8800, valid loss-0.3518, acc-0.9000, test loss-0.3576, acc-0.8964\n",
      "Iter-98150, train loss-0.4293, acc-0.8800, valid loss-0.3518, acc-0.9000, test loss-0.3576, acc-0.8964\n",
      "Iter-98160, train loss-0.3645, acc-0.9200, valid loss-0.3518, acc-0.9000, test loss-0.3576, acc-0.8964\n",
      "Iter-98170, train loss-0.2582, acc-0.9400, valid loss-0.3518, acc-0.9000, test loss-0.3576, acc-0.8964\n",
      "Iter-98180, train loss-0.3913, acc-0.9200, valid loss-0.3518, acc-0.9000, test loss-0.3576, acc-0.8964\n",
      "Iter-98190, train loss-0.3857, acc-0.9200, valid loss-0.3518, acc-0.8998, test loss-0.3576, acc-0.8964\n",
      "Iter-98200, train loss-0.4098, acc-0.8800, valid loss-0.3518, acc-0.9000, test loss-0.3576, acc-0.8963\n",
      "Iter-98210, train loss-0.3603, acc-0.8600, valid loss-0.3517, acc-0.8998, test loss-0.3575, acc-0.8964\n",
      "Iter-98220, train loss-0.3704, acc-0.8400, valid loss-0.3517, acc-0.8998, test loss-0.3575, acc-0.8964\n",
      "Iter-98230, train loss-0.2373, acc-0.9400, valid loss-0.3517, acc-0.9000, test loss-0.3575, acc-0.8963\n",
      "Iter-98240, train loss-0.4417, acc-0.8400, valid loss-0.3517, acc-0.9000, test loss-0.3575, acc-0.8963\n",
      "Iter-98250, train loss-0.3122, acc-0.8800, valid loss-0.3517, acc-0.9000, test loss-0.3575, acc-0.8963\n",
      "Iter-98260, train loss-0.2950, acc-0.9200, valid loss-0.3517, acc-0.9000, test loss-0.3575, acc-0.8964\n",
      "Iter-98270, train loss-0.2479, acc-0.9600, valid loss-0.3517, acc-0.9000, test loss-0.3575, acc-0.8963\n",
      "Iter-98280, train loss-0.3553, acc-0.9000, valid loss-0.3516, acc-0.9000, test loss-0.3574, acc-0.8963\n",
      "Iter-98290, train loss-0.3816, acc-0.8400, valid loss-0.3516, acc-0.9000, test loss-0.3575, acc-0.8964\n",
      "Iter-98300, train loss-0.1802, acc-0.9800, valid loss-0.3516, acc-0.9000, test loss-0.3574, acc-0.8964\n",
      "Iter-98310, train loss-0.2854, acc-0.9400, valid loss-0.3516, acc-0.9002, test loss-0.3574, acc-0.8964\n",
      "Iter-98320, train loss-0.2918, acc-0.9000, valid loss-0.3516, acc-0.9002, test loss-0.3574, acc-0.8963\n",
      "Iter-98330, train loss-0.3434, acc-0.8800, valid loss-0.3516, acc-0.9002, test loss-0.3574, acc-0.8963\n",
      "Iter-98340, train loss-0.2022, acc-0.9600, valid loss-0.3515, acc-0.9002, test loss-0.3574, acc-0.8963\n",
      "Iter-98350, train loss-0.4870, acc-0.7600, valid loss-0.3515, acc-0.9002, test loss-0.3574, acc-0.8963\n",
      "Iter-98360, train loss-0.2631, acc-0.9000, valid loss-0.3515, acc-0.9002, test loss-0.3574, acc-0.8963\n",
      "Iter-98370, train loss-0.2747, acc-0.9400, valid loss-0.3515, acc-0.9002, test loss-0.3574, acc-0.8962\n",
      "Iter-98380, train loss-0.7662, acc-0.8000, valid loss-0.3515, acc-0.9002, test loss-0.3573, acc-0.8962\n",
      "Iter-98390, train loss-0.4902, acc-0.8400, valid loss-0.3515, acc-0.9002, test loss-0.3573, acc-0.8962\n",
      "Iter-98400, train loss-0.3631, acc-0.9000, valid loss-0.3515, acc-0.9002, test loss-0.3573, acc-0.8962\n",
      "Iter-98410, train loss-0.4058, acc-0.8800, valid loss-0.3515, acc-0.9000, test loss-0.3573, acc-0.8964\n",
      "Iter-98420, train loss-0.4903, acc-0.8200, valid loss-0.3515, acc-0.9000, test loss-0.3573, acc-0.8964\n",
      "Iter-98430, train loss-0.3629, acc-0.9400, valid loss-0.3514, acc-0.9000, test loss-0.3573, acc-0.8965\n",
      "Iter-98440, train loss-0.2239, acc-0.9200, valid loss-0.3514, acc-0.9002, test loss-0.3573, acc-0.8963\n",
      "Iter-98450, train loss-0.2421, acc-0.9400, valid loss-0.3514, acc-0.9000, test loss-0.3572, acc-0.8962\n",
      "Iter-98460, train loss-0.4296, acc-0.8600, valid loss-0.3514, acc-0.9000, test loss-0.3572, acc-0.8963\n",
      "Iter-98470, train loss-0.3865, acc-0.9200, valid loss-0.3514, acc-0.9000, test loss-0.3572, acc-0.8963\n",
      "Iter-98480, train loss-0.4290, acc-0.8400, valid loss-0.3514, acc-0.9000, test loss-0.3572, acc-0.8963\n",
      "Iter-98490, train loss-0.3085, acc-0.9200, valid loss-0.3514, acc-0.9002, test loss-0.3572, acc-0.8963\n",
      "Iter-98500, train loss-0.3537, acc-0.9200, valid loss-0.3514, acc-0.9002, test loss-0.3572, acc-0.8965\n",
      "Iter-98510, train loss-0.5008, acc-0.8600, valid loss-0.3514, acc-0.9002, test loss-0.3572, acc-0.8965\n",
      "Iter-98520, train loss-0.3006, acc-0.9200, valid loss-0.3514, acc-0.9002, test loss-0.3572, acc-0.8964\n",
      "Iter-98530, train loss-0.4161, acc-0.8400, valid loss-0.3514, acc-0.9002, test loss-0.3572, acc-0.8963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-98540, train loss-0.5872, acc-0.8600, valid loss-0.3514, acc-0.9002, test loss-0.3571, acc-0.8963\n",
      "Iter-98550, train loss-0.3542, acc-0.9200, valid loss-0.3513, acc-0.9000, test loss-0.3571, acc-0.8962\n",
      "Iter-98560, train loss-0.3714, acc-0.9200, valid loss-0.3513, acc-0.9002, test loss-0.3571, acc-0.8963\n",
      "Iter-98570, train loss-0.4056, acc-0.8600, valid loss-0.3513, acc-0.9002, test loss-0.3571, acc-0.8962\n",
      "Iter-98580, train loss-0.4512, acc-0.8400, valid loss-0.3513, acc-0.9002, test loss-0.3571, acc-0.8962\n",
      "Iter-98590, train loss-0.7216, acc-0.8200, valid loss-0.3513, acc-0.9002, test loss-0.3571, acc-0.8962\n",
      "Iter-98600, train loss-0.2255, acc-0.9000, valid loss-0.3513, acc-0.9002, test loss-0.3571, acc-0.8963\n",
      "Iter-98610, train loss-0.1510, acc-0.9800, valid loss-0.3513, acc-0.9002, test loss-0.3571, acc-0.8963\n",
      "Iter-98620, train loss-0.4072, acc-0.8600, valid loss-0.3513, acc-0.9002, test loss-0.3571, acc-0.8963\n",
      "Iter-98630, train loss-0.3766, acc-0.9200, valid loss-0.3513, acc-0.9002, test loss-0.3570, acc-0.8963\n",
      "Iter-98640, train loss-0.5399, acc-0.8400, valid loss-0.3513, acc-0.9002, test loss-0.3570, acc-0.8963\n",
      "Iter-98650, train loss-0.3660, acc-0.9000, valid loss-0.3513, acc-0.9002, test loss-0.3570, acc-0.8964\n",
      "Iter-98660, train loss-0.3601, acc-0.9200, valid loss-0.3512, acc-0.9002, test loss-0.3570, acc-0.8962\n",
      "Iter-98670, train loss-0.2593, acc-0.9400, valid loss-0.3512, acc-0.9002, test loss-0.3570, acc-0.8962\n",
      "Iter-98680, train loss-0.3845, acc-0.9000, valid loss-0.3512, acc-0.9002, test loss-0.3570, acc-0.8962\n",
      "Iter-98690, train loss-0.2025, acc-0.9600, valid loss-0.3512, acc-0.9002, test loss-0.3570, acc-0.8962\n",
      "Iter-98700, train loss-0.3599, acc-0.8800, valid loss-0.3512, acc-0.9002, test loss-0.3570, acc-0.8962\n",
      "Iter-98710, train loss-0.2719, acc-0.9600, valid loss-0.3512, acc-0.9002, test loss-0.3569, acc-0.8963\n",
      "Iter-98720, train loss-0.2991, acc-0.9600, valid loss-0.3512, acc-0.9002, test loss-0.3569, acc-0.8964\n",
      "Iter-98730, train loss-0.4579, acc-0.8600, valid loss-0.3511, acc-0.9002, test loss-0.3569, acc-0.8963\n",
      "Iter-98740, train loss-0.2752, acc-0.9000, valid loss-0.3511, acc-0.9002, test loss-0.3569, acc-0.8964\n",
      "Iter-98750, train loss-0.2799, acc-0.9200, valid loss-0.3511, acc-0.9002, test loss-0.3569, acc-0.8964\n",
      "Iter-98760, train loss-0.2004, acc-0.9600, valid loss-0.3511, acc-0.9002, test loss-0.3569, acc-0.8964\n",
      "Iter-98770, train loss-0.3023, acc-0.9000, valid loss-0.3511, acc-0.9002, test loss-0.3569, acc-0.8964\n",
      "Iter-98780, train loss-0.4319, acc-0.8600, valid loss-0.3511, acc-0.9002, test loss-0.3569, acc-0.8964\n",
      "Iter-98790, train loss-0.4224, acc-0.8800, valid loss-0.3511, acc-0.9002, test loss-0.3568, acc-0.8964\n",
      "Iter-98800, train loss-0.2380, acc-0.9400, valid loss-0.3511, acc-0.9002, test loss-0.3568, acc-0.8963\n",
      "Iter-98810, train loss-0.4929, acc-0.8600, valid loss-0.3510, acc-0.9002, test loss-0.3568, acc-0.8963\n",
      "Iter-98820, train loss-0.3124, acc-0.8800, valid loss-0.3510, acc-0.9002, test loss-0.3568, acc-0.8963\n",
      "Iter-98830, train loss-0.4882, acc-0.8800, valid loss-0.3510, acc-0.9002, test loss-0.3568, acc-0.8964\n",
      "Iter-98840, train loss-0.4445, acc-0.8600, valid loss-0.3510, acc-0.9002, test loss-0.3568, acc-0.8963\n",
      "Iter-98850, train loss-0.3111, acc-0.9000, valid loss-0.3510, acc-0.9002, test loss-0.3568, acc-0.8963\n",
      "Iter-98860, train loss-0.3393, acc-0.9000, valid loss-0.3510, acc-0.9002, test loss-0.3568, acc-0.8963\n",
      "Iter-98870, train loss-0.4744, acc-0.8800, valid loss-0.3510, acc-0.9002, test loss-0.3567, acc-0.8963\n",
      "Iter-98880, train loss-0.4258, acc-0.8600, valid loss-0.3510, acc-0.9002, test loss-0.3567, acc-0.8964\n",
      "Iter-98890, train loss-0.4110, acc-0.8600, valid loss-0.3510, acc-0.9002, test loss-0.3567, acc-0.8964\n",
      "Iter-98900, train loss-0.4288, acc-0.8800, valid loss-0.3509, acc-0.9002, test loss-0.3567, acc-0.8964\n",
      "Iter-98910, train loss-0.2424, acc-0.9800, valid loss-0.3509, acc-0.9002, test loss-0.3567, acc-0.8964\n",
      "Iter-98920, train loss-0.4299, acc-0.9200, valid loss-0.3509, acc-0.9000, test loss-0.3567, acc-0.8964\n",
      "Iter-98930, train loss-0.4516, acc-0.9000, valid loss-0.3509, acc-0.9002, test loss-0.3567, acc-0.8964\n",
      "Iter-98940, train loss-0.4173, acc-0.8400, valid loss-0.3509, acc-0.9002, test loss-0.3566, acc-0.8964\n",
      "Iter-98950, train loss-0.4661, acc-0.8800, valid loss-0.3509, acc-0.9002, test loss-0.3566, acc-0.8963\n",
      "Iter-98960, train loss-0.4984, acc-0.8400, valid loss-0.3508, acc-0.9002, test loss-0.3566, acc-0.8964\n",
      "Iter-98970, train loss-0.3707, acc-0.9200, valid loss-0.3508, acc-0.9002, test loss-0.3566, acc-0.8965\n",
      "Iter-98980, train loss-0.2809, acc-0.9400, valid loss-0.3508, acc-0.9002, test loss-0.3566, acc-0.8966\n",
      "Iter-98990, train loss-0.2917, acc-0.9000, valid loss-0.3508, acc-0.9002, test loss-0.3566, acc-0.8966\n",
      "Iter-99000, train loss-0.1679, acc-0.9800, valid loss-0.3508, acc-0.9002, test loss-0.3566, acc-0.8965\n",
      "Iter-99010, train loss-0.3398, acc-0.9200, valid loss-0.3508, acc-0.9002, test loss-0.3566, acc-0.8964\n",
      "Iter-99020, train loss-0.6595, acc-0.7800, valid loss-0.3508, acc-0.9002, test loss-0.3566, acc-0.8964\n",
      "Iter-99030, train loss-0.1062, acc-0.9800, valid loss-0.3508, acc-0.9002, test loss-0.3565, acc-0.8965\n",
      "Iter-99040, train loss-0.2866, acc-0.9400, valid loss-0.3507, acc-0.9002, test loss-0.3565, acc-0.8965\n",
      "Iter-99050, train loss-0.4262, acc-0.8400, valid loss-0.3507, acc-0.9002, test loss-0.3565, acc-0.8965\n",
      "Iter-99060, train loss-0.3354, acc-0.9000, valid loss-0.3507, acc-0.9002, test loss-0.3565, acc-0.8966\n",
      "Iter-99070, train loss-0.3341, acc-0.9200, valid loss-0.3507, acc-0.9002, test loss-0.3565, acc-0.8964\n",
      "Iter-99080, train loss-0.2393, acc-0.9200, valid loss-0.3507, acc-0.9002, test loss-0.3565, acc-0.8964\n",
      "Iter-99090, train loss-0.4450, acc-0.8800, valid loss-0.3507, acc-0.9002, test loss-0.3565, acc-0.8964\n",
      "Iter-99100, train loss-0.3468, acc-0.8800, valid loss-0.3507, acc-0.9002, test loss-0.3565, acc-0.8964\n",
      "Iter-99110, train loss-0.4833, acc-0.8600, valid loss-0.3507, acc-0.9002, test loss-0.3565, acc-0.8964\n",
      "Iter-99120, train loss-0.5984, acc-0.7800, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8964\n",
      "Iter-99130, train loss-0.4335, acc-0.9000, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8966\n",
      "Iter-99140, train loss-0.1991, acc-0.9400, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8968\n",
      "Iter-99150, train loss-0.5844, acc-0.8000, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8968\n",
      "Iter-99160, train loss-0.3582, acc-0.9000, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8967\n",
      "Iter-99170, train loss-0.4495, acc-0.8800, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8966\n",
      "Iter-99180, train loss-0.2864, acc-0.9000, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8966\n",
      "Iter-99190, train loss-0.3163, acc-0.8600, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8967\n",
      "Iter-99200, train loss-0.5841, acc-0.8400, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8966\n",
      "Iter-99210, train loss-0.3003, acc-0.9200, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8966\n",
      "Iter-99220, train loss-0.3266, acc-0.8800, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8967\n",
      "Iter-99230, train loss-0.3311, acc-0.9000, valid loss-0.3506, acc-0.9002, test loss-0.3564, acc-0.8967\n",
      "Iter-99240, train loss-0.3970, acc-0.8800, valid loss-0.3505, acc-0.9002, test loss-0.3563, acc-0.8967\n",
      "Iter-99250, train loss-0.3056, acc-0.8800, valid loss-0.3505, acc-0.9002, test loss-0.3563, acc-0.8966\n",
      "Iter-99260, train loss-0.2823, acc-0.8600, valid loss-0.3505, acc-0.9002, test loss-0.3563, acc-0.8966\n",
      "Iter-99270, train loss-0.4354, acc-0.8600, valid loss-0.3505, acc-0.9002, test loss-0.3563, acc-0.8967\n",
      "Iter-99280, train loss-0.4074, acc-0.8200, valid loss-0.3505, acc-0.9002, test loss-0.3563, acc-0.8966\n",
      "Iter-99290, train loss-0.3484, acc-0.8800, valid loss-0.3505, acc-0.9002, test loss-0.3563, acc-0.8966\n",
      "Iter-99300, train loss-0.3609, acc-0.8600, valid loss-0.3505, acc-0.9002, test loss-0.3563, acc-0.8966\n",
      "Iter-99310, train loss-0.1850, acc-0.9400, valid loss-0.3505, acc-0.9002, test loss-0.3563, acc-0.8965\n",
      "Iter-99320, train loss-0.4203, acc-0.8800, valid loss-0.3504, acc-0.9002, test loss-0.3563, acc-0.8965\n",
      "Iter-99330, train loss-0.6454, acc-0.8400, valid loss-0.3504, acc-0.9002, test loss-0.3563, acc-0.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-99340, train loss-0.5705, acc-0.7600, valid loss-0.3504, acc-0.9002, test loss-0.3562, acc-0.8967\n",
      "Iter-99350, train loss-0.5967, acc-0.8000, valid loss-0.3504, acc-0.9002, test loss-0.3562, acc-0.8967\n",
      "Iter-99360, train loss-0.3180, acc-0.9200, valid loss-0.3504, acc-0.9002, test loss-0.3562, acc-0.8966\n",
      "Iter-99370, train loss-0.2450, acc-0.9800, valid loss-0.3504, acc-0.9002, test loss-0.3562, acc-0.8965\n",
      "Iter-99380, train loss-0.2539, acc-0.9400, valid loss-0.3504, acc-0.9002, test loss-0.3562, acc-0.8966\n",
      "Iter-99390, train loss-0.6408, acc-0.8400, valid loss-0.3504, acc-0.9002, test loss-0.3562, acc-0.8968\n",
      "Iter-99400, train loss-0.2514, acc-0.9400, valid loss-0.3504, acc-0.9002, test loss-0.3562, acc-0.8968\n",
      "Iter-99410, train loss-0.2675, acc-0.9200, valid loss-0.3503, acc-0.9002, test loss-0.3562, acc-0.8967\n",
      "Iter-99420, train loss-0.2337, acc-0.9600, valid loss-0.3503, acc-0.9002, test loss-0.3562, acc-0.8965\n",
      "Iter-99430, train loss-0.3471, acc-0.8400, valid loss-0.3503, acc-0.9002, test loss-0.3562, acc-0.8967\n",
      "Iter-99440, train loss-0.7118, acc-0.7800, valid loss-0.3503, acc-0.9002, test loss-0.3562, acc-0.8966\n",
      "Iter-99450, train loss-0.2767, acc-0.9000, valid loss-0.3503, acc-0.9002, test loss-0.3562, acc-0.8965\n",
      "Iter-99460, train loss-0.3167, acc-0.9200, valid loss-0.3503, acc-0.9002, test loss-0.3561, acc-0.8967\n",
      "Iter-99470, train loss-0.3238, acc-0.9000, valid loss-0.3503, acc-0.9002, test loss-0.3561, acc-0.8967\n",
      "Iter-99480, train loss-0.4163, acc-0.8400, valid loss-0.3503, acc-0.9002, test loss-0.3561, acc-0.8967\n",
      "Iter-99490, train loss-0.2325, acc-0.9600, valid loss-0.3502, acc-0.9002, test loss-0.3561, acc-0.8967\n",
      "Iter-99500, train loss-0.3418, acc-0.9000, valid loss-0.3502, acc-0.9002, test loss-0.3561, acc-0.8967\n",
      "Iter-99510, train loss-0.3676, acc-0.9200, valid loss-0.3502, acc-0.9002, test loss-0.3561, acc-0.8967\n",
      "Iter-99520, train loss-0.3098, acc-0.9200, valid loss-0.3502, acc-0.9002, test loss-0.3561, acc-0.8968\n",
      "Iter-99530, train loss-0.2398, acc-0.9400, valid loss-0.3502, acc-0.9002, test loss-0.3561, acc-0.8968\n",
      "Iter-99540, train loss-0.2641, acc-0.9600, valid loss-0.3502, acc-0.9002, test loss-0.3561, acc-0.8968\n",
      "Iter-99550, train loss-0.5983, acc-0.8400, valid loss-0.3502, acc-0.9002, test loss-0.3561, acc-0.8968\n",
      "Iter-99560, train loss-0.4601, acc-0.8400, valid loss-0.3502, acc-0.9002, test loss-0.3561, acc-0.8967\n",
      "Iter-99570, train loss-0.5737, acc-0.8200, valid loss-0.3502, acc-0.9002, test loss-0.3561, acc-0.8966\n",
      "Iter-99580, train loss-0.2721, acc-0.9400, valid loss-0.3502, acc-0.9002, test loss-0.3560, acc-0.8967\n",
      "Iter-99590, train loss-0.4114, acc-0.8800, valid loss-0.3501, acc-0.9002, test loss-0.3560, acc-0.8967\n",
      "Iter-99600, train loss-0.4579, acc-0.8200, valid loss-0.3501, acc-0.9002, test loss-0.3560, acc-0.8967\n",
      "Iter-99610, train loss-0.2935, acc-0.9200, valid loss-0.3501, acc-0.9002, test loss-0.3560, acc-0.8967\n",
      "Iter-99620, train loss-0.2392, acc-0.9400, valid loss-0.3501, acc-0.9002, test loss-0.3560, acc-0.8967\n",
      "Iter-99630, train loss-0.4914, acc-0.8400, valid loss-0.3501, acc-0.9002, test loss-0.3560, acc-0.8966\n",
      "Iter-99640, train loss-0.3124, acc-0.8800, valid loss-0.3501, acc-0.9002, test loss-0.3560, acc-0.8967\n",
      "Iter-99650, train loss-0.2174, acc-0.9800, valid loss-0.3501, acc-0.9002, test loss-0.3559, acc-0.8967\n",
      "Iter-99660, train loss-0.5631, acc-0.8200, valid loss-0.3501, acc-0.9002, test loss-0.3559, acc-0.8969\n",
      "Iter-99670, train loss-0.3824, acc-0.9000, valid loss-0.3501, acc-0.9002, test loss-0.3559, acc-0.8967\n",
      "Iter-99680, train loss-0.4990, acc-0.8400, valid loss-0.3501, acc-0.9002, test loss-0.3559, acc-0.8966\n",
      "Iter-99690, train loss-0.3714, acc-0.9400, valid loss-0.3501, acc-0.9002, test loss-0.3559, acc-0.8966\n",
      "Iter-99700, train loss-0.6874, acc-0.8000, valid loss-0.3500, acc-0.9002, test loss-0.3559, acc-0.8967\n",
      "Iter-99710, train loss-0.5275, acc-0.8800, valid loss-0.3500, acc-0.9002, test loss-0.3559, acc-0.8966\n",
      "Iter-99720, train loss-0.4294, acc-0.8600, valid loss-0.3500, acc-0.9002, test loss-0.3559, acc-0.8967\n",
      "Iter-99730, train loss-0.4400, acc-0.8400, valid loss-0.3500, acc-0.9002, test loss-0.3559, acc-0.8966\n",
      "Iter-99740, train loss-0.4112, acc-0.9000, valid loss-0.3500, acc-0.9002, test loss-0.3559, acc-0.8966\n",
      "Iter-99750, train loss-0.3605, acc-0.8800, valid loss-0.3500, acc-0.9002, test loss-0.3559, acc-0.8966\n",
      "Iter-99760, train loss-0.2375, acc-0.9000, valid loss-0.3500, acc-0.9002, test loss-0.3559, acc-0.8966\n",
      "Iter-99770, train loss-0.4706, acc-0.8400, valid loss-0.3500, acc-0.9002, test loss-0.3558, acc-0.8967\n",
      "Iter-99780, train loss-0.2073, acc-0.9000, valid loss-0.3500, acc-0.9002, test loss-0.3558, acc-0.8967\n",
      "Iter-99790, train loss-0.4395, acc-0.8600, valid loss-0.3500, acc-0.9002, test loss-0.3558, acc-0.8966\n",
      "Iter-99800, train loss-0.3783, acc-0.8600, valid loss-0.3499, acc-0.9002, test loss-0.3558, acc-0.8966\n",
      "Iter-99810, train loss-0.5849, acc-0.8400, valid loss-0.3499, acc-0.9002, test loss-0.3558, acc-0.8966\n",
      "Iter-99820, train loss-0.2913, acc-0.9400, valid loss-0.3499, acc-0.9002, test loss-0.3558, acc-0.8966\n",
      "Iter-99830, train loss-0.4572, acc-0.8800, valid loss-0.3499, acc-0.9002, test loss-0.3558, acc-0.8966\n",
      "Iter-99840, train loss-0.2751, acc-0.9200, valid loss-0.3499, acc-0.9002, test loss-0.3558, acc-0.8966\n",
      "Iter-99850, train loss-0.3420, acc-0.8800, valid loss-0.3499, acc-0.9002, test loss-0.3558, acc-0.8966\n",
      "Iter-99860, train loss-0.5797, acc-0.8600, valid loss-0.3499, acc-0.9002, test loss-0.3557, acc-0.8967\n",
      "Iter-99870, train loss-0.4759, acc-0.8800, valid loss-0.3499, acc-0.9002, test loss-0.3557, acc-0.8967\n",
      "Iter-99880, train loss-0.4145, acc-0.8800, valid loss-0.3499, acc-0.9002, test loss-0.3557, acc-0.8967\n",
      "Iter-99890, train loss-0.2671, acc-0.9200, valid loss-0.3499, acc-0.9002, test loss-0.3557, acc-0.8967\n",
      "Iter-99900, train loss-0.3936, acc-0.8200, valid loss-0.3498, acc-0.9002, test loss-0.3557, acc-0.8966\n",
      "Iter-99910, train loss-0.5475, acc-0.8800, valid loss-0.3498, acc-0.9002, test loss-0.3557, acc-0.8966\n",
      "Iter-99920, train loss-0.4246, acc-0.9200, valid loss-0.3498, acc-0.9002, test loss-0.3557, acc-0.8966\n",
      "Iter-99930, train loss-0.3098, acc-0.9400, valid loss-0.3498, acc-0.9002, test loss-0.3557, acc-0.8965\n",
      "Iter-99940, train loss-0.3403, acc-0.9000, valid loss-0.3498, acc-0.9002, test loss-0.3557, acc-0.8967\n",
      "Iter-99950, train loss-0.2819, acc-0.9400, valid loss-0.3498, acc-0.9002, test loss-0.3556, acc-0.8967\n",
      "Iter-99960, train loss-0.4292, acc-0.9200, valid loss-0.3498, acc-0.9002, test loss-0.3556, acc-0.8968\n",
      "Iter-99970, train loss-0.3666, acc-0.9000, valid loss-0.3498, acc-0.9002, test loss-0.3556, acc-0.8968\n",
      "Iter-99980, train loss-0.2810, acc-0.9200, valid loss-0.3498, acc-0.9002, test loss-0.3556, acc-0.8968\n",
      "Iter-99990, train loss-0.4207, acc-0.9200, valid loss-0.3498, acc-0.9002, test loss-0.3556, acc-0.8968\n",
      "Iter-100000, train loss-0.5564, acc-0.7800, valid loss-0.3497, acc-0.9002, test loss-0.3556, acc-0.8969\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIAFBCCGEhQQEIYQQAPw/KXuHxdBDsVcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1cc8eeea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1cc8eeef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

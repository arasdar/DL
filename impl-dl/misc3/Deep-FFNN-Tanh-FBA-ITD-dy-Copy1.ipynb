{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#         y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#         y *= 2.0 # uni-var/ std\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#             y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#             y *= 2.0 # uni-var/ std\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "#         dy =  dy @ self.W_fixed[2].T # done\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "#             dy =  dy @ self.W_fixed[2].T # done\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "        dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3219, acc-0.1200, valid loss-2.3191, acc-0.1034, test loss-2.3200, acc-0.0937\n",
      "Iter-20, train loss-2.3418, acc-0.1000, valid loss-2.3190, acc-0.1040, test loss-2.3199, acc-0.0940\n",
      "Iter-30, train loss-2.3174, acc-0.0600, valid loss-2.3188, acc-0.1044, test loss-2.3198, acc-0.0942\n",
      "Iter-40, train loss-2.3151, acc-0.1800, valid loss-2.3187, acc-0.1046, test loss-2.3196, acc-0.0946\n",
      "Iter-50, train loss-2.3148, acc-0.0800, valid loss-2.3185, acc-0.1046, test loss-2.3195, acc-0.0950\n",
      "Iter-60, train loss-2.3034, acc-0.1400, valid loss-2.3184, acc-0.1052, test loss-2.3193, acc-0.0953\n",
      "Iter-70, train loss-2.3074, acc-0.1200, valid loss-2.3183, acc-0.1052, test loss-2.3192, acc-0.0954\n",
      "Iter-80, train loss-2.3214, acc-0.0800, valid loss-2.3181, acc-0.1056, test loss-2.3190, acc-0.0955\n",
      "Iter-90, train loss-2.3169, acc-0.1200, valid loss-2.3180, acc-0.1058, test loss-2.3189, acc-0.0957\n",
      "Iter-100, train loss-2.3270, acc-0.1200, valid loss-2.3178, acc-0.1058, test loss-2.3188, acc-0.0957\n",
      "Iter-110, train loss-2.3057, acc-0.1400, valid loss-2.3177, acc-0.1056, test loss-2.3186, acc-0.0961\n",
      "Iter-120, train loss-2.3192, acc-0.0600, valid loss-2.3176, acc-0.1060, test loss-2.3185, acc-0.0962\n",
      "Iter-130, train loss-2.3221, acc-0.0800, valid loss-2.3174, acc-0.1058, test loss-2.3183, acc-0.0962\n",
      "Iter-140, train loss-2.3185, acc-0.1000, valid loss-2.3173, acc-0.1060, test loss-2.3182, acc-0.0965\n",
      "Iter-150, train loss-2.3348, acc-0.0600, valid loss-2.3171, acc-0.1060, test loss-2.3181, acc-0.0969\n",
      "Iter-160, train loss-2.3342, acc-0.0400, valid loss-2.3170, acc-0.1062, test loss-2.3179, acc-0.0971\n",
      "Iter-170, train loss-2.3173, acc-0.0800, valid loss-2.3168, acc-0.1066, test loss-2.3178, acc-0.0972\n",
      "Iter-180, train loss-2.3258, acc-0.0800, valid loss-2.3167, acc-0.1070, test loss-2.3176, acc-0.0975\n",
      "Iter-190, train loss-2.3474, acc-0.1000, valid loss-2.3166, acc-0.1070, test loss-2.3175, acc-0.0977\n",
      "Iter-200, train loss-2.3291, acc-0.1000, valid loss-2.3164, acc-0.1072, test loss-2.3174, acc-0.0980\n",
      "Iter-210, train loss-2.2914, acc-0.1200, valid loss-2.3163, acc-0.1072, test loss-2.3172, acc-0.0979\n",
      "Iter-220, train loss-2.3322, acc-0.0600, valid loss-2.3162, acc-0.1074, test loss-2.3171, acc-0.0982\n",
      "Iter-230, train loss-2.3330, acc-0.0800, valid loss-2.3160, acc-0.1072, test loss-2.3169, acc-0.0980\n",
      "Iter-240, train loss-2.3173, acc-0.0400, valid loss-2.3159, acc-0.1074, test loss-2.3168, acc-0.0986\n",
      "Iter-250, train loss-2.2968, acc-0.1600, valid loss-2.3157, acc-0.1076, test loss-2.3166, acc-0.0987\n",
      "Iter-260, train loss-2.2801, acc-0.1600, valid loss-2.3156, acc-0.1076, test loss-2.3165, acc-0.0992\n",
      "Iter-270, train loss-2.3240, acc-0.1000, valid loss-2.3155, acc-0.1076, test loss-2.3164, acc-0.0995\n",
      "Iter-280, train loss-2.3100, acc-0.0800, valid loss-2.3153, acc-0.1078, test loss-2.3162, acc-0.0998\n",
      "Iter-290, train loss-2.3204, acc-0.1000, valid loss-2.3152, acc-0.1080, test loss-2.3161, acc-0.1000\n",
      "Iter-300, train loss-2.2978, acc-0.1200, valid loss-2.3150, acc-0.1082, test loss-2.3160, acc-0.1002\n",
      "Iter-310, train loss-2.3246, acc-0.0800, valid loss-2.3149, acc-0.1082, test loss-2.3158, acc-0.1006\n",
      "Iter-320, train loss-2.3086, acc-0.1000, valid loss-2.3148, acc-0.1084, test loss-2.3157, acc-0.1007\n",
      "Iter-330, train loss-2.3302, acc-0.0800, valid loss-2.3146, acc-0.1082, test loss-2.3155, acc-0.1008\n",
      "Iter-340, train loss-2.3000, acc-0.1400, valid loss-2.3145, acc-0.1084, test loss-2.3154, acc-0.1012\n",
      "Iter-350, train loss-2.3133, acc-0.0600, valid loss-2.3143, acc-0.1084, test loss-2.3153, acc-0.1014\n",
      "Iter-360, train loss-2.3136, acc-0.1000, valid loss-2.3142, acc-0.1080, test loss-2.3151, acc-0.1013\n",
      "Iter-370, train loss-2.3010, acc-0.0600, valid loss-2.3141, acc-0.1084, test loss-2.3150, acc-0.1014\n",
      "Iter-380, train loss-2.3056, acc-0.1000, valid loss-2.3139, acc-0.1082, test loss-2.3148, acc-0.1015\n",
      "Iter-390, train loss-2.3180, acc-0.0800, valid loss-2.3138, acc-0.1082, test loss-2.3147, acc-0.1017\n",
      "Iter-400, train loss-2.3154, acc-0.0800, valid loss-2.3136, acc-0.1086, test loss-2.3146, acc-0.1017\n",
      "Iter-410, train loss-2.3165, acc-0.1200, valid loss-2.3135, acc-0.1092, test loss-2.3144, acc-0.1018\n",
      "Iter-420, train loss-2.3303, acc-0.0200, valid loss-2.3134, acc-0.1096, test loss-2.3143, acc-0.1021\n",
      "Iter-430, train loss-2.3424, acc-0.0600, valid loss-2.3132, acc-0.1098, test loss-2.3141, acc-0.1022\n",
      "Iter-440, train loss-2.3156, acc-0.1200, valid loss-2.3131, acc-0.1102, test loss-2.3140, acc-0.1025\n",
      "Iter-450, train loss-2.3035, acc-0.0800, valid loss-2.3129, acc-0.1106, test loss-2.3139, acc-0.1027\n",
      "Iter-460, train loss-2.2933, acc-0.1400, valid loss-2.3128, acc-0.1106, test loss-2.3137, acc-0.1029\n",
      "Iter-470, train loss-2.3088, acc-0.0800, valid loss-2.3127, acc-0.1106, test loss-2.3136, acc-0.1032\n",
      "Iter-480, train loss-2.3002, acc-0.1800, valid loss-2.3125, acc-0.1106, test loss-2.3134, acc-0.1033\n",
      "Iter-490, train loss-2.2852, acc-0.1400, valid loss-2.3124, acc-0.1110, test loss-2.3133, acc-0.1036\n",
      "Iter-500, train loss-2.3130, acc-0.1000, valid loss-2.3122, acc-0.1114, test loss-2.3132, acc-0.1038\n",
      "Iter-510, train loss-2.3124, acc-0.0600, valid loss-2.3121, acc-0.1118, test loss-2.3130, acc-0.1042\n",
      "Iter-520, train loss-2.3264, acc-0.0200, valid loss-2.3119, acc-0.1118, test loss-2.3129, acc-0.1047\n",
      "Iter-530, train loss-2.3308, acc-0.0400, valid loss-2.3118, acc-0.1118, test loss-2.3127, acc-0.1048\n",
      "Iter-540, train loss-2.2752, acc-0.2000, valid loss-2.3117, acc-0.1118, test loss-2.3126, acc-0.1050\n",
      "Iter-550, train loss-2.3098, acc-0.1000, valid loss-2.3115, acc-0.1120, test loss-2.3124, acc-0.1055\n",
      "Iter-560, train loss-2.3098, acc-0.0800, valid loss-2.3114, acc-0.1120, test loss-2.3123, acc-0.1057\n",
      "Iter-570, train loss-2.3088, acc-0.1000, valid loss-2.3112, acc-0.1120, test loss-2.3122, acc-0.1059\n",
      "Iter-580, train loss-2.2872, acc-0.2200, valid loss-2.3111, acc-0.1124, test loss-2.3120, acc-0.1064\n",
      "Iter-590, train loss-2.3083, acc-0.0800, valid loss-2.3110, acc-0.1126, test loss-2.3119, acc-0.1064\n",
      "Iter-600, train loss-2.3175, acc-0.1000, valid loss-2.3109, acc-0.1126, test loss-2.3118, acc-0.1063\n",
      "Iter-610, train loss-2.3211, acc-0.0600, valid loss-2.3107, acc-0.1126, test loss-2.3116, acc-0.1064\n",
      "Iter-620, train loss-2.3166, acc-0.0800, valid loss-2.3106, acc-0.1128, test loss-2.3115, acc-0.1067\n",
      "Iter-630, train loss-2.3104, acc-0.1200, valid loss-2.3104, acc-0.1136, test loss-2.3114, acc-0.1072\n",
      "Iter-640, train loss-2.2848, acc-0.1800, valid loss-2.3103, acc-0.1138, test loss-2.3112, acc-0.1075\n",
      "Iter-650, train loss-2.3043, acc-0.1600, valid loss-2.3101, acc-0.1138, test loss-2.3111, acc-0.1077\n",
      "Iter-660, train loss-2.2987, acc-0.1600, valid loss-2.3100, acc-0.1142, test loss-2.3109, acc-0.1079\n",
      "Iter-670, train loss-2.3234, acc-0.0600, valid loss-2.3099, acc-0.1144, test loss-2.3108, acc-0.1085\n",
      "Iter-680, train loss-2.3257, acc-0.0600, valid loss-2.3097, acc-0.1148, test loss-2.3106, acc-0.1089\n",
      "Iter-690, train loss-2.3154, acc-0.0400, valid loss-2.3096, acc-0.1152, test loss-2.3105, acc-0.1088\n",
      "Iter-700, train loss-2.2867, acc-0.1800, valid loss-2.3094, acc-0.1148, test loss-2.3104, acc-0.1091\n",
      "Iter-710, train loss-2.3062, acc-0.1200, valid loss-2.3093, acc-0.1150, test loss-2.3102, acc-0.1096\n",
      "Iter-720, train loss-2.3376, acc-0.0800, valid loss-2.3092, acc-0.1152, test loss-2.3101, acc-0.1095\n",
      "Iter-730, train loss-2.3078, acc-0.0600, valid loss-2.3090, acc-0.1156, test loss-2.3099, acc-0.1099\n",
      "Iter-740, train loss-2.2891, acc-0.2600, valid loss-2.3089, acc-0.1160, test loss-2.3098, acc-0.1100\n",
      "Iter-750, train loss-2.3344, acc-0.0800, valid loss-2.3087, acc-0.1160, test loss-2.3096, acc-0.1100\n",
      "Iter-760, train loss-2.3107, acc-0.1200, valid loss-2.3086, acc-0.1160, test loss-2.3095, acc-0.1101\n",
      "Iter-770, train loss-2.3279, acc-0.1000, valid loss-2.3085, acc-0.1164, test loss-2.3094, acc-0.1102\n",
      "Iter-780, train loss-2.3214, acc-0.0600, valid loss-2.3083, acc-0.1166, test loss-2.3092, acc-0.1103\n",
      "Iter-790, train loss-2.3312, acc-0.0800, valid loss-2.3082, acc-0.1168, test loss-2.3091, acc-0.1109\n",
      "Iter-800, train loss-2.3271, acc-0.0800, valid loss-2.3081, acc-0.1172, test loss-2.3090, acc-0.1111\n",
      "Iter-810, train loss-2.3079, acc-0.1200, valid loss-2.3079, acc-0.1176, test loss-2.3088, acc-0.1112\n",
      "Iter-820, train loss-2.3179, acc-0.0600, valid loss-2.3078, acc-0.1178, test loss-2.3087, acc-0.1113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.3097, acc-0.1000, valid loss-2.3076, acc-0.1178, test loss-2.3085, acc-0.1113\n",
      "Iter-840, train loss-2.3053, acc-0.1400, valid loss-2.3075, acc-0.1184, test loss-2.3084, acc-0.1118\n",
      "Iter-850, train loss-2.3083, acc-0.0600, valid loss-2.3073, acc-0.1184, test loss-2.3083, acc-0.1117\n",
      "Iter-860, train loss-2.3190, acc-0.1000, valid loss-2.3072, acc-0.1188, test loss-2.3081, acc-0.1119\n",
      "Iter-870, train loss-2.3070, acc-0.0800, valid loss-2.3071, acc-0.1190, test loss-2.3080, acc-0.1121\n",
      "Iter-880, train loss-2.3010, acc-0.0800, valid loss-2.3069, acc-0.1188, test loss-2.3078, acc-0.1122\n",
      "Iter-890, train loss-2.2944, acc-0.1000, valid loss-2.3068, acc-0.1192, test loss-2.3077, acc-0.1122\n",
      "Iter-900, train loss-2.3009, acc-0.1600, valid loss-2.3066, acc-0.1196, test loss-2.3075, acc-0.1124\n",
      "Iter-910, train loss-2.3344, acc-0.1000, valid loss-2.3065, acc-0.1202, test loss-2.3074, acc-0.1124\n",
      "Iter-920, train loss-2.2926, acc-0.1200, valid loss-2.3063, acc-0.1200, test loss-2.3073, acc-0.1124\n",
      "Iter-930, train loss-2.2958, acc-0.1600, valid loss-2.3062, acc-0.1200, test loss-2.3071, acc-0.1127\n",
      "Iter-940, train loss-2.3034, acc-0.1600, valid loss-2.3061, acc-0.1210, test loss-2.3070, acc-0.1130\n",
      "Iter-950, train loss-2.2937, acc-0.1000, valid loss-2.3059, acc-0.1212, test loss-2.3068, acc-0.1135\n",
      "Iter-960, train loss-2.2912, acc-0.2000, valid loss-2.3058, acc-0.1216, test loss-2.3067, acc-0.1137\n",
      "Iter-970, train loss-2.2894, acc-0.1600, valid loss-2.3056, acc-0.1214, test loss-2.3065, acc-0.1141\n",
      "Iter-980, train loss-2.3025, acc-0.1000, valid loss-2.3055, acc-0.1216, test loss-2.3064, acc-0.1146\n",
      "Iter-990, train loss-2.2935, acc-0.1000, valid loss-2.3053, acc-0.1216, test loss-2.3063, acc-0.1145\n",
      "Iter-1000, train loss-2.3347, acc-0.0400, valid loss-2.3052, acc-0.1222, test loss-2.3061, acc-0.1147\n",
      "Iter-1010, train loss-2.2848, acc-0.1600, valid loss-2.3051, acc-0.1224, test loss-2.3060, acc-0.1151\n",
      "Iter-1020, train loss-2.2927, acc-0.1600, valid loss-2.3049, acc-0.1228, test loss-2.3058, acc-0.1151\n",
      "Iter-1030, train loss-2.3042, acc-0.1000, valid loss-2.3048, acc-0.1228, test loss-2.3057, acc-0.1155\n",
      "Iter-1040, train loss-2.3136, acc-0.0800, valid loss-2.3046, acc-0.1228, test loss-2.3055, acc-0.1158\n",
      "Iter-1050, train loss-2.2857, acc-0.1800, valid loss-2.3045, acc-0.1228, test loss-2.3054, acc-0.1162\n",
      "Iter-1060, train loss-2.3042, acc-0.1200, valid loss-2.3044, acc-0.1230, test loss-2.3053, acc-0.1164\n",
      "Iter-1070, train loss-2.2942, acc-0.0800, valid loss-2.3042, acc-0.1234, test loss-2.3051, acc-0.1165\n",
      "Iter-1080, train loss-2.2734, acc-0.2400, valid loss-2.3040, acc-0.1236, test loss-2.3050, acc-0.1166\n",
      "Iter-1090, train loss-2.2992, acc-0.1400, valid loss-2.3039, acc-0.1242, test loss-2.3048, acc-0.1168\n",
      "Iter-1100, train loss-2.2998, acc-0.1400, valid loss-2.3038, acc-0.1242, test loss-2.3047, acc-0.1170\n",
      "Iter-1110, train loss-2.3121, acc-0.0800, valid loss-2.3036, acc-0.1244, test loss-2.3045, acc-0.1174\n",
      "Iter-1120, train loss-2.3125, acc-0.0800, valid loss-2.3035, acc-0.1248, test loss-2.3044, acc-0.1176\n",
      "Iter-1130, train loss-2.2955, acc-0.2200, valid loss-2.3033, acc-0.1252, test loss-2.3042, acc-0.1179\n",
      "Iter-1140, train loss-2.3062, acc-0.1000, valid loss-2.3032, acc-0.1250, test loss-2.3041, acc-0.1177\n",
      "Iter-1150, train loss-2.3099, acc-0.1200, valid loss-2.3031, acc-0.1254, test loss-2.3040, acc-0.1183\n",
      "Iter-1160, train loss-2.3109, acc-0.0600, valid loss-2.3029, acc-0.1256, test loss-2.3038, acc-0.1186\n",
      "Iter-1170, train loss-2.2797, acc-0.1200, valid loss-2.3028, acc-0.1262, test loss-2.3037, acc-0.1189\n",
      "Iter-1180, train loss-2.3078, acc-0.1600, valid loss-2.3026, acc-0.1264, test loss-2.3036, acc-0.1195\n",
      "Iter-1190, train loss-2.2849, acc-0.1000, valid loss-2.3025, acc-0.1264, test loss-2.3034, acc-0.1200\n",
      "Iter-1200, train loss-2.3184, acc-0.0400, valid loss-2.3024, acc-0.1272, test loss-2.3033, acc-0.1202\n",
      "Iter-1210, train loss-2.3158, acc-0.0800, valid loss-2.3022, acc-0.1276, test loss-2.3031, acc-0.1206\n",
      "Iter-1220, train loss-2.2928, acc-0.1400, valid loss-2.3021, acc-0.1282, test loss-2.3030, acc-0.1211\n",
      "Iter-1230, train loss-2.2991, acc-0.1400, valid loss-2.3019, acc-0.1282, test loss-2.3029, acc-0.1215\n",
      "Iter-1240, train loss-2.2908, acc-0.1800, valid loss-2.3018, acc-0.1286, test loss-2.3027, acc-0.1217\n",
      "Iter-1250, train loss-2.2726, acc-0.1200, valid loss-2.3017, acc-0.1292, test loss-2.3026, acc-0.1222\n",
      "Iter-1260, train loss-2.3067, acc-0.1000, valid loss-2.3015, acc-0.1290, test loss-2.3024, acc-0.1226\n",
      "Iter-1270, train loss-2.2918, acc-0.2400, valid loss-2.3014, acc-0.1294, test loss-2.3023, acc-0.1228\n",
      "Iter-1280, train loss-2.2986, acc-0.0400, valid loss-2.3013, acc-0.1300, test loss-2.3022, acc-0.1229\n",
      "Iter-1290, train loss-2.2934, acc-0.1600, valid loss-2.3011, acc-0.1300, test loss-2.3020, acc-0.1235\n",
      "Iter-1300, train loss-2.2987, acc-0.1200, valid loss-2.3010, acc-0.1304, test loss-2.3019, acc-0.1239\n",
      "Iter-1310, train loss-2.3227, acc-0.0800, valid loss-2.3009, acc-0.1306, test loss-2.3018, acc-0.1240\n",
      "Iter-1320, train loss-2.3221, acc-0.0800, valid loss-2.3007, acc-0.1308, test loss-2.3016, acc-0.1242\n",
      "Iter-1330, train loss-2.2955, acc-0.0600, valid loss-2.3006, acc-0.1308, test loss-2.3015, acc-0.1244\n",
      "Iter-1340, train loss-2.2929, acc-0.0800, valid loss-2.3004, acc-0.1312, test loss-2.3013, acc-0.1245\n",
      "Iter-1350, train loss-2.2827, acc-0.1600, valid loss-2.3003, acc-0.1314, test loss-2.3012, acc-0.1250\n",
      "Iter-1360, train loss-2.2873, acc-0.1600, valid loss-2.3002, acc-0.1318, test loss-2.3011, acc-0.1252\n",
      "Iter-1370, train loss-2.2927, acc-0.1200, valid loss-2.3000, acc-0.1320, test loss-2.3009, acc-0.1254\n",
      "Iter-1380, train loss-2.2807, acc-0.1600, valid loss-2.2999, acc-0.1324, test loss-2.3008, acc-0.1258\n",
      "Iter-1390, train loss-2.2848, acc-0.1200, valid loss-2.2997, acc-0.1324, test loss-2.3007, acc-0.1263\n",
      "Iter-1400, train loss-2.3125, acc-0.1000, valid loss-2.2996, acc-0.1326, test loss-2.3005, acc-0.1268\n",
      "Iter-1410, train loss-2.3072, acc-0.1000, valid loss-2.2995, acc-0.1330, test loss-2.3004, acc-0.1272\n",
      "Iter-1420, train loss-2.2969, acc-0.1600, valid loss-2.2993, acc-0.1334, test loss-2.3002, acc-0.1274\n",
      "Iter-1430, train loss-2.3173, acc-0.0600, valid loss-2.2992, acc-0.1334, test loss-2.3001, acc-0.1277\n",
      "Iter-1440, train loss-2.2847, acc-0.1400, valid loss-2.2990, acc-0.1340, test loss-2.2999, acc-0.1283\n",
      "Iter-1450, train loss-2.3038, acc-0.1200, valid loss-2.2989, acc-0.1340, test loss-2.2998, acc-0.1286\n",
      "Iter-1460, train loss-2.2820, acc-0.1200, valid loss-2.2988, acc-0.1340, test loss-2.2997, acc-0.1291\n",
      "Iter-1470, train loss-2.2902, acc-0.1600, valid loss-2.2986, acc-0.1348, test loss-2.2995, acc-0.1292\n",
      "Iter-1480, train loss-2.3004, acc-0.1800, valid loss-2.2985, acc-0.1350, test loss-2.2994, acc-0.1287\n",
      "Iter-1490, train loss-2.3359, acc-0.0200, valid loss-2.2983, acc-0.1350, test loss-2.2993, acc-0.1291\n",
      "Iter-1500, train loss-2.3001, acc-0.1200, valid loss-2.2982, acc-0.1352, test loss-2.2991, acc-0.1290\n",
      "Iter-1510, train loss-2.2922, acc-0.1400, valid loss-2.2981, acc-0.1358, test loss-2.2990, acc-0.1297\n",
      "Iter-1520, train loss-2.2962, acc-0.1400, valid loss-2.2979, acc-0.1362, test loss-2.2988, acc-0.1300\n",
      "Iter-1530, train loss-2.2841, acc-0.1400, valid loss-2.2978, acc-0.1362, test loss-2.2987, acc-0.1303\n",
      "Iter-1540, train loss-2.3130, acc-0.0800, valid loss-2.2976, acc-0.1366, test loss-2.2986, acc-0.1305\n",
      "Iter-1550, train loss-2.2723, acc-0.2200, valid loss-2.2975, acc-0.1372, test loss-2.2984, acc-0.1306\n",
      "Iter-1560, train loss-2.3107, acc-0.1200, valid loss-2.2974, acc-0.1376, test loss-2.2983, acc-0.1305\n",
      "Iter-1570, train loss-2.2871, acc-0.1400, valid loss-2.2972, acc-0.1384, test loss-2.2981, acc-0.1311\n",
      "Iter-1580, train loss-2.3007, acc-0.2000, valid loss-2.2971, acc-0.1384, test loss-2.2980, acc-0.1313\n",
      "Iter-1590, train loss-2.3288, acc-0.0800, valid loss-2.2969, acc-0.1382, test loss-2.2979, acc-0.1318\n",
      "Iter-1600, train loss-2.2965, acc-0.0800, valid loss-2.2968, acc-0.1384, test loss-2.2977, acc-0.1322\n",
      "Iter-1610, train loss-2.3034, acc-0.1600, valid loss-2.2967, acc-0.1388, test loss-2.2976, acc-0.1324\n",
      "Iter-1620, train loss-2.3064, acc-0.1000, valid loss-2.2965, acc-0.1398, test loss-2.2975, acc-0.1323\n",
      "Iter-1630, train loss-2.2986, acc-0.1200, valid loss-2.2964, acc-0.1398, test loss-2.2973, acc-0.1323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-2.3018, acc-0.1200, valid loss-2.2962, acc-0.1404, test loss-2.2972, acc-0.1326\n",
      "Iter-1650, train loss-2.3215, acc-0.0400, valid loss-2.2961, acc-0.1406, test loss-2.2970, acc-0.1332\n",
      "Iter-1660, train loss-2.2978, acc-0.1200, valid loss-2.2960, acc-0.1406, test loss-2.2969, acc-0.1330\n",
      "Iter-1670, train loss-2.2979, acc-0.1400, valid loss-2.2958, acc-0.1408, test loss-2.2967, acc-0.1333\n",
      "Iter-1680, train loss-2.2788, acc-0.1200, valid loss-2.2957, acc-0.1410, test loss-2.2966, acc-0.1334\n",
      "Iter-1690, train loss-2.3103, acc-0.0800, valid loss-2.2955, acc-0.1412, test loss-2.2965, acc-0.1335\n",
      "Iter-1700, train loss-2.2888, acc-0.1000, valid loss-2.2954, acc-0.1416, test loss-2.2963, acc-0.1338\n",
      "Iter-1710, train loss-2.3112, acc-0.1000, valid loss-2.2953, acc-0.1414, test loss-2.2962, acc-0.1339\n",
      "Iter-1720, train loss-2.2933, acc-0.1400, valid loss-2.2951, acc-0.1416, test loss-2.2960, acc-0.1346\n",
      "Iter-1730, train loss-2.3068, acc-0.1000, valid loss-2.2950, acc-0.1422, test loss-2.2959, acc-0.1349\n",
      "Iter-1740, train loss-2.2976, acc-0.1200, valid loss-2.2948, acc-0.1426, test loss-2.2957, acc-0.1353\n",
      "Iter-1750, train loss-2.2804, acc-0.1600, valid loss-2.2947, acc-0.1428, test loss-2.2956, acc-0.1355\n",
      "Iter-1760, train loss-2.3089, acc-0.0600, valid loss-2.2946, acc-0.1434, test loss-2.2955, acc-0.1359\n",
      "Iter-1770, train loss-2.2834, acc-0.1800, valid loss-2.2944, acc-0.1436, test loss-2.2953, acc-0.1361\n",
      "Iter-1780, train loss-2.2974, acc-0.0800, valid loss-2.2943, acc-0.1442, test loss-2.2952, acc-0.1362\n",
      "Iter-1790, train loss-2.2827, acc-0.2000, valid loss-2.2941, acc-0.1448, test loss-2.2951, acc-0.1367\n",
      "Iter-1800, train loss-2.2855, acc-0.1000, valid loss-2.2940, acc-0.1448, test loss-2.2949, acc-0.1372\n",
      "Iter-1810, train loss-2.3002, acc-0.1000, valid loss-2.2938, acc-0.1448, test loss-2.2948, acc-0.1374\n",
      "Iter-1820, train loss-2.2842, acc-0.1000, valid loss-2.2937, acc-0.1452, test loss-2.2946, acc-0.1380\n",
      "Iter-1830, train loss-2.2843, acc-0.1600, valid loss-2.2936, acc-0.1454, test loss-2.2945, acc-0.1380\n",
      "Iter-1840, train loss-2.2822, acc-0.1400, valid loss-2.2934, acc-0.1452, test loss-2.2944, acc-0.1384\n",
      "Iter-1850, train loss-2.3122, acc-0.1400, valid loss-2.2933, acc-0.1460, test loss-2.2942, acc-0.1389\n",
      "Iter-1860, train loss-2.2842, acc-0.1600, valid loss-2.2932, acc-0.1468, test loss-2.2941, acc-0.1388\n",
      "Iter-1870, train loss-2.2824, acc-0.1400, valid loss-2.2930, acc-0.1472, test loss-2.2940, acc-0.1397\n",
      "Iter-1880, train loss-2.2951, acc-0.1000, valid loss-2.2929, acc-0.1474, test loss-2.2938, acc-0.1401\n",
      "Iter-1890, train loss-2.3060, acc-0.1600, valid loss-2.2927, acc-0.1474, test loss-2.2937, acc-0.1405\n",
      "Iter-1900, train loss-2.3121, acc-0.0600, valid loss-2.2926, acc-0.1480, test loss-2.2935, acc-0.1411\n",
      "Iter-1910, train loss-2.2912, acc-0.0800, valid loss-2.2925, acc-0.1480, test loss-2.2934, acc-0.1414\n",
      "Iter-1920, train loss-2.2864, acc-0.2200, valid loss-2.2923, acc-0.1482, test loss-2.2933, acc-0.1413\n",
      "Iter-1930, train loss-2.3043, acc-0.1600, valid loss-2.2922, acc-0.1484, test loss-2.2932, acc-0.1420\n",
      "Iter-1940, train loss-2.2897, acc-0.1600, valid loss-2.2921, acc-0.1488, test loss-2.2930, acc-0.1424\n",
      "Iter-1950, train loss-2.2842, acc-0.1400, valid loss-2.2919, acc-0.1492, test loss-2.2929, acc-0.1423\n",
      "Iter-1960, train loss-2.2750, acc-0.2400, valid loss-2.2918, acc-0.1494, test loss-2.2927, acc-0.1427\n",
      "Iter-1970, train loss-2.3068, acc-0.0800, valid loss-2.2917, acc-0.1494, test loss-2.2926, acc-0.1432\n",
      "Iter-1980, train loss-2.3044, acc-0.1200, valid loss-2.2916, acc-0.1496, test loss-2.2925, acc-0.1438\n",
      "Iter-1990, train loss-2.3048, acc-0.1200, valid loss-2.2914, acc-0.1496, test loss-2.2924, acc-0.1439\n",
      "Iter-2000, train loss-2.3010, acc-0.1600, valid loss-2.2913, acc-0.1498, test loss-2.2922, acc-0.1441\n",
      "Iter-2010, train loss-2.3038, acc-0.0800, valid loss-2.2911, acc-0.1496, test loss-2.2921, acc-0.1445\n",
      "Iter-2020, train loss-2.3003, acc-0.0600, valid loss-2.2910, acc-0.1496, test loss-2.2920, acc-0.1448\n",
      "Iter-2030, train loss-2.2723, acc-0.1400, valid loss-2.2909, acc-0.1496, test loss-2.2918, acc-0.1449\n",
      "Iter-2040, train loss-2.3039, acc-0.1600, valid loss-2.2908, acc-0.1496, test loss-2.2917, acc-0.1448\n",
      "Iter-2050, train loss-2.2747, acc-0.2200, valid loss-2.2906, acc-0.1494, test loss-2.2915, acc-0.1449\n",
      "Iter-2060, train loss-2.3135, acc-0.0600, valid loss-2.2905, acc-0.1498, test loss-2.2914, acc-0.1456\n",
      "Iter-2070, train loss-2.2960, acc-0.1200, valid loss-2.2903, acc-0.1496, test loss-2.2913, acc-0.1456\n",
      "Iter-2080, train loss-2.2848, acc-0.2400, valid loss-2.2902, acc-0.1500, test loss-2.2911, acc-0.1463\n",
      "Iter-2090, train loss-2.3201, acc-0.1200, valid loss-2.2901, acc-0.1504, test loss-2.2910, acc-0.1464\n",
      "Iter-2100, train loss-2.2843, acc-0.2200, valid loss-2.2899, acc-0.1504, test loss-2.2909, acc-0.1465\n",
      "Iter-2110, train loss-2.2818, acc-0.1600, valid loss-2.2898, acc-0.1504, test loss-2.2907, acc-0.1468\n",
      "Iter-2120, train loss-2.2893, acc-0.1000, valid loss-2.2897, acc-0.1504, test loss-2.2906, acc-0.1468\n",
      "Iter-2130, train loss-2.2696, acc-0.1400, valid loss-2.2895, acc-0.1504, test loss-2.2905, acc-0.1469\n",
      "Iter-2140, train loss-2.2938, acc-0.1200, valid loss-2.2894, acc-0.1504, test loss-2.2903, acc-0.1468\n",
      "Iter-2150, train loss-2.2858, acc-0.1600, valid loss-2.2893, acc-0.1504, test loss-2.2902, acc-0.1468\n",
      "Iter-2160, train loss-2.2881, acc-0.2200, valid loss-2.2891, acc-0.1504, test loss-2.2900, acc-0.1471\n",
      "Iter-2170, train loss-2.2865, acc-0.0800, valid loss-2.2890, acc-0.1508, test loss-2.2899, acc-0.1473\n",
      "Iter-2180, train loss-2.2912, acc-0.0800, valid loss-2.2888, acc-0.1506, test loss-2.2898, acc-0.1475\n",
      "Iter-2190, train loss-2.2793, acc-0.2000, valid loss-2.2887, acc-0.1508, test loss-2.2896, acc-0.1478\n",
      "Iter-2200, train loss-2.2797, acc-0.1800, valid loss-2.2886, acc-0.1514, test loss-2.2895, acc-0.1480\n",
      "Iter-2210, train loss-2.3021, acc-0.1600, valid loss-2.2884, acc-0.1510, test loss-2.2893, acc-0.1483\n",
      "Iter-2220, train loss-2.3207, acc-0.0200, valid loss-2.2883, acc-0.1514, test loss-2.2892, acc-0.1491\n",
      "Iter-2230, train loss-2.2709, acc-0.2400, valid loss-2.2882, acc-0.1516, test loss-2.2891, acc-0.1494\n",
      "Iter-2240, train loss-2.2850, acc-0.1400, valid loss-2.2880, acc-0.1522, test loss-2.2889, acc-0.1493\n",
      "Iter-2250, train loss-2.3011, acc-0.1000, valid loss-2.2879, acc-0.1522, test loss-2.2888, acc-0.1494\n",
      "Iter-2260, train loss-2.2909, acc-0.1200, valid loss-2.2878, acc-0.1524, test loss-2.2887, acc-0.1500\n",
      "Iter-2270, train loss-2.2872, acc-0.2000, valid loss-2.2876, acc-0.1524, test loss-2.2885, acc-0.1501\n",
      "Iter-2280, train loss-2.2702, acc-0.1800, valid loss-2.2875, acc-0.1530, test loss-2.2884, acc-0.1506\n",
      "Iter-2290, train loss-2.2782, acc-0.1800, valid loss-2.2874, acc-0.1532, test loss-2.2883, acc-0.1510\n",
      "Iter-2300, train loss-2.2849, acc-0.2000, valid loss-2.2872, acc-0.1536, test loss-2.2881, acc-0.1513\n",
      "Iter-2310, train loss-2.2940, acc-0.1400, valid loss-2.2871, acc-0.1542, test loss-2.2880, acc-0.1515\n",
      "Iter-2320, train loss-2.2768, acc-0.1600, valid loss-2.2869, acc-0.1544, test loss-2.2879, acc-0.1519\n",
      "Iter-2330, train loss-2.2821, acc-0.1600, valid loss-2.2868, acc-0.1550, test loss-2.2877, acc-0.1528\n",
      "Iter-2340, train loss-2.2735, acc-0.0800, valid loss-2.2867, acc-0.1550, test loss-2.2876, acc-0.1526\n",
      "Iter-2350, train loss-2.2748, acc-0.1600, valid loss-2.2865, acc-0.1552, test loss-2.2874, acc-0.1527\n",
      "Iter-2360, train loss-2.3001, acc-0.1000, valid loss-2.2864, acc-0.1550, test loss-2.2873, acc-0.1534\n",
      "Iter-2370, train loss-2.2949, acc-0.1600, valid loss-2.2863, acc-0.1560, test loss-2.2872, acc-0.1534\n",
      "Iter-2380, train loss-2.2891, acc-0.1400, valid loss-2.2861, acc-0.1564, test loss-2.2870, acc-0.1539\n",
      "Iter-2390, train loss-2.2902, acc-0.2000, valid loss-2.2860, acc-0.1564, test loss-2.2869, acc-0.1538\n",
      "Iter-2400, train loss-2.2602, acc-0.2000, valid loss-2.2859, acc-0.1564, test loss-2.2868, acc-0.1542\n",
      "Iter-2410, train loss-2.2838, acc-0.1800, valid loss-2.2857, acc-0.1570, test loss-2.2866, acc-0.1545\n",
      "Iter-2420, train loss-2.2870, acc-0.1400, valid loss-2.2856, acc-0.1574, test loss-2.2865, acc-0.1547\n",
      "Iter-2430, train loss-2.2852, acc-0.1600, valid loss-2.2854, acc-0.1578, test loss-2.2864, acc-0.1549\n",
      "Iter-2440, train loss-2.2876, acc-0.1800, valid loss-2.2853, acc-0.1582, test loss-2.2862, acc-0.1551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-2.3015, acc-0.1000, valid loss-2.2852, acc-0.1582, test loss-2.2861, acc-0.1556\n",
      "Iter-2460, train loss-2.2904, acc-0.1600, valid loss-2.2850, acc-0.1586, test loss-2.2860, acc-0.1559\n",
      "Iter-2470, train loss-2.3017, acc-0.0600, valid loss-2.2849, acc-0.1590, test loss-2.2858, acc-0.1560\n",
      "Iter-2480, train loss-2.3011, acc-0.1200, valid loss-2.2848, acc-0.1590, test loss-2.2857, acc-0.1565\n",
      "Iter-2490, train loss-2.2801, acc-0.1600, valid loss-2.2846, acc-0.1594, test loss-2.2856, acc-0.1563\n",
      "Iter-2500, train loss-2.2655, acc-0.1600, valid loss-2.2845, acc-0.1592, test loss-2.2854, acc-0.1568\n",
      "Iter-2510, train loss-2.2815, acc-0.1800, valid loss-2.2844, acc-0.1600, test loss-2.2853, acc-0.1574\n",
      "Iter-2520, train loss-2.2906, acc-0.1400, valid loss-2.2842, acc-0.1602, test loss-2.2852, acc-0.1573\n",
      "Iter-2530, train loss-2.3126, acc-0.1000, valid loss-2.2841, acc-0.1612, test loss-2.2850, acc-0.1576\n",
      "Iter-2540, train loss-2.2872, acc-0.2000, valid loss-2.2840, acc-0.1616, test loss-2.2849, acc-0.1581\n",
      "Iter-2550, train loss-2.2954, acc-0.0800, valid loss-2.2838, acc-0.1616, test loss-2.2848, acc-0.1585\n",
      "Iter-2560, train loss-2.2884, acc-0.1400, valid loss-2.2837, acc-0.1622, test loss-2.2846, acc-0.1588\n",
      "Iter-2570, train loss-2.2600, acc-0.2400, valid loss-2.2835, acc-0.1622, test loss-2.2845, acc-0.1589\n",
      "Iter-2580, train loss-2.2774, acc-0.1800, valid loss-2.2834, acc-0.1626, test loss-2.2843, acc-0.1591\n",
      "Iter-2590, train loss-2.2860, acc-0.1600, valid loss-2.2833, acc-0.1636, test loss-2.2842, acc-0.1591\n",
      "Iter-2600, train loss-2.2735, acc-0.1600, valid loss-2.2831, acc-0.1638, test loss-2.2841, acc-0.1595\n",
      "Iter-2610, train loss-2.2766, acc-0.2000, valid loss-2.2830, acc-0.1636, test loss-2.2839, acc-0.1596\n",
      "Iter-2620, train loss-2.2865, acc-0.1600, valid loss-2.2829, acc-0.1642, test loss-2.2838, acc-0.1599\n",
      "Iter-2630, train loss-2.2907, acc-0.1400, valid loss-2.2827, acc-0.1644, test loss-2.2837, acc-0.1601\n",
      "Iter-2640, train loss-2.2778, acc-0.1600, valid loss-2.2826, acc-0.1650, test loss-2.2835, acc-0.1602\n",
      "Iter-2650, train loss-2.3027, acc-0.1400, valid loss-2.2825, acc-0.1652, test loss-2.2834, acc-0.1601\n",
      "Iter-2660, train loss-2.2954, acc-0.1400, valid loss-2.2824, acc-0.1658, test loss-2.2833, acc-0.1611\n",
      "Iter-2670, train loss-2.2647, acc-0.1800, valid loss-2.2822, acc-0.1664, test loss-2.2831, acc-0.1612\n",
      "Iter-2680, train loss-2.3021, acc-0.0600, valid loss-2.2821, acc-0.1668, test loss-2.2830, acc-0.1619\n",
      "Iter-2690, train loss-2.2905, acc-0.1600, valid loss-2.2820, acc-0.1668, test loss-2.2829, acc-0.1619\n",
      "Iter-2700, train loss-2.2639, acc-0.2000, valid loss-2.2818, acc-0.1668, test loss-2.2827, acc-0.1625\n",
      "Iter-2710, train loss-2.2714, acc-0.2000, valid loss-2.2817, acc-0.1668, test loss-2.2826, acc-0.1622\n",
      "Iter-2720, train loss-2.2798, acc-0.1000, valid loss-2.2816, acc-0.1672, test loss-2.2825, acc-0.1630\n",
      "Iter-2730, train loss-2.2843, acc-0.1600, valid loss-2.2814, acc-0.1688, test loss-2.2823, acc-0.1639\n",
      "Iter-2740, train loss-2.2787, acc-0.1200, valid loss-2.2813, acc-0.1688, test loss-2.2822, acc-0.1641\n",
      "Iter-2750, train loss-2.2634, acc-0.1200, valid loss-2.2812, acc-0.1694, test loss-2.2821, acc-0.1652\n",
      "Iter-2760, train loss-2.2771, acc-0.1200, valid loss-2.2810, acc-0.1696, test loss-2.2819, acc-0.1653\n",
      "Iter-2770, train loss-2.2987, acc-0.1400, valid loss-2.2809, acc-0.1702, test loss-2.2818, acc-0.1656\n",
      "Iter-2780, train loss-2.2893, acc-0.1800, valid loss-2.2808, acc-0.1704, test loss-2.2817, acc-0.1662\n",
      "Iter-2790, train loss-2.2827, acc-0.2000, valid loss-2.2806, acc-0.1700, test loss-2.2816, acc-0.1663\n",
      "Iter-2800, train loss-2.2841, acc-0.1600, valid loss-2.2805, acc-0.1704, test loss-2.2814, acc-0.1664\n",
      "Iter-2810, train loss-2.2896, acc-0.1400, valid loss-2.2804, acc-0.1708, test loss-2.2813, acc-0.1670\n",
      "Iter-2820, train loss-2.2869, acc-0.1000, valid loss-2.2803, acc-0.1712, test loss-2.2812, acc-0.1677\n",
      "Iter-2830, train loss-2.2966, acc-0.1600, valid loss-2.2801, acc-0.1718, test loss-2.2810, acc-0.1679\n",
      "Iter-2840, train loss-2.2741, acc-0.2200, valid loss-2.2800, acc-0.1720, test loss-2.2809, acc-0.1687\n",
      "Iter-2850, train loss-2.2788, acc-0.1800, valid loss-2.2799, acc-0.1722, test loss-2.2808, acc-0.1689\n",
      "Iter-2860, train loss-2.3054, acc-0.1000, valid loss-2.2797, acc-0.1724, test loss-2.2806, acc-0.1691\n",
      "Iter-2870, train loss-2.2658, acc-0.2000, valid loss-2.2796, acc-0.1724, test loss-2.2805, acc-0.1696\n",
      "Iter-2880, train loss-2.2917, acc-0.1800, valid loss-2.2795, acc-0.1728, test loss-2.2804, acc-0.1700\n",
      "Iter-2890, train loss-2.2715, acc-0.2400, valid loss-2.2793, acc-0.1730, test loss-2.2802, acc-0.1699\n",
      "Iter-2900, train loss-2.2762, acc-0.1200, valid loss-2.2792, acc-0.1732, test loss-2.2801, acc-0.1702\n",
      "Iter-2910, train loss-2.2874, acc-0.1200, valid loss-2.2791, acc-0.1732, test loss-2.2799, acc-0.1704\n",
      "Iter-2920, train loss-2.2600, acc-0.1800, valid loss-2.2789, acc-0.1736, test loss-2.2798, acc-0.1708\n",
      "Iter-2930, train loss-2.2722, acc-0.1600, valid loss-2.2788, acc-0.1742, test loss-2.2797, acc-0.1712\n",
      "Iter-2940, train loss-2.2742, acc-0.1200, valid loss-2.2787, acc-0.1744, test loss-2.2795, acc-0.1719\n",
      "Iter-2950, train loss-2.2709, acc-0.2400, valid loss-2.2785, acc-0.1746, test loss-2.2794, acc-0.1722\n",
      "Iter-2960, train loss-2.2821, acc-0.1800, valid loss-2.2784, acc-0.1750, test loss-2.2793, acc-0.1724\n",
      "Iter-2970, train loss-2.2954, acc-0.0800, valid loss-2.2783, acc-0.1758, test loss-2.2792, acc-0.1727\n",
      "Iter-2980, train loss-2.2907, acc-0.1600, valid loss-2.2781, acc-0.1762, test loss-2.2790, acc-0.1732\n",
      "Iter-2990, train loss-2.2722, acc-0.2200, valid loss-2.2780, acc-0.1764, test loss-2.2789, acc-0.1735\n",
      "Iter-3000, train loss-2.3042, acc-0.1600, valid loss-2.2779, acc-0.1764, test loss-2.2788, acc-0.1737\n",
      "Iter-3010, train loss-2.2569, acc-0.2200, valid loss-2.2777, acc-0.1764, test loss-2.2786, acc-0.1739\n",
      "Iter-3020, train loss-2.2619, acc-0.1600, valid loss-2.2776, acc-0.1768, test loss-2.2785, acc-0.1745\n",
      "Iter-3030, train loss-2.2771, acc-0.2200, valid loss-2.2775, acc-0.1770, test loss-2.2784, acc-0.1749\n",
      "Iter-3040, train loss-2.2909, acc-0.1800, valid loss-2.2773, acc-0.1776, test loss-2.2782, acc-0.1755\n",
      "Iter-3050, train loss-2.2647, acc-0.3000, valid loss-2.2772, acc-0.1778, test loss-2.2781, acc-0.1758\n",
      "Iter-3060, train loss-2.2805, acc-0.1200, valid loss-2.2771, acc-0.1784, test loss-2.2780, acc-0.1763\n",
      "Iter-3070, train loss-2.2621, acc-0.1400, valid loss-2.2769, acc-0.1786, test loss-2.2778, acc-0.1763\n",
      "Iter-3080, train loss-2.2821, acc-0.1000, valid loss-2.2768, acc-0.1796, test loss-2.2777, acc-0.1769\n",
      "Iter-3090, train loss-2.2605, acc-0.2000, valid loss-2.2767, acc-0.1800, test loss-2.2776, acc-0.1771\n",
      "Iter-3100, train loss-2.2565, acc-0.2200, valid loss-2.2765, acc-0.1802, test loss-2.2774, acc-0.1774\n",
      "Iter-3110, train loss-2.2793, acc-0.1600, valid loss-2.2764, acc-0.1806, test loss-2.2773, acc-0.1775\n",
      "Iter-3120, train loss-2.2575, acc-0.1400, valid loss-2.2763, acc-0.1808, test loss-2.2772, acc-0.1778\n",
      "Iter-3130, train loss-2.2640, acc-0.1800, valid loss-2.2762, acc-0.1810, test loss-2.2771, acc-0.1780\n",
      "Iter-3140, train loss-2.2895, acc-0.1200, valid loss-2.2760, acc-0.1812, test loss-2.2769, acc-0.1788\n",
      "Iter-3150, train loss-2.2956, acc-0.0400, valid loss-2.2759, acc-0.1812, test loss-2.2768, acc-0.1790\n",
      "Iter-3160, train loss-2.2599, acc-0.2000, valid loss-2.2758, acc-0.1812, test loss-2.2767, acc-0.1795\n",
      "Iter-3170, train loss-2.2902, acc-0.1400, valid loss-2.2756, acc-0.1818, test loss-2.2765, acc-0.1798\n",
      "Iter-3180, train loss-2.2357, acc-0.1600, valid loss-2.2755, acc-0.1822, test loss-2.2764, acc-0.1801\n",
      "Iter-3190, train loss-2.2924, acc-0.2000, valid loss-2.2754, acc-0.1820, test loss-2.2762, acc-0.1801\n",
      "Iter-3200, train loss-2.2794, acc-0.0800, valid loss-2.2752, acc-0.1820, test loss-2.2761, acc-0.1802\n",
      "Iter-3210, train loss-2.2577, acc-0.2400, valid loss-2.2751, acc-0.1822, test loss-2.2760, acc-0.1803\n",
      "Iter-3220, train loss-2.2986, acc-0.1400, valid loss-2.2750, acc-0.1822, test loss-2.2759, acc-0.1805\n",
      "Iter-3230, train loss-2.2695, acc-0.1200, valid loss-2.2748, acc-0.1820, test loss-2.2757, acc-0.1812\n",
      "Iter-3240, train loss-2.2779, acc-0.2200, valid loss-2.2747, acc-0.1834, test loss-2.2756, acc-0.1818\n",
      "Iter-3250, train loss-2.2828, acc-0.1000, valid loss-2.2746, acc-0.1846, test loss-2.2754, acc-0.1820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-2.2675, acc-0.1800, valid loss-2.2744, acc-0.1844, test loss-2.2753, acc-0.1822\n",
      "Iter-3270, train loss-2.2804, acc-0.1800, valid loss-2.2743, acc-0.1846, test loss-2.2752, acc-0.1823\n",
      "Iter-3280, train loss-2.2719, acc-0.1600, valid loss-2.2742, acc-0.1846, test loss-2.2750, acc-0.1827\n",
      "Iter-3290, train loss-2.2569, acc-0.1800, valid loss-2.2740, acc-0.1850, test loss-2.2749, acc-0.1832\n",
      "Iter-3300, train loss-2.2748, acc-0.1800, valid loss-2.2739, acc-0.1854, test loss-2.2748, acc-0.1836\n",
      "Iter-3310, train loss-2.2696, acc-0.1600, valid loss-2.2738, acc-0.1852, test loss-2.2747, acc-0.1840\n",
      "Iter-3320, train loss-2.2671, acc-0.1600, valid loss-2.2737, acc-0.1858, test loss-2.2745, acc-0.1839\n",
      "Iter-3330, train loss-2.2549, acc-0.2000, valid loss-2.2735, acc-0.1860, test loss-2.2744, acc-0.1844\n",
      "Iter-3340, train loss-2.2644, acc-0.2200, valid loss-2.2734, acc-0.1862, test loss-2.2742, acc-0.1848\n",
      "Iter-3350, train loss-2.2793, acc-0.1800, valid loss-2.2733, acc-0.1864, test loss-2.2741, acc-0.1848\n",
      "Iter-3360, train loss-2.2673, acc-0.2400, valid loss-2.2731, acc-0.1870, test loss-2.2740, acc-0.1850\n",
      "Iter-3370, train loss-2.2715, acc-0.2400, valid loss-2.2730, acc-0.1874, test loss-2.2739, acc-0.1852\n",
      "Iter-3380, train loss-2.2644, acc-0.2800, valid loss-2.2729, acc-0.1880, test loss-2.2737, acc-0.1855\n",
      "Iter-3390, train loss-2.2733, acc-0.1800, valid loss-2.2727, acc-0.1888, test loss-2.2736, acc-0.1855\n",
      "Iter-3400, train loss-2.2882, acc-0.1400, valid loss-2.2726, acc-0.1892, test loss-2.2735, acc-0.1858\n",
      "Iter-3410, train loss-2.2748, acc-0.1600, valid loss-2.2725, acc-0.1896, test loss-2.2733, acc-0.1865\n",
      "Iter-3420, train loss-2.2926, acc-0.1800, valid loss-2.2723, acc-0.1898, test loss-2.2732, acc-0.1871\n",
      "Iter-3430, train loss-2.2607, acc-0.1800, valid loss-2.2722, acc-0.1902, test loss-2.2731, acc-0.1875\n",
      "Iter-3440, train loss-2.2780, acc-0.2200, valid loss-2.2721, acc-0.1910, test loss-2.2729, acc-0.1878\n",
      "Iter-3450, train loss-2.2532, acc-0.2400, valid loss-2.2719, acc-0.1914, test loss-2.2728, acc-0.1883\n",
      "Iter-3460, train loss-2.2581, acc-0.1800, valid loss-2.2718, acc-0.1914, test loss-2.2727, acc-0.1884\n",
      "Iter-3470, train loss-2.2644, acc-0.2800, valid loss-2.2717, acc-0.1920, test loss-2.2725, acc-0.1886\n",
      "Iter-3480, train loss-2.2826, acc-0.1600, valid loss-2.2715, acc-0.1922, test loss-2.2724, acc-0.1884\n",
      "Iter-3490, train loss-2.2663, acc-0.1800, valid loss-2.2714, acc-0.1924, test loss-2.2723, acc-0.1886\n",
      "Iter-3500, train loss-2.3020, acc-0.0400, valid loss-2.2713, acc-0.1928, test loss-2.2722, acc-0.1886\n",
      "Iter-3510, train loss-2.2781, acc-0.1800, valid loss-2.2712, acc-0.1930, test loss-2.2720, acc-0.1888\n",
      "Iter-3520, train loss-2.2675, acc-0.2400, valid loss-2.2710, acc-0.1934, test loss-2.2719, acc-0.1894\n",
      "Iter-3530, train loss-2.2638, acc-0.2200, valid loss-2.2709, acc-0.1936, test loss-2.2718, acc-0.1897\n",
      "Iter-3540, train loss-2.2658, acc-0.2400, valid loss-2.2708, acc-0.1938, test loss-2.2716, acc-0.1896\n",
      "Iter-3550, train loss-2.2818, acc-0.1600, valid loss-2.2706, acc-0.1940, test loss-2.2715, acc-0.1900\n",
      "Iter-3560, train loss-2.2651, acc-0.2400, valid loss-2.2705, acc-0.1942, test loss-2.2714, acc-0.1899\n",
      "Iter-3570, train loss-2.2897, acc-0.1000, valid loss-2.2704, acc-0.1948, test loss-2.2712, acc-0.1904\n",
      "Iter-3580, train loss-2.2636, acc-0.2000, valid loss-2.2702, acc-0.1946, test loss-2.2711, acc-0.1905\n",
      "Iter-3590, train loss-2.2653, acc-0.2400, valid loss-2.2701, acc-0.1950, test loss-2.2710, acc-0.1908\n",
      "Iter-3600, train loss-2.2757, acc-0.1400, valid loss-2.2700, acc-0.1954, test loss-2.2709, acc-0.1913\n",
      "Iter-3610, train loss-2.2447, acc-0.2400, valid loss-2.2699, acc-0.1954, test loss-2.2707, acc-0.1917\n",
      "Iter-3620, train loss-2.3053, acc-0.1400, valid loss-2.2697, acc-0.1960, test loss-2.2706, acc-0.1921\n",
      "Iter-3630, train loss-2.2610, acc-0.1200, valid loss-2.2696, acc-0.1964, test loss-2.2705, acc-0.1926\n",
      "Iter-3640, train loss-2.2708, acc-0.2600, valid loss-2.2695, acc-0.1970, test loss-2.2703, acc-0.1931\n",
      "Iter-3650, train loss-2.2589, acc-0.2200, valid loss-2.2693, acc-0.1970, test loss-2.2702, acc-0.1939\n",
      "Iter-3660, train loss-2.2757, acc-0.1200, valid loss-2.2692, acc-0.1976, test loss-2.2701, acc-0.1940\n",
      "Iter-3670, train loss-2.2680, acc-0.0800, valid loss-2.2691, acc-0.1978, test loss-2.2700, acc-0.1940\n",
      "Iter-3680, train loss-2.2749, acc-0.1800, valid loss-2.2689, acc-0.1982, test loss-2.2698, acc-0.1943\n",
      "Iter-3690, train loss-2.2775, acc-0.1600, valid loss-2.2688, acc-0.1980, test loss-2.2697, acc-0.1945\n",
      "Iter-3700, train loss-2.2736, acc-0.0800, valid loss-2.2687, acc-0.1982, test loss-2.2695, acc-0.1945\n",
      "Iter-3710, train loss-2.2686, acc-0.2200, valid loss-2.2685, acc-0.1988, test loss-2.2694, acc-0.1952\n",
      "Iter-3720, train loss-2.2756, acc-0.1800, valid loss-2.2684, acc-0.1988, test loss-2.2693, acc-0.1956\n",
      "Iter-3730, train loss-2.2902, acc-0.1400, valid loss-2.2683, acc-0.1990, test loss-2.2691, acc-0.1965\n",
      "Iter-3740, train loss-2.2640, acc-0.2200, valid loss-2.2681, acc-0.1994, test loss-2.2690, acc-0.1966\n",
      "Iter-3750, train loss-2.2620, acc-0.2400, valid loss-2.2680, acc-0.1998, test loss-2.2689, acc-0.1965\n",
      "Iter-3760, train loss-2.2546, acc-0.2200, valid loss-2.2679, acc-0.2008, test loss-2.2687, acc-0.1976\n",
      "Iter-3770, train loss-2.2756, acc-0.1200, valid loss-2.2677, acc-0.2010, test loss-2.2686, acc-0.1979\n",
      "Iter-3780, train loss-2.3081, acc-0.1200, valid loss-2.2676, acc-0.2018, test loss-2.2685, acc-0.1981\n",
      "Iter-3790, train loss-2.2956, acc-0.1400, valid loss-2.2675, acc-0.2020, test loss-2.2683, acc-0.1984\n",
      "Iter-3800, train loss-2.2828, acc-0.1600, valid loss-2.2673, acc-0.2022, test loss-2.2682, acc-0.1986\n",
      "Iter-3810, train loss-2.2683, acc-0.1800, valid loss-2.2672, acc-0.2022, test loss-2.2681, acc-0.1992\n",
      "Iter-3820, train loss-2.2627, acc-0.1600, valid loss-2.2671, acc-0.2032, test loss-2.2679, acc-0.1995\n",
      "Iter-3830, train loss-2.2764, acc-0.2400, valid loss-2.2669, acc-0.2030, test loss-2.2678, acc-0.1996\n",
      "Iter-3840, train loss-2.2683, acc-0.2400, valid loss-2.2668, acc-0.2032, test loss-2.2677, acc-0.2004\n",
      "Iter-3850, train loss-2.2693, acc-0.2400, valid loss-2.2667, acc-0.2030, test loss-2.2675, acc-0.2009\n",
      "Iter-3860, train loss-2.2445, acc-0.3200, valid loss-2.2665, acc-0.2028, test loss-2.2674, acc-0.2013\n",
      "Iter-3870, train loss-2.2560, acc-0.2600, valid loss-2.2664, acc-0.2030, test loss-2.2673, acc-0.2018\n",
      "Iter-3880, train loss-2.2519, acc-0.3000, valid loss-2.2663, acc-0.2036, test loss-2.2671, acc-0.2022\n",
      "Iter-3890, train loss-2.2763, acc-0.1600, valid loss-2.2661, acc-0.2038, test loss-2.2670, acc-0.2025\n",
      "Iter-3900, train loss-2.2826, acc-0.1400, valid loss-2.2660, acc-0.2042, test loss-2.2669, acc-0.2032\n",
      "Iter-3910, train loss-2.2790, acc-0.2000, valid loss-2.2659, acc-0.2046, test loss-2.2668, acc-0.2038\n",
      "Iter-3920, train loss-2.2322, acc-0.3000, valid loss-2.2657, acc-0.2046, test loss-2.2666, acc-0.2037\n",
      "Iter-3930, train loss-2.2480, acc-0.2400, valid loss-2.2656, acc-0.2046, test loss-2.2665, acc-0.2041\n",
      "Iter-3940, train loss-2.2594, acc-0.3000, valid loss-2.2655, acc-0.2058, test loss-2.2663, acc-0.2043\n",
      "Iter-3950, train loss-2.2915, acc-0.1200, valid loss-2.2653, acc-0.2060, test loss-2.2662, acc-0.2050\n",
      "Iter-3960, train loss-2.2649, acc-0.2200, valid loss-2.2652, acc-0.2062, test loss-2.2661, acc-0.2054\n",
      "Iter-3970, train loss-2.2490, acc-0.2200, valid loss-2.2651, acc-0.2062, test loss-2.2660, acc-0.2058\n",
      "Iter-3980, train loss-2.2719, acc-0.1800, valid loss-2.2649, acc-0.2068, test loss-2.2658, acc-0.2061\n",
      "Iter-3990, train loss-2.2944, acc-0.1000, valid loss-2.2648, acc-0.2066, test loss-2.2657, acc-0.2066\n",
      "Iter-4000, train loss-2.2665, acc-0.2800, valid loss-2.2646, acc-0.2076, test loss-2.2655, acc-0.2071\n",
      "Iter-4010, train loss-2.2797, acc-0.2200, valid loss-2.2645, acc-0.2080, test loss-2.2654, acc-0.2069\n",
      "Iter-4020, train loss-2.2647, acc-0.2000, valid loss-2.2644, acc-0.2084, test loss-2.2653, acc-0.2072\n",
      "Iter-4030, train loss-2.2651, acc-0.2600, valid loss-2.2643, acc-0.2084, test loss-2.2652, acc-0.2076\n",
      "Iter-4040, train loss-2.2657, acc-0.1200, valid loss-2.2641, acc-0.2084, test loss-2.2650, acc-0.2076\n",
      "Iter-4050, train loss-2.2612, acc-0.1800, valid loss-2.2640, acc-0.2088, test loss-2.2649, acc-0.2079\n",
      "Iter-4060, train loss-2.2569, acc-0.1400, valid loss-2.2639, acc-0.2092, test loss-2.2648, acc-0.2083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-2.2342, acc-0.3400, valid loss-2.2637, acc-0.2106, test loss-2.2646, acc-0.2085\n",
      "Iter-4080, train loss-2.2435, acc-0.3600, valid loss-2.2636, acc-0.2112, test loss-2.2645, acc-0.2087\n",
      "Iter-4090, train loss-2.2806, acc-0.1800, valid loss-2.2635, acc-0.2114, test loss-2.2644, acc-0.2089\n",
      "Iter-4100, train loss-2.2670, acc-0.2000, valid loss-2.2634, acc-0.2118, test loss-2.2643, acc-0.2092\n",
      "Iter-4110, train loss-2.2644, acc-0.2200, valid loss-2.2632, acc-0.2118, test loss-2.2641, acc-0.2099\n",
      "Iter-4120, train loss-2.2813, acc-0.1800, valid loss-2.2631, acc-0.2124, test loss-2.2640, acc-0.2099\n",
      "Iter-4130, train loss-2.2534, acc-0.2800, valid loss-2.2629, acc-0.2126, test loss-2.2639, acc-0.2104\n",
      "Iter-4140, train loss-2.2799, acc-0.2000, valid loss-2.2628, acc-0.2132, test loss-2.2637, acc-0.2107\n",
      "Iter-4150, train loss-2.2640, acc-0.2000, valid loss-2.2627, acc-0.2136, test loss-2.2636, acc-0.2107\n",
      "Iter-4160, train loss-2.2603, acc-0.2400, valid loss-2.2626, acc-0.2134, test loss-2.2635, acc-0.2111\n",
      "Iter-4170, train loss-2.2713, acc-0.2200, valid loss-2.2624, acc-0.2134, test loss-2.2633, acc-0.2115\n",
      "Iter-4180, train loss-2.2541, acc-0.2600, valid loss-2.2623, acc-0.2144, test loss-2.2632, acc-0.2117\n",
      "Iter-4190, train loss-2.2675, acc-0.2200, valid loss-2.2622, acc-0.2144, test loss-2.2631, acc-0.2123\n",
      "Iter-4200, train loss-2.2628, acc-0.2200, valid loss-2.2620, acc-0.2158, test loss-2.2629, acc-0.2127\n",
      "Iter-4210, train loss-2.2630, acc-0.1800, valid loss-2.2619, acc-0.2160, test loss-2.2628, acc-0.2128\n",
      "Iter-4220, train loss-2.2703, acc-0.0800, valid loss-2.2618, acc-0.2166, test loss-2.2627, acc-0.2133\n",
      "Iter-4230, train loss-2.2863, acc-0.1400, valid loss-2.2616, acc-0.2174, test loss-2.2626, acc-0.2135\n",
      "Iter-4240, train loss-2.2634, acc-0.2600, valid loss-2.2615, acc-0.2172, test loss-2.2624, acc-0.2143\n",
      "Iter-4250, train loss-2.2481, acc-0.2800, valid loss-2.2614, acc-0.2174, test loss-2.2623, acc-0.2146\n",
      "Iter-4260, train loss-2.2760, acc-0.2000, valid loss-2.2613, acc-0.2182, test loss-2.2622, acc-0.2148\n",
      "Iter-4270, train loss-2.2688, acc-0.1600, valid loss-2.2611, acc-0.2188, test loss-2.2620, acc-0.2153\n",
      "Iter-4280, train loss-2.2570, acc-0.2000, valid loss-2.2610, acc-0.2190, test loss-2.2619, acc-0.2155\n",
      "Iter-4290, train loss-2.2623, acc-0.2000, valid loss-2.2609, acc-0.2198, test loss-2.2618, acc-0.2158\n",
      "Iter-4300, train loss-2.2590, acc-0.2000, valid loss-2.2608, acc-0.2200, test loss-2.2617, acc-0.2159\n",
      "Iter-4310, train loss-2.2672, acc-0.2200, valid loss-2.2606, acc-0.2200, test loss-2.2615, acc-0.2164\n",
      "Iter-4320, train loss-2.2549, acc-0.2000, valid loss-2.2605, acc-0.2202, test loss-2.2614, acc-0.2169\n",
      "Iter-4330, train loss-2.2600, acc-0.3200, valid loss-2.2604, acc-0.2208, test loss-2.2613, acc-0.2174\n",
      "Iter-4340, train loss-2.2876, acc-0.1000, valid loss-2.2602, acc-0.2208, test loss-2.2611, acc-0.2173\n",
      "Iter-4350, train loss-2.2625, acc-0.2000, valid loss-2.2601, acc-0.2210, test loss-2.2610, acc-0.2181\n",
      "Iter-4360, train loss-2.2383, acc-0.2800, valid loss-2.2600, acc-0.2208, test loss-2.2609, acc-0.2181\n",
      "Iter-4370, train loss-2.2542, acc-0.2200, valid loss-2.2599, acc-0.2206, test loss-2.2608, acc-0.2187\n",
      "Iter-4380, train loss-2.2521, acc-0.2000, valid loss-2.2597, acc-0.2216, test loss-2.2606, acc-0.2189\n",
      "Iter-4390, train loss-2.2609, acc-0.2600, valid loss-2.2596, acc-0.2214, test loss-2.2605, acc-0.2188\n",
      "Iter-4400, train loss-2.2517, acc-0.1800, valid loss-2.2595, acc-0.2224, test loss-2.2604, acc-0.2196\n",
      "Iter-4410, train loss-2.2617, acc-0.2400, valid loss-2.2593, acc-0.2232, test loss-2.2602, acc-0.2202\n",
      "Iter-4420, train loss-2.2560, acc-0.1800, valid loss-2.2592, acc-0.2234, test loss-2.2601, acc-0.2208\n",
      "Iter-4430, train loss-2.2485, acc-0.3400, valid loss-2.2590, acc-0.2238, test loss-2.2600, acc-0.2215\n",
      "Iter-4440, train loss-2.2275, acc-0.2200, valid loss-2.2589, acc-0.2236, test loss-2.2598, acc-0.2217\n",
      "Iter-4450, train loss-2.3021, acc-0.2200, valid loss-2.2588, acc-0.2246, test loss-2.2597, acc-0.2222\n",
      "Iter-4460, train loss-2.2835, acc-0.1600, valid loss-2.2586, acc-0.2248, test loss-2.2596, acc-0.2223\n",
      "Iter-4470, train loss-2.2558, acc-0.2200, valid loss-2.2585, acc-0.2254, test loss-2.2594, acc-0.2227\n",
      "Iter-4480, train loss-2.2454, acc-0.2400, valid loss-2.2584, acc-0.2254, test loss-2.2593, acc-0.2228\n",
      "Iter-4490, train loss-2.2538, acc-0.2000, valid loss-2.2582, acc-0.2260, test loss-2.2592, acc-0.2230\n",
      "Iter-4500, train loss-2.2585, acc-0.2200, valid loss-2.2581, acc-0.2260, test loss-2.2590, acc-0.2234\n",
      "Iter-4510, train loss-2.2942, acc-0.1000, valid loss-2.2580, acc-0.2266, test loss-2.2589, acc-0.2233\n",
      "Iter-4520, train loss-2.2624, acc-0.2600, valid loss-2.2579, acc-0.2266, test loss-2.2588, acc-0.2238\n",
      "Iter-4530, train loss-2.2355, acc-0.3200, valid loss-2.2577, acc-0.2266, test loss-2.2587, acc-0.2241\n",
      "Iter-4540, train loss-2.2569, acc-0.2800, valid loss-2.2576, acc-0.2272, test loss-2.2585, acc-0.2243\n",
      "Iter-4550, train loss-2.2731, acc-0.2000, valid loss-2.2575, acc-0.2274, test loss-2.2584, acc-0.2252\n",
      "Iter-4560, train loss-2.2603, acc-0.1600, valid loss-2.2573, acc-0.2278, test loss-2.2583, acc-0.2252\n",
      "Iter-4570, train loss-2.2804, acc-0.1600, valid loss-2.2572, acc-0.2282, test loss-2.2581, acc-0.2255\n",
      "Iter-4580, train loss-2.2764, acc-0.2600, valid loss-2.2571, acc-0.2286, test loss-2.2580, acc-0.2262\n",
      "Iter-4590, train loss-2.2537, acc-0.2000, valid loss-2.2569, acc-0.2290, test loss-2.2579, acc-0.2264\n",
      "Iter-4600, train loss-2.2354, acc-0.2400, valid loss-2.2568, acc-0.2294, test loss-2.2578, acc-0.2269\n",
      "Iter-4610, train loss-2.2397, acc-0.4000, valid loss-2.2567, acc-0.2294, test loss-2.2576, acc-0.2269\n",
      "Iter-4620, train loss-2.2638, acc-0.2600, valid loss-2.2566, acc-0.2300, test loss-2.2575, acc-0.2270\n",
      "Iter-4630, train loss-2.2486, acc-0.2400, valid loss-2.2564, acc-0.2304, test loss-2.2574, acc-0.2275\n",
      "Iter-4640, train loss-2.2554, acc-0.2200, valid loss-2.2563, acc-0.2306, test loss-2.2572, acc-0.2282\n",
      "Iter-4650, train loss-2.2370, acc-0.2600, valid loss-2.2562, acc-0.2308, test loss-2.2571, acc-0.2284\n",
      "Iter-4660, train loss-2.2754, acc-0.2000, valid loss-2.2561, acc-0.2316, test loss-2.2570, acc-0.2288\n",
      "Iter-4670, train loss-2.2571, acc-0.1400, valid loss-2.2559, acc-0.2318, test loss-2.2569, acc-0.2289\n",
      "Iter-4680, train loss-2.2492, acc-0.2400, valid loss-2.2558, acc-0.2324, test loss-2.2567, acc-0.2292\n",
      "Iter-4690, train loss-2.2780, acc-0.1400, valid loss-2.2557, acc-0.2322, test loss-2.2566, acc-0.2292\n",
      "Iter-4700, train loss-2.2345, acc-0.2600, valid loss-2.2555, acc-0.2326, test loss-2.2565, acc-0.2294\n",
      "Iter-4710, train loss-2.2655, acc-0.1600, valid loss-2.2554, acc-0.2334, test loss-2.2563, acc-0.2294\n",
      "Iter-4720, train loss-2.2329, acc-0.3000, valid loss-2.2553, acc-0.2346, test loss-2.2562, acc-0.2303\n",
      "Iter-4730, train loss-2.2655, acc-0.1600, valid loss-2.2551, acc-0.2350, test loss-2.2561, acc-0.2309\n",
      "Iter-4740, train loss-2.2435, acc-0.2400, valid loss-2.2550, acc-0.2350, test loss-2.2559, acc-0.2310\n",
      "Iter-4750, train loss-2.2532, acc-0.2800, valid loss-2.2549, acc-0.2352, test loss-2.2558, acc-0.2314\n",
      "Iter-4760, train loss-2.2295, acc-0.3600, valid loss-2.2547, acc-0.2358, test loss-2.2557, acc-0.2319\n",
      "Iter-4770, train loss-2.2261, acc-0.2800, valid loss-2.2546, acc-0.2362, test loss-2.2555, acc-0.2322\n",
      "Iter-4780, train loss-2.2784, acc-0.1600, valid loss-2.2545, acc-0.2372, test loss-2.2554, acc-0.2324\n",
      "Iter-4790, train loss-2.2607, acc-0.1800, valid loss-2.2544, acc-0.2374, test loss-2.2553, acc-0.2328\n",
      "Iter-4800, train loss-2.2552, acc-0.2400, valid loss-2.2542, acc-0.2374, test loss-2.2552, acc-0.2333\n",
      "Iter-4810, train loss-2.2658, acc-0.2400, valid loss-2.2541, acc-0.2378, test loss-2.2550, acc-0.2335\n",
      "Iter-4820, train loss-2.2778, acc-0.2600, valid loss-2.2540, acc-0.2376, test loss-2.2549, acc-0.2339\n",
      "Iter-4830, train loss-2.2882, acc-0.1800, valid loss-2.2538, acc-0.2378, test loss-2.2548, acc-0.2340\n",
      "Iter-4840, train loss-2.2505, acc-0.3000, valid loss-2.2537, acc-0.2388, test loss-2.2546, acc-0.2344\n",
      "Iter-4850, train loss-2.2296, acc-0.2400, valid loss-2.2536, acc-0.2390, test loss-2.2545, acc-0.2346\n",
      "Iter-4860, train loss-2.2236, acc-0.3200, valid loss-2.2534, acc-0.2390, test loss-2.2544, acc-0.2348\n",
      "Iter-4870, train loss-2.2719, acc-0.1800, valid loss-2.2533, acc-0.2394, test loss-2.2542, acc-0.2349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-2.2636, acc-0.2400, valid loss-2.2532, acc-0.2402, test loss-2.2541, acc-0.2357\n",
      "Iter-4890, train loss-2.2576, acc-0.2400, valid loss-2.2531, acc-0.2408, test loss-2.2540, acc-0.2360\n",
      "Iter-4900, train loss-2.2548, acc-0.2800, valid loss-2.2529, acc-0.2412, test loss-2.2538, acc-0.2363\n",
      "Iter-4910, train loss-2.2583, acc-0.2400, valid loss-2.2528, acc-0.2416, test loss-2.2537, acc-0.2367\n",
      "Iter-4920, train loss-2.2528, acc-0.2000, valid loss-2.2527, acc-0.2422, test loss-2.2536, acc-0.2369\n",
      "Iter-4930, train loss-2.2629, acc-0.2400, valid loss-2.2525, acc-0.2426, test loss-2.2535, acc-0.2375\n",
      "Iter-4940, train loss-2.2525, acc-0.2200, valid loss-2.2524, acc-0.2428, test loss-2.2533, acc-0.2377\n",
      "Iter-4950, train loss-2.2339, acc-0.3400, valid loss-2.2523, acc-0.2428, test loss-2.2532, acc-0.2381\n",
      "Iter-4960, train loss-2.2802, acc-0.1400, valid loss-2.2522, acc-0.2426, test loss-2.2531, acc-0.2388\n",
      "Iter-4970, train loss-2.2607, acc-0.2800, valid loss-2.2520, acc-0.2432, test loss-2.2530, acc-0.2390\n",
      "Iter-4980, train loss-2.2464, acc-0.3000, valid loss-2.2519, acc-0.2432, test loss-2.2528, acc-0.2395\n",
      "Iter-4990, train loss-2.2611, acc-0.1800, valid loss-2.2518, acc-0.2434, test loss-2.2527, acc-0.2397\n",
      "Iter-5000, train loss-2.2693, acc-0.2000, valid loss-2.2517, acc-0.2438, test loss-2.2526, acc-0.2400\n",
      "Iter-5010, train loss-2.2209, acc-0.3400, valid loss-2.2515, acc-0.2446, test loss-2.2524, acc-0.2403\n",
      "Iter-5020, train loss-2.2399, acc-0.2600, valid loss-2.2514, acc-0.2452, test loss-2.2523, acc-0.2405\n",
      "Iter-5030, train loss-2.2542, acc-0.1800, valid loss-2.2513, acc-0.2458, test loss-2.2522, acc-0.2411\n",
      "Iter-5040, train loss-2.2534, acc-0.2200, valid loss-2.2511, acc-0.2460, test loss-2.2521, acc-0.2415\n",
      "Iter-5050, train loss-2.2336, acc-0.3400, valid loss-2.2510, acc-0.2462, test loss-2.2519, acc-0.2421\n",
      "Iter-5060, train loss-2.2478, acc-0.2000, valid loss-2.2509, acc-0.2464, test loss-2.2518, acc-0.2433\n",
      "Iter-5070, train loss-2.2339, acc-0.3000, valid loss-2.2508, acc-0.2470, test loss-2.2517, acc-0.2435\n",
      "Iter-5080, train loss-2.2630, acc-0.2800, valid loss-2.2506, acc-0.2474, test loss-2.2516, acc-0.2441\n",
      "Iter-5090, train loss-2.2611, acc-0.1800, valid loss-2.2505, acc-0.2480, test loss-2.2514, acc-0.2445\n",
      "Iter-5100, train loss-2.2340, acc-0.2400, valid loss-2.2504, acc-0.2484, test loss-2.2513, acc-0.2448\n",
      "Iter-5110, train loss-2.2714, acc-0.2400, valid loss-2.2503, acc-0.2486, test loss-2.2512, acc-0.2455\n",
      "Iter-5120, train loss-2.2514, acc-0.1800, valid loss-2.2502, acc-0.2490, test loss-2.2511, acc-0.2459\n",
      "Iter-5130, train loss-2.2583, acc-0.2400, valid loss-2.2500, acc-0.2494, test loss-2.2509, acc-0.2463\n",
      "Iter-5140, train loss-2.2604, acc-0.1800, valid loss-2.2499, acc-0.2496, test loss-2.2508, acc-0.2467\n",
      "Iter-5150, train loss-2.2382, acc-0.3800, valid loss-2.2498, acc-0.2500, test loss-2.2507, acc-0.2470\n",
      "Iter-5160, train loss-2.2471, acc-0.2400, valid loss-2.2496, acc-0.2504, test loss-2.2506, acc-0.2482\n",
      "Iter-5170, train loss-2.2650, acc-0.2200, valid loss-2.2495, acc-0.2506, test loss-2.2504, acc-0.2489\n",
      "Iter-5180, train loss-2.2384, acc-0.2400, valid loss-2.2494, acc-0.2508, test loss-2.2503, acc-0.2488\n",
      "Iter-5190, train loss-2.2510, acc-0.2800, valid loss-2.2492, acc-0.2506, test loss-2.2502, acc-0.2489\n",
      "Iter-5200, train loss-2.2442, acc-0.3600, valid loss-2.2491, acc-0.2508, test loss-2.2500, acc-0.2492\n",
      "Iter-5210, train loss-2.2502, acc-0.2400, valid loss-2.2490, acc-0.2512, test loss-2.2499, acc-0.2495\n",
      "Iter-5220, train loss-2.2649, acc-0.1400, valid loss-2.2489, acc-0.2520, test loss-2.2498, acc-0.2500\n",
      "Iter-5230, train loss-2.2435, acc-0.1600, valid loss-2.2487, acc-0.2530, test loss-2.2497, acc-0.2502\n",
      "Iter-5240, train loss-2.2312, acc-0.2200, valid loss-2.2486, acc-0.2532, test loss-2.2495, acc-0.2507\n",
      "Iter-5250, train loss-2.2668, acc-0.2000, valid loss-2.2485, acc-0.2532, test loss-2.2494, acc-0.2510\n",
      "Iter-5260, train loss-2.2483, acc-0.2000, valid loss-2.2483, acc-0.2536, test loss-2.2493, acc-0.2513\n",
      "Iter-5270, train loss-2.2616, acc-0.2000, valid loss-2.2482, acc-0.2544, test loss-2.2492, acc-0.2515\n",
      "Iter-5280, train loss-2.2414, acc-0.2800, valid loss-2.2481, acc-0.2542, test loss-2.2490, acc-0.2523\n",
      "Iter-5290, train loss-2.2404, acc-0.2200, valid loss-2.2480, acc-0.2550, test loss-2.2489, acc-0.2527\n",
      "Iter-5300, train loss-2.2202, acc-0.3200, valid loss-2.2478, acc-0.2552, test loss-2.2488, acc-0.2531\n",
      "Iter-5310, train loss-2.2623, acc-0.2200, valid loss-2.2477, acc-0.2554, test loss-2.2486, acc-0.2529\n",
      "Iter-5320, train loss-2.2631, acc-0.2400, valid loss-2.2476, acc-0.2556, test loss-2.2485, acc-0.2539\n",
      "Iter-5330, train loss-2.2163, acc-0.3600, valid loss-2.2475, acc-0.2566, test loss-2.2484, acc-0.2541\n",
      "Iter-5340, train loss-2.2449, acc-0.3000, valid loss-2.2473, acc-0.2568, test loss-2.2483, acc-0.2541\n",
      "Iter-5350, train loss-2.2235, acc-0.1800, valid loss-2.2472, acc-0.2574, test loss-2.2481, acc-0.2544\n",
      "Iter-5360, train loss-2.2613, acc-0.2000, valid loss-2.2471, acc-0.2578, test loss-2.2480, acc-0.2546\n",
      "Iter-5370, train loss-2.2269, acc-0.3200, valid loss-2.2470, acc-0.2578, test loss-2.2479, acc-0.2554\n",
      "Iter-5380, train loss-2.2423, acc-0.2400, valid loss-2.2468, acc-0.2578, test loss-2.2478, acc-0.2559\n",
      "Iter-5390, train loss-2.2372, acc-0.2800, valid loss-2.2467, acc-0.2578, test loss-2.2476, acc-0.2566\n",
      "Iter-5400, train loss-2.2472, acc-0.2000, valid loss-2.2466, acc-0.2578, test loss-2.2475, acc-0.2569\n",
      "Iter-5410, train loss-2.2285, acc-0.2800, valid loss-2.2465, acc-0.2586, test loss-2.2474, acc-0.2573\n",
      "Iter-5420, train loss-2.2429, acc-0.2400, valid loss-2.2463, acc-0.2588, test loss-2.2473, acc-0.2578\n",
      "Iter-5430, train loss-2.2316, acc-0.3400, valid loss-2.2462, acc-0.2590, test loss-2.2471, acc-0.2585\n",
      "Iter-5440, train loss-2.2274, acc-0.2800, valid loss-2.2461, acc-0.2594, test loss-2.2470, acc-0.2589\n",
      "Iter-5450, train loss-2.2586, acc-0.2600, valid loss-2.2460, acc-0.2600, test loss-2.2469, acc-0.2593\n",
      "Iter-5460, train loss-2.2545, acc-0.2400, valid loss-2.2458, acc-0.2610, test loss-2.2468, acc-0.2596\n",
      "Iter-5470, train loss-2.2330, acc-0.2000, valid loss-2.2457, acc-0.2608, test loss-2.2466, acc-0.2599\n",
      "Iter-5480, train loss-2.2744, acc-0.2000, valid loss-2.2456, acc-0.2614, test loss-2.2465, acc-0.2605\n",
      "Iter-5490, train loss-2.2508, acc-0.3000, valid loss-2.2454, acc-0.2620, test loss-2.2464, acc-0.2611\n",
      "Iter-5500, train loss-2.2255, acc-0.2600, valid loss-2.2453, acc-0.2630, test loss-2.2462, acc-0.2615\n",
      "Iter-5510, train loss-2.2608, acc-0.3400, valid loss-2.2452, acc-0.2626, test loss-2.2461, acc-0.2620\n",
      "Iter-5520, train loss-2.2839, acc-0.1200, valid loss-2.2451, acc-0.2626, test loss-2.2460, acc-0.2623\n",
      "Iter-5530, train loss-2.2501, acc-0.2200, valid loss-2.2449, acc-0.2634, test loss-2.2459, acc-0.2625\n",
      "Iter-5540, train loss-2.2527, acc-0.2200, valid loss-2.2448, acc-0.2642, test loss-2.2457, acc-0.2627\n",
      "Iter-5550, train loss-2.2192, acc-0.4200, valid loss-2.2447, acc-0.2644, test loss-2.2456, acc-0.2634\n",
      "Iter-5560, train loss-2.2320, acc-0.3200, valid loss-2.2445, acc-0.2650, test loss-2.2455, acc-0.2635\n",
      "Iter-5570, train loss-2.2432, acc-0.3200, valid loss-2.2444, acc-0.2652, test loss-2.2453, acc-0.2640\n",
      "Iter-5580, train loss-2.2486, acc-0.2400, valid loss-2.2443, acc-0.2656, test loss-2.2452, acc-0.2642\n",
      "Iter-5590, train loss-2.2579, acc-0.2600, valid loss-2.2442, acc-0.2660, test loss-2.2451, acc-0.2649\n",
      "Iter-5600, train loss-2.2325, acc-0.3000, valid loss-2.2440, acc-0.2658, test loss-2.2450, acc-0.2651\n",
      "Iter-5610, train loss-2.2115, acc-0.4200, valid loss-2.2439, acc-0.2660, test loss-2.2448, acc-0.2652\n",
      "Iter-5620, train loss-2.2653, acc-0.2800, valid loss-2.2438, acc-0.2662, test loss-2.2447, acc-0.2657\n",
      "Iter-5630, train loss-2.2440, acc-0.2400, valid loss-2.2437, acc-0.2666, test loss-2.2446, acc-0.2659\n",
      "Iter-5640, train loss-2.2473, acc-0.1400, valid loss-2.2436, acc-0.2676, test loss-2.2445, acc-0.2664\n",
      "Iter-5650, train loss-2.2595, acc-0.2400, valid loss-2.2434, acc-0.2670, test loss-2.2444, acc-0.2665\n",
      "Iter-5660, train loss-2.2761, acc-0.2400, valid loss-2.2433, acc-0.2674, test loss-2.2442, acc-0.2673\n",
      "Iter-5670, train loss-2.2522, acc-0.2400, valid loss-2.2432, acc-0.2678, test loss-2.2441, acc-0.2676\n",
      "Iter-5680, train loss-2.2589, acc-0.2400, valid loss-2.2430, acc-0.2680, test loss-2.2440, acc-0.2678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-2.2338, acc-0.2400, valid loss-2.2429, acc-0.2684, test loss-2.2438, acc-0.2678\n",
      "Iter-5700, train loss-2.2148, acc-0.3600, valid loss-2.2428, acc-0.2680, test loss-2.2437, acc-0.2682\n",
      "Iter-5710, train loss-2.2642, acc-0.2200, valid loss-2.2427, acc-0.2682, test loss-2.2436, acc-0.2688\n",
      "Iter-5720, train loss-2.2555, acc-0.2800, valid loss-2.2425, acc-0.2684, test loss-2.2435, acc-0.2690\n",
      "Iter-5730, train loss-2.2203, acc-0.2800, valid loss-2.2424, acc-0.2686, test loss-2.2433, acc-0.2691\n",
      "Iter-5740, train loss-2.2257, acc-0.2400, valid loss-2.2423, acc-0.2694, test loss-2.2432, acc-0.2694\n",
      "Iter-5750, train loss-2.2258, acc-0.2800, valid loss-2.2421, acc-0.2696, test loss-2.2431, acc-0.2697\n",
      "Iter-5760, train loss-2.2047, acc-0.3200, valid loss-2.2420, acc-0.2698, test loss-2.2430, acc-0.2699\n",
      "Iter-5770, train loss-2.2494, acc-0.2000, valid loss-2.2419, acc-0.2704, test loss-2.2428, acc-0.2702\n",
      "Iter-5780, train loss-2.2185, acc-0.3400, valid loss-2.2417, acc-0.2708, test loss-2.2427, acc-0.2704\n",
      "Iter-5790, train loss-2.2403, acc-0.2400, valid loss-2.2416, acc-0.2712, test loss-2.2426, acc-0.2708\n",
      "Iter-5800, train loss-2.2623, acc-0.1800, valid loss-2.2415, acc-0.2714, test loss-2.2424, acc-0.2711\n",
      "Iter-5810, train loss-2.2289, acc-0.3800, valid loss-2.2413, acc-0.2720, test loss-2.2423, acc-0.2712\n",
      "Iter-5820, train loss-2.2377, acc-0.2600, valid loss-2.2412, acc-0.2728, test loss-2.2422, acc-0.2716\n",
      "Iter-5830, train loss-2.2415, acc-0.2000, valid loss-2.2411, acc-0.2734, test loss-2.2420, acc-0.2720\n",
      "Iter-5840, train loss-2.2773, acc-0.1800, valid loss-2.2410, acc-0.2738, test loss-2.2419, acc-0.2724\n",
      "Iter-5850, train loss-2.2275, acc-0.3000, valid loss-2.2408, acc-0.2738, test loss-2.2418, acc-0.2725\n",
      "Iter-5860, train loss-2.2177, acc-0.3800, valid loss-2.2407, acc-0.2748, test loss-2.2417, acc-0.2727\n",
      "Iter-5870, train loss-2.2539, acc-0.3000, valid loss-2.2406, acc-0.2748, test loss-2.2415, acc-0.2727\n",
      "Iter-5880, train loss-2.2415, acc-0.2000, valid loss-2.2404, acc-0.2756, test loss-2.2414, acc-0.2731\n",
      "Iter-5890, train loss-2.2457, acc-0.2400, valid loss-2.2403, acc-0.2758, test loss-2.2413, acc-0.2738\n",
      "Iter-5900, train loss-2.2436, acc-0.2600, valid loss-2.2402, acc-0.2758, test loss-2.2412, acc-0.2739\n",
      "Iter-5910, train loss-2.2407, acc-0.2800, valid loss-2.2401, acc-0.2766, test loss-2.2410, acc-0.2741\n",
      "Iter-5920, train loss-2.2233, acc-0.3000, valid loss-2.2399, acc-0.2770, test loss-2.2409, acc-0.2743\n",
      "Iter-5930, train loss-2.2472, acc-0.3200, valid loss-2.2398, acc-0.2776, test loss-2.2408, acc-0.2743\n",
      "Iter-5940, train loss-2.2290, acc-0.2800, valid loss-2.2397, acc-0.2774, test loss-2.2406, acc-0.2751\n",
      "Iter-5950, train loss-2.2311, acc-0.3600, valid loss-2.2396, acc-0.2776, test loss-2.2405, acc-0.2749\n",
      "Iter-5960, train loss-2.2307, acc-0.2800, valid loss-2.2394, acc-0.2778, test loss-2.2404, acc-0.2756\n",
      "Iter-5970, train loss-2.2685, acc-0.1800, valid loss-2.2393, acc-0.2782, test loss-2.2403, acc-0.2757\n",
      "Iter-5980, train loss-2.2618, acc-0.2400, valid loss-2.2392, acc-0.2786, test loss-2.2401, acc-0.2762\n",
      "Iter-5990, train loss-2.2537, acc-0.3200, valid loss-2.2391, acc-0.2788, test loss-2.2400, acc-0.2764\n",
      "Iter-6000, train loss-2.2333, acc-0.4000, valid loss-2.2389, acc-0.2790, test loss-2.2399, acc-0.2767\n",
      "Iter-6010, train loss-2.2485, acc-0.3200, valid loss-2.2388, acc-0.2790, test loss-2.2398, acc-0.2772\n",
      "Iter-6020, train loss-2.2354, acc-0.2000, valid loss-2.2387, acc-0.2796, test loss-2.2396, acc-0.2775\n",
      "Iter-6030, train loss-2.2668, acc-0.1400, valid loss-2.2386, acc-0.2806, test loss-2.2395, acc-0.2776\n",
      "Iter-6040, train loss-2.2492, acc-0.2000, valid loss-2.2384, acc-0.2806, test loss-2.2394, acc-0.2777\n",
      "Iter-6050, train loss-2.2286, acc-0.3000, valid loss-2.2383, acc-0.2810, test loss-2.2393, acc-0.2783\n",
      "Iter-6060, train loss-2.2575, acc-0.2000, valid loss-2.2382, acc-0.2814, test loss-2.2391, acc-0.2789\n",
      "Iter-6070, train loss-2.2460, acc-0.2600, valid loss-2.2381, acc-0.2814, test loss-2.2390, acc-0.2794\n",
      "Iter-6080, train loss-2.2423, acc-0.2000, valid loss-2.2379, acc-0.2816, test loss-2.2389, acc-0.2796\n",
      "Iter-6090, train loss-2.2258, acc-0.3400, valid loss-2.2378, acc-0.2820, test loss-2.2387, acc-0.2796\n",
      "Iter-6100, train loss-2.2151, acc-0.3000, valid loss-2.2377, acc-0.2820, test loss-2.2386, acc-0.2800\n",
      "Iter-6110, train loss-2.2470, acc-0.2200, valid loss-2.2375, acc-0.2818, test loss-2.2385, acc-0.2805\n",
      "Iter-6120, train loss-2.2272, acc-0.3800, valid loss-2.2374, acc-0.2820, test loss-2.2384, acc-0.2808\n",
      "Iter-6130, train loss-2.2422, acc-0.2400, valid loss-2.2373, acc-0.2820, test loss-2.2382, acc-0.2814\n",
      "Iter-6140, train loss-2.2261, acc-0.3000, valid loss-2.2372, acc-0.2822, test loss-2.2381, acc-0.2817\n",
      "Iter-6150, train loss-2.2224, acc-0.3400, valid loss-2.2370, acc-0.2826, test loss-2.2380, acc-0.2822\n",
      "Iter-6160, train loss-2.2546, acc-0.2000, valid loss-2.2369, acc-0.2830, test loss-2.2379, acc-0.2825\n",
      "Iter-6170, train loss-2.2559, acc-0.1400, valid loss-2.2368, acc-0.2838, test loss-2.2377, acc-0.2826\n",
      "Iter-6180, train loss-2.2073, acc-0.4000, valid loss-2.2366, acc-0.2840, test loss-2.2376, acc-0.2832\n",
      "Iter-6190, train loss-2.2434, acc-0.3200, valid loss-2.2365, acc-0.2838, test loss-2.2375, acc-0.2836\n",
      "Iter-6200, train loss-2.2207, acc-0.2600, valid loss-2.2364, acc-0.2842, test loss-2.2373, acc-0.2842\n",
      "Iter-6210, train loss-2.2306, acc-0.3400, valid loss-2.2363, acc-0.2840, test loss-2.2372, acc-0.2843\n",
      "Iter-6220, train loss-2.2593, acc-0.1800, valid loss-2.2362, acc-0.2840, test loss-2.2371, acc-0.2845\n",
      "Iter-6230, train loss-2.2429, acc-0.2000, valid loss-2.2360, acc-0.2844, test loss-2.2370, acc-0.2849\n",
      "Iter-6240, train loss-2.2391, acc-0.3400, valid loss-2.2359, acc-0.2844, test loss-2.2369, acc-0.2853\n",
      "Iter-6250, train loss-2.2306, acc-0.3400, valid loss-2.2358, acc-0.2854, test loss-2.2367, acc-0.2855\n",
      "Iter-6260, train loss-2.2386, acc-0.2200, valid loss-2.2357, acc-0.2858, test loss-2.2366, acc-0.2858\n",
      "Iter-6270, train loss-2.2390, acc-0.3600, valid loss-2.2355, acc-0.2858, test loss-2.2365, acc-0.2869\n",
      "Iter-6280, train loss-2.2436, acc-0.2600, valid loss-2.2354, acc-0.2860, test loss-2.2364, acc-0.2872\n",
      "Iter-6290, train loss-2.2480, acc-0.2000, valid loss-2.2353, acc-0.2862, test loss-2.2362, acc-0.2876\n",
      "Iter-6300, train loss-2.2382, acc-0.3200, valid loss-2.2352, acc-0.2864, test loss-2.2361, acc-0.2877\n",
      "Iter-6310, train loss-2.2598, acc-0.2600, valid loss-2.2350, acc-0.2868, test loss-2.2360, acc-0.2879\n",
      "Iter-6320, train loss-2.2356, acc-0.2400, valid loss-2.2349, acc-0.2872, test loss-2.2359, acc-0.2881\n",
      "Iter-6330, train loss-2.2355, acc-0.2600, valid loss-2.2348, acc-0.2878, test loss-2.2357, acc-0.2885\n",
      "Iter-6340, train loss-2.2327, acc-0.3600, valid loss-2.2347, acc-0.2880, test loss-2.2356, acc-0.2887\n",
      "Iter-6350, train loss-2.2440, acc-0.2400, valid loss-2.2346, acc-0.2882, test loss-2.2355, acc-0.2891\n",
      "Iter-6360, train loss-2.2264, acc-0.3200, valid loss-2.2344, acc-0.2880, test loss-2.2353, acc-0.2891\n",
      "Iter-6370, train loss-2.2355, acc-0.2800, valid loss-2.2343, acc-0.2886, test loss-2.2352, acc-0.2894\n",
      "Iter-6380, train loss-2.2665, acc-0.1400, valid loss-2.2342, acc-0.2886, test loss-2.2351, acc-0.2898\n",
      "Iter-6390, train loss-2.2091, acc-0.4400, valid loss-2.2340, acc-0.2888, test loss-2.2350, acc-0.2902\n",
      "Iter-6400, train loss-2.2360, acc-0.1800, valid loss-2.2339, acc-0.2892, test loss-2.2348, acc-0.2905\n",
      "Iter-6410, train loss-2.2324, acc-0.2800, valid loss-2.2338, acc-0.2892, test loss-2.2347, acc-0.2904\n",
      "Iter-6420, train loss-2.2125, acc-0.3200, valid loss-2.2337, acc-0.2902, test loss-2.2346, acc-0.2911\n",
      "Iter-6430, train loss-2.2210, acc-0.2800, valid loss-2.2336, acc-0.2906, test loss-2.2345, acc-0.2906\n",
      "Iter-6440, train loss-2.2141, acc-0.3600, valid loss-2.2334, acc-0.2906, test loss-2.2344, acc-0.2913\n",
      "Iter-6450, train loss-2.2313, acc-0.2600, valid loss-2.2333, acc-0.2914, test loss-2.2342, acc-0.2912\n",
      "Iter-6460, train loss-2.2065, acc-0.4000, valid loss-2.2332, acc-0.2912, test loss-2.2341, acc-0.2917\n",
      "Iter-6470, train loss-2.2530, acc-0.2000, valid loss-2.2331, acc-0.2914, test loss-2.2340, acc-0.2921\n",
      "Iter-6480, train loss-2.2199, acc-0.2600, valid loss-2.2329, acc-0.2920, test loss-2.2339, acc-0.2928\n",
      "Iter-6490, train loss-2.2140, acc-0.4000, valid loss-2.2328, acc-0.2930, test loss-2.2337, acc-0.2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-2.2002, acc-0.3000, valid loss-2.2327, acc-0.2930, test loss-2.2336, acc-0.2934\n",
      "Iter-6510, train loss-2.2405, acc-0.2400, valid loss-2.2326, acc-0.2928, test loss-2.2335, acc-0.2932\n",
      "Iter-6520, train loss-2.2507, acc-0.2600, valid loss-2.2324, acc-0.2934, test loss-2.2334, acc-0.2936\n",
      "Iter-6530, train loss-2.2163, acc-0.4200, valid loss-2.2323, acc-0.2942, test loss-2.2332, acc-0.2941\n",
      "Iter-6540, train loss-2.2208, acc-0.3000, valid loss-2.2322, acc-0.2942, test loss-2.2331, acc-0.2941\n",
      "Iter-6550, train loss-2.2425, acc-0.2800, valid loss-2.2321, acc-0.2944, test loss-2.2330, acc-0.2945\n",
      "Iter-6560, train loss-2.2458, acc-0.2600, valid loss-2.2319, acc-0.2952, test loss-2.2329, acc-0.2948\n",
      "Iter-6570, train loss-2.2425, acc-0.2200, valid loss-2.2318, acc-0.2954, test loss-2.2327, acc-0.2951\n",
      "Iter-6580, train loss-2.2550, acc-0.2400, valid loss-2.2317, acc-0.2956, test loss-2.2326, acc-0.2956\n",
      "Iter-6590, train loss-2.2406, acc-0.2600, valid loss-2.2316, acc-0.2964, test loss-2.2325, acc-0.2956\n",
      "Iter-6600, train loss-2.2367, acc-0.3000, valid loss-2.2314, acc-0.2968, test loss-2.2324, acc-0.2960\n",
      "Iter-6610, train loss-2.2493, acc-0.2600, valid loss-2.2313, acc-0.2970, test loss-2.2323, acc-0.2961\n",
      "Iter-6620, train loss-2.2080, acc-0.4000, valid loss-2.2312, acc-0.2974, test loss-2.2321, acc-0.2963\n",
      "Iter-6630, train loss-2.2539, acc-0.2800, valid loss-2.2311, acc-0.2978, test loss-2.2320, acc-0.2964\n",
      "Iter-6640, train loss-2.2174, acc-0.3600, valid loss-2.2310, acc-0.2978, test loss-2.2319, acc-0.2964\n",
      "Iter-6650, train loss-2.2408, acc-0.2600, valid loss-2.2308, acc-0.2978, test loss-2.2318, acc-0.2968\n",
      "Iter-6660, train loss-2.2238, acc-0.3000, valid loss-2.2307, acc-0.2980, test loss-2.2316, acc-0.2970\n",
      "Iter-6670, train loss-2.2507, acc-0.2600, valid loss-2.2306, acc-0.2982, test loss-2.2315, acc-0.2970\n",
      "Iter-6680, train loss-2.2157, acc-0.3200, valid loss-2.2305, acc-0.2986, test loss-2.2314, acc-0.2974\n",
      "Iter-6690, train loss-2.2493, acc-0.2400, valid loss-2.2303, acc-0.2986, test loss-2.2313, acc-0.2977\n",
      "Iter-6700, train loss-2.2293, acc-0.2800, valid loss-2.2302, acc-0.2990, test loss-2.2311, acc-0.2980\n",
      "Iter-6710, train loss-2.2348, acc-0.3600, valid loss-2.2301, acc-0.2992, test loss-2.2310, acc-0.2982\n",
      "Iter-6720, train loss-2.2329, acc-0.3000, valid loss-2.2300, acc-0.3004, test loss-2.2309, acc-0.2978\n",
      "Iter-6730, train loss-2.2435, acc-0.2200, valid loss-2.2298, acc-0.3008, test loss-2.2308, acc-0.2981\n",
      "Iter-6740, train loss-2.2251, acc-0.3000, valid loss-2.2297, acc-0.3008, test loss-2.2306, acc-0.2987\n",
      "Iter-6750, train loss-2.2247, acc-0.3600, valid loss-2.2296, acc-0.3014, test loss-2.2305, acc-0.2986\n",
      "Iter-6760, train loss-2.2159, acc-0.3000, valid loss-2.2294, acc-0.3014, test loss-2.2304, acc-0.2987\n",
      "Iter-6770, train loss-2.2268, acc-0.2800, valid loss-2.2293, acc-0.3018, test loss-2.2303, acc-0.2988\n",
      "Iter-6780, train loss-2.2244, acc-0.3800, valid loss-2.2292, acc-0.3018, test loss-2.2301, acc-0.2994\n",
      "Iter-6790, train loss-2.2330, acc-0.2400, valid loss-2.2291, acc-0.3030, test loss-2.2300, acc-0.2995\n",
      "Iter-6800, train loss-2.2479, acc-0.2600, valid loss-2.2290, acc-0.3024, test loss-2.2299, acc-0.2998\n",
      "Iter-6810, train loss-2.2528, acc-0.2400, valid loss-2.2288, acc-0.3032, test loss-2.2298, acc-0.3004\n",
      "Iter-6820, train loss-2.2288, acc-0.3400, valid loss-2.2287, acc-0.3036, test loss-2.2297, acc-0.3007\n",
      "Iter-6830, train loss-2.2321, acc-0.1800, valid loss-2.2286, acc-0.3034, test loss-2.2295, acc-0.3007\n",
      "Iter-6840, train loss-2.2229, acc-0.3600, valid loss-2.2285, acc-0.3040, test loss-2.2294, acc-0.3010\n",
      "Iter-6850, train loss-2.2204, acc-0.3200, valid loss-2.2283, acc-0.3050, test loss-2.2293, acc-0.3015\n",
      "Iter-6860, train loss-2.2262, acc-0.2600, valid loss-2.2282, acc-0.3046, test loss-2.2291, acc-0.3018\n",
      "Iter-6870, train loss-2.2393, acc-0.2400, valid loss-2.2281, acc-0.3046, test loss-2.2290, acc-0.3022\n",
      "Iter-6880, train loss-2.2451, acc-0.2200, valid loss-2.2279, acc-0.3046, test loss-2.2289, acc-0.3025\n",
      "Iter-6890, train loss-2.2208, acc-0.3400, valid loss-2.2278, acc-0.3046, test loss-2.2288, acc-0.3028\n",
      "Iter-6900, train loss-2.2371, acc-0.2800, valid loss-2.2277, acc-0.3048, test loss-2.2287, acc-0.3030\n",
      "Iter-6910, train loss-2.2287, acc-0.3400, valid loss-2.2276, acc-0.3050, test loss-2.2285, acc-0.3033\n",
      "Iter-6920, train loss-2.2218, acc-0.3400, valid loss-2.2275, acc-0.3052, test loss-2.2284, acc-0.3032\n",
      "Iter-6930, train loss-2.2298, acc-0.2600, valid loss-2.2273, acc-0.3056, test loss-2.2283, acc-0.3036\n",
      "Iter-6940, train loss-2.2126, acc-0.4200, valid loss-2.2272, acc-0.3056, test loss-2.2281, acc-0.3035\n",
      "Iter-6950, train loss-2.2299, acc-0.2800, valid loss-2.2271, acc-0.3060, test loss-2.2280, acc-0.3039\n",
      "Iter-6960, train loss-2.2171, acc-0.3800, valid loss-2.2270, acc-0.3062, test loss-2.2279, acc-0.3041\n",
      "Iter-6970, train loss-2.2295, acc-0.2600, valid loss-2.2268, acc-0.3064, test loss-2.2278, acc-0.3042\n",
      "Iter-6980, train loss-2.2383, acc-0.3400, valid loss-2.2267, acc-0.3066, test loss-2.2276, acc-0.3045\n",
      "Iter-6990, train loss-2.2385, acc-0.2600, valid loss-2.2266, acc-0.3068, test loss-2.2275, acc-0.3048\n",
      "Iter-7000, train loss-2.2316, acc-0.3600, valid loss-2.2265, acc-0.3072, test loss-2.2274, acc-0.3055\n",
      "Iter-7010, train loss-2.2366, acc-0.2800, valid loss-2.2263, acc-0.3070, test loss-2.2273, acc-0.3059\n",
      "Iter-7020, train loss-2.2060, acc-0.3400, valid loss-2.2262, acc-0.3078, test loss-2.2271, acc-0.3068\n",
      "Iter-7030, train loss-2.2189, acc-0.4000, valid loss-2.2261, acc-0.3082, test loss-2.2270, acc-0.3069\n",
      "Iter-7040, train loss-2.2275, acc-0.3000, valid loss-2.2259, acc-0.3082, test loss-2.2269, acc-0.3073\n",
      "Iter-7050, train loss-2.2577, acc-0.2600, valid loss-2.2258, acc-0.3080, test loss-2.2268, acc-0.3076\n",
      "Iter-7060, train loss-2.2329, acc-0.2400, valid loss-2.2257, acc-0.3084, test loss-2.2266, acc-0.3081\n",
      "Iter-7070, train loss-2.2261, acc-0.2600, valid loss-2.2256, acc-0.3086, test loss-2.2265, acc-0.3081\n",
      "Iter-7080, train loss-2.2027, acc-0.4000, valid loss-2.2255, acc-0.3090, test loss-2.2264, acc-0.3082\n",
      "Iter-7090, train loss-2.2384, acc-0.2800, valid loss-2.2253, acc-0.3094, test loss-2.2263, acc-0.3086\n",
      "Iter-7100, train loss-2.2637, acc-0.1400, valid loss-2.2252, acc-0.3096, test loss-2.2261, acc-0.3088\n",
      "Iter-7110, train loss-2.2099, acc-0.3200, valid loss-2.2251, acc-0.3096, test loss-2.2260, acc-0.3089\n",
      "Iter-7120, train loss-2.2321, acc-0.2800, valid loss-2.2250, acc-0.3098, test loss-2.2259, acc-0.3094\n",
      "Iter-7130, train loss-2.2282, acc-0.3000, valid loss-2.2249, acc-0.3096, test loss-2.2258, acc-0.3092\n",
      "Iter-7140, train loss-2.2353, acc-0.2800, valid loss-2.2247, acc-0.3104, test loss-2.2257, acc-0.3094\n",
      "Iter-7150, train loss-2.2284, acc-0.3600, valid loss-2.2246, acc-0.3106, test loss-2.2255, acc-0.3096\n",
      "Iter-7160, train loss-2.2174, acc-0.4200, valid loss-2.2245, acc-0.3110, test loss-2.2254, acc-0.3098\n",
      "Iter-7170, train loss-2.2153, acc-0.3800, valid loss-2.2244, acc-0.3112, test loss-2.2253, acc-0.3104\n",
      "Iter-7180, train loss-2.2430, acc-0.1600, valid loss-2.2242, acc-0.3112, test loss-2.2252, acc-0.3107\n",
      "Iter-7190, train loss-2.2435, acc-0.2600, valid loss-2.2241, acc-0.3114, test loss-2.2250, acc-0.3111\n",
      "Iter-7200, train loss-2.2096, acc-0.3600, valid loss-2.2240, acc-0.3116, test loss-2.2249, acc-0.3117\n",
      "Iter-7210, train loss-2.2488, acc-0.3000, valid loss-2.2239, acc-0.3118, test loss-2.2248, acc-0.3118\n",
      "Iter-7220, train loss-2.2657, acc-0.2200, valid loss-2.2238, acc-0.3120, test loss-2.2247, acc-0.3120\n",
      "Iter-7230, train loss-2.2061, acc-0.2400, valid loss-2.2236, acc-0.3122, test loss-2.2246, acc-0.3126\n",
      "Iter-7240, train loss-2.2566, acc-0.2200, valid loss-2.2235, acc-0.3122, test loss-2.2244, acc-0.3123\n",
      "Iter-7250, train loss-2.2226, acc-0.2800, valid loss-2.2234, acc-0.3122, test loss-2.2243, acc-0.3123\n",
      "Iter-7260, train loss-2.2236, acc-0.2800, valid loss-2.2233, acc-0.3126, test loss-2.2242, acc-0.3128\n",
      "Iter-7270, train loss-2.2155, acc-0.3400, valid loss-2.2231, acc-0.3134, test loss-2.2241, acc-0.3134\n",
      "Iter-7280, train loss-2.1995, acc-0.3600, valid loss-2.2230, acc-0.3140, test loss-2.2239, acc-0.3137\n",
      "Iter-7290, train loss-2.2205, acc-0.3000, valid loss-2.2229, acc-0.3140, test loss-2.2238, acc-0.3135\n",
      "Iter-7300, train loss-2.2002, acc-0.3400, valid loss-2.2228, acc-0.3144, test loss-2.2237, acc-0.3143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-2.2216, acc-0.4000, valid loss-2.2227, acc-0.3146, test loss-2.2236, acc-0.3144\n",
      "Iter-7320, train loss-2.2427, acc-0.2600, valid loss-2.2225, acc-0.3154, test loss-2.2235, acc-0.3148\n",
      "Iter-7330, train loss-2.1985, acc-0.4000, valid loss-2.2224, acc-0.3158, test loss-2.2233, acc-0.3145\n",
      "Iter-7340, train loss-2.2038, acc-0.3200, valid loss-2.2223, acc-0.3162, test loss-2.2232, acc-0.3146\n",
      "Iter-7350, train loss-2.2346, acc-0.3200, valid loss-2.2222, acc-0.3166, test loss-2.2231, acc-0.3149\n",
      "Iter-7360, train loss-2.2160, acc-0.3000, valid loss-2.2220, acc-0.3168, test loss-2.2230, acc-0.3153\n",
      "Iter-7370, train loss-2.1969, acc-0.4000, valid loss-2.2219, acc-0.3168, test loss-2.2228, acc-0.3153\n",
      "Iter-7380, train loss-2.1931, acc-0.3200, valid loss-2.2218, acc-0.3176, test loss-2.2227, acc-0.3157\n",
      "Iter-7390, train loss-2.2455, acc-0.2800, valid loss-2.2217, acc-0.3178, test loss-2.2226, acc-0.3158\n",
      "Iter-7400, train loss-2.1994, acc-0.5000, valid loss-2.2215, acc-0.3178, test loss-2.2225, acc-0.3158\n",
      "Iter-7410, train loss-2.2223, acc-0.3000, valid loss-2.2214, acc-0.3178, test loss-2.2224, acc-0.3162\n",
      "Iter-7420, train loss-2.2238, acc-0.3400, valid loss-2.2213, acc-0.3180, test loss-2.2222, acc-0.3165\n",
      "Iter-7430, train loss-2.2116, acc-0.3800, valid loss-2.2212, acc-0.3186, test loss-2.2221, acc-0.3164\n",
      "Iter-7440, train loss-2.2182, acc-0.3200, valid loss-2.2211, acc-0.3188, test loss-2.2220, acc-0.3165\n",
      "Iter-7450, train loss-2.2252, acc-0.3200, valid loss-2.2209, acc-0.3192, test loss-2.2219, acc-0.3170\n",
      "Iter-7460, train loss-2.2279, acc-0.3000, valid loss-2.2208, acc-0.3192, test loss-2.2218, acc-0.3168\n",
      "Iter-7470, train loss-2.2087, acc-0.2800, valid loss-2.2207, acc-0.3198, test loss-2.2216, acc-0.3175\n",
      "Iter-7480, train loss-2.2528, acc-0.3000, valid loss-2.2206, acc-0.3196, test loss-2.2215, acc-0.3174\n",
      "Iter-7490, train loss-2.2317, acc-0.2800, valid loss-2.2205, acc-0.3198, test loss-2.2214, acc-0.3172\n",
      "Iter-7500, train loss-2.2292, acc-0.3000, valid loss-2.2204, acc-0.3196, test loss-2.2213, acc-0.3178\n",
      "Iter-7510, train loss-2.2435, acc-0.2400, valid loss-2.2202, acc-0.3198, test loss-2.2212, acc-0.3178\n",
      "Iter-7520, train loss-2.2067, acc-0.4200, valid loss-2.2201, acc-0.3198, test loss-2.2211, acc-0.3179\n",
      "Iter-7530, train loss-2.1953, acc-0.3600, valid loss-2.2200, acc-0.3200, test loss-2.2209, acc-0.3181\n",
      "Iter-7540, train loss-2.2253, acc-0.3200, valid loss-2.2199, acc-0.3206, test loss-2.2208, acc-0.3184\n",
      "Iter-7550, train loss-2.2076, acc-0.4400, valid loss-2.2198, acc-0.3210, test loss-2.2207, acc-0.3187\n",
      "Iter-7560, train loss-2.2139, acc-0.3200, valid loss-2.2196, acc-0.3210, test loss-2.2206, acc-0.3187\n",
      "Iter-7570, train loss-2.2264, acc-0.3000, valid loss-2.2195, acc-0.3208, test loss-2.2205, acc-0.3189\n",
      "Iter-7580, train loss-2.2256, acc-0.3200, valid loss-2.2194, acc-0.3208, test loss-2.2203, acc-0.3193\n",
      "Iter-7590, train loss-2.2457, acc-0.3000, valid loss-2.2193, acc-0.3208, test loss-2.2202, acc-0.3193\n",
      "Iter-7600, train loss-2.1889, acc-0.3800, valid loss-2.2191, acc-0.3214, test loss-2.2201, acc-0.3197\n",
      "Iter-7610, train loss-2.2294, acc-0.2400, valid loss-2.2190, acc-0.3220, test loss-2.2200, acc-0.3199\n",
      "Iter-7620, train loss-2.2059, acc-0.4200, valid loss-2.2189, acc-0.3230, test loss-2.2198, acc-0.3200\n",
      "Iter-7630, train loss-2.2254, acc-0.3000, valid loss-2.2188, acc-0.3228, test loss-2.2197, acc-0.3203\n",
      "Iter-7640, train loss-2.2469, acc-0.2800, valid loss-2.2187, acc-0.3232, test loss-2.2196, acc-0.3205\n",
      "Iter-7650, train loss-2.2345, acc-0.2200, valid loss-2.2185, acc-0.3234, test loss-2.2195, acc-0.3204\n",
      "Iter-7660, train loss-2.1801, acc-0.4400, valid loss-2.2184, acc-0.3234, test loss-2.2194, acc-0.3210\n",
      "Iter-7670, train loss-2.2399, acc-0.3200, valid loss-2.2183, acc-0.3238, test loss-2.2192, acc-0.3208\n",
      "Iter-7680, train loss-2.2308, acc-0.2600, valid loss-2.2182, acc-0.3232, test loss-2.2191, acc-0.3211\n",
      "Iter-7690, train loss-2.1783, acc-0.4200, valid loss-2.2181, acc-0.3244, test loss-2.2190, acc-0.3212\n",
      "Iter-7700, train loss-2.2761, acc-0.1200, valid loss-2.2179, acc-0.3244, test loss-2.2189, acc-0.3213\n",
      "Iter-7710, train loss-2.2092, acc-0.3600, valid loss-2.2178, acc-0.3244, test loss-2.2188, acc-0.3219\n",
      "Iter-7720, train loss-2.2211, acc-0.3200, valid loss-2.2177, acc-0.3248, test loss-2.2187, acc-0.3220\n",
      "Iter-7730, train loss-2.2231, acc-0.3000, valid loss-2.2176, acc-0.3248, test loss-2.2185, acc-0.3224\n",
      "Iter-7740, train loss-2.2288, acc-0.2400, valid loss-2.2175, acc-0.3250, test loss-2.2184, acc-0.3226\n",
      "Iter-7750, train loss-2.2224, acc-0.3000, valid loss-2.2173, acc-0.3250, test loss-2.2183, acc-0.3227\n",
      "Iter-7760, train loss-2.2113, acc-0.2600, valid loss-2.2172, acc-0.3250, test loss-2.2182, acc-0.3230\n",
      "Iter-7770, train loss-2.2202, acc-0.3600, valid loss-2.2171, acc-0.3250, test loss-2.2180, acc-0.3233\n",
      "Iter-7780, train loss-2.2290, acc-0.2400, valid loss-2.2170, acc-0.3252, test loss-2.2179, acc-0.3238\n",
      "Iter-7790, train loss-2.2162, acc-0.3600, valid loss-2.2168, acc-0.3254, test loss-2.2178, acc-0.3243\n",
      "Iter-7800, train loss-2.2088, acc-0.3000, valid loss-2.2167, acc-0.3260, test loss-2.2177, acc-0.3247\n",
      "Iter-7810, train loss-2.2335, acc-0.2600, valid loss-2.2166, acc-0.3262, test loss-2.2175, acc-0.3248\n",
      "Iter-7820, train loss-2.2240, acc-0.2400, valid loss-2.2165, acc-0.3264, test loss-2.2174, acc-0.3245\n",
      "Iter-7830, train loss-2.2174, acc-0.2800, valid loss-2.2163, acc-0.3266, test loss-2.2173, acc-0.3246\n",
      "Iter-7840, train loss-2.2079, acc-0.3800, valid loss-2.2162, acc-0.3266, test loss-2.2172, acc-0.3252\n",
      "Iter-7850, train loss-2.2172, acc-0.3200, valid loss-2.2161, acc-0.3270, test loss-2.2170, acc-0.3253\n",
      "Iter-7860, train loss-2.2046, acc-0.3600, valid loss-2.2160, acc-0.3268, test loss-2.2169, acc-0.3254\n",
      "Iter-7870, train loss-2.2363, acc-0.3200, valid loss-2.2159, acc-0.3272, test loss-2.2168, acc-0.3258\n",
      "Iter-7880, train loss-2.2104, acc-0.4200, valid loss-2.2157, acc-0.3276, test loss-2.2167, acc-0.3261\n",
      "Iter-7890, train loss-2.2215, acc-0.3200, valid loss-2.2156, acc-0.3280, test loss-2.2166, acc-0.3264\n",
      "Iter-7900, train loss-2.1800, acc-0.4800, valid loss-2.2155, acc-0.3278, test loss-2.2164, acc-0.3262\n",
      "Iter-7910, train loss-2.2061, acc-0.3800, valid loss-2.2154, acc-0.3282, test loss-2.2163, acc-0.3266\n",
      "Iter-7920, train loss-2.2288, acc-0.2400, valid loss-2.2153, acc-0.3282, test loss-2.2162, acc-0.3266\n",
      "Iter-7930, train loss-2.2240, acc-0.3000, valid loss-2.2151, acc-0.3288, test loss-2.2161, acc-0.3268\n",
      "Iter-7940, train loss-2.2381, acc-0.3000, valid loss-2.2150, acc-0.3290, test loss-2.2160, acc-0.3270\n",
      "Iter-7950, train loss-2.2128, acc-0.3800, valid loss-2.2149, acc-0.3296, test loss-2.2158, acc-0.3270\n",
      "Iter-7960, train loss-2.1901, acc-0.3200, valid loss-2.2148, acc-0.3296, test loss-2.2157, acc-0.3272\n",
      "Iter-7970, train loss-2.2462, acc-0.2800, valid loss-2.2147, acc-0.3300, test loss-2.2156, acc-0.3273\n",
      "Iter-7980, train loss-2.1828, acc-0.4200, valid loss-2.2146, acc-0.3304, test loss-2.2155, acc-0.3279\n",
      "Iter-7990, train loss-2.1808, acc-0.4600, valid loss-2.2144, acc-0.3308, test loss-2.2154, acc-0.3283\n",
      "Iter-8000, train loss-2.2045, acc-0.3400, valid loss-2.2143, acc-0.3312, test loss-2.2153, acc-0.3286\n",
      "Iter-8010, train loss-2.1984, acc-0.3400, valid loss-2.2142, acc-0.3320, test loss-2.2151, acc-0.3286\n",
      "Iter-8020, train loss-2.1933, acc-0.4000, valid loss-2.2141, acc-0.3322, test loss-2.2150, acc-0.3286\n",
      "Iter-8030, train loss-2.2242, acc-0.2400, valid loss-2.2140, acc-0.3330, test loss-2.2149, acc-0.3286\n",
      "Iter-8040, train loss-2.2330, acc-0.2600, valid loss-2.2138, acc-0.3330, test loss-2.2148, acc-0.3292\n",
      "Iter-8050, train loss-2.2057, acc-0.3200, valid loss-2.2137, acc-0.3326, test loss-2.2147, acc-0.3291\n",
      "Iter-8060, train loss-2.2013, acc-0.3400, valid loss-2.2136, acc-0.3328, test loss-2.2145, acc-0.3295\n",
      "Iter-8070, train loss-2.2317, acc-0.2600, valid loss-2.2135, acc-0.3326, test loss-2.2144, acc-0.3297\n",
      "Iter-8080, train loss-2.2303, acc-0.3400, valid loss-2.2134, acc-0.3330, test loss-2.2143, acc-0.3297\n",
      "Iter-8090, train loss-2.2031, acc-0.3800, valid loss-2.2133, acc-0.3334, test loss-2.2142, acc-0.3303\n",
      "Iter-8100, train loss-2.2264, acc-0.3400, valid loss-2.2131, acc-0.3338, test loss-2.2141, acc-0.3303\n",
      "Iter-8110, train loss-2.2024, acc-0.3800, valid loss-2.2130, acc-0.3340, test loss-2.2139, acc-0.3306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-2.2278, acc-0.2800, valid loss-2.2129, acc-0.3340, test loss-2.2138, acc-0.3308\n",
      "Iter-8130, train loss-2.2391, acc-0.2600, valid loss-2.2128, acc-0.3342, test loss-2.2137, acc-0.3308\n",
      "Iter-8140, train loss-2.2135, acc-0.3200, valid loss-2.2126, acc-0.3352, test loss-2.2136, acc-0.3311\n",
      "Iter-8150, train loss-2.1765, acc-0.5200, valid loss-2.2125, acc-0.3352, test loss-2.2135, acc-0.3310\n",
      "Iter-8160, train loss-2.2272, acc-0.2000, valid loss-2.2124, acc-0.3354, test loss-2.2134, acc-0.3313\n",
      "Iter-8170, train loss-2.2061, acc-0.3000, valid loss-2.2123, acc-0.3356, test loss-2.2132, acc-0.3317\n",
      "Iter-8180, train loss-2.2157, acc-0.3000, valid loss-2.2122, acc-0.3356, test loss-2.2131, acc-0.3320\n",
      "Iter-8190, train loss-2.2364, acc-0.2800, valid loss-2.2121, acc-0.3358, test loss-2.2130, acc-0.3321\n",
      "Iter-8200, train loss-2.1748, acc-0.4600, valid loss-2.2119, acc-0.3362, test loss-2.2129, acc-0.3323\n",
      "Iter-8210, train loss-2.2327, acc-0.2800, valid loss-2.2118, acc-0.3364, test loss-2.2128, acc-0.3324\n",
      "Iter-8220, train loss-2.2374, acc-0.2600, valid loss-2.2117, acc-0.3364, test loss-2.2126, acc-0.3326\n",
      "Iter-8230, train loss-2.2092, acc-0.3000, valid loss-2.2116, acc-0.3364, test loss-2.2125, acc-0.3328\n",
      "Iter-8240, train loss-2.2048, acc-0.3800, valid loss-2.2115, acc-0.3366, test loss-2.2124, acc-0.3331\n",
      "Iter-8250, train loss-2.2171, acc-0.3000, valid loss-2.2113, acc-0.3372, test loss-2.2123, acc-0.3332\n",
      "Iter-8260, train loss-2.1962, acc-0.3400, valid loss-2.2112, acc-0.3372, test loss-2.2122, acc-0.3335\n",
      "Iter-8270, train loss-2.2151, acc-0.3000, valid loss-2.2111, acc-0.3372, test loss-2.2120, acc-0.3336\n",
      "Iter-8280, train loss-2.2350, acc-0.2400, valid loss-2.2110, acc-0.3376, test loss-2.2119, acc-0.3337\n",
      "Iter-8290, train loss-2.1861, acc-0.3800, valid loss-2.2108, acc-0.3378, test loss-2.2118, acc-0.3337\n",
      "Iter-8300, train loss-2.1976, acc-0.3400, valid loss-2.2107, acc-0.3378, test loss-2.2117, acc-0.3339\n",
      "Iter-8310, train loss-2.2344, acc-0.3000, valid loss-2.2106, acc-0.3386, test loss-2.2115, acc-0.3343\n",
      "Iter-8320, train loss-2.2409, acc-0.2000, valid loss-2.2105, acc-0.3390, test loss-2.2114, acc-0.3343\n",
      "Iter-8330, train loss-2.2062, acc-0.3200, valid loss-2.2104, acc-0.3392, test loss-2.2113, acc-0.3344\n",
      "Iter-8340, train loss-2.2008, acc-0.4200, valid loss-2.2102, acc-0.3390, test loss-2.2112, acc-0.3344\n",
      "Iter-8350, train loss-2.2259, acc-0.2800, valid loss-2.2101, acc-0.3396, test loss-2.2111, acc-0.3345\n",
      "Iter-8360, train loss-2.1852, acc-0.3200, valid loss-2.2100, acc-0.3396, test loss-2.2110, acc-0.3344\n",
      "Iter-8370, train loss-2.2062, acc-0.2600, valid loss-2.2099, acc-0.3400, test loss-2.2108, acc-0.3345\n",
      "Iter-8380, train loss-2.1934, acc-0.4200, valid loss-2.2098, acc-0.3402, test loss-2.2107, acc-0.3349\n",
      "Iter-8390, train loss-2.2284, acc-0.2800, valid loss-2.2097, acc-0.3404, test loss-2.2106, acc-0.3349\n",
      "Iter-8400, train loss-2.2234, acc-0.2800, valid loss-2.2095, acc-0.3408, test loss-2.2105, acc-0.3352\n",
      "Iter-8410, train loss-2.2433, acc-0.2200, valid loss-2.2094, acc-0.3406, test loss-2.2104, acc-0.3354\n",
      "Iter-8420, train loss-2.1997, acc-0.3200, valid loss-2.2093, acc-0.3412, test loss-2.2102, acc-0.3357\n",
      "Iter-8430, train loss-2.2228, acc-0.3000, valid loss-2.2092, acc-0.3412, test loss-2.2101, acc-0.3362\n",
      "Iter-8440, train loss-2.2333, acc-0.2800, valid loss-2.2091, acc-0.3410, test loss-2.2100, acc-0.3364\n",
      "Iter-8450, train loss-2.1711, acc-0.5000, valid loss-2.2090, acc-0.3412, test loss-2.2099, acc-0.3366\n",
      "Iter-8460, train loss-2.2190, acc-0.3600, valid loss-2.2088, acc-0.3412, test loss-2.2098, acc-0.3368\n",
      "Iter-8470, train loss-2.2151, acc-0.2600, valid loss-2.2087, acc-0.3414, test loss-2.2097, acc-0.3369\n",
      "Iter-8480, train loss-2.1899, acc-0.3600, valid loss-2.2086, acc-0.3420, test loss-2.2095, acc-0.3373\n",
      "Iter-8490, train loss-2.1895, acc-0.5600, valid loss-2.2085, acc-0.3422, test loss-2.2094, acc-0.3376\n",
      "Iter-8500, train loss-2.2237, acc-0.3400, valid loss-2.2084, acc-0.3422, test loss-2.2093, acc-0.3378\n",
      "Iter-8510, train loss-2.2219, acc-0.3800, valid loss-2.2082, acc-0.3426, test loss-2.2092, acc-0.3382\n",
      "Iter-8520, train loss-2.2239, acc-0.3000, valid loss-2.2081, acc-0.3422, test loss-2.2091, acc-0.3382\n",
      "Iter-8530, train loss-2.2001, acc-0.3200, valid loss-2.2080, acc-0.3426, test loss-2.2089, acc-0.3384\n",
      "Iter-8540, train loss-2.2074, acc-0.3600, valid loss-2.2079, acc-0.3430, test loss-2.2088, acc-0.3384\n",
      "Iter-8550, train loss-2.2094, acc-0.4000, valid loss-2.2078, acc-0.3430, test loss-2.2087, acc-0.3389\n",
      "Iter-8560, train loss-2.1669, acc-0.5000, valid loss-2.2077, acc-0.3438, test loss-2.2086, acc-0.3392\n",
      "Iter-8570, train loss-2.2136, acc-0.2400, valid loss-2.2075, acc-0.3442, test loss-2.2085, acc-0.3393\n",
      "Iter-8580, train loss-2.2428, acc-0.2800, valid loss-2.2074, acc-0.3442, test loss-2.2083, acc-0.3396\n",
      "Iter-8590, train loss-2.2117, acc-0.3000, valid loss-2.2073, acc-0.3438, test loss-2.2082, acc-0.3393\n",
      "Iter-8600, train loss-2.1677, acc-0.4800, valid loss-2.2072, acc-0.3442, test loss-2.2081, acc-0.3393\n",
      "Iter-8610, train loss-2.2087, acc-0.3600, valid loss-2.2071, acc-0.3442, test loss-2.2080, acc-0.3395\n",
      "Iter-8620, train loss-2.2003, acc-0.4200, valid loss-2.2069, acc-0.3442, test loss-2.2079, acc-0.3398\n",
      "Iter-8630, train loss-2.2231, acc-0.3000, valid loss-2.2068, acc-0.3444, test loss-2.2077, acc-0.3399\n",
      "Iter-8640, train loss-2.2200, acc-0.2600, valid loss-2.2067, acc-0.3450, test loss-2.2076, acc-0.3402\n",
      "Iter-8650, train loss-2.2087, acc-0.3800, valid loss-2.2066, acc-0.3448, test loss-2.2075, acc-0.3401\n",
      "Iter-8660, train loss-2.2157, acc-0.3400, valid loss-2.2065, acc-0.3452, test loss-2.2074, acc-0.3403\n",
      "Iter-8670, train loss-2.2117, acc-0.3000, valid loss-2.2063, acc-0.3454, test loss-2.2073, acc-0.3405\n",
      "Iter-8680, train loss-2.1975, acc-0.3200, valid loss-2.2062, acc-0.3456, test loss-2.2072, acc-0.3406\n",
      "Iter-8690, train loss-2.2236, acc-0.3600, valid loss-2.2061, acc-0.3458, test loss-2.2070, acc-0.3406\n",
      "Iter-8700, train loss-2.1851, acc-0.4200, valid loss-2.2060, acc-0.3460, test loss-2.2069, acc-0.3409\n",
      "Iter-8710, train loss-2.2354, acc-0.2800, valid loss-2.2059, acc-0.3460, test loss-2.2068, acc-0.3408\n",
      "Iter-8720, train loss-2.2416, acc-0.2200, valid loss-2.2058, acc-0.3458, test loss-2.2067, acc-0.3413\n",
      "Iter-8730, train loss-2.2232, acc-0.3400, valid loss-2.2056, acc-0.3460, test loss-2.2066, acc-0.3412\n",
      "Iter-8740, train loss-2.2268, acc-0.3000, valid loss-2.2055, acc-0.3460, test loss-2.2064, acc-0.3412\n",
      "Iter-8750, train loss-2.2022, acc-0.3200, valid loss-2.2054, acc-0.3466, test loss-2.2063, acc-0.3413\n",
      "Iter-8760, train loss-2.2045, acc-0.2800, valid loss-2.2053, acc-0.3464, test loss-2.2062, acc-0.3415\n",
      "Iter-8770, train loss-2.2189, acc-0.2200, valid loss-2.2052, acc-0.3464, test loss-2.2061, acc-0.3417\n",
      "Iter-8780, train loss-2.1945, acc-0.4000, valid loss-2.2051, acc-0.3462, test loss-2.2060, acc-0.3418\n",
      "Iter-8790, train loss-2.1994, acc-0.4200, valid loss-2.2049, acc-0.3464, test loss-2.2059, acc-0.3423\n",
      "Iter-8800, train loss-2.1918, acc-0.3800, valid loss-2.2048, acc-0.3464, test loss-2.2057, acc-0.3425\n",
      "Iter-8810, train loss-2.1562, acc-0.5000, valid loss-2.2047, acc-0.3462, test loss-2.2056, acc-0.3422\n",
      "Iter-8820, train loss-2.2116, acc-0.3600, valid loss-2.2046, acc-0.3460, test loss-2.2055, acc-0.3425\n",
      "Iter-8830, train loss-2.2167, acc-0.3200, valid loss-2.2045, acc-0.3462, test loss-2.2054, acc-0.3426\n",
      "Iter-8840, train loss-2.2041, acc-0.2800, valid loss-2.2044, acc-0.3462, test loss-2.2053, acc-0.3429\n",
      "Iter-8850, train loss-2.2010, acc-0.3600, valid loss-2.2042, acc-0.3462, test loss-2.2051, acc-0.3433\n",
      "Iter-8860, train loss-2.2174, acc-0.3800, valid loss-2.2041, acc-0.3466, test loss-2.2050, acc-0.3432\n",
      "Iter-8870, train loss-2.2197, acc-0.3400, valid loss-2.2040, acc-0.3470, test loss-2.2049, acc-0.3433\n",
      "Iter-8880, train loss-2.2194, acc-0.3000, valid loss-2.2039, acc-0.3472, test loss-2.2048, acc-0.3437\n",
      "Iter-8890, train loss-2.2157, acc-0.2800, valid loss-2.2038, acc-0.3470, test loss-2.2047, acc-0.3440\n",
      "Iter-8900, train loss-2.2168, acc-0.3400, valid loss-2.2036, acc-0.3470, test loss-2.2046, acc-0.3441\n",
      "Iter-8910, train loss-2.1869, acc-0.5200, valid loss-2.2035, acc-0.3474, test loss-2.2044, acc-0.3448\n",
      "Iter-8920, train loss-2.2143, acc-0.3800, valid loss-2.2034, acc-0.3478, test loss-2.2043, acc-0.3448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-2.1932, acc-0.3200, valid loss-2.2033, acc-0.3480, test loss-2.2042, acc-0.3446\n",
      "Iter-8940, train loss-2.2222, acc-0.3000, valid loss-2.2032, acc-0.3478, test loss-2.2041, acc-0.3446\n",
      "Iter-8950, train loss-2.2084, acc-0.3000, valid loss-2.2030, acc-0.3482, test loss-2.2040, acc-0.3449\n",
      "Iter-8960, train loss-2.2306, acc-0.3000, valid loss-2.2029, acc-0.3486, test loss-2.2038, acc-0.3448\n",
      "Iter-8970, train loss-2.1982, acc-0.3200, valid loss-2.2028, acc-0.3494, test loss-2.2037, acc-0.3448\n",
      "Iter-8980, train loss-2.2020, acc-0.3800, valid loss-2.2027, acc-0.3494, test loss-2.2036, acc-0.3449\n",
      "Iter-8990, train loss-2.2260, acc-0.3000, valid loss-2.2025, acc-0.3486, test loss-2.2035, acc-0.3452\n",
      "Iter-9000, train loss-2.2172, acc-0.3600, valid loss-2.2024, acc-0.3492, test loss-2.2033, acc-0.3451\n",
      "Iter-9010, train loss-2.2239, acc-0.3000, valid loss-2.2023, acc-0.3496, test loss-2.2032, acc-0.3452\n",
      "Iter-9020, train loss-2.2264, acc-0.3400, valid loss-2.2022, acc-0.3498, test loss-2.2031, acc-0.3454\n",
      "Iter-9030, train loss-2.1702, acc-0.4200, valid loss-2.2021, acc-0.3498, test loss-2.2030, acc-0.3457\n",
      "Iter-9040, train loss-2.2282, acc-0.2600, valid loss-2.2019, acc-0.3500, test loss-2.2029, acc-0.3460\n",
      "Iter-9050, train loss-2.2223, acc-0.2600, valid loss-2.2018, acc-0.3502, test loss-2.2027, acc-0.3461\n",
      "Iter-9060, train loss-2.1746, acc-0.4400, valid loss-2.2017, acc-0.3508, test loss-2.2026, acc-0.3463\n",
      "Iter-9070, train loss-2.2016, acc-0.3400, valid loss-2.2016, acc-0.3508, test loss-2.2025, acc-0.3463\n",
      "Iter-9080, train loss-2.2327, acc-0.2800, valid loss-2.2015, acc-0.3506, test loss-2.2024, acc-0.3466\n",
      "Iter-9090, train loss-2.1630, acc-0.4200, valid loss-2.2014, acc-0.3506, test loss-2.2023, acc-0.3467\n",
      "Iter-9100, train loss-2.1907, acc-0.3400, valid loss-2.2012, acc-0.3508, test loss-2.2022, acc-0.3469\n",
      "Iter-9110, train loss-2.1901, acc-0.3800, valid loss-2.2011, acc-0.3516, test loss-2.2020, acc-0.3477\n",
      "Iter-9120, train loss-2.1819, acc-0.4200, valid loss-2.2010, acc-0.3512, test loss-2.2019, acc-0.3477\n",
      "Iter-9130, train loss-2.1900, acc-0.4200, valid loss-2.2009, acc-0.3516, test loss-2.2018, acc-0.3477\n",
      "Iter-9140, train loss-2.2039, acc-0.2800, valid loss-2.2008, acc-0.3510, test loss-2.2017, acc-0.3476\n",
      "Iter-9150, train loss-2.1947, acc-0.3400, valid loss-2.2007, acc-0.3518, test loss-2.2016, acc-0.3477\n",
      "Iter-9160, train loss-2.2087, acc-0.3800, valid loss-2.2005, acc-0.3516, test loss-2.2015, acc-0.3478\n",
      "Iter-9170, train loss-2.2045, acc-0.3200, valid loss-2.2004, acc-0.3516, test loss-2.2013, acc-0.3477\n",
      "Iter-9180, train loss-2.1819, acc-0.4800, valid loss-2.2003, acc-0.3518, test loss-2.2012, acc-0.3475\n",
      "Iter-9190, train loss-2.1937, acc-0.3600, valid loss-2.2002, acc-0.3520, test loss-2.2011, acc-0.3473\n",
      "Iter-9200, train loss-2.1948, acc-0.3200, valid loss-2.2001, acc-0.3522, test loss-2.2010, acc-0.3478\n",
      "Iter-9210, train loss-2.2181, acc-0.3800, valid loss-2.1999, acc-0.3522, test loss-2.2009, acc-0.3482\n",
      "Iter-9220, train loss-2.2300, acc-0.2200, valid loss-2.1998, acc-0.3520, test loss-2.2007, acc-0.3481\n",
      "Iter-9230, train loss-2.2035, acc-0.3000, valid loss-2.1997, acc-0.3520, test loss-2.2006, acc-0.3487\n",
      "Iter-9240, train loss-2.1949, acc-0.2800, valid loss-2.1996, acc-0.3522, test loss-2.2005, acc-0.3488\n",
      "Iter-9250, train loss-2.2167, acc-0.3000, valid loss-2.1995, acc-0.3520, test loss-2.2004, acc-0.3487\n",
      "Iter-9260, train loss-2.2107, acc-0.3400, valid loss-2.1993, acc-0.3522, test loss-2.2003, acc-0.3488\n",
      "Iter-9270, train loss-2.1863, acc-0.4200, valid loss-2.1992, acc-0.3524, test loss-2.2001, acc-0.3491\n",
      "Iter-9280, train loss-2.1862, acc-0.4000, valid loss-2.1991, acc-0.3526, test loss-2.2000, acc-0.3492\n",
      "Iter-9290, train loss-2.1840, acc-0.3600, valid loss-2.1990, acc-0.3526, test loss-2.1999, acc-0.3492\n",
      "Iter-9300, train loss-2.2225, acc-0.4000, valid loss-2.1989, acc-0.3528, test loss-2.1998, acc-0.3493\n",
      "Iter-9310, train loss-2.1886, acc-0.4000, valid loss-2.1987, acc-0.3530, test loss-2.1997, acc-0.3492\n",
      "Iter-9320, train loss-2.2293, acc-0.2800, valid loss-2.1986, acc-0.3530, test loss-2.1995, acc-0.3494\n",
      "Iter-9330, train loss-2.1797, acc-0.4600, valid loss-2.1985, acc-0.3536, test loss-2.1994, acc-0.3496\n",
      "Iter-9340, train loss-2.1683, acc-0.4600, valid loss-2.1984, acc-0.3536, test loss-2.1993, acc-0.3498\n",
      "Iter-9350, train loss-2.1904, acc-0.4600, valid loss-2.1983, acc-0.3542, test loss-2.1992, acc-0.3500\n",
      "Iter-9360, train loss-2.1649, acc-0.5000, valid loss-2.1982, acc-0.3540, test loss-2.1991, acc-0.3503\n",
      "Iter-9370, train loss-2.1968, acc-0.3200, valid loss-2.1980, acc-0.3544, test loss-2.1989, acc-0.3503\n",
      "Iter-9380, train loss-2.2075, acc-0.3600, valid loss-2.1979, acc-0.3548, test loss-2.1988, acc-0.3505\n",
      "Iter-9390, train loss-2.1851, acc-0.3400, valid loss-2.1978, acc-0.3548, test loss-2.1987, acc-0.3507\n",
      "Iter-9400, train loss-2.1955, acc-0.3600, valid loss-2.1977, acc-0.3550, test loss-2.1986, acc-0.3505\n",
      "Iter-9410, train loss-2.2340, acc-0.2000, valid loss-2.1975, acc-0.3548, test loss-2.1985, acc-0.3506\n",
      "Iter-9420, train loss-2.1998, acc-0.4000, valid loss-2.1974, acc-0.3548, test loss-2.1983, acc-0.3508\n",
      "Iter-9430, train loss-2.1639, acc-0.4400, valid loss-2.1973, acc-0.3550, test loss-2.1982, acc-0.3510\n",
      "Iter-9440, train loss-2.1878, acc-0.4000, valid loss-2.1972, acc-0.3546, test loss-2.1981, acc-0.3510\n",
      "Iter-9450, train loss-2.2026, acc-0.2400, valid loss-2.1971, acc-0.3546, test loss-2.1980, acc-0.3510\n",
      "Iter-9460, train loss-2.2106, acc-0.3000, valid loss-2.1970, acc-0.3546, test loss-2.1979, acc-0.3508\n",
      "Iter-9470, train loss-2.1773, acc-0.3800, valid loss-2.1968, acc-0.3546, test loss-2.1977, acc-0.3510\n",
      "Iter-9480, train loss-2.2470, acc-0.2000, valid loss-2.1967, acc-0.3548, test loss-2.1976, acc-0.3512\n",
      "Iter-9490, train loss-2.1971, acc-0.3000, valid loss-2.1966, acc-0.3544, test loss-2.1975, acc-0.3512\n",
      "Iter-9500, train loss-2.2065, acc-0.3200, valid loss-2.1965, acc-0.3546, test loss-2.1974, acc-0.3515\n",
      "Iter-9510, train loss-2.1940, acc-0.3800, valid loss-2.1964, acc-0.3552, test loss-2.1973, acc-0.3519\n",
      "Iter-9520, train loss-2.1643, acc-0.4400, valid loss-2.1962, acc-0.3554, test loss-2.1971, acc-0.3521\n",
      "Iter-9530, train loss-2.1724, acc-0.3600, valid loss-2.1961, acc-0.3556, test loss-2.1970, acc-0.3520\n",
      "Iter-9540, train loss-2.1902, acc-0.3000, valid loss-2.1960, acc-0.3552, test loss-2.1969, acc-0.3527\n",
      "Iter-9550, train loss-2.2096, acc-0.3200, valid loss-2.1959, acc-0.3552, test loss-2.1968, acc-0.3524\n",
      "Iter-9560, train loss-2.2020, acc-0.2600, valid loss-2.1958, acc-0.3558, test loss-2.1967, acc-0.3530\n",
      "Iter-9570, train loss-2.1950, acc-0.4000, valid loss-2.1957, acc-0.3560, test loss-2.1965, acc-0.3529\n",
      "Iter-9580, train loss-2.1993, acc-0.3200, valid loss-2.1955, acc-0.3562, test loss-2.1964, acc-0.3529\n",
      "Iter-9590, train loss-2.2052, acc-0.3000, valid loss-2.1954, acc-0.3560, test loss-2.1963, acc-0.3533\n",
      "Iter-9600, train loss-2.2202, acc-0.2800, valid loss-2.1953, acc-0.3560, test loss-2.1962, acc-0.3529\n",
      "Iter-9610, train loss-2.1949, acc-0.4200, valid loss-2.1952, acc-0.3566, test loss-2.1961, acc-0.3533\n",
      "Iter-9620, train loss-2.1788, acc-0.4000, valid loss-2.1951, acc-0.3560, test loss-2.1959, acc-0.3535\n",
      "Iter-9630, train loss-2.2463, acc-0.1800, valid loss-2.1949, acc-0.3564, test loss-2.1958, acc-0.3531\n",
      "Iter-9640, train loss-2.2127, acc-0.2400, valid loss-2.1948, acc-0.3564, test loss-2.1957, acc-0.3534\n",
      "Iter-9650, train loss-2.2270, acc-0.2600, valid loss-2.1947, acc-0.3564, test loss-2.1956, acc-0.3537\n",
      "Iter-9660, train loss-2.1885, acc-0.4000, valid loss-2.1946, acc-0.3562, test loss-2.1955, acc-0.3536\n",
      "Iter-9670, train loss-2.1956, acc-0.3600, valid loss-2.1945, acc-0.3564, test loss-2.1954, acc-0.3540\n",
      "Iter-9680, train loss-2.2012, acc-0.3400, valid loss-2.1944, acc-0.3566, test loss-2.1953, acc-0.3541\n",
      "Iter-9690, train loss-2.2071, acc-0.3600, valid loss-2.1943, acc-0.3566, test loss-2.1951, acc-0.3538\n",
      "Iter-9700, train loss-2.1825, acc-0.3600, valid loss-2.1941, acc-0.3566, test loss-2.1950, acc-0.3537\n",
      "Iter-9710, train loss-2.2086, acc-0.2200, valid loss-2.1940, acc-0.3568, test loss-2.1949, acc-0.3540\n",
      "Iter-9720, train loss-2.1930, acc-0.3400, valid loss-2.1939, acc-0.3572, test loss-2.1948, acc-0.3543\n",
      "Iter-9730, train loss-2.1869, acc-0.3800, valid loss-2.1938, acc-0.3572, test loss-2.1947, acc-0.3545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-2.2040, acc-0.3800, valid loss-2.1937, acc-0.3578, test loss-2.1945, acc-0.3544\n",
      "Iter-9750, train loss-2.2096, acc-0.3000, valid loss-2.1935, acc-0.3576, test loss-2.1944, acc-0.3546\n",
      "Iter-9760, train loss-2.2235, acc-0.2400, valid loss-2.1934, acc-0.3580, test loss-2.1943, acc-0.3549\n",
      "Iter-9770, train loss-2.1967, acc-0.3000, valid loss-2.1933, acc-0.3586, test loss-2.1942, acc-0.3551\n",
      "Iter-9780, train loss-2.1863, acc-0.3800, valid loss-2.1932, acc-0.3582, test loss-2.1941, acc-0.3552\n",
      "Iter-9790, train loss-2.1417, acc-0.4600, valid loss-2.1931, acc-0.3582, test loss-2.1940, acc-0.3553\n",
      "Iter-9800, train loss-2.1942, acc-0.3200, valid loss-2.1930, acc-0.3584, test loss-2.1938, acc-0.3554\n",
      "Iter-9810, train loss-2.2157, acc-0.3000, valid loss-2.1928, acc-0.3582, test loss-2.1937, acc-0.3557\n",
      "Iter-9820, train loss-2.1677, acc-0.4200, valid loss-2.1927, acc-0.3578, test loss-2.1936, acc-0.3560\n",
      "Iter-9830, train loss-2.1983, acc-0.3400, valid loss-2.1926, acc-0.3568, test loss-2.1935, acc-0.3558\n",
      "Iter-9840, train loss-2.1971, acc-0.3400, valid loss-2.1925, acc-0.3580, test loss-2.1934, acc-0.3561\n",
      "Iter-9850, train loss-2.2237, acc-0.2200, valid loss-2.1923, acc-0.3582, test loss-2.1932, acc-0.3560\n",
      "Iter-9860, train loss-2.1718, acc-0.3600, valid loss-2.1922, acc-0.3580, test loss-2.1931, acc-0.3562\n",
      "Iter-9870, train loss-2.2057, acc-0.3000, valid loss-2.1921, acc-0.3582, test loss-2.1930, acc-0.3559\n",
      "Iter-9880, train loss-2.1950, acc-0.3600, valid loss-2.1920, acc-0.3584, test loss-2.1929, acc-0.3558\n",
      "Iter-9890, train loss-2.1992, acc-0.4000, valid loss-2.1919, acc-0.3586, test loss-2.1928, acc-0.3560\n",
      "Iter-9900, train loss-2.1839, acc-0.3600, valid loss-2.1918, acc-0.3586, test loss-2.1927, acc-0.3560\n",
      "Iter-9910, train loss-2.1946, acc-0.3800, valid loss-2.1916, acc-0.3590, test loss-2.1925, acc-0.3558\n",
      "Iter-9920, train loss-2.2021, acc-0.3400, valid loss-2.1915, acc-0.3590, test loss-2.1924, acc-0.3563\n",
      "Iter-9930, train loss-2.2099, acc-0.3200, valid loss-2.1914, acc-0.3590, test loss-2.1923, acc-0.3563\n",
      "Iter-9940, train loss-2.2125, acc-0.3000, valid loss-2.1913, acc-0.3596, test loss-2.1922, acc-0.3565\n",
      "Iter-9950, train loss-2.1968, acc-0.3000, valid loss-2.1912, acc-0.3598, test loss-2.1921, acc-0.3565\n",
      "Iter-9960, train loss-2.1744, acc-0.4000, valid loss-2.1911, acc-0.3598, test loss-2.1920, acc-0.3566\n",
      "Iter-9970, train loss-2.2108, acc-0.2800, valid loss-2.1910, acc-0.3602, test loss-2.1919, acc-0.3567\n",
      "Iter-9980, train loss-2.2140, acc-0.2200, valid loss-2.1908, acc-0.3606, test loss-2.1917, acc-0.3569\n",
      "Iter-9990, train loss-2.2012, acc-0.3200, valid loss-2.1907, acc-0.3608, test loss-2.1916, acc-0.3569\n",
      "Iter-10000, train loss-2.2054, acc-0.3600, valid loss-2.1906, acc-0.3606, test loss-2.1915, acc-0.3570\n",
      "Iter-10010, train loss-2.2006, acc-0.2800, valid loss-2.1905, acc-0.3608, test loss-2.1914, acc-0.3569\n",
      "Iter-10020, train loss-2.2229, acc-0.2400, valid loss-2.1904, acc-0.3606, test loss-2.1913, acc-0.3573\n",
      "Iter-10030, train loss-2.2034, acc-0.3800, valid loss-2.1902, acc-0.3604, test loss-2.1911, acc-0.3576\n",
      "Iter-10040, train loss-2.1966, acc-0.3000, valid loss-2.1901, acc-0.3608, test loss-2.1910, acc-0.3576\n",
      "Iter-10050, train loss-2.1669, acc-0.4800, valid loss-2.1900, acc-0.3602, test loss-2.1909, acc-0.3578\n",
      "Iter-10060, train loss-2.1594, acc-0.3600, valid loss-2.1899, acc-0.3608, test loss-2.1908, acc-0.3576\n",
      "Iter-10070, train loss-2.1654, acc-0.4200, valid loss-2.1898, acc-0.3614, test loss-2.1907, acc-0.3577\n",
      "Iter-10080, train loss-2.2133, acc-0.3400, valid loss-2.1897, acc-0.3606, test loss-2.1905, acc-0.3577\n",
      "Iter-10090, train loss-2.1765, acc-0.4000, valid loss-2.1895, acc-0.3610, test loss-2.1904, acc-0.3577\n",
      "Iter-10100, train loss-2.1718, acc-0.4000, valid loss-2.1894, acc-0.3608, test loss-2.1903, acc-0.3584\n",
      "Iter-10110, train loss-2.1930, acc-0.3200, valid loss-2.1893, acc-0.3610, test loss-2.1902, acc-0.3584\n",
      "Iter-10120, train loss-2.2053, acc-0.3800, valid loss-2.1892, acc-0.3606, test loss-2.1901, acc-0.3585\n",
      "Iter-10130, train loss-2.2121, acc-0.3000, valid loss-2.1891, acc-0.3604, test loss-2.1900, acc-0.3588\n",
      "Iter-10140, train loss-2.2150, acc-0.2600, valid loss-2.1890, acc-0.3608, test loss-2.1898, acc-0.3593\n",
      "Iter-10150, train loss-2.1754, acc-0.4000, valid loss-2.1888, acc-0.3614, test loss-2.1897, acc-0.3595\n",
      "Iter-10160, train loss-2.1902, acc-0.3000, valid loss-2.1887, acc-0.3610, test loss-2.1896, acc-0.3595\n",
      "Iter-10170, train loss-2.2135, acc-0.3200, valid loss-2.1886, acc-0.3610, test loss-2.1895, acc-0.3594\n",
      "Iter-10180, train loss-2.2077, acc-0.4000, valid loss-2.1885, acc-0.3614, test loss-2.1894, acc-0.3596\n",
      "Iter-10190, train loss-2.1889, acc-0.3600, valid loss-2.1884, acc-0.3612, test loss-2.1893, acc-0.3594\n",
      "Iter-10200, train loss-2.1895, acc-0.3600, valid loss-2.1883, acc-0.3614, test loss-2.1892, acc-0.3594\n",
      "Iter-10210, train loss-2.1717, acc-0.3800, valid loss-2.1881, acc-0.3612, test loss-2.1890, acc-0.3595\n",
      "Iter-10220, train loss-2.1745, acc-0.3400, valid loss-2.1880, acc-0.3610, test loss-2.1889, acc-0.3592\n",
      "Iter-10230, train loss-2.2041, acc-0.2400, valid loss-2.1879, acc-0.3614, test loss-2.1888, acc-0.3598\n",
      "Iter-10240, train loss-2.1759, acc-0.3400, valid loss-2.1878, acc-0.3616, test loss-2.1887, acc-0.3597\n",
      "Iter-10250, train loss-2.1553, acc-0.4200, valid loss-2.1877, acc-0.3614, test loss-2.1886, acc-0.3603\n",
      "Iter-10260, train loss-2.1906, acc-0.3600, valid loss-2.1876, acc-0.3616, test loss-2.1885, acc-0.3600\n",
      "Iter-10270, train loss-2.2084, acc-0.3000, valid loss-2.1875, acc-0.3616, test loss-2.1884, acc-0.3603\n",
      "Iter-10280, train loss-2.1994, acc-0.4000, valid loss-2.1873, acc-0.3620, test loss-2.1882, acc-0.3601\n",
      "Iter-10290, train loss-2.1955, acc-0.3400, valid loss-2.1872, acc-0.3618, test loss-2.1881, acc-0.3601\n",
      "Iter-10300, train loss-2.1774, acc-0.3600, valid loss-2.1871, acc-0.3622, test loss-2.1880, acc-0.3603\n",
      "Iter-10310, train loss-2.2021, acc-0.3000, valid loss-2.1870, acc-0.3620, test loss-2.1879, acc-0.3601\n",
      "Iter-10320, train loss-2.1637, acc-0.4400, valid loss-2.1869, acc-0.3616, test loss-2.1878, acc-0.3600\n",
      "Iter-10330, train loss-2.2186, acc-0.3000, valid loss-2.1868, acc-0.3620, test loss-2.1877, acc-0.3601\n",
      "Iter-10340, train loss-2.2049, acc-0.3400, valid loss-2.1866, acc-0.3616, test loss-2.1875, acc-0.3602\n",
      "Iter-10350, train loss-2.1975, acc-0.3000, valid loss-2.1865, acc-0.3620, test loss-2.1874, acc-0.3602\n",
      "Iter-10360, train loss-2.1775, acc-0.4400, valid loss-2.1864, acc-0.3622, test loss-2.1873, acc-0.3604\n",
      "Iter-10370, train loss-2.1840, acc-0.3800, valid loss-2.1863, acc-0.3626, test loss-2.1872, acc-0.3605\n",
      "Iter-10380, train loss-2.2086, acc-0.3000, valid loss-2.1862, acc-0.3630, test loss-2.1871, acc-0.3607\n",
      "Iter-10390, train loss-2.2247, acc-0.2600, valid loss-2.1860, acc-0.3630, test loss-2.1870, acc-0.3609\n",
      "Iter-10400, train loss-2.1801, acc-0.3200, valid loss-2.1859, acc-0.3634, test loss-2.1869, acc-0.3609\n",
      "Iter-10410, train loss-2.1932, acc-0.4200, valid loss-2.1858, acc-0.3634, test loss-2.1867, acc-0.3610\n",
      "Iter-10420, train loss-2.2039, acc-0.3400, valid loss-2.1857, acc-0.3638, test loss-2.1866, acc-0.3610\n",
      "Iter-10430, train loss-2.1748, acc-0.4400, valid loss-2.1856, acc-0.3636, test loss-2.1865, acc-0.3610\n",
      "Iter-10440, train loss-2.1983, acc-0.3600, valid loss-2.1855, acc-0.3638, test loss-2.1864, acc-0.3611\n",
      "Iter-10450, train loss-2.1957, acc-0.3000, valid loss-2.1854, acc-0.3636, test loss-2.1863, acc-0.3611\n",
      "Iter-10460, train loss-2.1703, acc-0.4800, valid loss-2.1852, acc-0.3638, test loss-2.1862, acc-0.3613\n",
      "Iter-10470, train loss-2.1668, acc-0.4400, valid loss-2.1851, acc-0.3646, test loss-2.1860, acc-0.3611\n",
      "Iter-10480, train loss-2.1777, acc-0.2800, valid loss-2.1850, acc-0.3648, test loss-2.1859, acc-0.3608\n",
      "Iter-10490, train loss-2.1923, acc-0.2600, valid loss-2.1849, acc-0.3648, test loss-2.1858, acc-0.3614\n",
      "Iter-10500, train loss-2.1955, acc-0.3400, valid loss-2.1848, acc-0.3650, test loss-2.1857, acc-0.3614\n",
      "Iter-10510, train loss-2.2097, acc-0.3000, valid loss-2.1847, acc-0.3648, test loss-2.1856, acc-0.3616\n",
      "Iter-10520, train loss-2.1842, acc-0.3800, valid loss-2.1845, acc-0.3650, test loss-2.1855, acc-0.3614\n",
      "Iter-10530, train loss-2.1686, acc-0.4200, valid loss-2.1844, acc-0.3650, test loss-2.1853, acc-0.3616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10540, train loss-2.1539, acc-0.4400, valid loss-2.1843, acc-0.3652, test loss-2.1852, acc-0.3618\n",
      "Iter-10550, train loss-2.1786, acc-0.3400, valid loss-2.1842, acc-0.3650, test loss-2.1851, acc-0.3617\n",
      "Iter-10560, train loss-2.2167, acc-0.2600, valid loss-2.1841, acc-0.3646, test loss-2.1850, acc-0.3616\n",
      "Iter-10570, train loss-2.1847, acc-0.3400, valid loss-2.1839, acc-0.3650, test loss-2.1849, acc-0.3621\n",
      "Iter-10580, train loss-2.1894, acc-0.3400, valid loss-2.1838, acc-0.3646, test loss-2.1847, acc-0.3620\n",
      "Iter-10590, train loss-2.2092, acc-0.3000, valid loss-2.1837, acc-0.3650, test loss-2.1846, acc-0.3623\n",
      "Iter-10600, train loss-2.1453, acc-0.4000, valid loss-2.1836, acc-0.3648, test loss-2.1845, acc-0.3622\n",
      "Iter-10610, train loss-2.1708, acc-0.4400, valid loss-2.1835, acc-0.3650, test loss-2.1844, acc-0.3625\n",
      "Iter-10620, train loss-2.1884, acc-0.3400, valid loss-2.1834, acc-0.3656, test loss-2.1843, acc-0.3625\n",
      "Iter-10630, train loss-2.1809, acc-0.4000, valid loss-2.1832, acc-0.3658, test loss-2.1842, acc-0.3624\n",
      "Iter-10640, train loss-2.1637, acc-0.4600, valid loss-2.1831, acc-0.3656, test loss-2.1840, acc-0.3622\n",
      "Iter-10650, train loss-2.1796, acc-0.3800, valid loss-2.1830, acc-0.3660, test loss-2.1839, acc-0.3625\n",
      "Iter-10660, train loss-2.1937, acc-0.3600, valid loss-2.1829, acc-0.3660, test loss-2.1838, acc-0.3627\n",
      "Iter-10670, train loss-2.1713, acc-0.3600, valid loss-2.1828, acc-0.3662, test loss-2.1837, acc-0.3628\n",
      "Iter-10680, train loss-2.1921, acc-0.3000, valid loss-2.1827, acc-0.3662, test loss-2.1836, acc-0.3627\n",
      "Iter-10690, train loss-2.1886, acc-0.3800, valid loss-2.1825, acc-0.3662, test loss-2.1835, acc-0.3629\n",
      "Iter-10700, train loss-2.1616, acc-0.5000, valid loss-2.1824, acc-0.3668, test loss-2.1834, acc-0.3632\n",
      "Iter-10710, train loss-2.2314, acc-0.2400, valid loss-2.1823, acc-0.3668, test loss-2.1832, acc-0.3635\n",
      "Iter-10720, train loss-2.1406, acc-0.4800, valid loss-2.1822, acc-0.3664, test loss-2.1831, acc-0.3635\n",
      "Iter-10730, train loss-2.1844, acc-0.3400, valid loss-2.1821, acc-0.3660, test loss-2.1830, acc-0.3635\n",
      "Iter-10740, train loss-2.1732, acc-0.4200, valid loss-2.1820, acc-0.3658, test loss-2.1829, acc-0.3633\n",
      "Iter-10750, train loss-2.1606, acc-0.4400, valid loss-2.1819, acc-0.3654, test loss-2.1828, acc-0.3631\n",
      "Iter-10760, train loss-2.1837, acc-0.3400, valid loss-2.1817, acc-0.3656, test loss-2.1827, acc-0.3629\n",
      "Iter-10770, train loss-2.1483, acc-0.5000, valid loss-2.1816, acc-0.3660, test loss-2.1825, acc-0.3630\n",
      "Iter-10780, train loss-2.1886, acc-0.3600, valid loss-2.1815, acc-0.3658, test loss-2.1824, acc-0.3629\n",
      "Iter-10790, train loss-2.1849, acc-0.4200, valid loss-2.1814, acc-0.3664, test loss-2.1823, acc-0.3633\n",
      "Iter-10800, train loss-2.1831, acc-0.3400, valid loss-2.1813, acc-0.3670, test loss-2.1822, acc-0.3636\n",
      "Iter-10810, train loss-2.2025, acc-0.3400, valid loss-2.1812, acc-0.3670, test loss-2.1821, acc-0.3635\n",
      "Iter-10820, train loss-2.1653, acc-0.4000, valid loss-2.1811, acc-0.3666, test loss-2.1820, acc-0.3636\n",
      "Iter-10830, train loss-2.2003, acc-0.3000, valid loss-2.1810, acc-0.3662, test loss-2.1819, acc-0.3637\n",
      "Iter-10840, train loss-2.2038, acc-0.3200, valid loss-2.1809, acc-0.3666, test loss-2.1818, acc-0.3639\n",
      "Iter-10850, train loss-2.2104, acc-0.3000, valid loss-2.1808, acc-0.3668, test loss-2.1817, acc-0.3641\n",
      "Iter-10860, train loss-2.1816, acc-0.3400, valid loss-2.1806, acc-0.3668, test loss-2.1815, acc-0.3641\n",
      "Iter-10870, train loss-2.1637, acc-0.3200, valid loss-2.1805, acc-0.3668, test loss-2.1814, acc-0.3642\n",
      "Iter-10880, train loss-2.1840, acc-0.3800, valid loss-2.1804, acc-0.3664, test loss-2.1813, acc-0.3640\n",
      "Iter-10890, train loss-2.2007, acc-0.3400, valid loss-2.1803, acc-0.3670, test loss-2.1812, acc-0.3643\n",
      "Iter-10900, train loss-2.1855, acc-0.3400, valid loss-2.1802, acc-0.3668, test loss-2.1811, acc-0.3647\n",
      "Iter-10910, train loss-2.1680, acc-0.3800, valid loss-2.1801, acc-0.3664, test loss-2.1810, acc-0.3647\n",
      "Iter-10920, train loss-2.2052, acc-0.3600, valid loss-2.1800, acc-0.3668, test loss-2.1809, acc-0.3648\n",
      "Iter-10930, train loss-2.1612, acc-0.3800, valid loss-2.1798, acc-0.3674, test loss-2.1807, acc-0.3648\n",
      "Iter-10940, train loss-2.1644, acc-0.4000, valid loss-2.1797, acc-0.3672, test loss-2.1806, acc-0.3651\n",
      "Iter-10950, train loss-2.1698, acc-0.4600, valid loss-2.1796, acc-0.3674, test loss-2.1805, acc-0.3651\n",
      "Iter-10960, train loss-2.2246, acc-0.2400, valid loss-2.1795, acc-0.3672, test loss-2.1804, acc-0.3652\n",
      "Iter-10970, train loss-2.1761, acc-0.4200, valid loss-2.1794, acc-0.3678, test loss-2.1803, acc-0.3652\n",
      "Iter-10980, train loss-2.1930, acc-0.2600, valid loss-2.1793, acc-0.3676, test loss-2.1802, acc-0.3652\n",
      "Iter-10990, train loss-2.1953, acc-0.3400, valid loss-2.1792, acc-0.3678, test loss-2.1800, acc-0.3651\n",
      "Iter-11000, train loss-2.1741, acc-0.3400, valid loss-2.1790, acc-0.3678, test loss-2.1799, acc-0.3654\n",
      "Iter-11010, train loss-2.2081, acc-0.3400, valid loss-2.1789, acc-0.3680, test loss-2.1798, acc-0.3651\n",
      "Iter-11020, train loss-2.1774, acc-0.4200, valid loss-2.1788, acc-0.3680, test loss-2.1797, acc-0.3653\n",
      "Iter-11030, train loss-2.1656, acc-0.3800, valid loss-2.1787, acc-0.3672, test loss-2.1796, acc-0.3650\n",
      "Iter-11040, train loss-2.1716, acc-0.3600, valid loss-2.1786, acc-0.3674, test loss-2.1795, acc-0.3650\n",
      "Iter-11050, train loss-2.1596, acc-0.4200, valid loss-2.1785, acc-0.3672, test loss-2.1794, acc-0.3648\n",
      "Iter-11060, train loss-2.1304, acc-0.5200, valid loss-2.1784, acc-0.3674, test loss-2.1792, acc-0.3649\n",
      "Iter-11070, train loss-2.1951, acc-0.4200, valid loss-2.1782, acc-0.3674, test loss-2.1791, acc-0.3651\n",
      "Iter-11080, train loss-2.1793, acc-0.4200, valid loss-2.1781, acc-0.3674, test loss-2.1790, acc-0.3653\n",
      "Iter-11090, train loss-2.1459, acc-0.3800, valid loss-2.1780, acc-0.3674, test loss-2.1789, acc-0.3653\n",
      "Iter-11100, train loss-2.1928, acc-0.3200, valid loss-2.1779, acc-0.3674, test loss-2.1788, acc-0.3655\n",
      "Iter-11110, train loss-2.1759, acc-0.3800, valid loss-2.1778, acc-0.3676, test loss-2.1787, acc-0.3657\n",
      "Iter-11120, train loss-2.1807, acc-0.3400, valid loss-2.1777, acc-0.3676, test loss-2.1786, acc-0.3657\n",
      "Iter-11130, train loss-2.1854, acc-0.3200, valid loss-2.1776, acc-0.3678, test loss-2.1784, acc-0.3662\n",
      "Iter-11140, train loss-2.1822, acc-0.3400, valid loss-2.1774, acc-0.3678, test loss-2.1783, acc-0.3663\n",
      "Iter-11150, train loss-2.1825, acc-0.4000, valid loss-2.1773, acc-0.3678, test loss-2.1782, acc-0.3665\n",
      "Iter-11160, train loss-2.1632, acc-0.3800, valid loss-2.1772, acc-0.3676, test loss-2.1781, acc-0.3664\n",
      "Iter-11170, train loss-2.2076, acc-0.2600, valid loss-2.1771, acc-0.3676, test loss-2.1780, acc-0.3665\n",
      "Iter-11180, train loss-2.1887, acc-0.3400, valid loss-2.1770, acc-0.3678, test loss-2.1779, acc-0.3665\n",
      "Iter-11190, train loss-2.1838, acc-0.3600, valid loss-2.1769, acc-0.3676, test loss-2.1778, acc-0.3668\n",
      "Iter-11200, train loss-2.2106, acc-0.3000, valid loss-2.1768, acc-0.3678, test loss-2.1776, acc-0.3669\n",
      "Iter-11210, train loss-2.1700, acc-0.3400, valid loss-2.1767, acc-0.3678, test loss-2.1775, acc-0.3670\n",
      "Iter-11220, train loss-2.1663, acc-0.3600, valid loss-2.1765, acc-0.3678, test loss-2.1774, acc-0.3670\n",
      "Iter-11230, train loss-2.1896, acc-0.3000, valid loss-2.1764, acc-0.3678, test loss-2.1773, acc-0.3671\n",
      "Iter-11240, train loss-2.1557, acc-0.5000, valid loss-2.1763, acc-0.3678, test loss-2.1772, acc-0.3670\n",
      "Iter-11250, train loss-2.1735, acc-0.4000, valid loss-2.1762, acc-0.3678, test loss-2.1771, acc-0.3673\n",
      "Iter-11260, train loss-2.1790, acc-0.4000, valid loss-2.1761, acc-0.3676, test loss-2.1770, acc-0.3672\n",
      "Iter-11270, train loss-2.2086, acc-0.2400, valid loss-2.1760, acc-0.3682, test loss-2.1768, acc-0.3669\n",
      "Iter-11280, train loss-2.1855, acc-0.3800, valid loss-2.1759, acc-0.3680, test loss-2.1767, acc-0.3670\n",
      "Iter-11290, train loss-2.1792, acc-0.3200, valid loss-2.1757, acc-0.3682, test loss-2.1766, acc-0.3671\n",
      "Iter-11300, train loss-2.1806, acc-0.3600, valid loss-2.1756, acc-0.3682, test loss-2.1765, acc-0.3673\n",
      "Iter-11310, train loss-2.1764, acc-0.3400, valid loss-2.1755, acc-0.3688, test loss-2.1764, acc-0.3673\n",
      "Iter-11320, train loss-2.1864, acc-0.3400, valid loss-2.1754, acc-0.3686, test loss-2.1763, acc-0.3676\n",
      "Iter-11330, train loss-2.1698, acc-0.4000, valid loss-2.1753, acc-0.3686, test loss-2.1762, acc-0.3679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11340, train loss-2.2087, acc-0.3000, valid loss-2.1752, acc-0.3688, test loss-2.1761, acc-0.3679\n",
      "Iter-11350, train loss-2.1872, acc-0.3200, valid loss-2.1751, acc-0.3688, test loss-2.1760, acc-0.3676\n",
      "Iter-11360, train loss-2.1654, acc-0.3200, valid loss-2.1750, acc-0.3692, test loss-2.1758, acc-0.3678\n",
      "Iter-11370, train loss-2.1674, acc-0.3600, valid loss-2.1748, acc-0.3686, test loss-2.1757, acc-0.3677\n",
      "Iter-11380, train loss-2.1914, acc-0.3200, valid loss-2.1747, acc-0.3688, test loss-2.1756, acc-0.3677\n",
      "Iter-11390, train loss-2.1718, acc-0.4000, valid loss-2.1746, acc-0.3688, test loss-2.1755, acc-0.3673\n",
      "Iter-11400, train loss-2.2104, acc-0.3400, valid loss-2.1745, acc-0.3690, test loss-2.1754, acc-0.3674\n",
      "Iter-11410, train loss-2.1724, acc-0.3800, valid loss-2.1744, acc-0.3692, test loss-2.1753, acc-0.3673\n",
      "Iter-11420, train loss-2.1604, acc-0.4000, valid loss-2.1743, acc-0.3692, test loss-2.1751, acc-0.3672\n",
      "Iter-11430, train loss-2.1446, acc-0.4000, valid loss-2.1742, acc-0.3694, test loss-2.1750, acc-0.3676\n",
      "Iter-11440, train loss-2.1784, acc-0.3400, valid loss-2.1741, acc-0.3696, test loss-2.1749, acc-0.3675\n",
      "Iter-11450, train loss-2.1639, acc-0.4600, valid loss-2.1739, acc-0.3694, test loss-2.1748, acc-0.3673\n",
      "Iter-11460, train loss-2.1766, acc-0.3800, valid loss-2.1738, acc-0.3692, test loss-2.1747, acc-0.3672\n",
      "Iter-11470, train loss-2.1434, acc-0.4200, valid loss-2.1737, acc-0.3692, test loss-2.1746, acc-0.3675\n",
      "Iter-11480, train loss-2.1946, acc-0.2800, valid loss-2.1736, acc-0.3692, test loss-2.1745, acc-0.3672\n",
      "Iter-11490, train loss-2.2016, acc-0.2600, valid loss-2.1735, acc-0.3696, test loss-2.1744, acc-0.3674\n",
      "Iter-11500, train loss-2.1340, acc-0.4800, valid loss-2.1734, acc-0.3702, test loss-2.1742, acc-0.3678\n",
      "Iter-11510, train loss-2.1735, acc-0.3800, valid loss-2.1733, acc-0.3702, test loss-2.1741, acc-0.3679\n",
      "Iter-11520, train loss-2.1686, acc-0.4200, valid loss-2.1731, acc-0.3702, test loss-2.1740, acc-0.3681\n",
      "Iter-11530, train loss-2.1918, acc-0.3400, valid loss-2.1730, acc-0.3704, test loss-2.1739, acc-0.3684\n",
      "Iter-11540, train loss-2.2087, acc-0.2600, valid loss-2.1729, acc-0.3706, test loss-2.1738, acc-0.3682\n",
      "Iter-11550, train loss-2.1682, acc-0.3400, valid loss-2.1728, acc-0.3708, test loss-2.1737, acc-0.3681\n",
      "Iter-11560, train loss-2.1919, acc-0.3600, valid loss-2.1727, acc-0.3706, test loss-2.1735, acc-0.3681\n",
      "Iter-11570, train loss-2.1921, acc-0.3400, valid loss-2.1726, acc-0.3706, test loss-2.1734, acc-0.3682\n",
      "Iter-11580, train loss-2.1687, acc-0.4200, valid loss-2.1724, acc-0.3708, test loss-2.1733, acc-0.3683\n",
      "Iter-11590, train loss-2.1866, acc-0.3800, valid loss-2.1723, acc-0.3708, test loss-2.1732, acc-0.3682\n",
      "Iter-11600, train loss-2.2062, acc-0.3000, valid loss-2.1722, acc-0.3712, test loss-2.1731, acc-0.3685\n",
      "Iter-11610, train loss-2.1824, acc-0.3800, valid loss-2.1721, acc-0.3710, test loss-2.1730, acc-0.3686\n",
      "Iter-11620, train loss-2.1901, acc-0.2800, valid loss-2.1720, acc-0.3712, test loss-2.1729, acc-0.3688\n",
      "Iter-11630, train loss-2.1889, acc-0.3400, valid loss-2.1719, acc-0.3712, test loss-2.1728, acc-0.3691\n",
      "Iter-11640, train loss-2.1968, acc-0.3000, valid loss-2.1718, acc-0.3718, test loss-2.1726, acc-0.3692\n",
      "Iter-11650, train loss-2.1666, acc-0.4000, valid loss-2.1717, acc-0.3716, test loss-2.1725, acc-0.3694\n",
      "Iter-11660, train loss-2.1571, acc-0.4600, valid loss-2.1715, acc-0.3716, test loss-2.1724, acc-0.3695\n",
      "Iter-11670, train loss-2.1902, acc-0.3400, valid loss-2.1714, acc-0.3716, test loss-2.1723, acc-0.3695\n",
      "Iter-11680, train loss-2.1793, acc-0.3600, valid loss-2.1713, acc-0.3718, test loss-2.1722, acc-0.3695\n",
      "Iter-11690, train loss-2.1555, acc-0.3400, valid loss-2.1712, acc-0.3720, test loss-2.1721, acc-0.3695\n",
      "Iter-11700, train loss-2.1719, acc-0.4200, valid loss-2.1711, acc-0.3724, test loss-2.1720, acc-0.3693\n",
      "Iter-11710, train loss-2.2185, acc-0.3400, valid loss-2.1710, acc-0.3724, test loss-2.1719, acc-0.3696\n",
      "Iter-11720, train loss-2.2156, acc-0.2800, valid loss-2.1709, acc-0.3726, test loss-2.1718, acc-0.3697\n",
      "Iter-11730, train loss-2.2050, acc-0.2800, valid loss-2.1708, acc-0.3722, test loss-2.1717, acc-0.3697\n",
      "Iter-11740, train loss-2.1833, acc-0.4200, valid loss-2.1707, acc-0.3726, test loss-2.1715, acc-0.3698\n",
      "Iter-11750, train loss-2.1601, acc-0.3200, valid loss-2.1706, acc-0.3724, test loss-2.1714, acc-0.3699\n",
      "Iter-11760, train loss-2.1730, acc-0.2800, valid loss-2.1705, acc-0.3724, test loss-2.1713, acc-0.3698\n",
      "Iter-11770, train loss-2.1738, acc-0.3600, valid loss-2.1703, acc-0.3726, test loss-2.1712, acc-0.3700\n",
      "Iter-11780, train loss-2.1789, acc-0.3000, valid loss-2.1702, acc-0.3730, test loss-2.1711, acc-0.3698\n",
      "Iter-11790, train loss-2.1624, acc-0.4200, valid loss-2.1701, acc-0.3728, test loss-2.1710, acc-0.3699\n",
      "Iter-11800, train loss-2.1750, acc-0.3200, valid loss-2.1700, acc-0.3726, test loss-2.1709, acc-0.3697\n",
      "Iter-11810, train loss-2.1663, acc-0.4400, valid loss-2.1699, acc-0.3728, test loss-2.1707, acc-0.3698\n",
      "Iter-11820, train loss-2.2191, acc-0.2800, valid loss-2.1698, acc-0.3732, test loss-2.1706, acc-0.3700\n",
      "Iter-11830, train loss-2.2051, acc-0.2600, valid loss-2.1697, acc-0.3736, test loss-2.1705, acc-0.3705\n",
      "Iter-11840, train loss-2.1587, acc-0.3200, valid loss-2.1695, acc-0.3736, test loss-2.1704, acc-0.3703\n",
      "Iter-11850, train loss-2.2185, acc-0.2800, valid loss-2.1694, acc-0.3744, test loss-2.1703, acc-0.3704\n",
      "Iter-11860, train loss-2.1840, acc-0.3800, valid loss-2.1693, acc-0.3742, test loss-2.1702, acc-0.3705\n",
      "Iter-11870, train loss-2.1586, acc-0.3600, valid loss-2.1692, acc-0.3742, test loss-2.1701, acc-0.3708\n",
      "Iter-11880, train loss-2.1983, acc-0.2800, valid loss-2.1691, acc-0.3742, test loss-2.1699, acc-0.3707\n",
      "Iter-11890, train loss-2.2243, acc-0.2800, valid loss-2.1690, acc-0.3742, test loss-2.1698, acc-0.3709\n",
      "Iter-11900, train loss-2.1501, acc-0.3800, valid loss-2.1689, acc-0.3744, test loss-2.1697, acc-0.3708\n",
      "Iter-11910, train loss-2.1578, acc-0.3600, valid loss-2.1688, acc-0.3750, test loss-2.1696, acc-0.3708\n",
      "Iter-11920, train loss-2.1860, acc-0.3400, valid loss-2.1687, acc-0.3754, test loss-2.1695, acc-0.3709\n",
      "Iter-11930, train loss-2.1740, acc-0.3200, valid loss-2.1686, acc-0.3754, test loss-2.1694, acc-0.3710\n",
      "Iter-11940, train loss-2.1499, acc-0.5200, valid loss-2.1684, acc-0.3752, test loss-2.1693, acc-0.3711\n",
      "Iter-11950, train loss-2.1425, acc-0.4200, valid loss-2.1683, acc-0.3752, test loss-2.1692, acc-0.3712\n",
      "Iter-11960, train loss-2.1825, acc-0.4200, valid loss-2.1682, acc-0.3752, test loss-2.1690, acc-0.3711\n",
      "Iter-11970, train loss-2.2028, acc-0.3800, valid loss-2.1681, acc-0.3752, test loss-2.1689, acc-0.3715\n",
      "Iter-11980, train loss-2.1761, acc-0.3800, valid loss-2.1680, acc-0.3752, test loss-2.1688, acc-0.3716\n",
      "Iter-11990, train loss-2.1765, acc-0.3000, valid loss-2.1679, acc-0.3754, test loss-2.1687, acc-0.3716\n",
      "Iter-12000, train loss-2.1852, acc-0.3200, valid loss-2.1678, acc-0.3754, test loss-2.1686, acc-0.3716\n",
      "Iter-12010, train loss-2.1869, acc-0.3600, valid loss-2.1677, acc-0.3754, test loss-2.1685, acc-0.3715\n",
      "Iter-12020, train loss-2.1875, acc-0.2800, valid loss-2.1675, acc-0.3756, test loss-2.1684, acc-0.3718\n",
      "Iter-12030, train loss-2.1502, acc-0.4400, valid loss-2.1674, acc-0.3756, test loss-2.1683, acc-0.3720\n",
      "Iter-12040, train loss-2.1607, acc-0.3400, valid loss-2.1673, acc-0.3758, test loss-2.1682, acc-0.3720\n",
      "Iter-12050, train loss-2.1730, acc-0.3600, valid loss-2.1672, acc-0.3758, test loss-2.1681, acc-0.3721\n",
      "Iter-12060, train loss-2.1655, acc-0.4000, valid loss-2.1671, acc-0.3760, test loss-2.1679, acc-0.3719\n",
      "Iter-12070, train loss-2.0989, acc-0.4200, valid loss-2.1670, acc-0.3760, test loss-2.1678, acc-0.3721\n",
      "Iter-12080, train loss-2.1820, acc-0.3600, valid loss-2.1669, acc-0.3758, test loss-2.1677, acc-0.3721\n",
      "Iter-12090, train loss-2.1435, acc-0.4000, valid loss-2.1668, acc-0.3756, test loss-2.1676, acc-0.3721\n",
      "Iter-12100, train loss-2.1962, acc-0.2800, valid loss-2.1667, acc-0.3756, test loss-2.1675, acc-0.3724\n",
      "Iter-12110, train loss-2.1746, acc-0.3400, valid loss-2.1666, acc-0.3760, test loss-2.1674, acc-0.3725\n",
      "Iter-12120, train loss-2.2129, acc-0.2600, valid loss-2.1665, acc-0.3760, test loss-2.1673, acc-0.3730\n",
      "Iter-12130, train loss-2.2010, acc-0.2000, valid loss-2.1664, acc-0.3758, test loss-2.1672, acc-0.3728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-12140, train loss-2.1517, acc-0.4400, valid loss-2.1662, acc-0.3754, test loss-2.1671, acc-0.3729\n",
      "Iter-12150, train loss-2.1487, acc-0.4000, valid loss-2.1661, acc-0.3758, test loss-2.1670, acc-0.3732\n",
      "Iter-12160, train loss-2.1589, acc-0.4000, valid loss-2.1660, acc-0.3760, test loss-2.1668, acc-0.3729\n",
      "Iter-12170, train loss-2.1802, acc-0.3800, valid loss-2.1659, acc-0.3760, test loss-2.1667, acc-0.3728\n",
      "Iter-12180, train loss-2.2041, acc-0.2800, valid loss-2.1658, acc-0.3760, test loss-2.1666, acc-0.3729\n",
      "Iter-12190, train loss-2.1759, acc-0.4200, valid loss-2.1657, acc-0.3758, test loss-2.1665, acc-0.3733\n",
      "Iter-12200, train loss-2.1792, acc-0.4200, valid loss-2.1656, acc-0.3760, test loss-2.1664, acc-0.3732\n",
      "Iter-12210, train loss-2.1193, acc-0.4400, valid loss-2.1654, acc-0.3760, test loss-2.1663, acc-0.3732\n",
      "Iter-12220, train loss-2.1619, acc-0.3600, valid loss-2.1653, acc-0.3760, test loss-2.1662, acc-0.3732\n",
      "Iter-12230, train loss-2.1671, acc-0.3800, valid loss-2.1652, acc-0.3766, test loss-2.1661, acc-0.3732\n",
      "Iter-12240, train loss-2.2043, acc-0.2800, valid loss-2.1651, acc-0.3764, test loss-2.1660, acc-0.3735\n",
      "Iter-12250, train loss-2.1841, acc-0.3800, valid loss-2.1650, acc-0.3764, test loss-2.1658, acc-0.3738\n",
      "Iter-12260, train loss-2.1923, acc-0.3000, valid loss-2.1649, acc-0.3764, test loss-2.1657, acc-0.3738\n",
      "Iter-12270, train loss-2.1498, acc-0.4400, valid loss-2.1648, acc-0.3762, test loss-2.1656, acc-0.3737\n",
      "Iter-12280, train loss-2.1482, acc-0.5200, valid loss-2.1647, acc-0.3762, test loss-2.1655, acc-0.3736\n",
      "Iter-12290, train loss-2.1633, acc-0.4000, valid loss-2.1645, acc-0.3758, test loss-2.1654, acc-0.3738\n",
      "Iter-12300, train loss-2.1707, acc-0.3400, valid loss-2.1644, acc-0.3758, test loss-2.1653, acc-0.3738\n",
      "Iter-12310, train loss-2.1612, acc-0.4600, valid loss-2.1643, acc-0.3766, test loss-2.1651, acc-0.3734\n",
      "Iter-12320, train loss-2.1741, acc-0.3600, valid loss-2.1642, acc-0.3762, test loss-2.1650, acc-0.3735\n",
      "Iter-12330, train loss-2.1765, acc-0.4600, valid loss-2.1641, acc-0.3764, test loss-2.1649, acc-0.3734\n",
      "Iter-12340, train loss-2.1709, acc-0.3600, valid loss-2.1640, acc-0.3762, test loss-2.1648, acc-0.3734\n",
      "Iter-12350, train loss-2.1476, acc-0.3800, valid loss-2.1639, acc-0.3760, test loss-2.1647, acc-0.3736\n",
      "Iter-12360, train loss-2.1613, acc-0.4400, valid loss-2.1637, acc-0.3758, test loss-2.1646, acc-0.3736\n",
      "Iter-12370, train loss-2.1577, acc-0.3400, valid loss-2.1636, acc-0.3762, test loss-2.1645, acc-0.3737\n",
      "Iter-12380, train loss-2.1868, acc-0.3000, valid loss-2.1635, acc-0.3760, test loss-2.1644, acc-0.3738\n",
      "Iter-12390, train loss-2.2199, acc-0.2200, valid loss-2.1634, acc-0.3760, test loss-2.1643, acc-0.3737\n",
      "Iter-12400, train loss-2.1547, acc-0.3000, valid loss-2.1633, acc-0.3760, test loss-2.1642, acc-0.3739\n",
      "Iter-12410, train loss-2.1715, acc-0.4600, valid loss-2.1632, acc-0.3762, test loss-2.1640, acc-0.3742\n",
      "Iter-12420, train loss-2.1339, acc-0.4000, valid loss-2.1631, acc-0.3764, test loss-2.1639, acc-0.3746\n",
      "Iter-12430, train loss-2.2222, acc-0.2200, valid loss-2.1630, acc-0.3764, test loss-2.1638, acc-0.3746\n",
      "Iter-12440, train loss-2.1843, acc-0.3200, valid loss-2.1628, acc-0.3764, test loss-2.1637, acc-0.3749\n",
      "Iter-12450, train loss-2.1849, acc-0.3400, valid loss-2.1627, acc-0.3766, test loss-2.1636, acc-0.3749\n",
      "Iter-12460, train loss-2.1724, acc-0.3600, valid loss-2.1626, acc-0.3766, test loss-2.1635, acc-0.3749\n",
      "Iter-12470, train loss-2.1637, acc-0.4600, valid loss-2.1625, acc-0.3762, test loss-2.1634, acc-0.3747\n",
      "Iter-12480, train loss-2.1645, acc-0.4000, valid loss-2.1624, acc-0.3764, test loss-2.1632, acc-0.3748\n",
      "Iter-12490, train loss-2.1871, acc-0.3200, valid loss-2.1623, acc-0.3760, test loss-2.1631, acc-0.3749\n",
      "Iter-12500, train loss-2.1431, acc-0.4200, valid loss-2.1622, acc-0.3760, test loss-2.1630, acc-0.3747\n",
      "Iter-12510, train loss-2.1933, acc-0.3000, valid loss-2.1621, acc-0.3762, test loss-2.1629, acc-0.3747\n",
      "Iter-12520, train loss-2.1696, acc-0.3400, valid loss-2.1619, acc-0.3760, test loss-2.1628, acc-0.3751\n",
      "Iter-12530, train loss-2.1423, acc-0.4600, valid loss-2.1618, acc-0.3758, test loss-2.1627, acc-0.3749\n",
      "Iter-12540, train loss-2.1481, acc-0.4400, valid loss-2.1617, acc-0.3760, test loss-2.1626, acc-0.3751\n",
      "Iter-12550, train loss-2.1763, acc-0.3200, valid loss-2.1616, acc-0.3764, test loss-2.1625, acc-0.3751\n",
      "Iter-12560, train loss-2.2016, acc-0.2600, valid loss-2.1615, acc-0.3764, test loss-2.1623, acc-0.3752\n",
      "Iter-12570, train loss-2.1725, acc-0.3400, valid loss-2.1614, acc-0.3766, test loss-2.1622, acc-0.3751\n",
      "Iter-12580, train loss-2.1895, acc-0.3000, valid loss-2.1613, acc-0.3766, test loss-2.1621, acc-0.3751\n",
      "Iter-12590, train loss-2.2089, acc-0.3200, valid loss-2.1612, acc-0.3762, test loss-2.1620, acc-0.3755\n",
      "Iter-12600, train loss-2.1464, acc-0.4000, valid loss-2.1610, acc-0.3760, test loss-2.1619, acc-0.3756\n",
      "Iter-12610, train loss-2.1455, acc-0.3600, valid loss-2.1609, acc-0.3766, test loss-2.1618, acc-0.3758\n",
      "Iter-12620, train loss-2.2295, acc-0.3000, valid loss-2.1608, acc-0.3766, test loss-2.1617, acc-0.3758\n",
      "Iter-12630, train loss-2.1719, acc-0.4400, valid loss-2.1607, acc-0.3766, test loss-2.1616, acc-0.3756\n",
      "Iter-12640, train loss-2.1781, acc-0.3800, valid loss-2.1606, acc-0.3770, test loss-2.1615, acc-0.3760\n",
      "Iter-12650, train loss-2.1642, acc-0.3200, valid loss-2.1605, acc-0.3766, test loss-2.1614, acc-0.3760\n",
      "Iter-12660, train loss-2.1627, acc-0.3800, valid loss-2.1604, acc-0.3766, test loss-2.1613, acc-0.3760\n",
      "Iter-12670, train loss-2.1767, acc-0.3600, valid loss-2.1603, acc-0.3762, test loss-2.1611, acc-0.3760\n",
      "Iter-12680, train loss-2.1625, acc-0.3600, valid loss-2.1602, acc-0.3768, test loss-2.1610, acc-0.3762\n",
      "Iter-12690, train loss-2.1730, acc-0.2600, valid loss-2.1601, acc-0.3768, test loss-2.1609, acc-0.3760\n",
      "Iter-12700, train loss-2.1666, acc-0.3600, valid loss-2.1600, acc-0.3770, test loss-2.1608, acc-0.3760\n",
      "Iter-12710, train loss-2.1651, acc-0.3000, valid loss-2.1598, acc-0.3768, test loss-2.1607, acc-0.3762\n",
      "Iter-12720, train loss-2.1504, acc-0.4000, valid loss-2.1597, acc-0.3772, test loss-2.1606, acc-0.3762\n",
      "Iter-12730, train loss-2.1781, acc-0.3600, valid loss-2.1596, acc-0.3770, test loss-2.1605, acc-0.3761\n",
      "Iter-12740, train loss-2.1937, acc-0.2600, valid loss-2.1595, acc-0.3770, test loss-2.1604, acc-0.3759\n",
      "Iter-12750, train loss-2.1760, acc-0.3600, valid loss-2.1594, acc-0.3774, test loss-2.1603, acc-0.3760\n",
      "Iter-12760, train loss-2.0980, acc-0.5000, valid loss-2.1593, acc-0.3778, test loss-2.1601, acc-0.3761\n",
      "Iter-12770, train loss-2.1460, acc-0.4800, valid loss-2.1592, acc-0.3774, test loss-2.1600, acc-0.3761\n",
      "Iter-12780, train loss-2.1748, acc-0.4000, valid loss-2.1591, acc-0.3776, test loss-2.1599, acc-0.3762\n",
      "Iter-12790, train loss-2.2234, acc-0.3200, valid loss-2.1590, acc-0.3782, test loss-2.1598, acc-0.3760\n",
      "Iter-12800, train loss-2.1166, acc-0.5400, valid loss-2.1588, acc-0.3784, test loss-2.1597, acc-0.3761\n",
      "Iter-12810, train loss-2.1443, acc-0.4000, valid loss-2.1587, acc-0.3782, test loss-2.1596, acc-0.3761\n",
      "Iter-12820, train loss-2.1486, acc-0.4000, valid loss-2.1586, acc-0.3788, test loss-2.1595, acc-0.3760\n",
      "Iter-12830, train loss-2.1418, acc-0.4200, valid loss-2.1585, acc-0.3784, test loss-2.1594, acc-0.3762\n",
      "Iter-12840, train loss-2.1432, acc-0.4200, valid loss-2.1584, acc-0.3790, test loss-2.1592, acc-0.3762\n",
      "Iter-12850, train loss-2.1870, acc-0.3200, valid loss-2.1583, acc-0.3786, test loss-2.1591, acc-0.3759\n",
      "Iter-12860, train loss-2.1637, acc-0.3400, valid loss-2.1582, acc-0.3788, test loss-2.1590, acc-0.3762\n",
      "Iter-12870, train loss-2.1854, acc-0.3200, valid loss-2.1581, acc-0.3792, test loss-2.1589, acc-0.3764\n",
      "Iter-12880, train loss-2.1396, acc-0.3600, valid loss-2.1579, acc-0.3794, test loss-2.1588, acc-0.3767\n",
      "Iter-12890, train loss-2.1488, acc-0.3800, valid loss-2.1578, acc-0.3794, test loss-2.1587, acc-0.3768\n",
      "Iter-12900, train loss-2.1599, acc-0.3400, valid loss-2.1577, acc-0.3798, test loss-2.1586, acc-0.3765\n",
      "Iter-12910, train loss-2.1819, acc-0.3200, valid loss-2.1576, acc-0.3802, test loss-2.1585, acc-0.3767\n",
      "Iter-12920, train loss-2.1985, acc-0.2800, valid loss-2.1575, acc-0.3804, test loss-2.1583, acc-0.3772\n",
      "Iter-12930, train loss-2.1315, acc-0.4800, valid loss-2.1574, acc-0.3808, test loss-2.1582, acc-0.3775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-12940, train loss-2.1657, acc-0.4400, valid loss-2.1573, acc-0.3812, test loss-2.1581, acc-0.3776\n",
      "Iter-12950, train loss-2.1514, acc-0.3400, valid loss-2.1572, acc-0.3812, test loss-2.1580, acc-0.3778\n",
      "Iter-12960, train loss-2.1384, acc-0.4000, valid loss-2.1570, acc-0.3806, test loss-2.1579, acc-0.3779\n",
      "Iter-12970, train loss-2.1668, acc-0.3600, valid loss-2.1569, acc-0.3806, test loss-2.1578, acc-0.3780\n",
      "Iter-12980, train loss-2.1416, acc-0.3800, valid loss-2.1568, acc-0.3812, test loss-2.1577, acc-0.3778\n",
      "Iter-12990, train loss-2.1515, acc-0.3400, valid loss-2.1567, acc-0.3808, test loss-2.1576, acc-0.3779\n",
      "Iter-13000, train loss-2.1502, acc-0.4600, valid loss-2.1566, acc-0.3812, test loss-2.1575, acc-0.3779\n",
      "Iter-13010, train loss-2.1722, acc-0.2800, valid loss-2.1565, acc-0.3814, test loss-2.1574, acc-0.3778\n",
      "Iter-13020, train loss-2.1496, acc-0.3600, valid loss-2.1564, acc-0.3814, test loss-2.1572, acc-0.3780\n",
      "Iter-13030, train loss-2.1560, acc-0.3400, valid loss-2.1563, acc-0.3814, test loss-2.1571, acc-0.3782\n",
      "Iter-13040, train loss-2.1329, acc-0.3600, valid loss-2.1562, acc-0.3814, test loss-2.1570, acc-0.3784\n",
      "Iter-13050, train loss-2.1597, acc-0.4000, valid loss-2.1560, acc-0.3816, test loss-2.1569, acc-0.3786\n",
      "Iter-13060, train loss-2.1516, acc-0.4400, valid loss-2.1559, acc-0.3822, test loss-2.1568, acc-0.3786\n",
      "Iter-13070, train loss-2.1389, acc-0.4200, valid loss-2.1558, acc-0.3820, test loss-2.1567, acc-0.3786\n",
      "Iter-13080, train loss-2.1696, acc-0.3400, valid loss-2.1557, acc-0.3822, test loss-2.1566, acc-0.3786\n",
      "Iter-13090, train loss-2.1696, acc-0.3800, valid loss-2.1556, acc-0.3820, test loss-2.1565, acc-0.3789\n",
      "Iter-13100, train loss-2.0909, acc-0.4800, valid loss-2.1555, acc-0.3818, test loss-2.1564, acc-0.3793\n",
      "Iter-13110, train loss-2.1916, acc-0.4800, valid loss-2.1554, acc-0.3818, test loss-2.1563, acc-0.3794\n",
      "Iter-13120, train loss-2.1347, acc-0.3800, valid loss-2.1553, acc-0.3820, test loss-2.1562, acc-0.3795\n",
      "Iter-13130, train loss-2.1952, acc-0.2800, valid loss-2.1552, acc-0.3820, test loss-2.1560, acc-0.3795\n",
      "Iter-13140, train loss-2.1717, acc-0.3600, valid loss-2.1551, acc-0.3822, test loss-2.1559, acc-0.3794\n",
      "Iter-13150, train loss-2.1421, acc-0.3800, valid loss-2.1550, acc-0.3828, test loss-2.1558, acc-0.3794\n",
      "Iter-13160, train loss-2.1428, acc-0.3800, valid loss-2.1549, acc-0.3830, test loss-2.1557, acc-0.3793\n",
      "Iter-13170, train loss-2.1740, acc-0.3600, valid loss-2.1548, acc-0.3832, test loss-2.1556, acc-0.3796\n",
      "Iter-13180, train loss-2.1307, acc-0.4400, valid loss-2.1547, acc-0.3832, test loss-2.1555, acc-0.3796\n",
      "Iter-13190, train loss-2.1724, acc-0.4000, valid loss-2.1545, acc-0.3832, test loss-2.1554, acc-0.3795\n",
      "Iter-13200, train loss-2.1144, acc-0.4800, valid loss-2.1544, acc-0.3832, test loss-2.1553, acc-0.3794\n",
      "Iter-13210, train loss-2.1853, acc-0.3000, valid loss-2.1543, acc-0.3836, test loss-2.1552, acc-0.3795\n",
      "Iter-13220, train loss-2.1468, acc-0.3800, valid loss-2.1542, acc-0.3838, test loss-2.1551, acc-0.3800\n",
      "Iter-13230, train loss-2.1213, acc-0.4600, valid loss-2.1541, acc-0.3836, test loss-2.1549, acc-0.3797\n",
      "Iter-13240, train loss-2.1616, acc-0.4400, valid loss-2.1540, acc-0.3836, test loss-2.1548, acc-0.3795\n",
      "Iter-13250, train loss-2.1891, acc-0.2800, valid loss-2.1539, acc-0.3842, test loss-2.1547, acc-0.3801\n",
      "Iter-13260, train loss-2.1971, acc-0.2200, valid loss-2.1538, acc-0.3838, test loss-2.1546, acc-0.3802\n",
      "Iter-13270, train loss-2.1410, acc-0.3600, valid loss-2.1537, acc-0.3838, test loss-2.1545, acc-0.3798\n",
      "Iter-13280, train loss-2.1762, acc-0.3400, valid loss-2.1536, acc-0.3840, test loss-2.1544, acc-0.3797\n",
      "Iter-13290, train loss-2.1310, acc-0.4600, valid loss-2.1535, acc-0.3840, test loss-2.1543, acc-0.3798\n",
      "Iter-13300, train loss-2.1820, acc-0.3200, valid loss-2.1533, acc-0.3842, test loss-2.1542, acc-0.3802\n",
      "Iter-13310, train loss-2.1587, acc-0.2600, valid loss-2.1532, acc-0.3844, test loss-2.1541, acc-0.3802\n",
      "Iter-13320, train loss-2.1863, acc-0.3400, valid loss-2.1531, acc-0.3844, test loss-2.1540, acc-0.3801\n",
      "Iter-13330, train loss-2.1216, acc-0.4600, valid loss-2.1530, acc-0.3846, test loss-2.1539, acc-0.3800\n",
      "Iter-13340, train loss-2.1489, acc-0.3600, valid loss-2.1529, acc-0.3844, test loss-2.1537, acc-0.3798\n",
      "Iter-13350, train loss-2.1523, acc-0.3400, valid loss-2.1528, acc-0.3842, test loss-2.1536, acc-0.3798\n",
      "Iter-13360, train loss-2.1406, acc-0.4000, valid loss-2.1527, acc-0.3838, test loss-2.1535, acc-0.3797\n",
      "Iter-13370, train loss-2.1400, acc-0.3800, valid loss-2.1526, acc-0.3838, test loss-2.1534, acc-0.3795\n",
      "Iter-13380, train loss-2.1756, acc-0.2600, valid loss-2.1525, acc-0.3838, test loss-2.1533, acc-0.3796\n",
      "Iter-13390, train loss-2.1845, acc-0.3400, valid loss-2.1524, acc-0.3840, test loss-2.1532, acc-0.3795\n",
      "Iter-13400, train loss-2.1487, acc-0.3600, valid loss-2.1523, acc-0.3840, test loss-2.1531, acc-0.3796\n",
      "Iter-13410, train loss-2.1496, acc-0.3600, valid loss-2.1522, acc-0.3840, test loss-2.1530, acc-0.3797\n",
      "Iter-13420, train loss-2.1145, acc-0.4400, valid loss-2.1521, acc-0.3844, test loss-2.1529, acc-0.3797\n",
      "Iter-13430, train loss-2.1991, acc-0.2600, valid loss-2.1520, acc-0.3850, test loss-2.1528, acc-0.3799\n",
      "Iter-13440, train loss-2.1690, acc-0.3400, valid loss-2.1518, acc-0.3846, test loss-2.1527, acc-0.3797\n",
      "Iter-13450, train loss-2.1406, acc-0.4600, valid loss-2.1517, acc-0.3850, test loss-2.1526, acc-0.3799\n",
      "Iter-13460, train loss-2.1610, acc-0.3800, valid loss-2.1516, acc-0.3852, test loss-2.1525, acc-0.3800\n",
      "Iter-13470, train loss-2.1430, acc-0.3600, valid loss-2.1515, acc-0.3848, test loss-2.1524, acc-0.3801\n",
      "Iter-13480, train loss-2.1535, acc-0.3400, valid loss-2.1514, acc-0.3848, test loss-2.1523, acc-0.3801\n",
      "Iter-13490, train loss-2.1554, acc-0.3600, valid loss-2.1513, acc-0.3846, test loss-2.1521, acc-0.3802\n",
      "Iter-13500, train loss-2.2154, acc-0.3000, valid loss-2.1512, acc-0.3852, test loss-2.1520, acc-0.3805\n",
      "Iter-13510, train loss-2.1366, acc-0.4200, valid loss-2.1511, acc-0.3850, test loss-2.1519, acc-0.3806\n",
      "Iter-13520, train loss-2.1843, acc-0.3000, valid loss-2.1510, acc-0.3850, test loss-2.1518, acc-0.3806\n",
      "Iter-13530, train loss-2.1530, acc-0.4000, valid loss-2.1508, acc-0.3852, test loss-2.1517, acc-0.3807\n",
      "Iter-13540, train loss-2.1514, acc-0.3400, valid loss-2.1507, acc-0.3854, test loss-2.1516, acc-0.3808\n",
      "Iter-13550, train loss-2.1590, acc-0.3200, valid loss-2.1506, acc-0.3858, test loss-2.1515, acc-0.3806\n",
      "Iter-13560, train loss-2.1360, acc-0.4000, valid loss-2.1505, acc-0.3858, test loss-2.1514, acc-0.3810\n",
      "Iter-13570, train loss-2.1847, acc-0.3600, valid loss-2.1504, acc-0.3864, test loss-2.1513, acc-0.3808\n",
      "Iter-13580, train loss-2.1539, acc-0.3600, valid loss-2.1503, acc-0.3862, test loss-2.1511, acc-0.3809\n",
      "Iter-13590, train loss-2.1280, acc-0.4400, valid loss-2.1502, acc-0.3862, test loss-2.1510, acc-0.3809\n",
      "Iter-13600, train loss-2.1456, acc-0.3800, valid loss-2.1501, acc-0.3866, test loss-2.1509, acc-0.3809\n",
      "Iter-13610, train loss-2.1953, acc-0.2200, valid loss-2.1500, acc-0.3868, test loss-2.1508, acc-0.3812\n",
      "Iter-13620, train loss-2.1475, acc-0.4200, valid loss-2.1499, acc-0.3870, test loss-2.1507, acc-0.3814\n",
      "Iter-13630, train loss-2.1626, acc-0.3400, valid loss-2.1498, acc-0.3870, test loss-2.1506, acc-0.3816\n",
      "Iter-13640, train loss-2.0972, acc-0.5200, valid loss-2.1497, acc-0.3870, test loss-2.1505, acc-0.3814\n",
      "Iter-13650, train loss-2.1360, acc-0.2800, valid loss-2.1496, acc-0.3872, test loss-2.1504, acc-0.3817\n",
      "Iter-13660, train loss-2.1808, acc-0.2800, valid loss-2.1495, acc-0.3870, test loss-2.1503, acc-0.3818\n",
      "Iter-13670, train loss-2.1240, acc-0.4600, valid loss-2.1493, acc-0.3872, test loss-2.1502, acc-0.3818\n",
      "Iter-13680, train loss-2.1777, acc-0.3400, valid loss-2.1492, acc-0.3874, test loss-2.1501, acc-0.3819\n",
      "Iter-13690, train loss-2.2009, acc-0.3600, valid loss-2.1491, acc-0.3876, test loss-2.1500, acc-0.3819\n",
      "Iter-13700, train loss-2.1605, acc-0.3800, valid loss-2.1490, acc-0.3880, test loss-2.1499, acc-0.3817\n",
      "Iter-13710, train loss-2.1678, acc-0.3400, valid loss-2.1489, acc-0.3880, test loss-2.1498, acc-0.3819\n",
      "Iter-13720, train loss-2.1871, acc-0.3600, valid loss-2.1488, acc-0.3882, test loss-2.1497, acc-0.3820\n",
      "Iter-13730, train loss-2.1554, acc-0.3800, valid loss-2.1487, acc-0.3882, test loss-2.1496, acc-0.3821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13740, train loss-2.1368, acc-0.4600, valid loss-2.1486, acc-0.3884, test loss-2.1494, acc-0.3820\n",
      "Iter-13750, train loss-2.1518, acc-0.4200, valid loss-2.1485, acc-0.3884, test loss-2.1493, acc-0.3820\n",
      "Iter-13760, train loss-2.1894, acc-0.2400, valid loss-2.1484, acc-0.3882, test loss-2.1492, acc-0.3822\n",
      "Iter-13770, train loss-2.1396, acc-0.3600, valid loss-2.1483, acc-0.3882, test loss-2.1491, acc-0.3823\n",
      "Iter-13780, train loss-2.1454, acc-0.3800, valid loss-2.1482, acc-0.3880, test loss-2.1490, acc-0.3825\n",
      "Iter-13790, train loss-2.1180, acc-0.4600, valid loss-2.1481, acc-0.3880, test loss-2.1489, acc-0.3823\n",
      "Iter-13800, train loss-2.1487, acc-0.4000, valid loss-2.1480, acc-0.3876, test loss-2.1488, acc-0.3822\n",
      "Iter-13810, train loss-2.1477, acc-0.4600, valid loss-2.1479, acc-0.3876, test loss-2.1487, acc-0.3823\n",
      "Iter-13820, train loss-2.2242, acc-0.2600, valid loss-2.1478, acc-0.3876, test loss-2.1486, acc-0.3823\n",
      "Iter-13830, train loss-2.1875, acc-0.2600, valid loss-2.1476, acc-0.3878, test loss-2.1485, acc-0.3825\n",
      "Iter-13840, train loss-2.1236, acc-0.4600, valid loss-2.1475, acc-0.3878, test loss-2.1484, acc-0.3826\n",
      "Iter-13850, train loss-2.1366, acc-0.5200, valid loss-2.1474, acc-0.3880, test loss-2.1483, acc-0.3826\n",
      "Iter-13860, train loss-2.1534, acc-0.3200, valid loss-2.1473, acc-0.3882, test loss-2.1482, acc-0.3826\n",
      "Iter-13870, train loss-2.1229, acc-0.5000, valid loss-2.1472, acc-0.3886, test loss-2.1481, acc-0.3830\n",
      "Iter-13880, train loss-2.1332, acc-0.5000, valid loss-2.1471, acc-0.3888, test loss-2.1479, acc-0.3830\n",
      "Iter-13890, train loss-2.0910, acc-0.5400, valid loss-2.1470, acc-0.3888, test loss-2.1478, acc-0.3830\n",
      "Iter-13900, train loss-2.1502, acc-0.3400, valid loss-2.1469, acc-0.3890, test loss-2.1477, acc-0.3832\n",
      "Iter-13910, train loss-2.1467, acc-0.4200, valid loss-2.1468, acc-0.3890, test loss-2.1476, acc-0.3831\n",
      "Iter-13920, train loss-2.0881, acc-0.4800, valid loss-2.1467, acc-0.3888, test loss-2.1475, acc-0.3830\n",
      "Iter-13930, train loss-2.1704, acc-0.4000, valid loss-2.1466, acc-0.3886, test loss-2.1474, acc-0.3832\n",
      "Iter-13940, train loss-2.0887, acc-0.5200, valid loss-2.1465, acc-0.3888, test loss-2.1473, acc-0.3832\n",
      "Iter-13950, train loss-2.1552, acc-0.4400, valid loss-2.1463, acc-0.3890, test loss-2.1472, acc-0.3830\n",
      "Iter-13960, train loss-2.1595, acc-0.3800, valid loss-2.1462, acc-0.3890, test loss-2.1471, acc-0.3831\n",
      "Iter-13970, train loss-2.1526, acc-0.3400, valid loss-2.1461, acc-0.3890, test loss-2.1470, acc-0.3833\n",
      "Iter-13980, train loss-2.1473, acc-0.3800, valid loss-2.1460, acc-0.3890, test loss-2.1469, acc-0.3831\n",
      "Iter-13990, train loss-2.1233, acc-0.4600, valid loss-2.1459, acc-0.3890, test loss-2.1467, acc-0.3831\n",
      "Iter-14000, train loss-2.1604, acc-0.3000, valid loss-2.1458, acc-0.3890, test loss-2.1466, acc-0.3832\n",
      "Iter-14010, train loss-2.1298, acc-0.4400, valid loss-2.1457, acc-0.3890, test loss-2.1465, acc-0.3832\n",
      "Iter-14020, train loss-2.1667, acc-0.3200, valid loss-2.1456, acc-0.3890, test loss-2.1464, acc-0.3835\n",
      "Iter-14030, train loss-2.1237, acc-0.3400, valid loss-2.1455, acc-0.3892, test loss-2.1463, acc-0.3835\n",
      "Iter-14040, train loss-2.1518, acc-0.3400, valid loss-2.1454, acc-0.3896, test loss-2.1462, acc-0.3834\n",
      "Iter-14050, train loss-2.1544, acc-0.3800, valid loss-2.1453, acc-0.3898, test loss-2.1461, acc-0.3836\n",
      "Iter-14060, train loss-2.1376, acc-0.4000, valid loss-2.1452, acc-0.3898, test loss-2.1460, acc-0.3835\n",
      "Iter-14070, train loss-2.1501, acc-0.3800, valid loss-2.1450, acc-0.3898, test loss-2.1459, acc-0.3833\n",
      "Iter-14080, train loss-2.1473, acc-0.4200, valid loss-2.1449, acc-0.3900, test loss-2.1458, acc-0.3837\n",
      "Iter-14090, train loss-2.1409, acc-0.3800, valid loss-2.1448, acc-0.3900, test loss-2.1457, acc-0.3837\n",
      "Iter-14100, train loss-2.1666, acc-0.4000, valid loss-2.1447, acc-0.3896, test loss-2.1455, acc-0.3837\n",
      "Iter-14110, train loss-2.1566, acc-0.3800, valid loss-2.1446, acc-0.3896, test loss-2.1454, acc-0.3839\n",
      "Iter-14120, train loss-2.1186, acc-0.3600, valid loss-2.1445, acc-0.3898, test loss-2.1453, acc-0.3838\n",
      "Iter-14130, train loss-2.1148, acc-0.4400, valid loss-2.1444, acc-0.3900, test loss-2.1452, acc-0.3837\n",
      "Iter-14140, train loss-2.1829, acc-0.2000, valid loss-2.1443, acc-0.3900, test loss-2.1451, acc-0.3837\n",
      "Iter-14150, train loss-2.1431, acc-0.4400, valid loss-2.1442, acc-0.3898, test loss-2.1450, acc-0.3837\n",
      "Iter-14160, train loss-2.1251, acc-0.4400, valid loss-2.1441, acc-0.3904, test loss-2.1449, acc-0.3837\n",
      "Iter-14170, train loss-2.1196, acc-0.4800, valid loss-2.1440, acc-0.3902, test loss-2.1448, acc-0.3836\n",
      "Iter-14180, train loss-2.1656, acc-0.3400, valid loss-2.1439, acc-0.3904, test loss-2.1447, acc-0.3837\n",
      "Iter-14190, train loss-2.1732, acc-0.3200, valid loss-2.1438, acc-0.3902, test loss-2.1446, acc-0.3838\n",
      "Iter-14200, train loss-2.1079, acc-0.4400, valid loss-2.1436, acc-0.3904, test loss-2.1445, acc-0.3837\n",
      "Iter-14210, train loss-2.1753, acc-0.2800, valid loss-2.1435, acc-0.3906, test loss-2.1444, acc-0.3837\n",
      "Iter-14220, train loss-2.1826, acc-0.3000, valid loss-2.1434, acc-0.3906, test loss-2.1443, acc-0.3838\n",
      "Iter-14230, train loss-2.1344, acc-0.4000, valid loss-2.1433, acc-0.3910, test loss-2.1442, acc-0.3840\n",
      "Iter-14240, train loss-2.1103, acc-0.4400, valid loss-2.1432, acc-0.3906, test loss-2.1440, acc-0.3848\n",
      "Iter-14250, train loss-2.1211, acc-0.4000, valid loss-2.1431, acc-0.3908, test loss-2.1439, acc-0.3847\n",
      "Iter-14260, train loss-2.1466, acc-0.3800, valid loss-2.1430, acc-0.3908, test loss-2.1438, acc-0.3848\n",
      "Iter-14270, train loss-2.1337, acc-0.3400, valid loss-2.1429, acc-0.3906, test loss-2.1437, acc-0.3847\n",
      "Iter-14280, train loss-2.1584, acc-0.3800, valid loss-2.1428, acc-0.3910, test loss-2.1436, acc-0.3850\n",
      "Iter-14290, train loss-2.1628, acc-0.4000, valid loss-2.1427, acc-0.3908, test loss-2.1435, acc-0.3849\n",
      "Iter-14300, train loss-2.1480, acc-0.4400, valid loss-2.1426, acc-0.3908, test loss-2.1434, acc-0.3848\n",
      "Iter-14310, train loss-2.1186, acc-0.4000, valid loss-2.1425, acc-0.3908, test loss-2.1433, acc-0.3848\n",
      "Iter-14320, train loss-2.1575, acc-0.3000, valid loss-2.1424, acc-0.3906, test loss-2.1432, acc-0.3848\n",
      "Iter-14330, train loss-2.0983, acc-0.4800, valid loss-2.1423, acc-0.3906, test loss-2.1431, acc-0.3850\n",
      "Iter-14340, train loss-2.1560, acc-0.3800, valid loss-2.1422, acc-0.3906, test loss-2.1430, acc-0.3848\n",
      "Iter-14350, train loss-2.1478, acc-0.4400, valid loss-2.1421, acc-0.3906, test loss-2.1429, acc-0.3846\n",
      "Iter-14360, train loss-2.1210, acc-0.4000, valid loss-2.1420, acc-0.3910, test loss-2.1427, acc-0.3848\n",
      "Iter-14370, train loss-2.1703, acc-0.3200, valid loss-2.1419, acc-0.3912, test loss-2.1426, acc-0.3851\n",
      "Iter-14380, train loss-2.1241, acc-0.4400, valid loss-2.1417, acc-0.3910, test loss-2.1425, acc-0.3852\n",
      "Iter-14390, train loss-2.1582, acc-0.3600, valid loss-2.1416, acc-0.3912, test loss-2.1424, acc-0.3851\n",
      "Iter-14400, train loss-2.1745, acc-0.2800, valid loss-2.1415, acc-0.3910, test loss-2.1423, acc-0.3854\n",
      "Iter-14410, train loss-2.1395, acc-0.3200, valid loss-2.1414, acc-0.3914, test loss-2.1422, acc-0.3856\n",
      "Iter-14420, train loss-2.1347, acc-0.4400, valid loss-2.1413, acc-0.3914, test loss-2.1421, acc-0.3856\n",
      "Iter-14430, train loss-2.1682, acc-0.2800, valid loss-2.1412, acc-0.3918, test loss-2.1420, acc-0.3853\n",
      "Iter-14440, train loss-2.1503, acc-0.3600, valid loss-2.1411, acc-0.3918, test loss-2.1419, acc-0.3855\n",
      "Iter-14450, train loss-2.1471, acc-0.4000, valid loss-2.1410, acc-0.3920, test loss-2.1418, acc-0.3854\n",
      "Iter-14460, train loss-2.1459, acc-0.3200, valid loss-2.1409, acc-0.3922, test loss-2.1417, acc-0.3856\n",
      "Iter-14470, train loss-2.2094, acc-0.2000, valid loss-2.1408, acc-0.3924, test loss-2.1416, acc-0.3859\n",
      "Iter-14480, train loss-2.1478, acc-0.4200, valid loss-2.1407, acc-0.3924, test loss-2.1415, acc-0.3858\n",
      "Iter-14490, train loss-2.1853, acc-0.2200, valid loss-2.1405, acc-0.3920, test loss-2.1414, acc-0.3858\n",
      "Iter-14500, train loss-2.1335, acc-0.4600, valid loss-2.1404, acc-0.3922, test loss-2.1412, acc-0.3863\n",
      "Iter-14510, train loss-2.1485, acc-0.3600, valid loss-2.1403, acc-0.3922, test loss-2.1411, acc-0.3861\n",
      "Iter-14520, train loss-2.1672, acc-0.3400, valid loss-2.1402, acc-0.3922, test loss-2.1410, acc-0.3862\n",
      "Iter-14530, train loss-2.1101, acc-0.4800, valid loss-2.1401, acc-0.3922, test loss-2.1409, acc-0.3865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-14540, train loss-2.1480, acc-0.3600, valid loss-2.1400, acc-0.3926, test loss-2.1408, acc-0.3864\n",
      "Iter-14550, train loss-2.1524, acc-0.3000, valid loss-2.1399, acc-0.3926, test loss-2.1407, acc-0.3866\n",
      "Iter-14560, train loss-2.1601, acc-0.3400, valid loss-2.1398, acc-0.3924, test loss-2.1406, acc-0.3868\n",
      "Iter-14570, train loss-2.1542, acc-0.3800, valid loss-2.1397, acc-0.3922, test loss-2.1405, acc-0.3869\n",
      "Iter-14580, train loss-2.1284, acc-0.4400, valid loss-2.1396, acc-0.3926, test loss-2.1404, acc-0.3868\n",
      "Iter-14590, train loss-2.1505, acc-0.4200, valid loss-2.1395, acc-0.3924, test loss-2.1403, acc-0.3870\n",
      "Iter-14600, train loss-2.1381, acc-0.3000, valid loss-2.1394, acc-0.3924, test loss-2.1402, acc-0.3869\n",
      "Iter-14610, train loss-2.1621, acc-0.4000, valid loss-2.1393, acc-0.3924, test loss-2.1401, acc-0.3873\n",
      "Iter-14620, train loss-2.1633, acc-0.4000, valid loss-2.1392, acc-0.3926, test loss-2.1400, acc-0.3873\n",
      "Iter-14630, train loss-2.1718, acc-0.3200, valid loss-2.1391, acc-0.3926, test loss-2.1399, acc-0.3872\n",
      "Iter-14640, train loss-2.1481, acc-0.3800, valid loss-2.1390, acc-0.3928, test loss-2.1398, acc-0.3872\n",
      "Iter-14650, train loss-2.1506, acc-0.3200, valid loss-2.1389, acc-0.3926, test loss-2.1397, acc-0.3871\n",
      "Iter-14660, train loss-2.1317, acc-0.3400, valid loss-2.1388, acc-0.3926, test loss-2.1396, acc-0.3876\n",
      "Iter-14670, train loss-2.1018, acc-0.4800, valid loss-2.1387, acc-0.3924, test loss-2.1395, acc-0.3876\n",
      "Iter-14680, train loss-2.1159, acc-0.4600, valid loss-2.1385, acc-0.3926, test loss-2.1393, acc-0.3877\n",
      "Iter-14690, train loss-2.1481, acc-0.3200, valid loss-2.1384, acc-0.3928, test loss-2.1392, acc-0.3878\n",
      "Iter-14700, train loss-2.1741, acc-0.3600, valid loss-2.1383, acc-0.3932, test loss-2.1391, acc-0.3880\n",
      "Iter-14710, train loss-2.1287, acc-0.4600, valid loss-2.1382, acc-0.3932, test loss-2.1390, acc-0.3880\n",
      "Iter-14720, train loss-2.1136, acc-0.4800, valid loss-2.1381, acc-0.3936, test loss-2.1389, acc-0.3882\n",
      "Iter-14730, train loss-2.1680, acc-0.3800, valid loss-2.1380, acc-0.3930, test loss-2.1388, acc-0.3883\n",
      "Iter-14740, train loss-2.1497, acc-0.3000, valid loss-2.1379, acc-0.3932, test loss-2.1387, acc-0.3883\n",
      "Iter-14750, train loss-2.1198, acc-0.4400, valid loss-2.1378, acc-0.3932, test loss-2.1386, acc-0.3884\n",
      "Iter-14760, train loss-2.1686, acc-0.2600, valid loss-2.1377, acc-0.3934, test loss-2.1385, acc-0.3887\n",
      "Iter-14770, train loss-2.1440, acc-0.4200, valid loss-2.1376, acc-0.3932, test loss-2.1384, acc-0.3887\n",
      "Iter-14780, train loss-2.1070, acc-0.4200, valid loss-2.1375, acc-0.3934, test loss-2.1383, acc-0.3886\n",
      "Iter-14790, train loss-2.1414, acc-0.4400, valid loss-2.1374, acc-0.3934, test loss-2.1382, acc-0.3888\n",
      "Iter-14800, train loss-2.1488, acc-0.3600, valid loss-2.1373, acc-0.3930, test loss-2.1381, acc-0.3885\n",
      "Iter-14810, train loss-2.1027, acc-0.4000, valid loss-2.1372, acc-0.3932, test loss-2.1380, acc-0.3885\n",
      "Iter-14820, train loss-2.1543, acc-0.3800, valid loss-2.1371, acc-0.3932, test loss-2.1379, acc-0.3890\n",
      "Iter-14830, train loss-2.1971, acc-0.2000, valid loss-2.1369, acc-0.3934, test loss-2.1377, acc-0.3887\n",
      "Iter-14840, train loss-2.1229, acc-0.3400, valid loss-2.1368, acc-0.3936, test loss-2.1376, acc-0.3887\n",
      "Iter-14850, train loss-2.1320, acc-0.4600, valid loss-2.1367, acc-0.3932, test loss-2.1375, acc-0.3887\n",
      "Iter-14860, train loss-2.1081, acc-0.4400, valid loss-2.1366, acc-0.3932, test loss-2.1374, acc-0.3890\n",
      "Iter-14870, train loss-2.1688, acc-0.3400, valid loss-2.1365, acc-0.3932, test loss-2.1373, acc-0.3891\n",
      "Iter-14880, train loss-2.1520, acc-0.3600, valid loss-2.1364, acc-0.3934, test loss-2.1372, acc-0.3890\n",
      "Iter-14890, train loss-2.1376, acc-0.4400, valid loss-2.1363, acc-0.3934, test loss-2.1371, acc-0.3890\n",
      "Iter-14900, train loss-2.1738, acc-0.2400, valid loss-2.1362, acc-0.3932, test loss-2.1370, acc-0.3891\n",
      "Iter-14910, train loss-2.1670, acc-0.3200, valid loss-2.1361, acc-0.3934, test loss-2.1369, acc-0.3893\n",
      "Iter-14920, train loss-2.1378, acc-0.4000, valid loss-2.1360, acc-0.3934, test loss-2.1368, acc-0.3892\n",
      "Iter-14930, train loss-2.1139, acc-0.3800, valid loss-2.1359, acc-0.3936, test loss-2.1367, acc-0.3895\n",
      "Iter-14940, train loss-2.1427, acc-0.4000, valid loss-2.1358, acc-0.3936, test loss-2.1366, acc-0.3899\n",
      "Iter-14950, train loss-2.2034, acc-0.2400, valid loss-2.1357, acc-0.3936, test loss-2.1365, acc-0.3900\n",
      "Iter-14960, train loss-2.1606, acc-0.3600, valid loss-2.1356, acc-0.3938, test loss-2.1364, acc-0.3901\n",
      "Iter-14970, train loss-2.1908, acc-0.3200, valid loss-2.1355, acc-0.3938, test loss-2.1363, acc-0.3902\n",
      "Iter-14980, train loss-2.0908, acc-0.4400, valid loss-2.1354, acc-0.3938, test loss-2.1362, acc-0.3902\n",
      "Iter-14990, train loss-2.1166, acc-0.4000, valid loss-2.1353, acc-0.3940, test loss-2.1361, acc-0.3903\n",
      "Iter-15000, train loss-2.1524, acc-0.3200, valid loss-2.1352, acc-0.3938, test loss-2.1360, acc-0.3901\n",
      "Iter-15010, train loss-2.1601, acc-0.3200, valid loss-2.1351, acc-0.3940, test loss-2.1359, acc-0.3901\n",
      "Iter-15020, train loss-2.1220, acc-0.4000, valid loss-2.1350, acc-0.3940, test loss-2.1358, acc-0.3901\n",
      "Iter-15030, train loss-2.1211, acc-0.4200, valid loss-2.1349, acc-0.3942, test loss-2.1357, acc-0.3901\n",
      "Iter-15040, train loss-2.1563, acc-0.2800, valid loss-2.1348, acc-0.3942, test loss-2.1356, acc-0.3900\n",
      "Iter-15050, train loss-2.1233, acc-0.4200, valid loss-2.1347, acc-0.3942, test loss-2.1355, acc-0.3903\n",
      "Iter-15060, train loss-2.1362, acc-0.3800, valid loss-2.1346, acc-0.3944, test loss-2.1353, acc-0.3904\n",
      "Iter-15070, train loss-2.1438, acc-0.3000, valid loss-2.1345, acc-0.3948, test loss-2.1352, acc-0.3902\n",
      "Iter-15080, train loss-2.1105, acc-0.4400, valid loss-2.1343, acc-0.3944, test loss-2.1351, acc-0.3903\n",
      "Iter-15090, train loss-2.1185, acc-0.4800, valid loss-2.1342, acc-0.3948, test loss-2.1350, acc-0.3905\n",
      "Iter-15100, train loss-2.1748, acc-0.3200, valid loss-2.1341, acc-0.3950, test loss-2.1349, acc-0.3907\n",
      "Iter-15110, train loss-2.1462, acc-0.3400, valid loss-2.1340, acc-0.3946, test loss-2.1348, acc-0.3908\n",
      "Iter-15120, train loss-2.1300, acc-0.4600, valid loss-2.1339, acc-0.3948, test loss-2.1347, acc-0.3906\n",
      "Iter-15130, train loss-2.1463, acc-0.4000, valid loss-2.1338, acc-0.3946, test loss-2.1346, acc-0.3907\n",
      "Iter-15140, train loss-2.1193, acc-0.4800, valid loss-2.1337, acc-0.3946, test loss-2.1345, acc-0.3908\n",
      "Iter-15150, train loss-2.1660, acc-0.3800, valid loss-2.1336, acc-0.3946, test loss-2.1344, acc-0.3908\n",
      "Iter-15160, train loss-2.1255, acc-0.4600, valid loss-2.1335, acc-0.3948, test loss-2.1343, acc-0.3910\n",
      "Iter-15170, train loss-2.1730, acc-0.3200, valid loss-2.1334, acc-0.3950, test loss-2.1342, acc-0.3912\n",
      "Iter-15180, train loss-2.1551, acc-0.3400, valid loss-2.1333, acc-0.3950, test loss-2.1341, acc-0.3909\n",
      "Iter-15190, train loss-2.1758, acc-0.3000, valid loss-2.1332, acc-0.3948, test loss-2.1340, acc-0.3905\n",
      "Iter-15200, train loss-2.1528, acc-0.3800, valid loss-2.1331, acc-0.3948, test loss-2.1339, acc-0.3910\n",
      "Iter-15210, train loss-2.1384, acc-0.4200, valid loss-2.1330, acc-0.3950, test loss-2.1338, acc-0.3908\n",
      "Iter-15220, train loss-2.1504, acc-0.4000, valid loss-2.1329, acc-0.3948, test loss-2.1337, acc-0.3906\n",
      "Iter-15230, train loss-2.1180, acc-0.4400, valid loss-2.1328, acc-0.3952, test loss-2.1336, acc-0.3908\n",
      "Iter-15240, train loss-2.1029, acc-0.4600, valid loss-2.1327, acc-0.3952, test loss-2.1335, acc-0.3907\n",
      "Iter-15250, train loss-2.1707, acc-0.4200, valid loss-2.1326, acc-0.3954, test loss-2.1334, acc-0.3909\n",
      "Iter-15260, train loss-2.1150, acc-0.3800, valid loss-2.1324, acc-0.3952, test loss-2.1332, acc-0.3909\n",
      "Iter-15270, train loss-2.1242, acc-0.4400, valid loss-2.1323, acc-0.3952, test loss-2.1331, acc-0.3910\n",
      "Iter-15280, train loss-2.1477, acc-0.3800, valid loss-2.1322, acc-0.3952, test loss-2.1330, acc-0.3909\n",
      "Iter-15290, train loss-2.1459, acc-0.4200, valid loss-2.1321, acc-0.3954, test loss-2.1329, acc-0.3910\n",
      "Iter-15300, train loss-2.1473, acc-0.3400, valid loss-2.1320, acc-0.3958, test loss-2.1328, acc-0.3911\n",
      "Iter-15310, train loss-2.1530, acc-0.4000, valid loss-2.1319, acc-0.3958, test loss-2.1327, acc-0.3913\n",
      "Iter-15320, train loss-2.1522, acc-0.3200, valid loss-2.1318, acc-0.3956, test loss-2.1326, acc-0.3911\n",
      "Iter-15330, train loss-2.1422, acc-0.3200, valid loss-2.1317, acc-0.3956, test loss-2.1325, acc-0.3912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-15340, train loss-2.1397, acc-0.3000, valid loss-2.1316, acc-0.3958, test loss-2.1324, acc-0.3914\n",
      "Iter-15350, train loss-2.1478, acc-0.3200, valid loss-2.1315, acc-0.3956, test loss-2.1323, acc-0.3914\n",
      "Iter-15360, train loss-2.1191, acc-0.4600, valid loss-2.1314, acc-0.3954, test loss-2.1322, acc-0.3911\n",
      "Iter-15370, train loss-2.1665, acc-0.3600, valid loss-2.1313, acc-0.3956, test loss-2.1321, acc-0.3913\n",
      "Iter-15380, train loss-2.1823, acc-0.2800, valid loss-2.1312, acc-0.3958, test loss-2.1320, acc-0.3913\n",
      "Iter-15390, train loss-2.1667, acc-0.4000, valid loss-2.1311, acc-0.3960, test loss-2.1319, acc-0.3914\n",
      "Iter-15400, train loss-2.1781, acc-0.3000, valid loss-2.1310, acc-0.3962, test loss-2.1318, acc-0.3916\n",
      "Iter-15410, train loss-2.1849, acc-0.2600, valid loss-2.1309, acc-0.3962, test loss-2.1317, acc-0.3917\n",
      "Iter-15420, train loss-2.1466, acc-0.3400, valid loss-2.1308, acc-0.3964, test loss-2.1316, acc-0.3914\n",
      "Iter-15430, train loss-2.1335, acc-0.4200, valid loss-2.1307, acc-0.3966, test loss-2.1315, acc-0.3914\n",
      "Iter-15440, train loss-2.1153, acc-0.4000, valid loss-2.1306, acc-0.3964, test loss-2.1314, acc-0.3913\n",
      "Iter-15450, train loss-2.1500, acc-0.3600, valid loss-2.1305, acc-0.3968, test loss-2.1313, acc-0.3914\n",
      "Iter-15460, train loss-2.0887, acc-0.4800, valid loss-2.1304, acc-0.3972, test loss-2.1312, acc-0.3914\n",
      "Iter-15470, train loss-2.1074, acc-0.4800, valid loss-2.1303, acc-0.3964, test loss-2.1311, acc-0.3913\n",
      "Iter-15480, train loss-2.0856, acc-0.5800, valid loss-2.1302, acc-0.3966, test loss-2.1310, acc-0.3915\n",
      "Iter-15490, train loss-2.1264, acc-0.3600, valid loss-2.1301, acc-0.3968, test loss-2.1309, acc-0.3913\n",
      "Iter-15500, train loss-2.1272, acc-0.3600, valid loss-2.1299, acc-0.3972, test loss-2.1308, acc-0.3911\n",
      "Iter-15510, train loss-2.1480, acc-0.4400, valid loss-2.1298, acc-0.3974, test loss-2.1307, acc-0.3914\n",
      "Iter-15520, train loss-2.1202, acc-0.4600, valid loss-2.1297, acc-0.3972, test loss-2.1305, acc-0.3914\n",
      "Iter-15530, train loss-2.1112, acc-0.5200, valid loss-2.1296, acc-0.3972, test loss-2.1304, acc-0.3917\n",
      "Iter-15540, train loss-2.1619, acc-0.4000, valid loss-2.1295, acc-0.3972, test loss-2.1303, acc-0.3919\n",
      "Iter-15550, train loss-2.1301, acc-0.3600, valid loss-2.1294, acc-0.3976, test loss-2.1302, acc-0.3924\n",
      "Iter-15560, train loss-2.1237, acc-0.4200, valid loss-2.1293, acc-0.3972, test loss-2.1301, acc-0.3923\n",
      "Iter-15570, train loss-2.0760, acc-0.4600, valid loss-2.1292, acc-0.3972, test loss-2.1300, acc-0.3924\n",
      "Iter-15580, train loss-2.1568, acc-0.3200, valid loss-2.1291, acc-0.3972, test loss-2.1299, acc-0.3925\n",
      "Iter-15590, train loss-2.1235, acc-0.4000, valid loss-2.1290, acc-0.3970, test loss-2.1298, acc-0.3921\n",
      "Iter-15600, train loss-2.1261, acc-0.4200, valid loss-2.1289, acc-0.3970, test loss-2.1297, acc-0.3920\n",
      "Iter-15610, train loss-2.1166, acc-0.4400, valid loss-2.1288, acc-0.3970, test loss-2.1296, acc-0.3925\n",
      "Iter-15620, train loss-2.1089, acc-0.4200, valid loss-2.1287, acc-0.3966, test loss-2.1295, acc-0.3925\n",
      "Iter-15630, train loss-2.1705, acc-0.2800, valid loss-2.1286, acc-0.3968, test loss-2.1294, acc-0.3924\n",
      "Iter-15640, train loss-2.1400, acc-0.2800, valid loss-2.1285, acc-0.3970, test loss-2.1293, acc-0.3925\n",
      "Iter-15650, train loss-2.1603, acc-0.2800, valid loss-2.1284, acc-0.3966, test loss-2.1292, acc-0.3926\n",
      "Iter-15660, train loss-2.1000, acc-0.4400, valid loss-2.1283, acc-0.3968, test loss-2.1291, acc-0.3925\n",
      "Iter-15670, train loss-2.1753, acc-0.3800, valid loss-2.1282, acc-0.3972, test loss-2.1290, acc-0.3925\n",
      "Iter-15680, train loss-2.1495, acc-0.3800, valid loss-2.1281, acc-0.3972, test loss-2.1289, acc-0.3926\n",
      "Iter-15690, train loss-2.1468, acc-0.3800, valid loss-2.1280, acc-0.3974, test loss-2.1288, acc-0.3928\n",
      "Iter-15700, train loss-2.1551, acc-0.3000, valid loss-2.1279, acc-0.3976, test loss-2.1287, acc-0.3929\n",
      "Iter-15710, train loss-2.1229, acc-0.4000, valid loss-2.1277, acc-0.3976, test loss-2.1286, acc-0.3929\n",
      "Iter-15720, train loss-2.1199, acc-0.4200, valid loss-2.1276, acc-0.3976, test loss-2.1285, acc-0.3929\n",
      "Iter-15730, train loss-2.1295, acc-0.4400, valid loss-2.1275, acc-0.3978, test loss-2.1283, acc-0.3926\n",
      "Iter-15740, train loss-2.1408, acc-0.3600, valid loss-2.1274, acc-0.3976, test loss-2.1282, acc-0.3925\n",
      "Iter-15750, train loss-2.1334, acc-0.3600, valid loss-2.1273, acc-0.3978, test loss-2.1281, acc-0.3926\n",
      "Iter-15760, train loss-2.1318, acc-0.3600, valid loss-2.1272, acc-0.3986, test loss-2.1280, acc-0.3928\n",
      "Iter-15770, train loss-2.1257, acc-0.4400, valid loss-2.1271, acc-0.3986, test loss-2.1279, acc-0.3929\n",
      "Iter-15780, train loss-2.1000, acc-0.4600, valid loss-2.1270, acc-0.3986, test loss-2.1278, acc-0.3929\n",
      "Iter-15790, train loss-2.1373, acc-0.3600, valid loss-2.1269, acc-0.3986, test loss-2.1277, acc-0.3929\n",
      "Iter-15800, train loss-2.1155, acc-0.4000, valid loss-2.1268, acc-0.3984, test loss-2.1276, acc-0.3931\n",
      "Iter-15810, train loss-2.1648, acc-0.3600, valid loss-2.1267, acc-0.3986, test loss-2.1275, acc-0.3929\n",
      "Iter-15820, train loss-2.1214, acc-0.3600, valid loss-2.1266, acc-0.3990, test loss-2.1274, acc-0.3927\n",
      "Iter-15830, train loss-2.1284, acc-0.4600, valid loss-2.1265, acc-0.3990, test loss-2.1273, acc-0.3929\n",
      "Iter-15840, train loss-2.1700, acc-0.3000, valid loss-2.1264, acc-0.3992, test loss-2.1272, acc-0.3930\n",
      "Iter-15850, train loss-2.1642, acc-0.3200, valid loss-2.1263, acc-0.3988, test loss-2.1271, acc-0.3930\n",
      "Iter-15860, train loss-2.1356, acc-0.4000, valid loss-2.1262, acc-0.3990, test loss-2.1270, acc-0.3932\n",
      "Iter-15870, train loss-2.1561, acc-0.3200, valid loss-2.1261, acc-0.3992, test loss-2.1269, acc-0.3933\n",
      "Iter-15880, train loss-2.1483, acc-0.3600, valid loss-2.1260, acc-0.3994, test loss-2.1268, acc-0.3933\n",
      "Iter-15890, train loss-2.1233, acc-0.4000, valid loss-2.1259, acc-0.3992, test loss-2.1267, acc-0.3933\n",
      "Iter-15900, train loss-2.1295, acc-0.4600, valid loss-2.1258, acc-0.3994, test loss-2.1266, acc-0.3931\n",
      "Iter-15910, train loss-2.1481, acc-0.4200, valid loss-2.1257, acc-0.3994, test loss-2.1265, acc-0.3930\n",
      "Iter-15920, train loss-2.0958, acc-0.4600, valid loss-2.1256, acc-0.3994, test loss-2.1264, acc-0.3933\n",
      "Iter-15930, train loss-2.1462, acc-0.3400, valid loss-2.1255, acc-0.3996, test loss-2.1263, acc-0.3935\n",
      "Iter-15940, train loss-2.1055, acc-0.3800, valid loss-2.1253, acc-0.3996, test loss-2.1262, acc-0.3936\n",
      "Iter-15950, train loss-2.0695, acc-0.5000, valid loss-2.1252, acc-0.3994, test loss-2.1261, acc-0.3934\n",
      "Iter-15960, train loss-2.1225, acc-0.3600, valid loss-2.1251, acc-0.3992, test loss-2.1259, acc-0.3935\n",
      "Iter-15970, train loss-2.1557, acc-0.2800, valid loss-2.1250, acc-0.3992, test loss-2.1258, acc-0.3936\n",
      "Iter-15980, train loss-2.1549, acc-0.3200, valid loss-2.1249, acc-0.3994, test loss-2.1257, acc-0.3934\n",
      "Iter-15990, train loss-2.1415, acc-0.4200, valid loss-2.1248, acc-0.3994, test loss-2.1256, acc-0.3935\n",
      "Iter-16000, train loss-2.1205, acc-0.4200, valid loss-2.1247, acc-0.3998, test loss-2.1255, acc-0.3930\n",
      "Iter-16010, train loss-2.1280, acc-0.4200, valid loss-2.1246, acc-0.3998, test loss-2.1254, acc-0.3930\n",
      "Iter-16020, train loss-2.1012, acc-0.5000, valid loss-2.1245, acc-0.3998, test loss-2.1253, acc-0.3932\n",
      "Iter-16030, train loss-2.1360, acc-0.4000, valid loss-2.1244, acc-0.4000, test loss-2.1252, acc-0.3935\n",
      "Iter-16040, train loss-2.1430, acc-0.3600, valid loss-2.1243, acc-0.4000, test loss-2.1251, acc-0.3936\n",
      "Iter-16050, train loss-2.0868, acc-0.4200, valid loss-2.1242, acc-0.4000, test loss-2.1250, acc-0.3936\n",
      "Iter-16060, train loss-2.1527, acc-0.3600, valid loss-2.1241, acc-0.4000, test loss-2.1249, acc-0.3937\n",
      "Iter-16070, train loss-2.1557, acc-0.3600, valid loss-2.1240, acc-0.3996, test loss-2.1248, acc-0.3939\n",
      "Iter-16080, train loss-2.1364, acc-0.4200, valid loss-2.1239, acc-0.3996, test loss-2.1247, acc-0.3937\n",
      "Iter-16090, train loss-2.1549, acc-0.3200, valid loss-2.1238, acc-0.3998, test loss-2.1246, acc-0.3939\n",
      "Iter-16100, train loss-2.1649, acc-0.3200, valid loss-2.1237, acc-0.3996, test loss-2.1245, acc-0.3940\n",
      "Iter-16110, train loss-2.1255, acc-0.3200, valid loss-2.1236, acc-0.3996, test loss-2.1244, acc-0.3939\n",
      "Iter-16120, train loss-2.1075, acc-0.4200, valid loss-2.1235, acc-0.4000, test loss-2.1243, acc-0.3940\n",
      "Iter-16130, train loss-2.0824, acc-0.5000, valid loss-2.1234, acc-0.4000, test loss-2.1242, acc-0.3944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-16140, train loss-2.1096, acc-0.3600, valid loss-2.1233, acc-0.4002, test loss-2.1241, acc-0.3943\n",
      "Iter-16150, train loss-2.1351, acc-0.3800, valid loss-2.1232, acc-0.3996, test loss-2.1240, acc-0.3942\n",
      "Iter-16160, train loss-2.1184, acc-0.4200, valid loss-2.1231, acc-0.3998, test loss-2.1239, acc-0.3945\n",
      "Iter-16170, train loss-2.1576, acc-0.3000, valid loss-2.1230, acc-0.4002, test loss-2.1238, acc-0.3944\n",
      "Iter-16180, train loss-2.1221, acc-0.3600, valid loss-2.1229, acc-0.4004, test loss-2.1237, acc-0.3942\n",
      "Iter-16190, train loss-2.1076, acc-0.4200, valid loss-2.1228, acc-0.4006, test loss-2.1236, acc-0.3945\n",
      "Iter-16200, train loss-2.1162, acc-0.3400, valid loss-2.1227, acc-0.4006, test loss-2.1235, acc-0.3945\n",
      "Iter-16210, train loss-2.1466, acc-0.4000, valid loss-2.1226, acc-0.4006, test loss-2.1234, acc-0.3944\n",
      "Iter-16220, train loss-2.1653, acc-0.3000, valid loss-2.1225, acc-0.4006, test loss-2.1233, acc-0.3943\n",
      "Iter-16230, train loss-2.1454, acc-0.3600, valid loss-2.1224, acc-0.4010, test loss-2.1232, acc-0.3944\n",
      "Iter-16240, train loss-2.1293, acc-0.3600, valid loss-2.1223, acc-0.4010, test loss-2.1231, acc-0.3945\n",
      "Iter-16250, train loss-2.1428, acc-0.4200, valid loss-2.1221, acc-0.4010, test loss-2.1229, acc-0.3944\n",
      "Iter-16260, train loss-2.1023, acc-0.4400, valid loss-2.1220, acc-0.4014, test loss-2.1228, acc-0.3946\n",
      "Iter-16270, train loss-2.1058, acc-0.4400, valid loss-2.1220, acc-0.4014, test loss-2.1227, acc-0.3947\n",
      "Iter-16280, train loss-2.1003, acc-0.4600, valid loss-2.1219, acc-0.4018, test loss-2.1227, acc-0.3946\n",
      "Iter-16290, train loss-2.1218, acc-0.4000, valid loss-2.1218, acc-0.4016, test loss-2.1226, acc-0.3948\n",
      "Iter-16300, train loss-2.0947, acc-0.3800, valid loss-2.1217, acc-0.4014, test loss-2.1225, acc-0.3947\n",
      "Iter-16310, train loss-2.0925, acc-0.4000, valid loss-2.1215, acc-0.4014, test loss-2.1223, acc-0.3947\n",
      "Iter-16320, train loss-2.1332, acc-0.3200, valid loss-2.1214, acc-0.4016, test loss-2.1222, acc-0.3948\n",
      "Iter-16330, train loss-2.1084, acc-0.4600, valid loss-2.1213, acc-0.4018, test loss-2.1221, acc-0.3947\n",
      "Iter-16340, train loss-2.1443, acc-0.3000, valid loss-2.1212, acc-0.4014, test loss-2.1220, acc-0.3951\n",
      "Iter-16350, train loss-2.1154, acc-0.4000, valid loss-2.1211, acc-0.4016, test loss-2.1219, acc-0.3951\n",
      "Iter-16360, train loss-2.0747, acc-0.5600, valid loss-2.1210, acc-0.4016, test loss-2.1218, acc-0.3951\n",
      "Iter-16370, train loss-2.1343, acc-0.4200, valid loss-2.1209, acc-0.4018, test loss-2.1217, acc-0.3953\n",
      "Iter-16380, train loss-2.1630, acc-0.3800, valid loss-2.1208, acc-0.4018, test loss-2.1216, acc-0.3954\n",
      "Iter-16390, train loss-2.1120, acc-0.4000, valid loss-2.1207, acc-0.4016, test loss-2.1215, acc-0.3954\n",
      "Iter-16400, train loss-2.0838, acc-0.5000, valid loss-2.1206, acc-0.4016, test loss-2.1214, acc-0.3953\n",
      "Iter-16410, train loss-2.1008, acc-0.4600, valid loss-2.1205, acc-0.4016, test loss-2.1213, acc-0.3953\n",
      "Iter-16420, train loss-2.0785, acc-0.5000, valid loss-2.1204, acc-0.4016, test loss-2.1212, acc-0.3954\n",
      "Iter-16430, train loss-2.1157, acc-0.4600, valid loss-2.1203, acc-0.4016, test loss-2.1211, acc-0.3954\n",
      "Iter-16440, train loss-2.0883, acc-0.4800, valid loss-2.1202, acc-0.4020, test loss-2.1210, acc-0.3954\n",
      "Iter-16450, train loss-2.0988, acc-0.4000, valid loss-2.1201, acc-0.4022, test loss-2.1209, acc-0.3957\n",
      "Iter-16460, train loss-2.0901, acc-0.4200, valid loss-2.1200, acc-0.4022, test loss-2.1208, acc-0.3957\n",
      "Iter-16470, train loss-2.1701, acc-0.3600, valid loss-2.1199, acc-0.4026, test loss-2.1207, acc-0.3959\n",
      "Iter-16480, train loss-2.1151, acc-0.4200, valid loss-2.1198, acc-0.4024, test loss-2.1206, acc-0.3958\n",
      "Iter-16490, train loss-2.1312, acc-0.3400, valid loss-2.1197, acc-0.4026, test loss-2.1205, acc-0.3959\n",
      "Iter-16500, train loss-2.1408, acc-0.3800, valid loss-2.1196, acc-0.4022, test loss-2.1204, acc-0.3961\n",
      "Iter-16510, train loss-2.1573, acc-0.3000, valid loss-2.1195, acc-0.4028, test loss-2.1203, acc-0.3959\n",
      "Iter-16520, train loss-2.1102, acc-0.4200, valid loss-2.1194, acc-0.4026, test loss-2.1202, acc-0.3959\n",
      "Iter-16530, train loss-2.1262, acc-0.4400, valid loss-2.1193, acc-0.4026, test loss-2.1201, acc-0.3962\n",
      "Iter-16540, train loss-2.1186, acc-0.4000, valid loss-2.1192, acc-0.4026, test loss-2.1200, acc-0.3962\n",
      "Iter-16550, train loss-2.1126, acc-0.4000, valid loss-2.1191, acc-0.4026, test loss-2.1199, acc-0.3965\n",
      "Iter-16560, train loss-2.1072, acc-0.3800, valid loss-2.1190, acc-0.4026, test loss-2.1198, acc-0.3966\n",
      "Iter-16570, train loss-2.0993, acc-0.4400, valid loss-2.1189, acc-0.4026, test loss-2.1197, acc-0.3965\n",
      "Iter-16580, train loss-2.0948, acc-0.3800, valid loss-2.1188, acc-0.4026, test loss-2.1196, acc-0.3964\n",
      "Iter-16590, train loss-2.1281, acc-0.3800, valid loss-2.1187, acc-0.4028, test loss-2.1195, acc-0.3970\n",
      "Iter-16600, train loss-2.0973, acc-0.4200, valid loss-2.1186, acc-0.4028, test loss-2.1194, acc-0.3968\n",
      "Iter-16610, train loss-2.1118, acc-0.4800, valid loss-2.1185, acc-0.4028, test loss-2.1193, acc-0.3968\n",
      "Iter-16620, train loss-2.1889, acc-0.2600, valid loss-2.1184, acc-0.4028, test loss-2.1192, acc-0.3969\n",
      "Iter-16630, train loss-2.1280, acc-0.3200, valid loss-2.1183, acc-0.4026, test loss-2.1191, acc-0.3972\n",
      "Iter-16640, train loss-2.1443, acc-0.3400, valid loss-2.1182, acc-0.4026, test loss-2.1190, acc-0.3974\n",
      "Iter-16650, train loss-2.1274, acc-0.3800, valid loss-2.1181, acc-0.4026, test loss-2.1189, acc-0.3974\n",
      "Iter-16660, train loss-2.1415, acc-0.3600, valid loss-2.1180, acc-0.4028, test loss-2.1188, acc-0.3974\n",
      "Iter-16670, train loss-2.0907, acc-0.4400, valid loss-2.1179, acc-0.4028, test loss-2.1187, acc-0.3970\n",
      "Iter-16680, train loss-2.1138, acc-0.4400, valid loss-2.1178, acc-0.4032, test loss-2.1186, acc-0.3970\n",
      "Iter-16690, train loss-2.1043, acc-0.4600, valid loss-2.1177, acc-0.4032, test loss-2.1185, acc-0.3971\n",
      "Iter-16700, train loss-2.1290, acc-0.4600, valid loss-2.1176, acc-0.4032, test loss-2.1184, acc-0.3972\n",
      "Iter-16710, train loss-2.1152, acc-0.4000, valid loss-2.1175, acc-0.4032, test loss-2.1183, acc-0.3974\n",
      "Iter-16720, train loss-2.0958, acc-0.4200, valid loss-2.1174, acc-0.4030, test loss-2.1182, acc-0.3976\n",
      "Iter-16730, train loss-2.1780, acc-0.2600, valid loss-2.1173, acc-0.4030, test loss-2.1181, acc-0.3975\n",
      "Iter-16740, train loss-2.0924, acc-0.4600, valid loss-2.1172, acc-0.4032, test loss-2.1180, acc-0.3977\n",
      "Iter-16750, train loss-2.1341, acc-0.3600, valid loss-2.1171, acc-0.4034, test loss-2.1179, acc-0.3978\n",
      "Iter-16760, train loss-2.0956, acc-0.5000, valid loss-2.1170, acc-0.4036, test loss-2.1178, acc-0.3978\n",
      "Iter-16770, train loss-2.1163, acc-0.4200, valid loss-2.1169, acc-0.4038, test loss-2.1177, acc-0.3978\n",
      "Iter-16780, train loss-2.1276, acc-0.3800, valid loss-2.1168, acc-0.4036, test loss-2.1176, acc-0.3981\n",
      "Iter-16790, train loss-2.1205, acc-0.3400, valid loss-2.1167, acc-0.4042, test loss-2.1175, acc-0.3978\n",
      "Iter-16800, train loss-2.1141, acc-0.4400, valid loss-2.1166, acc-0.4040, test loss-2.1174, acc-0.3980\n",
      "Iter-16810, train loss-2.1001, acc-0.4800, valid loss-2.1165, acc-0.4040, test loss-2.1173, acc-0.3980\n",
      "Iter-16820, train loss-2.1236, acc-0.2600, valid loss-2.1164, acc-0.4042, test loss-2.1172, acc-0.3979\n",
      "Iter-16830, train loss-2.1074, acc-0.3800, valid loss-2.1163, acc-0.4042, test loss-2.1171, acc-0.3979\n",
      "Iter-16840, train loss-2.1486, acc-0.4000, valid loss-2.1162, acc-0.4042, test loss-2.1170, acc-0.3979\n",
      "Iter-16850, train loss-2.1206, acc-0.3800, valid loss-2.1161, acc-0.4044, test loss-2.1169, acc-0.3980\n",
      "Iter-16860, train loss-2.1170, acc-0.4200, valid loss-2.1160, acc-0.4046, test loss-2.1168, acc-0.3982\n",
      "Iter-16870, train loss-2.0788, acc-0.5600, valid loss-2.1159, acc-0.4046, test loss-2.1167, acc-0.3984\n",
      "Iter-16880, train loss-2.0829, acc-0.4400, valid loss-2.1158, acc-0.4046, test loss-2.1166, acc-0.3982\n",
      "Iter-16890, train loss-2.1424, acc-0.2600, valid loss-2.1157, acc-0.4046, test loss-2.1165, acc-0.3983\n",
      "Iter-16900, train loss-2.1064, acc-0.4200, valid loss-2.1156, acc-0.4048, test loss-2.1164, acc-0.3984\n",
      "Iter-16910, train loss-2.1554, acc-0.2200, valid loss-2.1155, acc-0.4050, test loss-2.1163, acc-0.3982\n",
      "Iter-16920, train loss-2.1330, acc-0.4000, valid loss-2.1154, acc-0.4048, test loss-2.1162, acc-0.3985\n",
      "Iter-16930, train loss-2.0725, acc-0.4400, valid loss-2.1153, acc-0.4050, test loss-2.1161, acc-0.3984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-16940, train loss-2.1480, acc-0.3200, valid loss-2.1152, acc-0.4048, test loss-2.1160, acc-0.3985\n",
      "Iter-16950, train loss-2.1337, acc-0.3200, valid loss-2.1151, acc-0.4046, test loss-2.1159, acc-0.3985\n",
      "Iter-16960, train loss-2.1242, acc-0.3600, valid loss-2.1149, acc-0.4048, test loss-2.1158, acc-0.3986\n",
      "Iter-16970, train loss-2.1557, acc-0.3200, valid loss-2.1148, acc-0.4052, test loss-2.1157, acc-0.3984\n",
      "Iter-16980, train loss-2.0972, acc-0.4200, valid loss-2.1147, acc-0.4054, test loss-2.1156, acc-0.3987\n",
      "Iter-16990, train loss-2.1461, acc-0.4000, valid loss-2.1146, acc-0.4054, test loss-2.1155, acc-0.3988\n",
      "Iter-17000, train loss-2.0747, acc-0.4400, valid loss-2.1145, acc-0.4052, test loss-2.1154, acc-0.3988\n",
      "Iter-17010, train loss-2.1851, acc-0.3400, valid loss-2.1144, acc-0.4052, test loss-2.1152, acc-0.3989\n",
      "Iter-17020, train loss-2.1423, acc-0.3800, valid loss-2.1143, acc-0.4052, test loss-2.1151, acc-0.3988\n",
      "Iter-17030, train loss-2.0883, acc-0.4600, valid loss-2.1142, acc-0.4052, test loss-2.1150, acc-0.3988\n",
      "Iter-17040, train loss-2.1919, acc-0.3000, valid loss-2.1141, acc-0.4054, test loss-2.1149, acc-0.3992\n",
      "Iter-17050, train loss-2.0968, acc-0.4800, valid loss-2.1140, acc-0.4054, test loss-2.1148, acc-0.3991\n",
      "Iter-17060, train loss-2.1033, acc-0.4600, valid loss-2.1139, acc-0.4052, test loss-2.1147, acc-0.3990\n",
      "Iter-17070, train loss-2.1143, acc-0.3600, valid loss-2.1138, acc-0.4052, test loss-2.1146, acc-0.3994\n",
      "Iter-17080, train loss-2.1121, acc-0.4200, valid loss-2.1137, acc-0.4052, test loss-2.1145, acc-0.3992\n",
      "Iter-17090, train loss-2.1569, acc-0.3400, valid loss-2.1136, acc-0.4054, test loss-2.1144, acc-0.3997\n",
      "Iter-17100, train loss-2.1276, acc-0.3200, valid loss-2.1135, acc-0.4054, test loss-2.1143, acc-0.3998\n",
      "Iter-17110, train loss-2.1039, acc-0.5000, valid loss-2.1134, acc-0.4054, test loss-2.1142, acc-0.4001\n",
      "Iter-17120, train loss-2.1362, acc-0.4000, valid loss-2.1133, acc-0.4054, test loss-2.1141, acc-0.3999\n",
      "Iter-17130, train loss-2.1161, acc-0.4200, valid loss-2.1132, acc-0.4054, test loss-2.1140, acc-0.4005\n",
      "Iter-17140, train loss-2.1323, acc-0.3600, valid loss-2.1131, acc-0.4054, test loss-2.1139, acc-0.4002\n",
      "Iter-17150, train loss-2.1264, acc-0.3800, valid loss-2.1130, acc-0.4056, test loss-2.1138, acc-0.4000\n",
      "Iter-17160, train loss-2.1127, acc-0.5000, valid loss-2.1129, acc-0.4060, test loss-2.1137, acc-0.3999\n",
      "Iter-17170, train loss-2.0883, acc-0.4600, valid loss-2.1128, acc-0.4056, test loss-2.1136, acc-0.3999\n",
      "Iter-17180, train loss-2.1228, acc-0.3800, valid loss-2.1127, acc-0.4058, test loss-2.1135, acc-0.4000\n",
      "Iter-17190, train loss-2.0957, acc-0.4400, valid loss-2.1126, acc-0.4056, test loss-2.1134, acc-0.3998\n",
      "Iter-17200, train loss-2.0945, acc-0.4000, valid loss-2.1125, acc-0.4058, test loss-2.1133, acc-0.4000\n",
      "Iter-17210, train loss-2.0790, acc-0.4600, valid loss-2.1124, acc-0.4058, test loss-2.1132, acc-0.3997\n",
      "Iter-17220, train loss-2.1557, acc-0.3600, valid loss-2.1123, acc-0.4058, test loss-2.1131, acc-0.4000\n",
      "Iter-17230, train loss-2.1361, acc-0.3800, valid loss-2.1122, acc-0.4058, test loss-2.1130, acc-0.4002\n",
      "Iter-17240, train loss-2.1468, acc-0.3200, valid loss-2.1121, acc-0.4064, test loss-2.1129, acc-0.3999\n",
      "Iter-17250, train loss-2.1213, acc-0.3400, valid loss-2.1120, acc-0.4062, test loss-2.1128, acc-0.4001\n",
      "Iter-17260, train loss-2.0486, acc-0.5000, valid loss-2.1119, acc-0.4062, test loss-2.1127, acc-0.4003\n",
      "Iter-17270, train loss-2.1114, acc-0.3800, valid loss-2.1118, acc-0.4062, test loss-2.1127, acc-0.4003\n",
      "Iter-17280, train loss-2.0871, acc-0.4400, valid loss-2.1117, acc-0.4060, test loss-2.1125, acc-0.4004\n",
      "Iter-17290, train loss-2.1114, acc-0.4200, valid loss-2.1116, acc-0.4064, test loss-2.1124, acc-0.4004\n",
      "Iter-17300, train loss-2.0977, acc-0.4000, valid loss-2.1115, acc-0.4066, test loss-2.1123, acc-0.4006\n",
      "Iter-17310, train loss-2.1353, acc-0.3200, valid loss-2.1114, acc-0.4068, test loss-2.1122, acc-0.4006\n",
      "Iter-17320, train loss-2.1616, acc-0.3400, valid loss-2.1113, acc-0.4068, test loss-2.1121, acc-0.4007\n",
      "Iter-17330, train loss-2.1286, acc-0.3800, valid loss-2.1112, acc-0.4068, test loss-2.1120, acc-0.4007\n",
      "Iter-17340, train loss-2.1369, acc-0.3800, valid loss-2.1111, acc-0.4064, test loss-2.1119, acc-0.4008\n",
      "Iter-17350, train loss-2.0743, acc-0.4600, valid loss-2.1110, acc-0.4062, test loss-2.1118, acc-0.4006\n",
      "Iter-17360, train loss-2.1303, acc-0.3600, valid loss-2.1109, acc-0.4060, test loss-2.1117, acc-0.4003\n",
      "Iter-17370, train loss-2.1085, acc-0.4000, valid loss-2.1108, acc-0.4064, test loss-2.1116, acc-0.4005\n",
      "Iter-17380, train loss-2.1207, acc-0.4400, valid loss-2.1107, acc-0.4062, test loss-2.1115, acc-0.4007\n",
      "Iter-17390, train loss-2.1417, acc-0.3600, valid loss-2.1106, acc-0.4068, test loss-2.1114, acc-0.4010\n",
      "Iter-17400, train loss-2.0875, acc-0.4400, valid loss-2.1105, acc-0.4070, test loss-2.1113, acc-0.4012\n",
      "Iter-17410, train loss-2.1094, acc-0.4000, valid loss-2.1104, acc-0.4068, test loss-2.1112, acc-0.4013\n",
      "Iter-17420, train loss-2.1160, acc-0.3600, valid loss-2.1103, acc-0.4068, test loss-2.1111, acc-0.4012\n",
      "Iter-17430, train loss-2.0810, acc-0.5000, valid loss-2.1102, acc-0.4068, test loss-2.1110, acc-0.4012\n",
      "Iter-17440, train loss-2.1633, acc-0.2800, valid loss-2.1101, acc-0.4066, test loss-2.1109, acc-0.4012\n",
      "Iter-17450, train loss-2.1667, acc-0.3400, valid loss-2.1100, acc-0.4070, test loss-2.1108, acc-0.4014\n",
      "Iter-17460, train loss-2.1255, acc-0.3400, valid loss-2.1099, acc-0.4066, test loss-2.1107, acc-0.4014\n",
      "Iter-17470, train loss-2.1462, acc-0.2600, valid loss-2.1098, acc-0.4064, test loss-2.1106, acc-0.4015\n",
      "Iter-17480, train loss-2.1714, acc-0.2400, valid loss-2.1097, acc-0.4064, test loss-2.1105, acc-0.4016\n",
      "Iter-17490, train loss-2.0916, acc-0.5000, valid loss-2.1096, acc-0.4068, test loss-2.1104, acc-0.4018\n",
      "Iter-17500, train loss-2.1687, acc-0.3000, valid loss-2.1095, acc-0.4066, test loss-2.1103, acc-0.4019\n",
      "Iter-17510, train loss-2.1058, acc-0.4200, valid loss-2.1094, acc-0.4066, test loss-2.1102, acc-0.4015\n",
      "Iter-17520, train loss-2.1284, acc-0.3800, valid loss-2.1093, acc-0.4064, test loss-2.1102, acc-0.4017\n",
      "Iter-17530, train loss-2.1195, acc-0.4000, valid loss-2.1092, acc-0.4068, test loss-2.1101, acc-0.4016\n",
      "Iter-17540, train loss-2.0794, acc-0.4400, valid loss-2.1091, acc-0.4068, test loss-2.1099, acc-0.4018\n",
      "Iter-17550, train loss-2.1342, acc-0.3000, valid loss-2.1090, acc-0.4068, test loss-2.1099, acc-0.4018\n",
      "Iter-17560, train loss-2.1582, acc-0.3400, valid loss-2.1089, acc-0.4070, test loss-2.1098, acc-0.4019\n",
      "Iter-17570, train loss-2.0912, acc-0.4400, valid loss-2.1088, acc-0.4072, test loss-2.1097, acc-0.4020\n",
      "Iter-17580, train loss-2.1409, acc-0.4200, valid loss-2.1088, acc-0.4074, test loss-2.1096, acc-0.4021\n",
      "Iter-17590, train loss-2.1057, acc-0.4000, valid loss-2.1087, acc-0.4078, test loss-2.1095, acc-0.4020\n",
      "Iter-17600, train loss-2.1102, acc-0.3400, valid loss-2.1086, acc-0.4074, test loss-2.1094, acc-0.4021\n",
      "Iter-17610, train loss-2.1018, acc-0.5200, valid loss-2.1085, acc-0.4080, test loss-2.1093, acc-0.4022\n",
      "Iter-17620, train loss-2.1318, acc-0.4000, valid loss-2.1084, acc-0.4080, test loss-2.1092, acc-0.4023\n",
      "Iter-17630, train loss-2.0822, acc-0.5000, valid loss-2.1083, acc-0.4080, test loss-2.1091, acc-0.4023\n",
      "Iter-17640, train loss-2.1597, acc-0.4000, valid loss-2.1082, acc-0.4080, test loss-2.1090, acc-0.4023\n",
      "Iter-17650, train loss-2.0816, acc-0.5000, valid loss-2.1081, acc-0.4078, test loss-2.1089, acc-0.4025\n",
      "Iter-17660, train loss-2.1201, acc-0.4000, valid loss-2.1079, acc-0.4078, test loss-2.1088, acc-0.4026\n",
      "Iter-17670, train loss-2.1040, acc-0.4200, valid loss-2.1079, acc-0.4084, test loss-2.1087, acc-0.4024\n",
      "Iter-17680, train loss-2.1326, acc-0.3600, valid loss-2.1078, acc-0.4084, test loss-2.1086, acc-0.4024\n",
      "Iter-17690, train loss-2.1125, acc-0.4000, valid loss-2.1077, acc-0.4082, test loss-2.1085, acc-0.4025\n",
      "Iter-17700, train loss-2.0931, acc-0.5400, valid loss-2.1076, acc-0.4078, test loss-2.1084, acc-0.4028\n",
      "Iter-17710, train loss-2.1261, acc-0.4200, valid loss-2.1074, acc-0.4082, test loss-2.1083, acc-0.4029\n",
      "Iter-17720, train loss-2.0357, acc-0.5600, valid loss-2.1073, acc-0.4080, test loss-2.1082, acc-0.4031\n",
      "Iter-17730, train loss-2.1058, acc-0.4200, valid loss-2.1072, acc-0.4078, test loss-2.1081, acc-0.4027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-17740, train loss-2.1376, acc-0.3200, valid loss-2.1071, acc-0.4084, test loss-2.1080, acc-0.4031\n",
      "Iter-17750, train loss-2.1330, acc-0.4000, valid loss-2.1070, acc-0.4082, test loss-2.1079, acc-0.4030\n",
      "Iter-17760, train loss-2.0772, acc-0.4000, valid loss-2.1069, acc-0.4082, test loss-2.1078, acc-0.4030\n",
      "Iter-17770, train loss-2.0343, acc-0.5000, valid loss-2.1068, acc-0.4082, test loss-2.1077, acc-0.4031\n",
      "Iter-17780, train loss-2.0879, acc-0.4200, valid loss-2.1067, acc-0.4082, test loss-2.1076, acc-0.4033\n",
      "Iter-17790, train loss-2.1442, acc-0.3600, valid loss-2.1066, acc-0.4080, test loss-2.1075, acc-0.4034\n",
      "Iter-17800, train loss-2.1258, acc-0.3400, valid loss-2.1065, acc-0.4082, test loss-2.1074, acc-0.4033\n",
      "Iter-17810, train loss-2.1283, acc-0.3800, valid loss-2.1064, acc-0.4082, test loss-2.1073, acc-0.4034\n",
      "Iter-17820, train loss-2.1710, acc-0.3600, valid loss-2.1063, acc-0.4082, test loss-2.1072, acc-0.4033\n",
      "Iter-17830, train loss-2.1199, acc-0.3800, valid loss-2.1062, acc-0.4082, test loss-2.1070, acc-0.4034\n",
      "Iter-17840, train loss-2.1336, acc-0.3400, valid loss-2.1061, acc-0.4080, test loss-2.1069, acc-0.4034\n",
      "Iter-17850, train loss-2.0874, acc-0.5000, valid loss-2.1060, acc-0.4082, test loss-2.1069, acc-0.4035\n",
      "Iter-17860, train loss-2.1276, acc-0.3200, valid loss-2.1059, acc-0.4082, test loss-2.1067, acc-0.4035\n",
      "Iter-17870, train loss-2.1025, acc-0.3800, valid loss-2.1058, acc-0.4084, test loss-2.1067, acc-0.4036\n",
      "Iter-17880, train loss-2.1281, acc-0.3200, valid loss-2.1057, acc-0.4084, test loss-2.1066, acc-0.4036\n",
      "Iter-17890, train loss-2.1040, acc-0.4400, valid loss-2.1056, acc-0.4084, test loss-2.1065, acc-0.4034\n",
      "Iter-17900, train loss-2.0944, acc-0.4400, valid loss-2.1055, acc-0.4084, test loss-2.1064, acc-0.4034\n",
      "Iter-17910, train loss-2.1276, acc-0.3600, valid loss-2.1054, acc-0.4084, test loss-2.1063, acc-0.4034\n",
      "Iter-17920, train loss-2.0951, acc-0.4200, valid loss-2.1053, acc-0.4082, test loss-2.1062, acc-0.4039\n",
      "Iter-17930, train loss-2.0464, acc-0.4600, valid loss-2.1052, acc-0.4082, test loss-2.1061, acc-0.4036\n",
      "Iter-17940, train loss-2.1148, acc-0.3400, valid loss-2.1051, acc-0.4084, test loss-2.1060, acc-0.4039\n",
      "Iter-17950, train loss-2.1629, acc-0.2800, valid loss-2.1050, acc-0.4084, test loss-2.1059, acc-0.4039\n",
      "Iter-17960, train loss-2.0750, acc-0.4600, valid loss-2.1049, acc-0.4082, test loss-2.1057, acc-0.4039\n",
      "Iter-17970, train loss-2.1164, acc-0.3800, valid loss-2.1048, acc-0.4080, test loss-2.1057, acc-0.4039\n",
      "Iter-17980, train loss-2.1406, acc-0.3600, valid loss-2.1047, acc-0.4082, test loss-2.1055, acc-0.4039\n",
      "Iter-17990, train loss-2.0857, acc-0.4400, valid loss-2.1046, acc-0.4084, test loss-2.1054, acc-0.4039\n",
      "Iter-18000, train loss-2.0901, acc-0.4400, valid loss-2.1045, acc-0.4082, test loss-2.1054, acc-0.4037\n",
      "Iter-18010, train loss-2.1549, acc-0.3400, valid loss-2.1044, acc-0.4084, test loss-2.1052, acc-0.4038\n",
      "Iter-18020, train loss-2.0848, acc-0.4600, valid loss-2.1043, acc-0.4082, test loss-2.1051, acc-0.4038\n",
      "Iter-18030, train loss-2.1221, acc-0.3600, valid loss-2.1042, acc-0.4082, test loss-2.1051, acc-0.4042\n",
      "Iter-18040, train loss-2.1212, acc-0.3600, valid loss-2.1041, acc-0.4082, test loss-2.1049, acc-0.4042\n",
      "Iter-18050, train loss-2.1074, acc-0.3200, valid loss-2.1040, acc-0.4084, test loss-2.1048, acc-0.4041\n",
      "Iter-18060, train loss-2.0480, acc-0.4000, valid loss-2.1039, acc-0.4084, test loss-2.1047, acc-0.4038\n",
      "Iter-18070, train loss-2.1368, acc-0.3000, valid loss-2.1038, acc-0.4082, test loss-2.1046, acc-0.4040\n",
      "Iter-18080, train loss-2.0986, acc-0.4000, valid loss-2.1037, acc-0.4084, test loss-2.1045, acc-0.4038\n",
      "Iter-18090, train loss-2.1100, acc-0.4600, valid loss-2.1036, acc-0.4084, test loss-2.1044, acc-0.4038\n",
      "Iter-18100, train loss-2.1173, acc-0.3800, valid loss-2.1035, acc-0.4084, test loss-2.1043, acc-0.4037\n",
      "Iter-18110, train loss-2.1316, acc-0.3800, valid loss-2.1034, acc-0.4084, test loss-2.1042, acc-0.4037\n",
      "Iter-18120, train loss-2.1339, acc-0.3200, valid loss-2.1033, acc-0.4084, test loss-2.1041, acc-0.4039\n",
      "Iter-18130, train loss-2.0790, acc-0.4400, valid loss-2.1032, acc-0.4084, test loss-2.1040, acc-0.4041\n",
      "Iter-18140, train loss-2.0620, acc-0.5000, valid loss-2.1031, acc-0.4084, test loss-2.1039, acc-0.4041\n",
      "Iter-18150, train loss-2.1158, acc-0.4800, valid loss-2.1030, acc-0.4084, test loss-2.1038, acc-0.4039\n",
      "Iter-18160, train loss-2.0897, acc-0.4600, valid loss-2.1029, acc-0.4084, test loss-2.1037, acc-0.4041\n",
      "Iter-18170, train loss-2.1057, acc-0.4600, valid loss-2.1028, acc-0.4082, test loss-2.1036, acc-0.4041\n",
      "Iter-18180, train loss-2.1380, acc-0.3800, valid loss-2.1027, acc-0.4082, test loss-2.1035, acc-0.4041\n",
      "Iter-18190, train loss-2.1769, acc-0.3600, valid loss-2.1026, acc-0.4084, test loss-2.1034, acc-0.4042\n",
      "Iter-18200, train loss-2.0486, acc-0.4800, valid loss-2.1025, acc-0.4082, test loss-2.1033, acc-0.4041\n",
      "Iter-18210, train loss-2.1401, acc-0.3800, valid loss-2.1024, acc-0.4084, test loss-2.1032, acc-0.4041\n",
      "Iter-18220, train loss-2.0817, acc-0.3800, valid loss-2.1023, acc-0.4086, test loss-2.1031, acc-0.4042\n",
      "Iter-18230, train loss-2.0635, acc-0.4800, valid loss-2.1022, acc-0.4084, test loss-2.1030, acc-0.4043\n",
      "Iter-18240, train loss-2.0728, acc-0.4000, valid loss-2.1021, acc-0.4086, test loss-2.1029, acc-0.4044\n",
      "Iter-18250, train loss-2.1570, acc-0.2800, valid loss-2.1020, acc-0.4086, test loss-2.1028, acc-0.4042\n",
      "Iter-18260, train loss-2.1176, acc-0.3800, valid loss-2.1019, acc-0.4086, test loss-2.1027, acc-0.4046\n",
      "Iter-18270, train loss-2.0957, acc-0.4400, valid loss-2.1018, acc-0.4086, test loss-2.1026, acc-0.4043\n",
      "Iter-18280, train loss-2.1083, acc-0.3400, valid loss-2.1017, acc-0.4086, test loss-2.1025, acc-0.4046\n",
      "Iter-18290, train loss-2.1400, acc-0.3600, valid loss-2.1016, acc-0.4086, test loss-2.1024, acc-0.4045\n",
      "Iter-18300, train loss-2.0726, acc-0.3800, valid loss-2.1015, acc-0.4086, test loss-2.1023, acc-0.4043\n",
      "Iter-18310, train loss-2.1469, acc-0.3000, valid loss-2.1014, acc-0.4086, test loss-2.1022, acc-0.4049\n",
      "Iter-18320, train loss-2.1043, acc-0.4000, valid loss-2.1013, acc-0.4086, test loss-2.1021, acc-0.4047\n",
      "Iter-18330, train loss-2.0698, acc-0.4800, valid loss-2.1012, acc-0.4086, test loss-2.1020, acc-0.4050\n",
      "Iter-18340, train loss-2.0782, acc-0.5000, valid loss-2.1011, acc-0.4086, test loss-2.1019, acc-0.4055\n",
      "Iter-18350, train loss-2.1290, acc-0.3800, valid loss-2.1010, acc-0.4086, test loss-2.1018, acc-0.4056\n",
      "Iter-18360, train loss-2.1305, acc-0.4200, valid loss-2.1009, acc-0.4090, test loss-2.1017, acc-0.4057\n",
      "Iter-18370, train loss-2.0825, acc-0.5400, valid loss-2.1008, acc-0.4090, test loss-2.1016, acc-0.4057\n",
      "Iter-18380, train loss-2.1109, acc-0.3800, valid loss-2.1007, acc-0.4090, test loss-2.1015, acc-0.4059\n",
      "Iter-18390, train loss-2.0881, acc-0.3600, valid loss-2.1006, acc-0.4088, test loss-2.1014, acc-0.4061\n",
      "Iter-18400, train loss-2.1119, acc-0.3800, valid loss-2.1005, acc-0.4086, test loss-2.1013, acc-0.4058\n",
      "Iter-18410, train loss-2.0618, acc-0.4200, valid loss-2.1004, acc-0.4086, test loss-2.1012, acc-0.4058\n",
      "Iter-18420, train loss-2.1261, acc-0.3800, valid loss-2.1003, acc-0.4088, test loss-2.1011, acc-0.4058\n",
      "Iter-18430, train loss-2.0758, acc-0.4000, valid loss-2.1002, acc-0.4088, test loss-2.1010, acc-0.4056\n",
      "Iter-18440, train loss-2.1553, acc-0.2800, valid loss-2.1001, acc-0.4090, test loss-2.1009, acc-0.4060\n",
      "Iter-18450, train loss-2.1010, acc-0.4000, valid loss-2.1000, acc-0.4094, test loss-2.1008, acc-0.4059\n",
      "Iter-18460, train loss-2.0823, acc-0.4600, valid loss-2.0999, acc-0.4092, test loss-2.1007, acc-0.4061\n",
      "Iter-18470, train loss-2.1189, acc-0.4400, valid loss-2.0998, acc-0.4090, test loss-2.1006, acc-0.4060\n",
      "Iter-18480, train loss-2.0809, acc-0.4400, valid loss-2.0997, acc-0.4092, test loss-2.1005, acc-0.4059\n",
      "Iter-18490, train loss-2.0737, acc-0.3800, valid loss-2.0996, acc-0.4096, test loss-2.1004, acc-0.4060\n",
      "Iter-18500, train loss-2.0698, acc-0.5400, valid loss-2.0995, acc-0.4094, test loss-2.1003, acc-0.4060\n",
      "Iter-18510, train loss-2.1071, acc-0.4400, valid loss-2.0994, acc-0.4094, test loss-2.1002, acc-0.4058\n",
      "Iter-18520, train loss-2.1109, acc-0.4200, valid loss-2.0993, acc-0.4094, test loss-2.1001, acc-0.4060\n",
      "Iter-18530, train loss-2.1029, acc-0.4000, valid loss-2.0992, acc-0.4096, test loss-2.1000, acc-0.4060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-18540, train loss-2.1044, acc-0.3200, valid loss-2.0991, acc-0.4096, test loss-2.0999, acc-0.4062\n",
      "Iter-18550, train loss-2.1265, acc-0.4000, valid loss-2.0990, acc-0.4098, test loss-2.0998, acc-0.4060\n",
      "Iter-18560, train loss-2.0866, acc-0.3200, valid loss-2.0989, acc-0.4098, test loss-2.0998, acc-0.4060\n",
      "Iter-18570, train loss-2.0487, acc-0.4000, valid loss-2.0988, acc-0.4096, test loss-2.0997, acc-0.4060\n",
      "Iter-18580, train loss-2.0664, acc-0.4400, valid loss-2.0987, acc-0.4102, test loss-2.0996, acc-0.4061\n",
      "Iter-18590, train loss-2.0620, acc-0.4400, valid loss-2.0986, acc-0.4098, test loss-2.0995, acc-0.4058\n",
      "Iter-18600, train loss-2.0557, acc-0.5400, valid loss-2.0985, acc-0.4100, test loss-2.0993, acc-0.4060\n",
      "Iter-18610, train loss-2.1166, acc-0.3400, valid loss-2.0984, acc-0.4102, test loss-2.0992, acc-0.4059\n",
      "Iter-18620, train loss-2.1150, acc-0.3400, valid loss-2.0983, acc-0.4102, test loss-2.0991, acc-0.4060\n",
      "Iter-18630, train loss-2.0713, acc-0.4400, valid loss-2.0982, acc-0.4100, test loss-2.0990, acc-0.4060\n",
      "Iter-18640, train loss-2.1226, acc-0.3000, valid loss-2.0981, acc-0.4102, test loss-2.0989, acc-0.4062\n",
      "Iter-18650, train loss-2.1064, acc-0.4200, valid loss-2.0980, acc-0.4100, test loss-2.0989, acc-0.4061\n",
      "Iter-18660, train loss-2.0907, acc-0.4200, valid loss-2.0979, acc-0.4100, test loss-2.0987, acc-0.4063\n",
      "Iter-18670, train loss-2.0586, acc-0.4600, valid loss-2.0978, acc-0.4098, test loss-2.0987, acc-0.4063\n",
      "Iter-18680, train loss-2.1298, acc-0.4200, valid loss-2.0977, acc-0.4098, test loss-2.0986, acc-0.4064\n",
      "Iter-18690, train loss-2.0775, acc-0.4400, valid loss-2.0976, acc-0.4100, test loss-2.0985, acc-0.4063\n",
      "Iter-18700, train loss-2.0953, acc-0.4400, valid loss-2.0975, acc-0.4102, test loss-2.0984, acc-0.4063\n",
      "Iter-18710, train loss-2.1139, acc-0.4000, valid loss-2.0974, acc-0.4102, test loss-2.0983, acc-0.4062\n",
      "Iter-18720, train loss-2.1596, acc-0.3000, valid loss-2.0973, acc-0.4106, test loss-2.0982, acc-0.4064\n",
      "Iter-18730, train loss-2.1489, acc-0.3200, valid loss-2.0972, acc-0.4108, test loss-2.0981, acc-0.4062\n",
      "Iter-18740, train loss-2.0951, acc-0.4600, valid loss-2.0971, acc-0.4110, test loss-2.0980, acc-0.4064\n",
      "Iter-18750, train loss-2.1412, acc-0.3200, valid loss-2.0970, acc-0.4112, test loss-2.0979, acc-0.4066\n",
      "Iter-18760, train loss-2.1067, acc-0.4200, valid loss-2.0969, acc-0.4112, test loss-2.0978, acc-0.4067\n",
      "Iter-18770, train loss-2.0921, acc-0.3400, valid loss-2.0968, acc-0.4112, test loss-2.0977, acc-0.4070\n",
      "Iter-18780, train loss-2.0889, acc-0.4800, valid loss-2.0967, acc-0.4114, test loss-2.0976, acc-0.4071\n",
      "Iter-18790, train loss-2.0842, acc-0.4000, valid loss-2.0966, acc-0.4114, test loss-2.0975, acc-0.4071\n",
      "Iter-18800, train loss-2.1379, acc-0.2800, valid loss-2.0965, acc-0.4114, test loss-2.0974, acc-0.4071\n",
      "Iter-18810, train loss-2.0923, acc-0.3600, valid loss-2.0964, acc-0.4112, test loss-2.0973, acc-0.4074\n",
      "Iter-18820, train loss-2.0839, acc-0.4400, valid loss-2.0963, acc-0.4114, test loss-2.0972, acc-0.4076\n",
      "Iter-18830, train loss-2.1337, acc-0.3600, valid loss-2.0962, acc-0.4112, test loss-2.0971, acc-0.4076\n",
      "Iter-18840, train loss-2.0908, acc-0.4600, valid loss-2.0961, acc-0.4112, test loss-2.0970, acc-0.4076\n",
      "Iter-18850, train loss-2.0543, acc-0.5400, valid loss-2.0960, acc-0.4112, test loss-2.0969, acc-0.4076\n",
      "Iter-18860, train loss-2.1125, acc-0.4000, valid loss-2.0959, acc-0.4118, test loss-2.0968, acc-0.4077\n",
      "Iter-18870, train loss-2.1874, acc-0.2400, valid loss-2.0958, acc-0.4126, test loss-2.0967, acc-0.4075\n",
      "Iter-18880, train loss-2.1052, acc-0.4600, valid loss-2.0957, acc-0.4122, test loss-2.0966, acc-0.4077\n",
      "Iter-18890, train loss-2.0972, acc-0.4600, valid loss-2.0956, acc-0.4122, test loss-2.0965, acc-0.4077\n",
      "Iter-18900, train loss-2.1365, acc-0.3200, valid loss-2.0955, acc-0.4126, test loss-2.0964, acc-0.4076\n",
      "Iter-18910, train loss-2.1752, acc-0.2400, valid loss-2.0954, acc-0.4128, test loss-2.0963, acc-0.4077\n",
      "Iter-18920, train loss-2.0955, acc-0.3400, valid loss-2.0953, acc-0.4128, test loss-2.0962, acc-0.4079\n",
      "Iter-18930, train loss-2.1331, acc-0.4200, valid loss-2.0953, acc-0.4126, test loss-2.0961, acc-0.4080\n",
      "Iter-18940, train loss-2.0892, acc-0.4200, valid loss-2.0952, acc-0.4128, test loss-2.0960, acc-0.4079\n",
      "Iter-18950, train loss-2.0996, acc-0.4000, valid loss-2.0951, acc-0.4130, test loss-2.0959, acc-0.4079\n",
      "Iter-18960, train loss-2.0988, acc-0.4400, valid loss-2.0950, acc-0.4130, test loss-2.0958, acc-0.4079\n",
      "Iter-18970, train loss-2.1152, acc-0.4400, valid loss-2.0949, acc-0.4128, test loss-2.0957, acc-0.4081\n",
      "Iter-18980, train loss-2.0944, acc-0.4400, valid loss-2.0948, acc-0.4132, test loss-2.0956, acc-0.4080\n",
      "Iter-18990, train loss-2.0649, acc-0.4800, valid loss-2.0946, acc-0.4134, test loss-2.0955, acc-0.4081\n",
      "Iter-19000, train loss-2.1228, acc-0.4200, valid loss-2.0946, acc-0.4138, test loss-2.0954, acc-0.4082\n",
      "Iter-19010, train loss-2.1326, acc-0.3400, valid loss-2.0945, acc-0.4138, test loss-2.0953, acc-0.4081\n",
      "Iter-19020, train loss-2.1144, acc-0.4600, valid loss-2.0944, acc-0.4134, test loss-2.0952, acc-0.4081\n",
      "Iter-19030, train loss-2.0836, acc-0.3800, valid loss-2.0943, acc-0.4136, test loss-2.0951, acc-0.4081\n",
      "Iter-19040, train loss-2.1003, acc-0.3600, valid loss-2.0942, acc-0.4134, test loss-2.0950, acc-0.4084\n",
      "Iter-19050, train loss-2.0868, acc-0.4200, valid loss-2.0941, acc-0.4134, test loss-2.0949, acc-0.4083\n",
      "Iter-19060, train loss-2.0973, acc-0.4200, valid loss-2.0940, acc-0.4138, test loss-2.0948, acc-0.4083\n",
      "Iter-19070, train loss-2.1746, acc-0.3000, valid loss-2.0939, acc-0.4138, test loss-2.0947, acc-0.4083\n",
      "Iter-19080, train loss-2.0948, acc-0.3800, valid loss-2.0938, acc-0.4136, test loss-2.0946, acc-0.4084\n",
      "Iter-19090, train loss-2.0756, acc-0.3800, valid loss-2.0937, acc-0.4136, test loss-2.0945, acc-0.4084\n",
      "Iter-19100, train loss-2.1345, acc-0.3000, valid loss-2.0936, acc-0.4136, test loss-2.0944, acc-0.4083\n",
      "Iter-19110, train loss-2.0831, acc-0.3600, valid loss-2.0935, acc-0.4136, test loss-2.0943, acc-0.4086\n",
      "Iter-19120, train loss-2.1257, acc-0.4200, valid loss-2.0934, acc-0.4142, test loss-2.0942, acc-0.4082\n",
      "Iter-19130, train loss-2.0699, acc-0.5200, valid loss-2.0933, acc-0.4140, test loss-2.0941, acc-0.4085\n",
      "Iter-19140, train loss-2.0770, acc-0.4200, valid loss-2.0932, acc-0.4138, test loss-2.0941, acc-0.4086\n",
      "Iter-19150, train loss-2.1212, acc-0.3400, valid loss-2.0931, acc-0.4142, test loss-2.0940, acc-0.4084\n",
      "Iter-19160, train loss-2.1143, acc-0.4600, valid loss-2.0930, acc-0.4144, test loss-2.0939, acc-0.4087\n",
      "Iter-19170, train loss-2.0966, acc-0.3600, valid loss-2.0929, acc-0.4144, test loss-2.0938, acc-0.4088\n",
      "Iter-19180, train loss-2.1036, acc-0.3400, valid loss-2.0928, acc-0.4144, test loss-2.0937, acc-0.4087\n",
      "Iter-19190, train loss-2.1261, acc-0.3600, valid loss-2.0927, acc-0.4144, test loss-2.0936, acc-0.4089\n",
      "Iter-19200, train loss-2.0935, acc-0.4400, valid loss-2.0926, acc-0.4144, test loss-2.0935, acc-0.4093\n",
      "Iter-19210, train loss-2.1048, acc-0.3800, valid loss-2.0925, acc-0.4144, test loss-2.0934, acc-0.4092\n",
      "Iter-19220, train loss-2.1586, acc-0.2800, valid loss-2.0924, acc-0.4144, test loss-2.0933, acc-0.4095\n",
      "Iter-19230, train loss-2.0911, acc-0.3000, valid loss-2.0923, acc-0.4150, test loss-2.0932, acc-0.4095\n",
      "Iter-19240, train loss-2.1245, acc-0.4000, valid loss-2.0922, acc-0.4150, test loss-2.0931, acc-0.4094\n",
      "Iter-19250, train loss-2.0750, acc-0.5000, valid loss-2.0921, acc-0.4150, test loss-2.0930, acc-0.4096\n",
      "Iter-19260, train loss-2.0992, acc-0.3600, valid loss-2.0920, acc-0.4148, test loss-2.0929, acc-0.4096\n",
      "Iter-19270, train loss-2.1006, acc-0.3600, valid loss-2.0919, acc-0.4148, test loss-2.0928, acc-0.4098\n",
      "Iter-19280, train loss-2.0062, acc-0.6400, valid loss-2.0918, acc-0.4150, test loss-2.0927, acc-0.4097\n",
      "Iter-19290, train loss-2.0709, acc-0.5200, valid loss-2.0917, acc-0.4148, test loss-2.0926, acc-0.4097\n",
      "Iter-19300, train loss-2.0992, acc-0.4400, valid loss-2.0917, acc-0.4154, test loss-2.0925, acc-0.4096\n",
      "Iter-19310, train loss-2.0902, acc-0.5200, valid loss-2.0915, acc-0.4154, test loss-2.0924, acc-0.4096\n",
      "Iter-19320, train loss-2.0968, acc-0.4000, valid loss-2.0914, acc-0.4156, test loss-2.0923, acc-0.4094\n",
      "Iter-19330, train loss-2.0510, acc-0.4400, valid loss-2.0913, acc-0.4156, test loss-2.0922, acc-0.4094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-19340, train loss-2.0811, acc-0.4600, valid loss-2.0913, acc-0.4154, test loss-2.0921, acc-0.4094\n",
      "Iter-19350, train loss-2.0783, acc-0.4200, valid loss-2.0911, acc-0.4154, test loss-2.0920, acc-0.4097\n",
      "Iter-19360, train loss-2.0622, acc-0.4600, valid loss-2.0911, acc-0.4154, test loss-2.0919, acc-0.4096\n",
      "Iter-19370, train loss-2.0956, acc-0.5000, valid loss-2.0909, acc-0.4154, test loss-2.0918, acc-0.4094\n",
      "Iter-19380, train loss-2.0703, acc-0.4200, valid loss-2.0908, acc-0.4152, test loss-2.0917, acc-0.4094\n",
      "Iter-19390, train loss-2.1022, acc-0.3600, valid loss-2.0907, acc-0.4152, test loss-2.0916, acc-0.4096\n",
      "Iter-19400, train loss-2.0826, acc-0.4000, valid loss-2.0906, acc-0.4154, test loss-2.0915, acc-0.4097\n",
      "Iter-19410, train loss-2.0834, acc-0.4800, valid loss-2.0905, acc-0.4152, test loss-2.0914, acc-0.4097\n",
      "Iter-19420, train loss-2.0496, acc-0.4800, valid loss-2.0905, acc-0.4154, test loss-2.0913, acc-0.4093\n",
      "Iter-19430, train loss-2.1035, acc-0.4400, valid loss-2.0904, acc-0.4152, test loss-2.0912, acc-0.4095\n",
      "Iter-19440, train loss-2.1412, acc-0.3800, valid loss-2.0903, acc-0.4154, test loss-2.0911, acc-0.4095\n",
      "Iter-19450, train loss-2.0815, acc-0.4400, valid loss-2.0902, acc-0.4154, test loss-2.0910, acc-0.4096\n",
      "Iter-19460, train loss-2.0741, acc-0.3600, valid loss-2.0900, acc-0.4152, test loss-2.0909, acc-0.4095\n",
      "Iter-19470, train loss-2.1343, acc-0.3200, valid loss-2.0899, acc-0.4150, test loss-2.0908, acc-0.4096\n",
      "Iter-19480, train loss-2.1259, acc-0.3400, valid loss-2.0898, acc-0.4148, test loss-2.0907, acc-0.4094\n",
      "Iter-19490, train loss-2.0856, acc-0.3600, valid loss-2.0898, acc-0.4150, test loss-2.0906, acc-0.4094\n",
      "Iter-19500, train loss-2.1102, acc-0.4200, valid loss-2.0896, acc-0.4152, test loss-2.0905, acc-0.4096\n",
      "Iter-19510, train loss-2.1433, acc-0.2800, valid loss-2.0895, acc-0.4150, test loss-2.0904, acc-0.4097\n",
      "Iter-19520, train loss-2.1205, acc-0.4600, valid loss-2.0894, acc-0.4148, test loss-2.0903, acc-0.4096\n",
      "Iter-19530, train loss-2.0744, acc-0.4200, valid loss-2.0893, acc-0.4152, test loss-2.0902, acc-0.4099\n",
      "Iter-19540, train loss-2.0493, acc-0.4600, valid loss-2.0892, acc-0.4152, test loss-2.0901, acc-0.4099\n",
      "Iter-19550, train loss-2.1099, acc-0.4000, valid loss-2.0892, acc-0.4150, test loss-2.0900, acc-0.4099\n",
      "Iter-19560, train loss-2.0633, acc-0.4400, valid loss-2.0891, acc-0.4148, test loss-2.0899, acc-0.4097\n",
      "Iter-19570, train loss-2.1045, acc-0.3200, valid loss-2.0890, acc-0.4152, test loss-2.0898, acc-0.4099\n",
      "Iter-19580, train loss-2.0657, acc-0.4200, valid loss-2.0889, acc-0.4150, test loss-2.0897, acc-0.4099\n",
      "Iter-19590, train loss-2.0880, acc-0.4400, valid loss-2.0888, acc-0.4152, test loss-2.0896, acc-0.4099\n",
      "Iter-19600, train loss-2.1003, acc-0.3600, valid loss-2.0887, acc-0.4152, test loss-2.0895, acc-0.4101\n",
      "Iter-19610, train loss-2.0909, acc-0.3800, valid loss-2.0886, acc-0.4154, test loss-2.0894, acc-0.4102\n",
      "Iter-19620, train loss-2.1093, acc-0.4200, valid loss-2.0885, acc-0.4152, test loss-2.0893, acc-0.4101\n",
      "Iter-19630, train loss-2.0588, acc-0.4600, valid loss-2.0884, acc-0.4152, test loss-2.0892, acc-0.4101\n",
      "Iter-19640, train loss-2.1074, acc-0.2800, valid loss-2.0883, acc-0.4156, test loss-2.0891, acc-0.4104\n",
      "Iter-19650, train loss-2.1258, acc-0.3800, valid loss-2.0882, acc-0.4158, test loss-2.0890, acc-0.4103\n",
      "Iter-19660, train loss-2.0998, acc-0.4000, valid loss-2.0881, acc-0.4156, test loss-2.0890, acc-0.4101\n",
      "Iter-19670, train loss-2.0871, acc-0.3800, valid loss-2.0880, acc-0.4156, test loss-2.0889, acc-0.4104\n",
      "Iter-19680, train loss-2.0492, acc-0.4400, valid loss-2.0879, acc-0.4154, test loss-2.0888, acc-0.4105\n",
      "Iter-19690, train loss-2.1233, acc-0.3000, valid loss-2.0878, acc-0.4154, test loss-2.0887, acc-0.4105\n",
      "Iter-19700, train loss-2.0353, acc-0.4800, valid loss-2.0877, acc-0.4154, test loss-2.0886, acc-0.4103\n",
      "Iter-19710, train loss-2.0881, acc-0.4400, valid loss-2.0876, acc-0.4154, test loss-2.0884, acc-0.4104\n",
      "Iter-19720, train loss-2.0975, acc-0.3400, valid loss-2.0875, acc-0.4156, test loss-2.0883, acc-0.4105\n",
      "Iter-19730, train loss-2.0410, acc-0.5000, valid loss-2.0874, acc-0.4154, test loss-2.0883, acc-0.4104\n",
      "Iter-19740, train loss-2.0909, acc-0.4200, valid loss-2.0873, acc-0.4156, test loss-2.0882, acc-0.4109\n",
      "Iter-19750, train loss-2.1200, acc-0.3200, valid loss-2.0872, acc-0.4156, test loss-2.0881, acc-0.4107\n",
      "Iter-19760, train loss-2.1170, acc-0.3400, valid loss-2.0871, acc-0.4158, test loss-2.0880, acc-0.4107\n",
      "Iter-19770, train loss-2.0910, acc-0.3000, valid loss-2.0870, acc-0.4158, test loss-2.0879, acc-0.4108\n",
      "Iter-19780, train loss-2.1048, acc-0.3600, valid loss-2.0869, acc-0.4158, test loss-2.0878, acc-0.4109\n",
      "Iter-19790, train loss-2.0932, acc-0.4000, valid loss-2.0869, acc-0.4158, test loss-2.0877, acc-0.4110\n",
      "Iter-19800, train loss-2.1064, acc-0.3200, valid loss-2.0868, acc-0.4160, test loss-2.0876, acc-0.4111\n",
      "Iter-19810, train loss-2.1127, acc-0.4200, valid loss-2.0867, acc-0.4158, test loss-2.0875, acc-0.4112\n",
      "Iter-19820, train loss-2.0879, acc-0.3600, valid loss-2.0866, acc-0.4158, test loss-2.0874, acc-0.4113\n",
      "Iter-19830, train loss-2.1156, acc-0.3600, valid loss-2.0865, acc-0.4158, test loss-2.0873, acc-0.4112\n",
      "Iter-19840, train loss-2.0650, acc-0.3800, valid loss-2.0864, acc-0.4158, test loss-2.0872, acc-0.4112\n",
      "Iter-19850, train loss-2.1017, acc-0.4800, valid loss-2.0863, acc-0.4160, test loss-2.0871, acc-0.4114\n",
      "Iter-19860, train loss-2.1103, acc-0.3800, valid loss-2.0862, acc-0.4162, test loss-2.0870, acc-0.4118\n",
      "Iter-19870, train loss-2.0052, acc-0.5800, valid loss-2.0861, acc-0.4160, test loss-2.0869, acc-0.4118\n",
      "Iter-19880, train loss-2.0693, acc-0.4600, valid loss-2.0860, acc-0.4160, test loss-2.0868, acc-0.4120\n",
      "Iter-19890, train loss-2.0817, acc-0.4000, valid loss-2.0859, acc-0.4160, test loss-2.0868, acc-0.4119\n",
      "Iter-19900, train loss-2.0407, acc-0.5600, valid loss-2.0858, acc-0.4160, test loss-2.0867, acc-0.4117\n",
      "Iter-19910, train loss-2.0830, acc-0.4400, valid loss-2.0857, acc-0.4162, test loss-2.0865, acc-0.4117\n",
      "Iter-19920, train loss-2.0964, acc-0.4400, valid loss-2.0856, acc-0.4162, test loss-2.0864, acc-0.4118\n",
      "Iter-19930, train loss-2.0619, acc-0.5400, valid loss-2.0855, acc-0.4164, test loss-2.0863, acc-0.4120\n",
      "Iter-19940, train loss-2.1157, acc-0.3800, valid loss-2.0854, acc-0.4164, test loss-2.0863, acc-0.4119\n",
      "Iter-19950, train loss-2.0852, acc-0.4000, valid loss-2.0853, acc-0.4162, test loss-2.0862, acc-0.4120\n",
      "Iter-19960, train loss-2.0649, acc-0.4400, valid loss-2.0852, acc-0.4168, test loss-2.0861, acc-0.4120\n",
      "Iter-19970, train loss-2.1159, acc-0.4400, valid loss-2.0851, acc-0.4172, test loss-2.0860, acc-0.4125\n",
      "Iter-19980, train loss-2.1195, acc-0.3800, valid loss-2.0850, acc-0.4174, test loss-2.0859, acc-0.4126\n",
      "Iter-19990, train loss-2.1146, acc-0.3600, valid loss-2.0849, acc-0.4176, test loss-2.0858, acc-0.4120\n",
      "Iter-20000, train loss-2.0831, acc-0.3800, valid loss-2.0848, acc-0.4174, test loss-2.0857, acc-0.4122\n",
      "Iter-20010, train loss-2.1121, acc-0.4000, valid loss-2.0847, acc-0.4178, test loss-2.0856, acc-0.4124\n",
      "Iter-20020, train loss-2.0778, acc-0.4400, valid loss-2.0847, acc-0.4176, test loss-2.0855, acc-0.4126\n",
      "Iter-20030, train loss-2.1173, acc-0.3200, valid loss-2.0846, acc-0.4176, test loss-2.0854, acc-0.4125\n",
      "Iter-20040, train loss-2.1408, acc-0.2800, valid loss-2.0845, acc-0.4178, test loss-2.0853, acc-0.4123\n",
      "Iter-20050, train loss-2.1200, acc-0.3200, valid loss-2.0844, acc-0.4178, test loss-2.0852, acc-0.4121\n",
      "Iter-20060, train loss-2.0186, acc-0.4800, valid loss-2.0843, acc-0.4180, test loss-2.0851, acc-0.4125\n",
      "Iter-20070, train loss-2.1140, acc-0.3000, valid loss-2.0842, acc-0.4182, test loss-2.0850, acc-0.4126\n",
      "Iter-20080, train loss-2.0624, acc-0.4400, valid loss-2.0841, acc-0.4184, test loss-2.0849, acc-0.4126\n",
      "Iter-20090, train loss-2.0414, acc-0.4400, valid loss-2.0840, acc-0.4184, test loss-2.0848, acc-0.4125\n",
      "Iter-20100, train loss-2.1127, acc-0.3800, valid loss-2.0839, acc-0.4186, test loss-2.0847, acc-0.4126\n",
      "Iter-20110, train loss-2.1630, acc-0.3000, valid loss-2.0838, acc-0.4186, test loss-2.0847, acc-0.4127\n",
      "Iter-20120, train loss-2.0959, acc-0.3800, valid loss-2.0837, acc-0.4186, test loss-2.0846, acc-0.4130\n",
      "Iter-20130, train loss-2.1206, acc-0.3400, valid loss-2.0836, acc-0.4186, test loss-2.0845, acc-0.4129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-20140, train loss-2.1081, acc-0.4000, valid loss-2.0835, acc-0.4184, test loss-2.0844, acc-0.4128\n",
      "Iter-20150, train loss-2.1241, acc-0.3600, valid loss-2.0834, acc-0.4186, test loss-2.0843, acc-0.4129\n",
      "Iter-20160, train loss-2.0845, acc-0.4400, valid loss-2.0833, acc-0.4188, test loss-2.0842, acc-0.4129\n",
      "Iter-20170, train loss-2.1101, acc-0.3000, valid loss-2.0832, acc-0.4188, test loss-2.0841, acc-0.4129\n",
      "Iter-20180, train loss-2.0572, acc-0.4000, valid loss-2.0831, acc-0.4190, test loss-2.0840, acc-0.4128\n",
      "Iter-20190, train loss-2.0634, acc-0.4600, valid loss-2.0831, acc-0.4190, test loss-2.0839, acc-0.4128\n",
      "Iter-20200, train loss-2.1059, acc-0.4000, valid loss-2.0830, acc-0.4190, test loss-2.0838, acc-0.4127\n",
      "Iter-20210, train loss-2.0742, acc-0.4200, valid loss-2.0829, acc-0.4184, test loss-2.0837, acc-0.4127\n",
      "Iter-20220, train loss-2.0041, acc-0.6600, valid loss-2.0828, acc-0.4184, test loss-2.0836, acc-0.4126\n",
      "Iter-20230, train loss-2.0774, acc-0.4000, valid loss-2.0827, acc-0.4186, test loss-2.0835, acc-0.4126\n",
      "Iter-20240, train loss-2.0923, acc-0.3800, valid loss-2.0826, acc-0.4184, test loss-2.0834, acc-0.4126\n",
      "Iter-20250, train loss-2.0389, acc-0.3800, valid loss-2.0825, acc-0.4184, test loss-2.0833, acc-0.4127\n",
      "Iter-20260, train loss-2.1058, acc-0.4400, valid loss-2.0824, acc-0.4184, test loss-2.0832, acc-0.4128\n",
      "Iter-20270, train loss-2.0735, acc-0.4800, valid loss-2.0823, acc-0.4184, test loss-2.0831, acc-0.4129\n",
      "Iter-20280, train loss-2.0860, acc-0.3200, valid loss-2.0822, acc-0.4186, test loss-2.0830, acc-0.4128\n",
      "Iter-20290, train loss-2.1004, acc-0.3800, valid loss-2.0821, acc-0.4184, test loss-2.0829, acc-0.4128\n",
      "Iter-20300, train loss-2.1396, acc-0.3600, valid loss-2.0820, acc-0.4186, test loss-2.0828, acc-0.4129\n",
      "Iter-20310, train loss-2.0692, acc-0.4400, valid loss-2.0819, acc-0.4184, test loss-2.0828, acc-0.4130\n",
      "Iter-20320, train loss-2.1080, acc-0.3600, valid loss-2.0818, acc-0.4184, test loss-2.0827, acc-0.4130\n",
      "Iter-20330, train loss-2.0909, acc-0.5400, valid loss-2.0817, acc-0.4188, test loss-2.0826, acc-0.4129\n",
      "Iter-20340, train loss-2.0451, acc-0.5000, valid loss-2.0816, acc-0.4184, test loss-2.0825, acc-0.4131\n",
      "Iter-20350, train loss-2.0745, acc-0.4400, valid loss-2.0815, acc-0.4188, test loss-2.0824, acc-0.4129\n",
      "Iter-20360, train loss-2.0818, acc-0.3800, valid loss-2.0814, acc-0.4188, test loss-2.0823, acc-0.4132\n",
      "Iter-20370, train loss-2.1209, acc-0.2800, valid loss-2.0813, acc-0.4194, test loss-2.0822, acc-0.4133\n",
      "Iter-20380, train loss-2.0607, acc-0.5400, valid loss-2.0812, acc-0.4194, test loss-2.0821, acc-0.4137\n",
      "Iter-20390, train loss-2.0648, acc-0.4200, valid loss-2.0811, acc-0.4194, test loss-2.0820, acc-0.4135\n",
      "Iter-20400, train loss-2.1186, acc-0.3600, valid loss-2.0811, acc-0.4192, test loss-2.0819, acc-0.4136\n",
      "Iter-20410, train loss-2.0656, acc-0.3400, valid loss-2.0810, acc-0.4192, test loss-2.0818, acc-0.4135\n",
      "Iter-20420, train loss-2.0993, acc-0.4200, valid loss-2.0809, acc-0.4192, test loss-2.0817, acc-0.4135\n",
      "Iter-20430, train loss-2.0772, acc-0.3600, valid loss-2.0808, acc-0.4194, test loss-2.0816, acc-0.4137\n",
      "Iter-20440, train loss-2.0584, acc-0.5200, valid loss-2.0807, acc-0.4196, test loss-2.0815, acc-0.4139\n",
      "Iter-20450, train loss-2.0725, acc-0.4200, valid loss-2.0806, acc-0.4194, test loss-2.0814, acc-0.4140\n",
      "Iter-20460, train loss-2.0648, acc-0.3800, valid loss-2.0805, acc-0.4198, test loss-2.0813, acc-0.4140\n",
      "Iter-20470, train loss-2.1173, acc-0.3600, valid loss-2.0804, acc-0.4198, test loss-2.0812, acc-0.4141\n",
      "Iter-20480, train loss-2.0661, acc-0.3800, valid loss-2.0803, acc-0.4198, test loss-2.0811, acc-0.4144\n",
      "Iter-20490, train loss-2.0806, acc-0.4200, valid loss-2.0802, acc-0.4198, test loss-2.0810, acc-0.4144\n",
      "Iter-20500, train loss-2.0873, acc-0.4400, valid loss-2.0801, acc-0.4198, test loss-2.0809, acc-0.4142\n",
      "Iter-20510, train loss-2.0529, acc-0.5400, valid loss-2.0800, acc-0.4198, test loss-2.0809, acc-0.4142\n",
      "Iter-20520, train loss-2.1606, acc-0.2800, valid loss-2.0799, acc-0.4198, test loss-2.0808, acc-0.4144\n",
      "Iter-20530, train loss-2.1038, acc-0.3600, valid loss-2.0798, acc-0.4200, test loss-2.0807, acc-0.4144\n",
      "Iter-20540, train loss-2.0901, acc-0.4600, valid loss-2.0797, acc-0.4198, test loss-2.0806, acc-0.4147\n",
      "Iter-20550, train loss-2.0870, acc-0.3600, valid loss-2.0796, acc-0.4198, test loss-2.0805, acc-0.4149\n",
      "Iter-20560, train loss-2.0277, acc-0.4800, valid loss-2.0795, acc-0.4198, test loss-2.0804, acc-0.4147\n",
      "Iter-20570, train loss-2.1213, acc-0.3400, valid loss-2.0794, acc-0.4198, test loss-2.0803, acc-0.4149\n",
      "Iter-20580, train loss-2.1158, acc-0.3400, valid loss-2.0793, acc-0.4200, test loss-2.0802, acc-0.4148\n",
      "Iter-20590, train loss-2.0479, acc-0.4600, valid loss-2.0792, acc-0.4198, test loss-2.0801, acc-0.4144\n",
      "Iter-20600, train loss-2.0540, acc-0.4200, valid loss-2.0791, acc-0.4198, test loss-2.0800, acc-0.4147\n",
      "Iter-20610, train loss-2.0492, acc-0.5000, valid loss-2.0790, acc-0.4198, test loss-2.0799, acc-0.4145\n",
      "Iter-20620, train loss-2.0926, acc-0.3600, valid loss-2.0789, acc-0.4200, test loss-2.0798, acc-0.4146\n",
      "Iter-20630, train loss-2.1335, acc-0.4000, valid loss-2.0788, acc-0.4198, test loss-2.0797, acc-0.4147\n",
      "Iter-20640, train loss-2.1371, acc-0.2800, valid loss-2.0787, acc-0.4198, test loss-2.0796, acc-0.4147\n",
      "Iter-20650, train loss-2.0458, acc-0.5000, valid loss-2.0786, acc-0.4198, test loss-2.0795, acc-0.4147\n",
      "Iter-20660, train loss-2.1422, acc-0.3200, valid loss-2.0785, acc-0.4198, test loss-2.0794, acc-0.4146\n",
      "Iter-20670, train loss-2.1058, acc-0.4600, valid loss-2.0784, acc-0.4198, test loss-2.0793, acc-0.4149\n",
      "Iter-20680, train loss-2.0129, acc-0.5800, valid loss-2.0784, acc-0.4198, test loss-2.0792, acc-0.4148\n",
      "Iter-20690, train loss-2.1014, acc-0.4000, valid loss-2.0783, acc-0.4198, test loss-2.0791, acc-0.4151\n",
      "Iter-20700, train loss-2.1014, acc-0.4000, valid loss-2.0782, acc-0.4198, test loss-2.0790, acc-0.4149\n",
      "Iter-20710, train loss-2.1233, acc-0.3200, valid loss-2.0781, acc-0.4198, test loss-2.0789, acc-0.4146\n",
      "Iter-20720, train loss-2.0794, acc-0.5400, valid loss-2.0780, acc-0.4200, test loss-2.0788, acc-0.4148\n",
      "Iter-20730, train loss-2.0918, acc-0.4400, valid loss-2.0779, acc-0.4200, test loss-2.0787, acc-0.4145\n",
      "Iter-20740, train loss-2.0989, acc-0.3800, valid loss-2.0778, acc-0.4200, test loss-2.0786, acc-0.4142\n",
      "Iter-20750, train loss-2.0610, acc-0.4600, valid loss-2.0777, acc-0.4196, test loss-2.0785, acc-0.4144\n",
      "Iter-20760, train loss-2.0800, acc-0.3400, valid loss-2.0776, acc-0.4200, test loss-2.0784, acc-0.4143\n",
      "Iter-20770, train loss-2.0424, acc-0.4800, valid loss-2.0775, acc-0.4200, test loss-2.0783, acc-0.4146\n",
      "Iter-20780, train loss-2.0617, acc-0.4400, valid loss-2.0774, acc-0.4198, test loss-2.0783, acc-0.4145\n",
      "Iter-20790, train loss-2.0814, acc-0.4200, valid loss-2.0773, acc-0.4198, test loss-2.0782, acc-0.4145\n",
      "Iter-20800, train loss-1.9866, acc-0.5000, valid loss-2.0772, acc-0.4198, test loss-2.0781, acc-0.4149\n",
      "Iter-20810, train loss-2.1030, acc-0.3800, valid loss-2.0771, acc-0.4198, test loss-2.0780, acc-0.4151\n",
      "Iter-20820, train loss-2.0680, acc-0.4400, valid loss-2.0771, acc-0.4196, test loss-2.0779, acc-0.4151\n",
      "Iter-20830, train loss-2.0290, acc-0.4800, valid loss-2.0770, acc-0.4198, test loss-2.0778, acc-0.4151\n",
      "Iter-20840, train loss-2.0312, acc-0.4600, valid loss-2.0769, acc-0.4200, test loss-2.0777, acc-0.4152\n",
      "Iter-20850, train loss-2.0397, acc-0.4400, valid loss-2.0768, acc-0.4200, test loss-2.0776, acc-0.4151\n",
      "Iter-20860, train loss-2.0656, acc-0.3800, valid loss-2.0767, acc-0.4200, test loss-2.0775, acc-0.4152\n",
      "Iter-20870, train loss-2.1313, acc-0.2800, valid loss-2.0766, acc-0.4200, test loss-2.0774, acc-0.4153\n",
      "Iter-20880, train loss-2.1280, acc-0.3600, valid loss-2.0765, acc-0.4200, test loss-2.0773, acc-0.4154\n",
      "Iter-20890, train loss-2.1090, acc-0.3600, valid loss-2.0764, acc-0.4200, test loss-2.0772, acc-0.4154\n",
      "Iter-20900, train loss-2.0977, acc-0.3400, valid loss-2.0763, acc-0.4200, test loss-2.0771, acc-0.4154\n",
      "Iter-20910, train loss-2.0751, acc-0.4400, valid loss-2.0762, acc-0.4200, test loss-2.0770, acc-0.4153\n",
      "Iter-20920, train loss-2.0685, acc-0.4200, valid loss-2.0761, acc-0.4202, test loss-2.0769, acc-0.4154\n",
      "Iter-20930, train loss-2.0553, acc-0.4600, valid loss-2.0760, acc-0.4202, test loss-2.0768, acc-0.4154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-20940, train loss-2.0849, acc-0.4800, valid loss-2.0759, acc-0.4204, test loss-2.0767, acc-0.4154\n",
      "Iter-20950, train loss-2.0775, acc-0.3800, valid loss-2.0758, acc-0.4204, test loss-2.0766, acc-0.4154\n",
      "Iter-20960, train loss-2.1003, acc-0.3800, valid loss-2.0757, acc-0.4202, test loss-2.0765, acc-0.4154\n",
      "Iter-20970, train loss-2.1061, acc-0.3600, valid loss-2.0756, acc-0.4204, test loss-2.0764, acc-0.4155\n",
      "Iter-20980, train loss-2.0372, acc-0.4600, valid loss-2.0755, acc-0.4202, test loss-2.0764, acc-0.4155\n",
      "Iter-20990, train loss-2.1054, acc-0.3600, valid loss-2.0754, acc-0.4200, test loss-2.0763, acc-0.4155\n",
      "Iter-21000, train loss-2.0576, acc-0.3400, valid loss-2.0753, acc-0.4200, test loss-2.0762, acc-0.4154\n",
      "Iter-21010, train loss-2.0770, acc-0.4000, valid loss-2.0752, acc-0.4200, test loss-2.0761, acc-0.4153\n",
      "Iter-21020, train loss-2.0597, acc-0.4800, valid loss-2.0751, acc-0.4200, test loss-2.0760, acc-0.4154\n",
      "Iter-21030, train loss-2.0877, acc-0.4000, valid loss-2.0750, acc-0.4200, test loss-2.0759, acc-0.4154\n",
      "Iter-21040, train loss-2.0560, acc-0.4600, valid loss-2.0750, acc-0.4200, test loss-2.0758, acc-0.4154\n",
      "Iter-21050, train loss-2.0929, acc-0.3200, valid loss-2.0749, acc-0.4200, test loss-2.0757, acc-0.4156\n",
      "Iter-21060, train loss-2.0124, acc-0.6000, valid loss-2.0748, acc-0.4202, test loss-2.0756, acc-0.4156\n",
      "Iter-21070, train loss-2.1167, acc-0.2800, valid loss-2.0747, acc-0.4200, test loss-2.0755, acc-0.4156\n",
      "Iter-21080, train loss-2.1104, acc-0.3400, valid loss-2.0746, acc-0.4198, test loss-2.0754, acc-0.4156\n",
      "Iter-21090, train loss-2.0646, acc-0.4200, valid loss-2.0745, acc-0.4198, test loss-2.0753, acc-0.4156\n",
      "Iter-21100, train loss-2.1168, acc-0.3600, valid loss-2.0744, acc-0.4200, test loss-2.0752, acc-0.4156\n",
      "Iter-21110, train loss-2.0163, acc-0.6000, valid loss-2.0743, acc-0.4200, test loss-2.0751, acc-0.4157\n",
      "Iter-21120, train loss-2.0837, acc-0.3600, valid loss-2.0742, acc-0.4204, test loss-2.0750, acc-0.4158\n",
      "Iter-21130, train loss-2.0932, acc-0.4600, valid loss-2.0741, acc-0.4204, test loss-2.0749, acc-0.4158\n",
      "Iter-21140, train loss-2.0801, acc-0.4200, valid loss-2.0740, acc-0.4202, test loss-2.0748, acc-0.4159\n",
      "Iter-21150, train loss-2.0645, acc-0.4000, valid loss-2.0739, acc-0.4202, test loss-2.0747, acc-0.4162\n",
      "Iter-21160, train loss-2.0389, acc-0.4600, valid loss-2.0738, acc-0.4202, test loss-2.0746, acc-0.4161\n",
      "Iter-21170, train loss-2.0533, acc-0.4400, valid loss-2.0737, acc-0.4204, test loss-2.0746, acc-0.4161\n",
      "Iter-21180, train loss-2.0826, acc-0.3800, valid loss-2.0736, acc-0.4206, test loss-2.0745, acc-0.4160\n",
      "Iter-21190, train loss-2.0413, acc-0.4400, valid loss-2.0735, acc-0.4206, test loss-2.0744, acc-0.4158\n",
      "Iter-21200, train loss-2.0877, acc-0.3800, valid loss-2.0735, acc-0.4202, test loss-2.0743, acc-0.4158\n",
      "Iter-21210, train loss-2.1040, acc-0.4400, valid loss-2.0734, acc-0.4202, test loss-2.0742, acc-0.4159\n",
      "Iter-21220, train loss-2.0973, acc-0.3800, valid loss-2.0733, acc-0.4202, test loss-2.0741, acc-0.4160\n",
      "Iter-21230, train loss-2.1154, acc-0.3000, valid loss-2.0732, acc-0.4206, test loss-2.0740, acc-0.4160\n",
      "Iter-21240, train loss-2.0872, acc-0.3000, valid loss-2.0731, acc-0.4206, test loss-2.0739, acc-0.4164\n",
      "Iter-21250, train loss-2.0464, acc-0.4600, valid loss-2.0730, acc-0.4208, test loss-2.0738, acc-0.4165\n",
      "Iter-21260, train loss-2.0747, acc-0.3800, valid loss-2.0729, acc-0.4206, test loss-2.0737, acc-0.4164\n",
      "Iter-21270, train loss-2.0708, acc-0.4400, valid loss-2.0728, acc-0.4204, test loss-2.0736, acc-0.4163\n",
      "Iter-21280, train loss-2.0911, acc-0.3600, valid loss-2.0727, acc-0.4204, test loss-2.0735, acc-0.4164\n",
      "Iter-21290, train loss-2.0483, acc-0.4000, valid loss-2.0726, acc-0.4204, test loss-2.0734, acc-0.4163\n",
      "Iter-21300, train loss-2.0298, acc-0.4200, valid loss-2.0725, acc-0.4202, test loss-2.0733, acc-0.4163\n",
      "Iter-21310, train loss-2.0485, acc-0.5400, valid loss-2.0724, acc-0.4202, test loss-2.0732, acc-0.4165\n",
      "Iter-21320, train loss-2.0844, acc-0.4600, valid loss-2.0723, acc-0.4204, test loss-2.0731, acc-0.4167\n",
      "Iter-21330, train loss-2.0629, acc-0.4800, valid loss-2.0723, acc-0.4200, test loss-2.0730, acc-0.4164\n",
      "Iter-21340, train loss-2.0779, acc-0.3600, valid loss-2.0722, acc-0.4198, test loss-2.0730, acc-0.4163\n",
      "Iter-21350, train loss-2.1029, acc-0.3000, valid loss-2.0721, acc-0.4196, test loss-2.0729, acc-0.4163\n",
      "Iter-21360, train loss-2.0698, acc-0.5000, valid loss-2.0720, acc-0.4196, test loss-2.0728, acc-0.4162\n",
      "Iter-21370, train loss-2.0936, acc-0.3600, valid loss-2.0719, acc-0.4198, test loss-2.0727, acc-0.4163\n",
      "Iter-21380, train loss-2.1093, acc-0.3600, valid loss-2.0718, acc-0.4196, test loss-2.0726, acc-0.4164\n",
      "Iter-21390, train loss-2.1187, acc-0.3200, valid loss-2.0717, acc-0.4196, test loss-2.0725, acc-0.4163\n",
      "Iter-21400, train loss-2.1020, acc-0.4400, valid loss-2.0716, acc-0.4204, test loss-2.0724, acc-0.4164\n",
      "Iter-21410, train loss-2.0380, acc-0.4200, valid loss-2.0715, acc-0.4200, test loss-2.0723, acc-0.4164\n",
      "Iter-21420, train loss-2.0401, acc-0.4800, valid loss-2.0714, acc-0.4202, test loss-2.0722, acc-0.4162\n",
      "Iter-21430, train loss-2.1092, acc-0.3000, valid loss-2.0713, acc-0.4204, test loss-2.0721, acc-0.4162\n",
      "Iter-21440, train loss-2.0572, acc-0.3800, valid loss-2.0712, acc-0.4202, test loss-2.0720, acc-0.4165\n",
      "Iter-21450, train loss-2.0672, acc-0.3600, valid loss-2.0711, acc-0.4204, test loss-2.0719, acc-0.4165\n",
      "Iter-21460, train loss-2.0690, acc-0.4000, valid loss-2.0710, acc-0.4202, test loss-2.0718, acc-0.4163\n",
      "Iter-21470, train loss-2.0996, acc-0.4000, valid loss-2.0709, acc-0.4204, test loss-2.0717, acc-0.4165\n",
      "Iter-21480, train loss-2.1105, acc-0.3000, valid loss-2.0708, acc-0.4206, test loss-2.0716, acc-0.4167\n",
      "Iter-21490, train loss-2.1059, acc-0.3800, valid loss-2.0707, acc-0.4210, test loss-2.0715, acc-0.4168\n",
      "Iter-21500, train loss-2.0842, acc-0.3800, valid loss-2.0707, acc-0.4210, test loss-2.0715, acc-0.4169\n",
      "Iter-21510, train loss-2.0600, acc-0.3800, valid loss-2.0706, acc-0.4214, test loss-2.0714, acc-0.4168\n",
      "Iter-21520, train loss-2.0410, acc-0.5200, valid loss-2.0705, acc-0.4214, test loss-2.0713, acc-0.4169\n",
      "Iter-21530, train loss-2.0760, acc-0.5000, valid loss-2.0704, acc-0.4214, test loss-2.0712, acc-0.4170\n",
      "Iter-21540, train loss-2.0657, acc-0.4600, valid loss-2.0703, acc-0.4212, test loss-2.0711, acc-0.4170\n",
      "Iter-21550, train loss-2.0639, acc-0.4400, valid loss-2.0702, acc-0.4212, test loss-2.0710, acc-0.4169\n",
      "Iter-21560, train loss-2.0777, acc-0.3800, valid loss-2.0701, acc-0.4212, test loss-2.0709, acc-0.4170\n",
      "Iter-21570, train loss-2.0992, acc-0.3600, valid loss-2.0700, acc-0.4212, test loss-2.0708, acc-0.4169\n",
      "Iter-21580, train loss-2.0319, acc-0.5400, valid loss-2.0699, acc-0.4212, test loss-2.0707, acc-0.4171\n",
      "Iter-21590, train loss-2.0883, acc-0.3600, valid loss-2.0698, acc-0.4218, test loss-2.0706, acc-0.4170\n",
      "Iter-21600, train loss-2.0667, acc-0.4400, valid loss-2.0697, acc-0.4216, test loss-2.0705, acc-0.4172\n",
      "Iter-21610, train loss-2.1033, acc-0.3400, valid loss-2.0696, acc-0.4218, test loss-2.0704, acc-0.4174\n",
      "Iter-21620, train loss-2.0861, acc-0.4200, valid loss-2.0696, acc-0.4220, test loss-2.0703, acc-0.4173\n",
      "Iter-21630, train loss-2.0860, acc-0.3400, valid loss-2.0695, acc-0.4222, test loss-2.0702, acc-0.4173\n",
      "Iter-21640, train loss-2.1492, acc-0.2600, valid loss-2.0694, acc-0.4216, test loss-2.0702, acc-0.4172\n",
      "Iter-21650, train loss-2.0574, acc-0.3600, valid loss-2.0693, acc-0.4212, test loss-2.0701, acc-0.4170\n",
      "Iter-21660, train loss-2.0759, acc-0.4800, valid loss-2.0692, acc-0.4216, test loss-2.0700, acc-0.4171\n",
      "Iter-21670, train loss-2.0216, acc-0.5200, valid loss-2.0691, acc-0.4216, test loss-2.0699, acc-0.4170\n",
      "Iter-21680, train loss-2.1055, acc-0.3200, valid loss-2.0690, acc-0.4220, test loss-2.0698, acc-0.4170\n",
      "Iter-21690, train loss-2.0557, acc-0.4400, valid loss-2.0689, acc-0.4222, test loss-2.0697, acc-0.4171\n",
      "Iter-21700, train loss-2.0889, acc-0.3800, valid loss-2.0688, acc-0.4220, test loss-2.0696, acc-0.4170\n",
      "Iter-21710, train loss-2.0142, acc-0.4600, valid loss-2.0687, acc-0.4220, test loss-2.0695, acc-0.4170\n",
      "Iter-21720, train loss-2.0951, acc-0.4400, valid loss-2.0686, acc-0.4220, test loss-2.0694, acc-0.4171\n",
      "Iter-21730, train loss-2.0206, acc-0.4600, valid loss-2.0685, acc-0.4222, test loss-2.0693, acc-0.4170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-21740, train loss-2.0668, acc-0.4400, valid loss-2.0684, acc-0.4218, test loss-2.0692, acc-0.4171\n",
      "Iter-21750, train loss-2.0963, acc-0.3600, valid loss-2.0683, acc-0.4222, test loss-2.0691, acc-0.4171\n",
      "Iter-21760, train loss-2.0320, acc-0.4800, valid loss-2.0682, acc-0.4222, test loss-2.0690, acc-0.4167\n",
      "Iter-21770, train loss-2.0904, acc-0.3400, valid loss-2.0682, acc-0.4224, test loss-2.0689, acc-0.4169\n",
      "Iter-21780, train loss-2.1066, acc-0.3400, valid loss-2.0681, acc-0.4226, test loss-2.0688, acc-0.4171\n",
      "Iter-21790, train loss-2.0563, acc-0.4400, valid loss-2.0680, acc-0.4224, test loss-2.0687, acc-0.4176\n",
      "Iter-21800, train loss-2.0804, acc-0.4000, valid loss-2.0679, acc-0.4228, test loss-2.0687, acc-0.4177\n",
      "Iter-21810, train loss-2.0218, acc-0.4600, valid loss-2.0678, acc-0.4228, test loss-2.0686, acc-0.4174\n",
      "Iter-21820, train loss-2.0854, acc-0.4400, valid loss-2.0677, acc-0.4228, test loss-2.0685, acc-0.4173\n",
      "Iter-21830, train loss-2.0613, acc-0.3800, valid loss-2.0676, acc-0.4232, test loss-2.0684, acc-0.4172\n",
      "Iter-21840, train loss-2.0998, acc-0.3400, valid loss-2.0675, acc-0.4230, test loss-2.0683, acc-0.4172\n",
      "Iter-21850, train loss-2.0160, acc-0.4600, valid loss-2.0674, acc-0.4230, test loss-2.0682, acc-0.4172\n",
      "Iter-21860, train loss-2.0668, acc-0.4400, valid loss-2.0673, acc-0.4230, test loss-2.0681, acc-0.4172\n",
      "Iter-21870, train loss-2.0386, acc-0.3800, valid loss-2.0672, acc-0.4230, test loss-2.0680, acc-0.4173\n",
      "Iter-21880, train loss-2.0602, acc-0.4400, valid loss-2.0671, acc-0.4232, test loss-2.0679, acc-0.4178\n",
      "Iter-21890, train loss-2.0678, acc-0.4600, valid loss-2.0670, acc-0.4230, test loss-2.0678, acc-0.4176\n",
      "Iter-21900, train loss-2.0580, acc-0.4200, valid loss-2.0670, acc-0.4232, test loss-2.0677, acc-0.4178\n",
      "Iter-21910, train loss-2.0502, acc-0.5000, valid loss-2.0669, acc-0.4230, test loss-2.0676, acc-0.4176\n",
      "Iter-21920, train loss-2.0508, acc-0.4000, valid loss-2.0668, acc-0.4230, test loss-2.0675, acc-0.4174\n",
      "Iter-21930, train loss-2.0818, acc-0.3600, valid loss-2.0667, acc-0.4234, test loss-2.0674, acc-0.4175\n",
      "Iter-21940, train loss-2.0548, acc-0.4200, valid loss-2.0666, acc-0.4232, test loss-2.0673, acc-0.4175\n",
      "Iter-21950, train loss-2.1513, acc-0.2800, valid loss-2.0665, acc-0.4234, test loss-2.0673, acc-0.4172\n",
      "Iter-21960, train loss-2.0586, acc-0.4800, valid loss-2.0664, acc-0.4232, test loss-2.0672, acc-0.4174\n",
      "Iter-21970, train loss-2.0554, acc-0.4600, valid loss-2.0663, acc-0.4236, test loss-2.0671, acc-0.4174\n",
      "Iter-21980, train loss-2.0666, acc-0.3200, valid loss-2.0662, acc-0.4240, test loss-2.0670, acc-0.4176\n",
      "Iter-21990, train loss-2.1145, acc-0.3000, valid loss-2.0661, acc-0.4240, test loss-2.0669, acc-0.4176\n",
      "Iter-22000, train loss-2.1199, acc-0.4200, valid loss-2.0660, acc-0.4240, test loss-2.0668, acc-0.4177\n",
      "Iter-22010, train loss-2.0805, acc-0.4200, valid loss-2.0659, acc-0.4242, test loss-2.0667, acc-0.4177\n",
      "Iter-22020, train loss-2.1062, acc-0.3000, valid loss-2.0658, acc-0.4242, test loss-2.0666, acc-0.4177\n",
      "Iter-22030, train loss-2.0946, acc-0.3600, valid loss-2.0657, acc-0.4242, test loss-2.0665, acc-0.4178\n",
      "Iter-22040, train loss-2.1041, acc-0.4200, valid loss-2.0656, acc-0.4240, test loss-2.0664, acc-0.4180\n",
      "Iter-22050, train loss-2.1077, acc-0.3200, valid loss-2.0655, acc-0.4244, test loss-2.0663, acc-0.4180\n",
      "Iter-22060, train loss-2.0870, acc-0.4000, valid loss-2.0654, acc-0.4238, test loss-2.0662, acc-0.4176\n",
      "Iter-22070, train loss-2.0457, acc-0.4400, valid loss-2.0653, acc-0.4238, test loss-2.0661, acc-0.4175\n",
      "Iter-22080, train loss-2.0408, acc-0.4200, valid loss-2.0652, acc-0.4236, test loss-2.0660, acc-0.4172\n",
      "Iter-22090, train loss-2.1008, acc-0.3400, valid loss-2.0651, acc-0.4240, test loss-2.0659, acc-0.4174\n",
      "Iter-22100, train loss-2.0751, acc-0.4200, valid loss-2.0651, acc-0.4240, test loss-2.0658, acc-0.4174\n",
      "Iter-22110, train loss-2.0586, acc-0.3800, valid loss-2.0650, acc-0.4236, test loss-2.0658, acc-0.4173\n",
      "Iter-22120, train loss-2.0939, acc-0.4400, valid loss-2.0649, acc-0.4238, test loss-2.0657, acc-0.4174\n",
      "Iter-22130, train loss-2.0695, acc-0.4200, valid loss-2.0648, acc-0.4238, test loss-2.0656, acc-0.4171\n",
      "Iter-22140, train loss-2.0370, acc-0.4200, valid loss-2.0647, acc-0.4234, test loss-2.0655, acc-0.4170\n",
      "Iter-22150, train loss-2.0410, acc-0.4600, valid loss-2.0646, acc-0.4238, test loss-2.0654, acc-0.4172\n",
      "Iter-22160, train loss-2.0505, acc-0.5200, valid loss-2.0645, acc-0.4236, test loss-2.0653, acc-0.4172\n",
      "Iter-22170, train loss-2.0353, acc-0.4400, valid loss-2.0644, acc-0.4236, test loss-2.0652, acc-0.4173\n",
      "Iter-22180, train loss-2.0927, acc-0.4000, valid loss-2.0643, acc-0.4238, test loss-2.0651, acc-0.4175\n",
      "Iter-22190, train loss-2.0921, acc-0.4200, valid loss-2.0642, acc-0.4238, test loss-2.0650, acc-0.4172\n",
      "Iter-22200, train loss-2.0572, acc-0.4400, valid loss-2.0641, acc-0.4238, test loss-2.0649, acc-0.4171\n",
      "Iter-22210, train loss-2.0795, acc-0.4200, valid loss-2.0640, acc-0.4240, test loss-2.0648, acc-0.4171\n",
      "Iter-22220, train loss-2.0551, acc-0.4200, valid loss-2.0639, acc-0.4242, test loss-2.0647, acc-0.4173\n",
      "Iter-22230, train loss-2.0827, acc-0.4000, valid loss-2.0638, acc-0.4242, test loss-2.0646, acc-0.4173\n",
      "Iter-22240, train loss-2.1132, acc-0.4200, valid loss-2.0637, acc-0.4244, test loss-2.0645, acc-0.4173\n",
      "Iter-22250, train loss-2.0586, acc-0.4600, valid loss-2.0636, acc-0.4246, test loss-2.0644, acc-0.4173\n",
      "Iter-22260, train loss-2.0718, acc-0.4600, valid loss-2.0636, acc-0.4244, test loss-2.0644, acc-0.4174\n",
      "Iter-22270, train loss-2.0367, acc-0.4400, valid loss-2.0635, acc-0.4246, test loss-2.0643, acc-0.4174\n",
      "Iter-22280, train loss-2.1129, acc-0.3400, valid loss-2.0634, acc-0.4246, test loss-2.0642, acc-0.4172\n",
      "Iter-22290, train loss-2.0771, acc-0.3600, valid loss-2.0633, acc-0.4244, test loss-2.0641, acc-0.4172\n",
      "Iter-22300, train loss-2.0595, acc-0.4600, valid loss-2.0632, acc-0.4244, test loss-2.0640, acc-0.4171\n",
      "Iter-22310, train loss-2.0803, acc-0.4600, valid loss-2.0631, acc-0.4246, test loss-2.0639, acc-0.4171\n",
      "Iter-22320, train loss-1.9551, acc-0.6400, valid loss-2.0630, acc-0.4246, test loss-2.0638, acc-0.4172\n",
      "Iter-22330, train loss-2.1066, acc-0.3000, valid loss-2.0629, acc-0.4244, test loss-2.0637, acc-0.4173\n",
      "Iter-22340, train loss-2.0763, acc-0.4200, valid loss-2.0628, acc-0.4244, test loss-2.0636, acc-0.4174\n",
      "Iter-22350, train loss-2.0344, acc-0.5200, valid loss-2.0627, acc-0.4244, test loss-2.0635, acc-0.4175\n",
      "Iter-22360, train loss-2.0703, acc-0.4000, valid loss-2.0626, acc-0.4246, test loss-2.0634, acc-0.4177\n",
      "Iter-22370, train loss-2.1109, acc-0.3600, valid loss-2.0625, acc-0.4244, test loss-2.0633, acc-0.4174\n",
      "Iter-22380, train loss-2.0512, acc-0.4800, valid loss-2.0624, acc-0.4244, test loss-2.0632, acc-0.4175\n",
      "Iter-22390, train loss-2.0903, acc-0.3400, valid loss-2.0623, acc-0.4250, test loss-2.0632, acc-0.4178\n",
      "Iter-22400, train loss-2.0663, acc-0.4600, valid loss-2.0623, acc-0.4250, test loss-2.0631, acc-0.4179\n",
      "Iter-22410, train loss-2.1072, acc-0.3200, valid loss-2.0622, acc-0.4250, test loss-2.0630, acc-0.4177\n",
      "Iter-22420, train loss-2.0656, acc-0.4600, valid loss-2.0621, acc-0.4246, test loss-2.0629, acc-0.4176\n",
      "Iter-22430, train loss-2.0555, acc-0.3600, valid loss-2.0620, acc-0.4248, test loss-2.0628, acc-0.4176\n",
      "Iter-22440, train loss-2.0299, acc-0.5000, valid loss-2.0619, acc-0.4250, test loss-2.0627, acc-0.4175\n",
      "Iter-22450, train loss-2.0482, acc-0.5400, valid loss-2.0618, acc-0.4248, test loss-2.0626, acc-0.4177\n",
      "Iter-22460, train loss-2.0328, acc-0.4400, valid loss-2.0617, acc-0.4252, test loss-2.0625, acc-0.4175\n",
      "Iter-22470, train loss-2.0476, acc-0.4000, valid loss-2.0616, acc-0.4254, test loss-2.0624, acc-0.4177\n",
      "Iter-22480, train loss-2.0274, acc-0.3800, valid loss-2.0615, acc-0.4254, test loss-2.0623, acc-0.4179\n",
      "Iter-22490, train loss-2.0412, acc-0.4000, valid loss-2.0614, acc-0.4252, test loss-2.0622, acc-0.4178\n",
      "Iter-22500, train loss-2.0965, acc-0.4400, valid loss-2.0613, acc-0.4252, test loss-2.0621, acc-0.4177\n",
      "Iter-22510, train loss-2.0749, acc-0.3800, valid loss-2.0612, acc-0.4252, test loss-2.0620, acc-0.4177\n",
      "Iter-22520, train loss-2.0869, acc-0.3600, valid loss-2.0611, acc-0.4252, test loss-2.0619, acc-0.4175\n",
      "Iter-22530, train loss-2.0677, acc-0.4000, valid loss-2.0610, acc-0.4254, test loss-2.0618, acc-0.4179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22540, train loss-2.0752, acc-0.4200, valid loss-2.0609, acc-0.4256, test loss-2.0618, acc-0.4177\n",
      "Iter-22550, train loss-2.0437, acc-0.3400, valid loss-2.0608, acc-0.4250, test loss-2.0617, acc-0.4179\n",
      "Iter-22560, train loss-2.1083, acc-0.2800, valid loss-2.0607, acc-0.4248, test loss-2.0616, acc-0.4175\n",
      "Iter-22570, train loss-2.0240, acc-0.3400, valid loss-2.0607, acc-0.4246, test loss-2.0615, acc-0.4176\n",
      "Iter-22580, train loss-2.1060, acc-0.3600, valid loss-2.0606, acc-0.4244, test loss-2.0614, acc-0.4179\n",
      "Iter-22590, train loss-2.0257, acc-0.3800, valid loss-2.0605, acc-0.4246, test loss-2.0613, acc-0.4178\n",
      "Iter-22600, train loss-2.0671, acc-0.3400, valid loss-2.0604, acc-0.4252, test loss-2.0612, acc-0.4177\n",
      "Iter-22610, train loss-2.0553, acc-0.4000, valid loss-2.0603, acc-0.4254, test loss-2.0611, acc-0.4179\n",
      "Iter-22620, train loss-2.0633, acc-0.3400, valid loss-2.0602, acc-0.4252, test loss-2.0610, acc-0.4175\n",
      "Iter-22630, train loss-2.0686, acc-0.4400, valid loss-2.0601, acc-0.4254, test loss-2.0609, acc-0.4177\n",
      "Iter-22640, train loss-2.1094, acc-0.3600, valid loss-2.0600, acc-0.4262, test loss-2.0609, acc-0.4180\n",
      "Iter-22650, train loss-2.0928, acc-0.3800, valid loss-2.0599, acc-0.4262, test loss-2.0608, acc-0.4175\n",
      "Iter-22660, train loss-2.1235, acc-0.3400, valid loss-2.0599, acc-0.4258, test loss-2.0607, acc-0.4176\n",
      "Iter-22670, train loss-2.0621, acc-0.5000, valid loss-2.0598, acc-0.4256, test loss-2.0606, acc-0.4176\n",
      "Iter-22680, train loss-2.0898, acc-0.4600, valid loss-2.0597, acc-0.4258, test loss-2.0605, acc-0.4178\n",
      "Iter-22690, train loss-2.0504, acc-0.4000, valid loss-2.0596, acc-0.4260, test loss-2.0604, acc-0.4178\n",
      "Iter-22700, train loss-2.0897, acc-0.4400, valid loss-2.0595, acc-0.4258, test loss-2.0603, acc-0.4180\n",
      "Iter-22710, train loss-2.0009, acc-0.5800, valid loss-2.0594, acc-0.4256, test loss-2.0602, acc-0.4181\n",
      "Iter-22720, train loss-2.0789, acc-0.4000, valid loss-2.0593, acc-0.4256, test loss-2.0601, acc-0.4181\n",
      "Iter-22730, train loss-2.0548, acc-0.4400, valid loss-2.0592, acc-0.4256, test loss-2.0600, acc-0.4180\n",
      "Iter-22740, train loss-2.1278, acc-0.2600, valid loss-2.0591, acc-0.4256, test loss-2.0599, acc-0.4183\n",
      "Iter-22750, train loss-2.1434, acc-0.2200, valid loss-2.0590, acc-0.4258, test loss-2.0598, acc-0.4181\n",
      "Iter-22760, train loss-2.0564, acc-0.4000, valid loss-2.0589, acc-0.4258, test loss-2.0597, acc-0.4180\n",
      "Iter-22770, train loss-2.0849, acc-0.3400, valid loss-2.0588, acc-0.4260, test loss-2.0597, acc-0.4181\n",
      "Iter-22780, train loss-1.9757, acc-0.5600, valid loss-2.0587, acc-0.4262, test loss-2.0596, acc-0.4183\n",
      "Iter-22790, train loss-2.0614, acc-0.4200, valid loss-2.0586, acc-0.4262, test loss-2.0595, acc-0.4183\n",
      "Iter-22800, train loss-2.0686, acc-0.4000, valid loss-2.0586, acc-0.4262, test loss-2.0594, acc-0.4184\n",
      "Iter-22810, train loss-2.0775, acc-0.4400, valid loss-2.0585, acc-0.4264, test loss-2.0593, acc-0.4184\n",
      "Iter-22820, train loss-2.0995, acc-0.4000, valid loss-2.0584, acc-0.4264, test loss-2.0592, acc-0.4184\n",
      "Iter-22830, train loss-2.0785, acc-0.3600, valid loss-2.0583, acc-0.4264, test loss-2.0591, acc-0.4185\n",
      "Iter-22840, train loss-2.1352, acc-0.2600, valid loss-2.0582, acc-0.4262, test loss-2.0590, acc-0.4185\n",
      "Iter-22850, train loss-2.0184, acc-0.5200, valid loss-2.0581, acc-0.4268, test loss-2.0589, acc-0.4186\n",
      "Iter-22860, train loss-2.0498, acc-0.4800, valid loss-2.0580, acc-0.4268, test loss-2.0588, acc-0.4185\n",
      "Iter-22870, train loss-2.0668, acc-0.3800, valid loss-2.0579, acc-0.4270, test loss-2.0587, acc-0.4189\n",
      "Iter-22880, train loss-2.0828, acc-0.4600, valid loss-2.0578, acc-0.4270, test loss-2.0586, acc-0.4191\n",
      "Iter-22890, train loss-2.1076, acc-0.3600, valid loss-2.0577, acc-0.4268, test loss-2.0586, acc-0.4189\n",
      "Iter-22900, train loss-2.0343, acc-0.4400, valid loss-2.0577, acc-0.4270, test loss-2.0585, acc-0.4190\n",
      "Iter-22910, train loss-2.0480, acc-0.5400, valid loss-2.0576, acc-0.4278, test loss-2.0584, acc-0.4191\n",
      "Iter-22920, train loss-2.0525, acc-0.4000, valid loss-2.0575, acc-0.4274, test loss-2.0583, acc-0.4189\n",
      "Iter-22930, train loss-2.1248, acc-0.4600, valid loss-2.0574, acc-0.4276, test loss-2.0582, acc-0.4189\n",
      "Iter-22940, train loss-2.0722, acc-0.4200, valid loss-2.0573, acc-0.4278, test loss-2.0581, acc-0.4191\n",
      "Iter-22950, train loss-2.0335, acc-0.4800, valid loss-2.0572, acc-0.4276, test loss-2.0580, acc-0.4191\n",
      "Iter-22960, train loss-2.0775, acc-0.3600, valid loss-2.0571, acc-0.4272, test loss-2.0579, acc-0.4190\n",
      "Iter-22970, train loss-2.0427, acc-0.4200, valid loss-2.0570, acc-0.4274, test loss-2.0578, acc-0.4191\n",
      "Iter-22980, train loss-2.0929, acc-0.3800, valid loss-2.0569, acc-0.4272, test loss-2.0577, acc-0.4191\n",
      "Iter-22990, train loss-2.0502, acc-0.4000, valid loss-2.0568, acc-0.4272, test loss-2.0576, acc-0.4190\n",
      "Iter-23000, train loss-2.0572, acc-0.4800, valid loss-2.0567, acc-0.4268, test loss-2.0576, acc-0.4191\n",
      "Iter-23010, train loss-2.0155, acc-0.4800, valid loss-2.0567, acc-0.4272, test loss-2.0575, acc-0.4192\n",
      "Iter-23020, train loss-2.0255, acc-0.4600, valid loss-2.0566, acc-0.4272, test loss-2.0574, acc-0.4192\n",
      "Iter-23030, train loss-2.0611, acc-0.4200, valid loss-2.0565, acc-0.4276, test loss-2.0573, acc-0.4189\n",
      "Iter-23040, train loss-2.0504, acc-0.5200, valid loss-2.0564, acc-0.4274, test loss-2.0572, acc-0.4189\n",
      "Iter-23050, train loss-2.0617, acc-0.4600, valid loss-2.0563, acc-0.4274, test loss-2.0571, acc-0.4188\n",
      "Iter-23060, train loss-2.0520, acc-0.3200, valid loss-2.0562, acc-0.4276, test loss-2.0570, acc-0.4188\n",
      "Iter-23070, train loss-2.0941, acc-0.3000, valid loss-2.0561, acc-0.4276, test loss-2.0569, acc-0.4189\n",
      "Iter-23080, train loss-2.0488, acc-0.4000, valid loss-2.0560, acc-0.4274, test loss-2.0568, acc-0.4192\n",
      "Iter-23090, train loss-2.1288, acc-0.3400, valid loss-2.0559, acc-0.4276, test loss-2.0567, acc-0.4191\n",
      "Iter-23100, train loss-2.0988, acc-0.3600, valid loss-2.0558, acc-0.4278, test loss-2.0566, acc-0.4191\n",
      "Iter-23110, train loss-2.0400, acc-0.4400, valid loss-2.0557, acc-0.4276, test loss-2.0565, acc-0.4195\n",
      "Iter-23120, train loss-2.0424, acc-0.4800, valid loss-2.0557, acc-0.4274, test loss-2.0564, acc-0.4194\n",
      "Iter-23130, train loss-2.0164, acc-0.4600, valid loss-2.0556, acc-0.4278, test loss-2.0564, acc-0.4195\n",
      "Iter-23140, train loss-2.0373, acc-0.5000, valid loss-2.0555, acc-0.4280, test loss-2.0563, acc-0.4196\n",
      "Iter-23150, train loss-2.0803, acc-0.3200, valid loss-2.0554, acc-0.4276, test loss-2.0562, acc-0.4193\n",
      "Iter-23160, train loss-2.0996, acc-0.3800, valid loss-2.0553, acc-0.4276, test loss-2.0561, acc-0.4193\n",
      "Iter-23170, train loss-2.0912, acc-0.3600, valid loss-2.0552, acc-0.4278, test loss-2.0560, acc-0.4192\n",
      "Iter-23180, train loss-2.0706, acc-0.5200, valid loss-2.0551, acc-0.4274, test loss-2.0559, acc-0.4195\n",
      "Iter-23190, train loss-2.0401, acc-0.4800, valid loss-2.0550, acc-0.4274, test loss-2.0558, acc-0.4194\n",
      "Iter-23200, train loss-2.0045, acc-0.6000, valid loss-2.0549, acc-0.4280, test loss-2.0557, acc-0.4194\n",
      "Iter-23210, train loss-2.0830, acc-0.4000, valid loss-2.0548, acc-0.4278, test loss-2.0556, acc-0.4197\n",
      "Iter-23220, train loss-2.0541, acc-0.4600, valid loss-2.0547, acc-0.4280, test loss-2.0555, acc-0.4197\n",
      "Iter-23230, train loss-2.0265, acc-0.4000, valid loss-2.0547, acc-0.4276, test loss-2.0554, acc-0.4196\n",
      "Iter-23240, train loss-2.1331, acc-0.2800, valid loss-2.0546, acc-0.4274, test loss-2.0554, acc-0.4199\n",
      "Iter-23250, train loss-2.0691, acc-0.4200, valid loss-2.0545, acc-0.4280, test loss-2.0553, acc-0.4200\n",
      "Iter-23260, train loss-2.0401, acc-0.4400, valid loss-2.0544, acc-0.4278, test loss-2.0552, acc-0.4199\n",
      "Iter-23270, train loss-2.0682, acc-0.3400, valid loss-2.0543, acc-0.4276, test loss-2.0551, acc-0.4200\n",
      "Iter-23280, train loss-2.0632, acc-0.3400, valid loss-2.0542, acc-0.4276, test loss-2.0550, acc-0.4199\n",
      "Iter-23290, train loss-2.0195, acc-0.4400, valid loss-2.0541, acc-0.4276, test loss-2.0549, acc-0.4199\n",
      "Iter-23300, train loss-2.0539, acc-0.4600, valid loss-2.0540, acc-0.4276, test loss-2.0548, acc-0.4202\n",
      "Iter-23310, train loss-2.0548, acc-0.4400, valid loss-2.0540, acc-0.4278, test loss-2.0547, acc-0.4198\n",
      "Iter-23320, train loss-2.0318, acc-0.5000, valid loss-2.0539, acc-0.4278, test loss-2.0546, acc-0.4199\n",
      "Iter-23330, train loss-2.0272, acc-0.3800, valid loss-2.0538, acc-0.4278, test loss-2.0546, acc-0.4202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-23340, train loss-1.9598, acc-0.5600, valid loss-2.0537, acc-0.4280, test loss-2.0545, acc-0.4203\n",
      "Iter-23350, train loss-2.0660, acc-0.3800, valid loss-2.0536, acc-0.4280, test loss-2.0544, acc-0.4202\n",
      "Iter-23360, train loss-2.1164, acc-0.4200, valid loss-2.0535, acc-0.4278, test loss-2.0543, acc-0.4203\n",
      "Iter-23370, train loss-2.0448, acc-0.3800, valid loss-2.0534, acc-0.4280, test loss-2.0542, acc-0.4202\n",
      "Iter-23380, train loss-1.9900, acc-0.6000, valid loss-2.0533, acc-0.4278, test loss-2.0541, acc-0.4203\n",
      "Iter-23390, train loss-2.1008, acc-0.4000, valid loss-2.0532, acc-0.4280, test loss-2.0540, acc-0.4203\n",
      "Iter-23400, train loss-2.0810, acc-0.3400, valid loss-2.0531, acc-0.4282, test loss-2.0539, acc-0.4205\n",
      "Iter-23410, train loss-2.0632, acc-0.4600, valid loss-2.0530, acc-0.4284, test loss-2.0538, acc-0.4206\n",
      "Iter-23420, train loss-2.0579, acc-0.4200, valid loss-2.0529, acc-0.4288, test loss-2.0537, acc-0.4205\n",
      "Iter-23430, train loss-2.1253, acc-0.3200, valid loss-2.0529, acc-0.4284, test loss-2.0536, acc-0.4208\n",
      "Iter-23440, train loss-2.0527, acc-0.4200, valid loss-2.0528, acc-0.4284, test loss-2.0536, acc-0.4205\n",
      "Iter-23450, train loss-2.0838, acc-0.3800, valid loss-2.0527, acc-0.4282, test loss-2.0535, acc-0.4207\n",
      "Iter-23460, train loss-2.0662, acc-0.4200, valid loss-2.0526, acc-0.4282, test loss-2.0534, acc-0.4206\n",
      "Iter-23470, train loss-2.1125, acc-0.3400, valid loss-2.0525, acc-0.4280, test loss-2.0533, acc-0.4206\n",
      "Iter-23480, train loss-2.0383, acc-0.3600, valid loss-2.0524, acc-0.4278, test loss-2.0532, acc-0.4208\n",
      "Iter-23490, train loss-2.0275, acc-0.4600, valid loss-2.0523, acc-0.4276, test loss-2.0531, acc-0.4211\n",
      "Iter-23500, train loss-2.0666, acc-0.4200, valid loss-2.0522, acc-0.4278, test loss-2.0530, acc-0.4210\n",
      "Iter-23510, train loss-2.0678, acc-0.3600, valid loss-2.0521, acc-0.4274, test loss-2.0529, acc-0.4211\n",
      "Iter-23520, train loss-2.0781, acc-0.3800, valid loss-2.0520, acc-0.4274, test loss-2.0528, acc-0.4211\n",
      "Iter-23530, train loss-2.0942, acc-0.3800, valid loss-2.0519, acc-0.4276, test loss-2.0527, acc-0.4211\n",
      "Iter-23540, train loss-2.0536, acc-0.4600, valid loss-2.0519, acc-0.4276, test loss-2.0526, acc-0.4212\n",
      "Iter-23550, train loss-2.0526, acc-0.4000, valid loss-2.0518, acc-0.4278, test loss-2.0526, acc-0.4213\n",
      "Iter-23560, train loss-2.1000, acc-0.3400, valid loss-2.0517, acc-0.4276, test loss-2.0525, acc-0.4215\n",
      "Iter-23570, train loss-2.0220, acc-0.4200, valid loss-2.0516, acc-0.4278, test loss-2.0524, acc-0.4215\n",
      "Iter-23580, train loss-2.0443, acc-0.4000, valid loss-2.0515, acc-0.4280, test loss-2.0523, acc-0.4218\n",
      "Iter-23590, train loss-2.0527, acc-0.4800, valid loss-2.0514, acc-0.4280, test loss-2.0522, acc-0.4220\n",
      "Iter-23600, train loss-2.0756, acc-0.4200, valid loss-2.0513, acc-0.4284, test loss-2.0521, acc-0.4218\n",
      "Iter-23610, train loss-2.0039, acc-0.4800, valid loss-2.0512, acc-0.4282, test loss-2.0520, acc-0.4218\n",
      "Iter-23620, train loss-2.0230, acc-0.4200, valid loss-2.0511, acc-0.4282, test loss-2.0519, acc-0.4220\n",
      "Iter-23630, train loss-2.0925, acc-0.3200, valid loss-2.0510, acc-0.4286, test loss-2.0518, acc-0.4218\n",
      "Iter-23640, train loss-2.0332, acc-0.3600, valid loss-2.0510, acc-0.4286, test loss-2.0517, acc-0.4221\n",
      "Iter-23650, train loss-2.0575, acc-0.4000, valid loss-2.0509, acc-0.4284, test loss-2.0517, acc-0.4224\n",
      "Iter-23660, train loss-2.0337, acc-0.5800, valid loss-2.0508, acc-0.4286, test loss-2.0516, acc-0.4224\n",
      "Iter-23670, train loss-2.0451, acc-0.4000, valid loss-2.0507, acc-0.4284, test loss-2.0515, acc-0.4225\n",
      "Iter-23680, train loss-2.0184, acc-0.5000, valid loss-2.0506, acc-0.4286, test loss-2.0514, acc-0.4226\n",
      "Iter-23690, train loss-2.1148, acc-0.3200, valid loss-2.0505, acc-0.4290, test loss-2.0513, acc-0.4225\n",
      "Iter-23700, train loss-2.0942, acc-0.3600, valid loss-2.0504, acc-0.4288, test loss-2.0512, acc-0.4226\n",
      "Iter-23710, train loss-2.0213, acc-0.4600, valid loss-2.0503, acc-0.4296, test loss-2.0511, acc-0.4226\n",
      "Iter-23720, train loss-2.0712, acc-0.3800, valid loss-2.0502, acc-0.4294, test loss-2.0510, acc-0.4226\n",
      "Iter-23730, train loss-2.0240, acc-0.4200, valid loss-2.0501, acc-0.4294, test loss-2.0510, acc-0.4226\n",
      "Iter-23740, train loss-2.1016, acc-0.3800, valid loss-2.0501, acc-0.4294, test loss-2.0509, acc-0.4228\n",
      "Iter-23750, train loss-2.0926, acc-0.4200, valid loss-2.0500, acc-0.4292, test loss-2.0508, acc-0.4229\n",
      "Iter-23760, train loss-2.0803, acc-0.4800, valid loss-2.0499, acc-0.4292, test loss-2.0507, acc-0.4230\n",
      "Iter-23770, train loss-2.0422, acc-0.4800, valid loss-2.0498, acc-0.4290, test loss-2.0506, acc-0.4230\n",
      "Iter-23780, train loss-2.0472, acc-0.5200, valid loss-2.0497, acc-0.4290, test loss-2.0505, acc-0.4231\n",
      "Iter-23790, train loss-2.0214, acc-0.5000, valid loss-2.0496, acc-0.4290, test loss-2.0504, acc-0.4229\n",
      "Iter-23800, train loss-2.1076, acc-0.3800, valid loss-2.0495, acc-0.4286, test loss-2.0503, acc-0.4227\n",
      "Iter-23810, train loss-2.0435, acc-0.3800, valid loss-2.0494, acc-0.4286, test loss-2.0502, acc-0.4227\n",
      "Iter-23820, train loss-2.0460, acc-0.4600, valid loss-2.0493, acc-0.4284, test loss-2.0501, acc-0.4230\n",
      "Iter-23830, train loss-2.0087, acc-0.5600, valid loss-2.0493, acc-0.4286, test loss-2.0501, acc-0.4230\n",
      "Iter-23840, train loss-2.0223, acc-0.5000, valid loss-2.0492, acc-0.4292, test loss-2.0500, acc-0.4231\n",
      "Iter-23850, train loss-2.0715, acc-0.4000, valid loss-2.0491, acc-0.4290, test loss-2.0499, acc-0.4230\n",
      "Iter-23860, train loss-2.0309, acc-0.3600, valid loss-2.0490, acc-0.4294, test loss-2.0498, acc-0.4230\n",
      "Iter-23870, train loss-2.0584, acc-0.3800, valid loss-2.0489, acc-0.4292, test loss-2.0497, acc-0.4231\n",
      "Iter-23880, train loss-2.0295, acc-0.5200, valid loss-2.0488, acc-0.4292, test loss-2.0496, acc-0.4230\n",
      "Iter-23890, train loss-2.0674, acc-0.4200, valid loss-2.0487, acc-0.4294, test loss-2.0495, acc-0.4231\n",
      "Iter-23900, train loss-2.0303, acc-0.4800, valid loss-2.0486, acc-0.4294, test loss-2.0494, acc-0.4232\n",
      "Iter-23910, train loss-2.1036, acc-0.4400, valid loss-2.0485, acc-0.4292, test loss-2.0493, acc-0.4233\n",
      "Iter-23920, train loss-2.0488, acc-0.4400, valid loss-2.0484, acc-0.4294, test loss-2.0492, acc-0.4235\n",
      "Iter-23930, train loss-2.0393, acc-0.5000, valid loss-2.0484, acc-0.4290, test loss-2.0491, acc-0.4235\n",
      "Iter-23940, train loss-2.0825, acc-0.3200, valid loss-2.0483, acc-0.4288, test loss-2.0491, acc-0.4234\n",
      "Iter-23950, train loss-2.0793, acc-0.4400, valid loss-2.0482, acc-0.4290, test loss-2.0490, acc-0.4234\n",
      "Iter-23960, train loss-2.0139, acc-0.4600, valid loss-2.0481, acc-0.4288, test loss-2.0489, acc-0.4234\n",
      "Iter-23970, train loss-2.0567, acc-0.4000, valid loss-2.0480, acc-0.4288, test loss-2.0488, acc-0.4234\n",
      "Iter-23980, train loss-2.0528, acc-0.4600, valid loss-2.0479, acc-0.4292, test loss-2.0487, acc-0.4236\n",
      "Iter-23990, train loss-2.0576, acc-0.4200, valid loss-2.0478, acc-0.4290, test loss-2.0486, acc-0.4240\n",
      "Iter-24000, train loss-2.0611, acc-0.4400, valid loss-2.0478, acc-0.4290, test loss-2.0485, acc-0.4237\n",
      "Iter-24010, train loss-2.0062, acc-0.4200, valid loss-2.0477, acc-0.4290, test loss-2.0484, acc-0.4237\n",
      "Iter-24020, train loss-2.0667, acc-0.4600, valid loss-2.0476, acc-0.4290, test loss-2.0484, acc-0.4236\n",
      "Iter-24030, train loss-2.0621, acc-0.3800, valid loss-2.0475, acc-0.4290, test loss-2.0483, acc-0.4238\n",
      "Iter-24040, train loss-2.0430, acc-0.4800, valid loss-2.0474, acc-0.4288, test loss-2.0482, acc-0.4241\n",
      "Iter-24050, train loss-2.0008, acc-0.5000, valid loss-2.0473, acc-0.4288, test loss-2.0481, acc-0.4243\n",
      "Iter-24060, train loss-2.1219, acc-0.2600, valid loss-2.0472, acc-0.4288, test loss-2.0480, acc-0.4239\n",
      "Iter-24070, train loss-1.9445, acc-0.6800, valid loss-2.0471, acc-0.4288, test loss-2.0479, acc-0.4241\n",
      "Iter-24080, train loss-2.1135, acc-0.3400, valid loss-2.0470, acc-0.4290, test loss-2.0478, acc-0.4244\n",
      "Iter-24090, train loss-2.0345, acc-0.4200, valid loss-2.0470, acc-0.4294, test loss-2.0477, acc-0.4243\n",
      "Iter-24100, train loss-2.0437, acc-0.5000, valid loss-2.0469, acc-0.4294, test loss-2.0477, acc-0.4244\n",
      "Iter-24110, train loss-2.0114, acc-0.4400, valid loss-2.0468, acc-0.4290, test loss-2.0476, acc-0.4244\n",
      "Iter-24120, train loss-2.0654, acc-0.3200, valid loss-2.0467, acc-0.4294, test loss-2.0475, acc-0.4245\n",
      "Iter-24130, train loss-2.0450, acc-0.4400, valid loss-2.0466, acc-0.4290, test loss-2.0474, acc-0.4243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-24140, train loss-2.0197, acc-0.4200, valid loss-2.0465, acc-0.4292, test loss-2.0473, acc-0.4241\n",
      "Iter-24150, train loss-2.0121, acc-0.5000, valid loss-2.0464, acc-0.4292, test loss-2.0472, acc-0.4242\n",
      "Iter-24160, train loss-2.0458, acc-0.4200, valid loss-2.0463, acc-0.4296, test loss-2.0471, acc-0.4241\n",
      "Iter-24170, train loss-2.1157, acc-0.3000, valid loss-2.0462, acc-0.4296, test loss-2.0470, acc-0.4242\n",
      "Iter-24180, train loss-2.0820, acc-0.3600, valid loss-2.0461, acc-0.4300, test loss-2.0469, acc-0.4245\n",
      "Iter-24190, train loss-2.1332, acc-0.3400, valid loss-2.0461, acc-0.4300, test loss-2.0469, acc-0.4244\n",
      "Iter-24200, train loss-2.1033, acc-0.4200, valid loss-2.0460, acc-0.4300, test loss-2.0468, acc-0.4244\n",
      "Iter-24210, train loss-2.0741, acc-0.3800, valid loss-2.0459, acc-0.4304, test loss-2.0467, acc-0.4244\n",
      "Iter-24220, train loss-2.0398, acc-0.4600, valid loss-2.0458, acc-0.4302, test loss-2.0466, acc-0.4242\n",
      "Iter-24230, train loss-2.0669, acc-0.4400, valid loss-2.0457, acc-0.4302, test loss-2.0465, acc-0.4242\n",
      "Iter-24240, train loss-2.0238, acc-0.5200, valid loss-2.0456, acc-0.4300, test loss-2.0464, acc-0.4241\n",
      "Iter-24250, train loss-2.1082, acc-0.3400, valid loss-2.0455, acc-0.4304, test loss-2.0463, acc-0.4244\n",
      "Iter-24260, train loss-2.0633, acc-0.4000, valid loss-2.0454, acc-0.4306, test loss-2.0462, acc-0.4243\n",
      "Iter-24270, train loss-2.0853, acc-0.3200, valid loss-2.0454, acc-0.4304, test loss-2.0461, acc-0.4245\n",
      "Iter-24280, train loss-2.0770, acc-0.4000, valid loss-2.0453, acc-0.4308, test loss-2.0460, acc-0.4244\n",
      "Iter-24290, train loss-2.0276, acc-0.4600, valid loss-2.0452, acc-0.4312, test loss-2.0459, acc-0.4245\n",
      "Iter-24300, train loss-2.0336, acc-0.4600, valid loss-2.0451, acc-0.4310, test loss-2.0459, acc-0.4246\n",
      "Iter-24310, train loss-2.0028, acc-0.5600, valid loss-2.0450, acc-0.4310, test loss-2.0458, acc-0.4245\n",
      "Iter-24320, train loss-2.0536, acc-0.4000, valid loss-2.0449, acc-0.4312, test loss-2.0457, acc-0.4244\n",
      "Iter-24330, train loss-2.0331, acc-0.4800, valid loss-2.0448, acc-0.4312, test loss-2.0456, acc-0.4246\n",
      "Iter-24340, train loss-2.0825, acc-0.3800, valid loss-2.0447, acc-0.4312, test loss-2.0455, acc-0.4248\n",
      "Iter-24350, train loss-2.1283, acc-0.2800, valid loss-2.0446, acc-0.4312, test loss-2.0454, acc-0.4247\n",
      "Iter-24360, train loss-2.0544, acc-0.3800, valid loss-2.0446, acc-0.4312, test loss-2.0453, acc-0.4246\n",
      "Iter-24370, train loss-2.1180, acc-0.3200, valid loss-2.0445, acc-0.4310, test loss-2.0452, acc-0.4245\n",
      "Iter-24380, train loss-2.0573, acc-0.3400, valid loss-2.0444, acc-0.4314, test loss-2.0452, acc-0.4247\n",
      "Iter-24390, train loss-2.0662, acc-0.3800, valid loss-2.0443, acc-0.4314, test loss-2.0451, acc-0.4248\n",
      "Iter-24400, train loss-2.1166, acc-0.3000, valid loss-2.0442, acc-0.4314, test loss-2.0450, acc-0.4249\n",
      "Iter-24410, train loss-2.0624, acc-0.4600, valid loss-2.0441, acc-0.4314, test loss-2.0449, acc-0.4249\n",
      "Iter-24420, train loss-2.0514, acc-0.3600, valid loss-2.0441, acc-0.4314, test loss-2.0448, acc-0.4252\n",
      "Iter-24430, train loss-2.0506, acc-0.4400, valid loss-2.0440, acc-0.4312, test loss-2.0447, acc-0.4251\n",
      "Iter-24440, train loss-2.0368, acc-0.4200, valid loss-2.0439, acc-0.4316, test loss-2.0446, acc-0.4252\n",
      "Iter-24450, train loss-2.0639, acc-0.3800, valid loss-2.0438, acc-0.4316, test loss-2.0446, acc-0.4256\n",
      "Iter-24460, train loss-2.1021, acc-0.2800, valid loss-2.0437, acc-0.4320, test loss-2.0445, acc-0.4254\n",
      "Iter-24470, train loss-2.0157, acc-0.5000, valid loss-2.0436, acc-0.4324, test loss-2.0444, acc-0.4252\n",
      "Iter-24480, train loss-2.0802, acc-0.4200, valid loss-2.0435, acc-0.4320, test loss-2.0443, acc-0.4252\n",
      "Iter-24490, train loss-2.0095, acc-0.4600, valid loss-2.0434, acc-0.4318, test loss-2.0442, acc-0.4251\n",
      "Iter-24500, train loss-2.0169, acc-0.5600, valid loss-2.0434, acc-0.4316, test loss-2.0441, acc-0.4251\n",
      "Iter-24510, train loss-2.0517, acc-0.4200, valid loss-2.0433, acc-0.4324, test loss-2.0440, acc-0.4252\n",
      "Iter-24520, train loss-2.0502, acc-0.2800, valid loss-2.0432, acc-0.4322, test loss-2.0439, acc-0.4252\n",
      "Iter-24530, train loss-2.0838, acc-0.3800, valid loss-2.0431, acc-0.4318, test loss-2.0439, acc-0.4252\n",
      "Iter-24540, train loss-2.0377, acc-0.5000, valid loss-2.0430, acc-0.4322, test loss-2.0438, acc-0.4253\n",
      "Iter-24550, train loss-2.0863, acc-0.3600, valid loss-2.0429, acc-0.4322, test loss-2.0437, acc-0.4255\n",
      "Iter-24560, train loss-2.0559, acc-0.4200, valid loss-2.0428, acc-0.4318, test loss-2.0436, acc-0.4252\n",
      "Iter-24570, train loss-2.0096, acc-0.4400, valid loss-2.0427, acc-0.4318, test loss-2.0435, acc-0.4254\n",
      "Iter-24580, train loss-2.0232, acc-0.3800, valid loss-2.0426, acc-0.4320, test loss-2.0434, acc-0.4254\n",
      "Iter-24590, train loss-2.1640, acc-0.2200, valid loss-2.0425, acc-0.4318, test loss-2.0433, acc-0.4254\n",
      "Iter-24600, train loss-2.0373, acc-0.4000, valid loss-2.0424, acc-0.4322, test loss-2.0432, acc-0.4256\n",
      "Iter-24610, train loss-2.0756, acc-0.4200, valid loss-2.0424, acc-0.4320, test loss-2.0431, acc-0.4256\n",
      "Iter-24620, train loss-2.0833, acc-0.4000, valid loss-2.0423, acc-0.4320, test loss-2.0431, acc-0.4256\n",
      "Iter-24630, train loss-1.9944, acc-0.4200, valid loss-2.0422, acc-0.4324, test loss-2.0430, acc-0.4258\n",
      "Iter-24640, train loss-2.0418, acc-0.4000, valid loss-2.0421, acc-0.4320, test loss-2.0429, acc-0.4259\n",
      "Iter-24650, train loss-2.0330, acc-0.4400, valid loss-2.0420, acc-0.4324, test loss-2.0428, acc-0.4258\n",
      "Iter-24660, train loss-1.9991, acc-0.4800, valid loss-2.0419, acc-0.4324, test loss-2.0427, acc-0.4257\n",
      "Iter-24670, train loss-2.0504, acc-0.4200, valid loss-2.0418, acc-0.4324, test loss-2.0426, acc-0.4259\n",
      "Iter-24680, train loss-2.1240, acc-0.3000, valid loss-2.0417, acc-0.4324, test loss-2.0425, acc-0.4256\n",
      "Iter-24690, train loss-2.1139, acc-0.2800, valid loss-2.0417, acc-0.4322, test loss-2.0424, acc-0.4258\n",
      "Iter-24700, train loss-2.0289, acc-0.4200, valid loss-2.0416, acc-0.4320, test loss-2.0423, acc-0.4258\n",
      "Iter-24710, train loss-2.0899, acc-0.2600, valid loss-2.0415, acc-0.4328, test loss-2.0423, acc-0.4258\n",
      "Iter-24720, train loss-2.0244, acc-0.4200, valid loss-2.0414, acc-0.4326, test loss-2.0422, acc-0.4257\n",
      "Iter-24730, train loss-2.0515, acc-0.4200, valid loss-2.0413, acc-0.4322, test loss-2.0421, acc-0.4256\n",
      "Iter-24740, train loss-2.0168, acc-0.4400, valid loss-2.0412, acc-0.4324, test loss-2.0420, acc-0.4258\n",
      "Iter-24750, train loss-1.9762, acc-0.5400, valid loss-2.0411, acc-0.4318, test loss-2.0419, acc-0.4259\n",
      "Iter-24760, train loss-2.0468, acc-0.4000, valid loss-2.0410, acc-0.4318, test loss-2.0418, acc-0.4260\n",
      "Iter-24770, train loss-2.1173, acc-0.3000, valid loss-2.0409, acc-0.4322, test loss-2.0417, acc-0.4259\n",
      "Iter-24780, train loss-2.0685, acc-0.3800, valid loss-2.0409, acc-0.4322, test loss-2.0416, acc-0.4258\n",
      "Iter-24790, train loss-2.0374, acc-0.4600, valid loss-2.0408, acc-0.4324, test loss-2.0416, acc-0.4260\n",
      "Iter-24800, train loss-2.0350, acc-0.4600, valid loss-2.0407, acc-0.4320, test loss-2.0415, acc-0.4260\n",
      "Iter-24810, train loss-2.0545, acc-0.3800, valid loss-2.0406, acc-0.4324, test loss-2.0414, acc-0.4262\n",
      "Iter-24820, train loss-2.0738, acc-0.3600, valid loss-2.0405, acc-0.4326, test loss-2.0413, acc-0.4262\n",
      "Iter-24830, train loss-2.0886, acc-0.3400, valid loss-2.0404, acc-0.4326, test loss-2.0412, acc-0.4265\n",
      "Iter-24840, train loss-2.0552, acc-0.4200, valid loss-2.0403, acc-0.4328, test loss-2.0411, acc-0.4265\n",
      "Iter-24850, train loss-2.0472, acc-0.5200, valid loss-2.0402, acc-0.4332, test loss-2.0410, acc-0.4266\n",
      "Iter-24860, train loss-2.0081, acc-0.5200, valid loss-2.0402, acc-0.4326, test loss-2.0409, acc-0.4265\n",
      "Iter-24870, train loss-2.0616, acc-0.3600, valid loss-2.0401, acc-0.4330, test loss-2.0409, acc-0.4264\n",
      "Iter-24880, train loss-2.0718, acc-0.3800, valid loss-2.0400, acc-0.4330, test loss-2.0408, acc-0.4265\n",
      "Iter-24890, train loss-2.0358, acc-0.4400, valid loss-2.0399, acc-0.4326, test loss-2.0407, acc-0.4263\n",
      "Iter-24900, train loss-1.9991, acc-0.5200, valid loss-2.0398, acc-0.4330, test loss-2.0406, acc-0.4265\n",
      "Iter-24910, train loss-2.0540, acc-0.4400, valid loss-2.0397, acc-0.4328, test loss-2.0405, acc-0.4266\n",
      "Iter-24920, train loss-2.0754, acc-0.4200, valid loss-2.0396, acc-0.4328, test loss-2.0404, acc-0.4266\n",
      "Iter-24930, train loss-2.0383, acc-0.4600, valid loss-2.0395, acc-0.4332, test loss-2.0403, acc-0.4266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-24940, train loss-2.0276, acc-0.4800, valid loss-2.0395, acc-0.4330, test loss-2.0402, acc-0.4266\n",
      "Iter-24950, train loss-2.0531, acc-0.4200, valid loss-2.0394, acc-0.4334, test loss-2.0402, acc-0.4268\n",
      "Iter-24960, train loss-2.0894, acc-0.3400, valid loss-2.0393, acc-0.4334, test loss-2.0401, acc-0.4267\n",
      "Iter-24970, train loss-2.0744, acc-0.3000, valid loss-2.0392, acc-0.4334, test loss-2.0400, acc-0.4266\n",
      "Iter-24980, train loss-2.0560, acc-0.3800, valid loss-2.0391, acc-0.4338, test loss-2.0399, acc-0.4267\n",
      "Iter-24990, train loss-2.0356, acc-0.4200, valid loss-2.0390, acc-0.4342, test loss-2.0398, acc-0.4268\n",
      "Iter-25000, train loss-2.0505, acc-0.4600, valid loss-2.0389, acc-0.4344, test loss-2.0397, acc-0.4267\n",
      "Iter-25010, train loss-2.0456, acc-0.3800, valid loss-2.0388, acc-0.4344, test loss-2.0396, acc-0.4270\n",
      "Iter-25020, train loss-2.0457, acc-0.4000, valid loss-2.0388, acc-0.4342, test loss-2.0396, acc-0.4269\n",
      "Iter-25030, train loss-2.0119, acc-0.4600, valid loss-2.0387, acc-0.4344, test loss-2.0395, acc-0.4269\n",
      "Iter-25040, train loss-2.0400, acc-0.4600, valid loss-2.0386, acc-0.4348, test loss-2.0394, acc-0.4269\n",
      "Iter-25050, train loss-2.1035, acc-0.4400, valid loss-2.0385, acc-0.4342, test loss-2.0393, acc-0.4269\n",
      "Iter-25060, train loss-2.0214, acc-0.4800, valid loss-2.0384, acc-0.4340, test loss-2.0392, acc-0.4269\n",
      "Iter-25070, train loss-2.0144, acc-0.4400, valid loss-2.0383, acc-0.4342, test loss-2.0391, acc-0.4268\n",
      "Iter-25080, train loss-2.0243, acc-0.4600, valid loss-2.0382, acc-0.4338, test loss-2.0390, acc-0.4268\n",
      "Iter-25090, train loss-1.9955, acc-0.4800, valid loss-2.0381, acc-0.4338, test loss-2.0389, acc-0.4268\n",
      "Iter-25100, train loss-1.9965, acc-0.5400, valid loss-2.0380, acc-0.4342, test loss-2.0388, acc-0.4271\n",
      "Iter-25110, train loss-2.0308, acc-0.5200, valid loss-2.0380, acc-0.4340, test loss-2.0388, acc-0.4269\n",
      "Iter-25120, train loss-2.0036, acc-0.4800, valid loss-2.0379, acc-0.4344, test loss-2.0387, acc-0.4269\n",
      "Iter-25130, train loss-2.0302, acc-0.3600, valid loss-2.0378, acc-0.4342, test loss-2.0386, acc-0.4270\n",
      "Iter-25140, train loss-2.0421, acc-0.4400, valid loss-2.0377, acc-0.4342, test loss-2.0385, acc-0.4270\n",
      "Iter-25150, train loss-2.0768, acc-0.3600, valid loss-2.0376, acc-0.4346, test loss-2.0384, acc-0.4270\n",
      "Iter-25160, train loss-2.0757, acc-0.3000, valid loss-2.0375, acc-0.4352, test loss-2.0383, acc-0.4273\n",
      "Iter-25170, train loss-2.0126, acc-0.5400, valid loss-2.0374, acc-0.4348, test loss-2.0382, acc-0.4273\n",
      "Iter-25180, train loss-2.0924, acc-0.3200, valid loss-2.0374, acc-0.4350, test loss-2.0381, acc-0.4275\n",
      "Iter-25190, train loss-2.0585, acc-0.4000, valid loss-2.0373, acc-0.4350, test loss-2.0381, acc-0.4277\n",
      "Iter-25200, train loss-2.0266, acc-0.5000, valid loss-2.0372, acc-0.4352, test loss-2.0380, acc-0.4279\n",
      "Iter-25210, train loss-2.0450, acc-0.4000, valid loss-2.0371, acc-0.4354, test loss-2.0379, acc-0.4281\n",
      "Iter-25220, train loss-2.0426, acc-0.4600, valid loss-2.0370, acc-0.4352, test loss-2.0378, acc-0.4281\n",
      "Iter-25230, train loss-2.0625, acc-0.3800, valid loss-2.0369, acc-0.4356, test loss-2.0377, acc-0.4281\n",
      "Iter-25240, train loss-2.0626, acc-0.4200, valid loss-2.0368, acc-0.4352, test loss-2.0376, acc-0.4283\n",
      "Iter-25250, train loss-2.0651, acc-0.4000, valid loss-2.0367, acc-0.4358, test loss-2.0375, acc-0.4282\n",
      "Iter-25260, train loss-2.0455, acc-0.4600, valid loss-2.0367, acc-0.4360, test loss-2.0375, acc-0.4285\n",
      "Iter-25270, train loss-2.0519, acc-0.4600, valid loss-2.0366, acc-0.4362, test loss-2.0374, acc-0.4284\n",
      "Iter-25280, train loss-2.0654, acc-0.3600, valid loss-2.0365, acc-0.4362, test loss-2.0373, acc-0.4286\n",
      "Iter-25290, train loss-2.0225, acc-0.4400, valid loss-2.0364, acc-0.4362, test loss-2.0372, acc-0.4285\n",
      "Iter-25300, train loss-2.0357, acc-0.4800, valid loss-2.0363, acc-0.4360, test loss-2.0371, acc-0.4286\n",
      "Iter-25310, train loss-2.0620, acc-0.4000, valid loss-2.0362, acc-0.4360, test loss-2.0370, acc-0.4286\n",
      "Iter-25320, train loss-2.0672, acc-0.3800, valid loss-2.0361, acc-0.4358, test loss-2.0369, acc-0.4286\n",
      "Iter-25330, train loss-2.0419, acc-0.4800, valid loss-2.0361, acc-0.4356, test loss-2.0369, acc-0.4287\n",
      "Iter-25340, train loss-2.0333, acc-0.3400, valid loss-2.0360, acc-0.4360, test loss-2.0368, acc-0.4286\n",
      "Iter-25350, train loss-1.9948, acc-0.4800, valid loss-2.0359, acc-0.4360, test loss-2.0367, acc-0.4288\n",
      "Iter-25360, train loss-2.0790, acc-0.3400, valid loss-2.0358, acc-0.4358, test loss-2.0366, acc-0.4290\n",
      "Iter-25370, train loss-2.0626, acc-0.4000, valid loss-2.0357, acc-0.4360, test loss-2.0365, acc-0.4291\n",
      "Iter-25380, train loss-1.9758, acc-0.6000, valid loss-2.0356, acc-0.4360, test loss-2.0364, acc-0.4291\n",
      "Iter-25390, train loss-2.1006, acc-0.3800, valid loss-2.0356, acc-0.4362, test loss-2.0363, acc-0.4293\n",
      "Iter-25400, train loss-1.9883, acc-0.5000, valid loss-2.0355, acc-0.4366, test loss-2.0362, acc-0.4294\n",
      "Iter-25410, train loss-2.0599, acc-0.4200, valid loss-2.0354, acc-0.4370, test loss-2.0361, acc-0.4294\n",
      "Iter-25420, train loss-2.0478, acc-0.3600, valid loss-2.0353, acc-0.4366, test loss-2.0361, acc-0.4295\n",
      "Iter-25430, train loss-2.0665, acc-0.4400, valid loss-2.0352, acc-0.4366, test loss-2.0360, acc-0.4295\n",
      "Iter-25440, train loss-2.0292, acc-0.5400, valid loss-2.0351, acc-0.4372, test loss-2.0359, acc-0.4296\n",
      "Iter-25450, train loss-2.0364, acc-0.4000, valid loss-2.0350, acc-0.4374, test loss-2.0358, acc-0.4294\n",
      "Iter-25460, train loss-2.0402, acc-0.4600, valid loss-2.0349, acc-0.4370, test loss-2.0357, acc-0.4291\n",
      "Iter-25470, train loss-2.0079, acc-0.4600, valid loss-2.0348, acc-0.4370, test loss-2.0356, acc-0.4292\n",
      "Iter-25480, train loss-2.0514, acc-0.4600, valid loss-2.0348, acc-0.4376, test loss-2.0355, acc-0.4296\n",
      "Iter-25490, train loss-2.0437, acc-0.4600, valid loss-2.0347, acc-0.4372, test loss-2.0355, acc-0.4295\n",
      "Iter-25500, train loss-2.0133, acc-0.5200, valid loss-2.0346, acc-0.4372, test loss-2.0354, acc-0.4298\n",
      "Iter-25510, train loss-2.0476, acc-0.4400, valid loss-2.0345, acc-0.4376, test loss-2.0353, acc-0.4297\n",
      "Iter-25520, train loss-2.0178, acc-0.5200, valid loss-2.0344, acc-0.4376, test loss-2.0352, acc-0.4299\n",
      "Iter-25530, train loss-2.0743, acc-0.4000, valid loss-2.0343, acc-0.4374, test loss-2.0351, acc-0.4299\n",
      "Iter-25540, train loss-1.9971, acc-0.4800, valid loss-2.0342, acc-0.4376, test loss-2.0350, acc-0.4297\n",
      "Iter-25550, train loss-2.0693, acc-0.4000, valid loss-2.0341, acc-0.4376, test loss-2.0349, acc-0.4299\n",
      "Iter-25560, train loss-2.0294, acc-0.4200, valid loss-2.0341, acc-0.4376, test loss-2.0348, acc-0.4300\n",
      "Iter-25570, train loss-2.0054, acc-0.4400, valid loss-2.0340, acc-0.4376, test loss-2.0347, acc-0.4298\n",
      "Iter-25580, train loss-2.0948, acc-0.3200, valid loss-2.0339, acc-0.4376, test loss-2.0347, acc-0.4299\n",
      "Iter-25590, train loss-2.0738, acc-0.2600, valid loss-2.0338, acc-0.4376, test loss-2.0346, acc-0.4298\n",
      "Iter-25600, train loss-1.9909, acc-0.5400, valid loss-2.0337, acc-0.4376, test loss-2.0345, acc-0.4297\n",
      "Iter-25610, train loss-1.9502, acc-0.6400, valid loss-2.0336, acc-0.4378, test loss-2.0344, acc-0.4296\n",
      "Iter-25620, train loss-2.1137, acc-0.3000, valid loss-2.0335, acc-0.4376, test loss-2.0343, acc-0.4297\n",
      "Iter-25630, train loss-2.0753, acc-0.4000, valid loss-2.0334, acc-0.4380, test loss-2.0342, acc-0.4298\n",
      "Iter-25640, train loss-2.0257, acc-0.5200, valid loss-2.0333, acc-0.4378, test loss-2.0341, acc-0.4300\n",
      "Iter-25650, train loss-2.0722, acc-0.4400, valid loss-2.0333, acc-0.4380, test loss-2.0340, acc-0.4300\n",
      "Iter-25660, train loss-2.0641, acc-0.4200, valid loss-2.0332, acc-0.4380, test loss-2.0340, acc-0.4304\n",
      "Iter-25670, train loss-2.0230, acc-0.4600, valid loss-2.0331, acc-0.4376, test loss-2.0339, acc-0.4302\n",
      "Iter-25680, train loss-2.0036, acc-0.4800, valid loss-2.0330, acc-0.4376, test loss-2.0338, acc-0.4303\n",
      "Iter-25690, train loss-2.0798, acc-0.3600, valid loss-2.0329, acc-0.4380, test loss-2.0337, acc-0.4304\n",
      "Iter-25700, train loss-2.0657, acc-0.3800, valid loss-2.0328, acc-0.4380, test loss-2.0336, acc-0.4303\n",
      "Iter-25710, train loss-2.0301, acc-0.4200, valid loss-2.0327, acc-0.4384, test loss-2.0335, acc-0.4304\n",
      "Iter-25720, train loss-2.0431, acc-0.4600, valid loss-2.0326, acc-0.4382, test loss-2.0334, acc-0.4305\n",
      "Iter-25730, train loss-2.0089, acc-0.4400, valid loss-2.0326, acc-0.4386, test loss-2.0333, acc-0.4306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-25740, train loss-2.0718, acc-0.3400, valid loss-2.0325, acc-0.4384, test loss-2.0332, acc-0.4306\n",
      "Iter-25750, train loss-2.0400, acc-0.4600, valid loss-2.0324, acc-0.4388, test loss-2.0332, acc-0.4306\n",
      "Iter-25760, train loss-2.0861, acc-0.3000, valid loss-2.0323, acc-0.4388, test loss-2.0331, acc-0.4307\n",
      "Iter-25770, train loss-2.0201, acc-0.3800, valid loss-2.0322, acc-0.4388, test loss-2.0330, acc-0.4308\n",
      "Iter-25780, train loss-2.0589, acc-0.4000, valid loss-2.0321, acc-0.4386, test loss-2.0329, acc-0.4304\n",
      "Iter-25790, train loss-2.0546, acc-0.3000, valid loss-2.0320, acc-0.4386, test loss-2.0328, acc-0.4307\n",
      "Iter-25800, train loss-2.0394, acc-0.3600, valid loss-2.0319, acc-0.4386, test loss-2.0327, acc-0.4307\n",
      "Iter-25810, train loss-2.0279, acc-0.4600, valid loss-2.0319, acc-0.4388, test loss-2.0326, acc-0.4307\n",
      "Iter-25820, train loss-2.0017, acc-0.4600, valid loss-2.0318, acc-0.4384, test loss-2.0326, acc-0.4308\n",
      "Iter-25830, train loss-2.0179, acc-0.5000, valid loss-2.0317, acc-0.4390, test loss-2.0325, acc-0.4308\n",
      "Iter-25840, train loss-2.0698, acc-0.3800, valid loss-2.0316, acc-0.4388, test loss-2.0324, acc-0.4308\n",
      "Iter-25850, train loss-2.0460, acc-0.4600, valid loss-2.0315, acc-0.4386, test loss-2.0323, acc-0.4309\n",
      "Iter-25860, train loss-1.9555, acc-0.5600, valid loss-2.0314, acc-0.4388, test loss-2.0322, acc-0.4310\n",
      "Iter-25870, train loss-1.9922, acc-0.5200, valid loss-2.0313, acc-0.4390, test loss-2.0321, acc-0.4309\n",
      "Iter-25880, train loss-2.0298, acc-0.4000, valid loss-2.0313, acc-0.4390, test loss-2.0320, acc-0.4307\n",
      "Iter-25890, train loss-2.0385, acc-0.4400, valid loss-2.0312, acc-0.4394, test loss-2.0320, acc-0.4309\n",
      "Iter-25900, train loss-2.0187, acc-0.4600, valid loss-2.0311, acc-0.4394, test loss-2.0319, acc-0.4309\n",
      "Iter-25910, train loss-2.0733, acc-0.4400, valid loss-2.0310, acc-0.4390, test loss-2.0318, acc-0.4311\n",
      "Iter-25920, train loss-2.0240, acc-0.4400, valid loss-2.0309, acc-0.4384, test loss-2.0317, acc-0.4311\n",
      "Iter-25930, train loss-1.9928, acc-0.4800, valid loss-2.0308, acc-0.4390, test loss-2.0316, acc-0.4312\n",
      "Iter-25940, train loss-1.9762, acc-0.4200, valid loss-2.0307, acc-0.4390, test loss-2.0315, acc-0.4312\n",
      "Iter-25950, train loss-2.0433, acc-0.4600, valid loss-2.0306, acc-0.4394, test loss-2.0314, acc-0.4312\n",
      "Iter-25960, train loss-1.9897, acc-0.5200, valid loss-2.0306, acc-0.4392, test loss-2.0313, acc-0.4311\n",
      "Iter-25970, train loss-2.0604, acc-0.4800, valid loss-2.0305, acc-0.4396, test loss-2.0313, acc-0.4313\n",
      "Iter-25980, train loss-2.0544, acc-0.4400, valid loss-2.0304, acc-0.4392, test loss-2.0312, acc-0.4313\n",
      "Iter-25990, train loss-2.0669, acc-0.3800, valid loss-2.0303, acc-0.4396, test loss-2.0311, acc-0.4313\n",
      "Iter-26000, train loss-2.0222, acc-0.3600, valid loss-2.0302, acc-0.4398, test loss-2.0310, acc-0.4310\n",
      "Iter-26010, train loss-2.0308, acc-0.4000, valid loss-2.0301, acc-0.4392, test loss-2.0309, acc-0.4313\n",
      "Iter-26020, train loss-2.0564, acc-0.4000, valid loss-2.0300, acc-0.4396, test loss-2.0308, acc-0.4314\n",
      "Iter-26030, train loss-2.0071, acc-0.5200, valid loss-2.0300, acc-0.4390, test loss-2.0308, acc-0.4315\n",
      "Iter-26040, train loss-2.0034, acc-0.4400, valid loss-2.0299, acc-0.4398, test loss-2.0307, acc-0.4313\n",
      "Iter-26050, train loss-2.0040, acc-0.4400, valid loss-2.0298, acc-0.4398, test loss-2.0306, acc-0.4314\n",
      "Iter-26060, train loss-2.0673, acc-0.3800, valid loss-2.0297, acc-0.4402, test loss-2.0305, acc-0.4314\n",
      "Iter-26070, train loss-2.0746, acc-0.3600, valid loss-2.0296, acc-0.4398, test loss-2.0304, acc-0.4314\n",
      "Iter-26080, train loss-2.0743, acc-0.4000, valid loss-2.0296, acc-0.4402, test loss-2.0303, acc-0.4317\n",
      "Iter-26090, train loss-2.0196, acc-0.4000, valid loss-2.0295, acc-0.4404, test loss-2.0303, acc-0.4314\n",
      "Iter-26100, train loss-2.0699, acc-0.3600, valid loss-2.0294, acc-0.4402, test loss-2.0302, acc-0.4316\n",
      "Iter-26110, train loss-2.0473, acc-0.4200, valid loss-2.0293, acc-0.4404, test loss-2.0301, acc-0.4318\n",
      "Iter-26120, train loss-2.1264, acc-0.3200, valid loss-2.0292, acc-0.4404, test loss-2.0300, acc-0.4323\n",
      "Iter-26130, train loss-2.1009, acc-0.2600, valid loss-2.0291, acc-0.4404, test loss-2.0299, acc-0.4323\n",
      "Iter-26140, train loss-2.0081, acc-0.5400, valid loss-2.0290, acc-0.4404, test loss-2.0298, acc-0.4321\n",
      "Iter-26150, train loss-2.0821, acc-0.3600, valid loss-2.0289, acc-0.4406, test loss-2.0297, acc-0.4320\n",
      "Iter-26160, train loss-2.0463, acc-0.3400, valid loss-2.0289, acc-0.4402, test loss-2.0297, acc-0.4322\n",
      "Iter-26170, train loss-2.0387, acc-0.3400, valid loss-2.0288, acc-0.4400, test loss-2.0296, acc-0.4323\n",
      "Iter-26180, train loss-2.0535, acc-0.3800, valid loss-2.0287, acc-0.4404, test loss-2.0295, acc-0.4325\n",
      "Iter-26190, train loss-2.0400, acc-0.4400, valid loss-2.0286, acc-0.4406, test loss-2.0294, acc-0.4326\n",
      "Iter-26200, train loss-2.0996, acc-0.3600, valid loss-2.0285, acc-0.4408, test loss-2.0293, acc-0.4327\n",
      "Iter-26210, train loss-2.0169, acc-0.5400, valid loss-2.0284, acc-0.4408, test loss-2.0292, acc-0.4325\n",
      "Iter-26220, train loss-2.0621, acc-0.4200, valid loss-2.0283, acc-0.4404, test loss-2.0291, acc-0.4327\n",
      "Iter-26230, train loss-2.0916, acc-0.2800, valid loss-2.0283, acc-0.4404, test loss-2.0290, acc-0.4328\n",
      "Iter-26240, train loss-1.9866, acc-0.5000, valid loss-2.0282, acc-0.4402, test loss-2.0290, acc-0.4324\n",
      "Iter-26250, train loss-2.0540, acc-0.3800, valid loss-2.0281, acc-0.4408, test loss-2.0289, acc-0.4322\n",
      "Iter-26260, train loss-2.0612, acc-0.3200, valid loss-2.0280, acc-0.4410, test loss-2.0288, acc-0.4325\n",
      "Iter-26270, train loss-2.0364, acc-0.4000, valid loss-2.0279, acc-0.4410, test loss-2.0287, acc-0.4327\n",
      "Iter-26280, train loss-2.0222, acc-0.4400, valid loss-2.0278, acc-0.4412, test loss-2.0286, acc-0.4327\n",
      "Iter-26290, train loss-2.0165, acc-0.5000, valid loss-2.0277, acc-0.4404, test loss-2.0285, acc-0.4325\n",
      "Iter-26300, train loss-2.0897, acc-0.2800, valid loss-2.0276, acc-0.4406, test loss-2.0284, acc-0.4325\n",
      "Iter-26310, train loss-2.0845, acc-0.3200, valid loss-2.0276, acc-0.4410, test loss-2.0283, acc-0.4324\n",
      "Iter-26320, train loss-1.9958, acc-0.5000, valid loss-2.0275, acc-0.4404, test loss-2.0283, acc-0.4325\n",
      "Iter-26330, train loss-2.0280, acc-0.4800, valid loss-2.0274, acc-0.4404, test loss-2.0282, acc-0.4324\n",
      "Iter-26340, train loss-2.0061, acc-0.4800, valid loss-2.0273, acc-0.4404, test loss-2.0281, acc-0.4325\n",
      "Iter-26350, train loss-2.0149, acc-0.5600, valid loss-2.0272, acc-0.4402, test loss-2.0280, acc-0.4323\n",
      "Iter-26360, train loss-2.0285, acc-0.5000, valid loss-2.0271, acc-0.4404, test loss-2.0279, acc-0.4326\n",
      "Iter-26370, train loss-2.0863, acc-0.2400, valid loss-2.0270, acc-0.4404, test loss-2.0278, acc-0.4328\n",
      "Iter-26380, train loss-2.0468, acc-0.4000, valid loss-2.0270, acc-0.4406, test loss-2.0277, acc-0.4328\n",
      "Iter-26390, train loss-2.0985, acc-0.3200, valid loss-2.0269, acc-0.4406, test loss-2.0277, acc-0.4328\n",
      "Iter-26400, train loss-2.0094, acc-0.3600, valid loss-2.0268, acc-0.4412, test loss-2.0276, acc-0.4323\n",
      "Iter-26410, train loss-2.0502, acc-0.4800, valid loss-2.0267, acc-0.4412, test loss-2.0275, acc-0.4327\n",
      "Iter-26420, train loss-2.0012, acc-0.5600, valid loss-2.0266, acc-0.4412, test loss-2.0274, acc-0.4327\n",
      "Iter-26430, train loss-2.0035, acc-0.5600, valid loss-2.0265, acc-0.4410, test loss-2.0273, acc-0.4328\n",
      "Iter-26440, train loss-1.9588, acc-0.4800, valid loss-2.0264, acc-0.4408, test loss-2.0272, acc-0.4327\n",
      "Iter-26450, train loss-2.0675, acc-0.3600, valid loss-2.0264, acc-0.4408, test loss-2.0272, acc-0.4328\n",
      "Iter-26460, train loss-2.0657, acc-0.3600, valid loss-2.0263, acc-0.4410, test loss-2.0271, acc-0.4328\n",
      "Iter-26470, train loss-2.0245, acc-0.5800, valid loss-2.0262, acc-0.4410, test loss-2.0270, acc-0.4327\n",
      "Iter-26480, train loss-2.0337, acc-0.4000, valid loss-2.0261, acc-0.4412, test loss-2.0269, acc-0.4328\n",
      "Iter-26490, train loss-2.0241, acc-0.4000, valid loss-2.0260, acc-0.4412, test loss-2.0268, acc-0.4327\n",
      "Iter-26500, train loss-1.9896, acc-0.5000, valid loss-2.0259, acc-0.4412, test loss-2.0267, acc-0.4328\n",
      "Iter-26510, train loss-2.0984, acc-0.3200, valid loss-2.0259, acc-0.4414, test loss-2.0267, acc-0.4330\n",
      "Iter-26520, train loss-2.0337, acc-0.4000, valid loss-2.0258, acc-0.4410, test loss-2.0266, acc-0.4328\n",
      "Iter-26530, train loss-2.0337, acc-0.3200, valid loss-2.0257, acc-0.4408, test loss-2.0265, acc-0.4329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-26540, train loss-2.0667, acc-0.3400, valid loss-2.0256, acc-0.4410, test loss-2.0264, acc-0.4329\n",
      "Iter-26550, train loss-1.9991, acc-0.5000, valid loss-2.0255, acc-0.4408, test loss-2.0263, acc-0.4330\n",
      "Iter-26560, train loss-2.0598, acc-0.4400, valid loss-2.0254, acc-0.4412, test loss-2.0262, acc-0.4330\n",
      "Iter-26570, train loss-1.9789, acc-0.4800, valid loss-2.0254, acc-0.4410, test loss-2.0261, acc-0.4331\n",
      "Iter-26580, train loss-2.0840, acc-0.4000, valid loss-2.0253, acc-0.4412, test loss-2.0261, acc-0.4331\n",
      "Iter-26590, train loss-2.0754, acc-0.3000, valid loss-2.0252, acc-0.4418, test loss-2.0260, acc-0.4330\n",
      "Iter-26600, train loss-2.0063, acc-0.4000, valid loss-2.0251, acc-0.4410, test loss-2.0259, acc-0.4331\n",
      "Iter-26610, train loss-2.0960, acc-0.3200, valid loss-2.0250, acc-0.4410, test loss-2.0258, acc-0.4328\n",
      "Iter-26620, train loss-2.0493, acc-0.3200, valid loss-2.0249, acc-0.4410, test loss-2.0257, acc-0.4329\n",
      "Iter-26630, train loss-1.9911, acc-0.4800, valid loss-2.0248, acc-0.4410, test loss-2.0256, acc-0.4330\n",
      "Iter-26640, train loss-1.9654, acc-0.5400, valid loss-2.0247, acc-0.4408, test loss-2.0255, acc-0.4332\n",
      "Iter-26650, train loss-2.0297, acc-0.4200, valid loss-2.0247, acc-0.4406, test loss-2.0255, acc-0.4331\n",
      "Iter-26660, train loss-2.0199, acc-0.4600, valid loss-2.0246, acc-0.4406, test loss-2.0254, acc-0.4333\n",
      "Iter-26670, train loss-2.0227, acc-0.4800, valid loss-2.0245, acc-0.4406, test loss-2.0253, acc-0.4333\n",
      "Iter-26680, train loss-2.0014, acc-0.4400, valid loss-2.0244, acc-0.4406, test loss-2.0252, acc-0.4331\n",
      "Iter-26690, train loss-2.0056, acc-0.4200, valid loss-2.0243, acc-0.4406, test loss-2.0251, acc-0.4332\n",
      "Iter-26700, train loss-2.0561, acc-0.4000, valid loss-2.0242, acc-0.4404, test loss-2.0250, acc-0.4332\n",
      "Iter-26710, train loss-1.9924, acc-0.4800, valid loss-2.0242, acc-0.4410, test loss-2.0250, acc-0.4333\n",
      "Iter-26720, train loss-2.0601, acc-0.4400, valid loss-2.0241, acc-0.4412, test loss-2.0249, acc-0.4331\n",
      "Iter-26730, train loss-2.0112, acc-0.4400, valid loss-2.0240, acc-0.4412, test loss-2.0248, acc-0.4332\n",
      "Iter-26740, train loss-2.0860, acc-0.3400, valid loss-2.0239, acc-0.4414, test loss-2.0247, acc-0.4333\n",
      "Iter-26750, train loss-2.0162, acc-0.5200, valid loss-2.0238, acc-0.4422, test loss-2.0246, acc-0.4334\n",
      "Iter-26760, train loss-1.9865, acc-0.4200, valid loss-2.0237, acc-0.4420, test loss-2.0245, acc-0.4335\n",
      "Iter-26770, train loss-2.0720, acc-0.3800, valid loss-2.0237, acc-0.4416, test loss-2.0245, acc-0.4334\n",
      "Iter-26780, train loss-2.0298, acc-0.3800, valid loss-2.0236, acc-0.4422, test loss-2.0244, acc-0.4336\n",
      "Iter-26790, train loss-2.0852, acc-0.2800, valid loss-2.0235, acc-0.4422, test loss-2.0243, acc-0.4336\n",
      "Iter-26800, train loss-2.0194, acc-0.5200, valid loss-2.0234, acc-0.4428, test loss-2.0242, acc-0.4334\n",
      "Iter-26810, train loss-1.9934, acc-0.4000, valid loss-2.0233, acc-0.4422, test loss-2.0241, acc-0.4334\n",
      "Iter-26820, train loss-1.9741, acc-0.5200, valid loss-2.0232, acc-0.4422, test loss-2.0240, acc-0.4332\n",
      "Iter-26830, train loss-2.1024, acc-0.3000, valid loss-2.0231, acc-0.4424, test loss-2.0239, acc-0.4335\n",
      "Iter-26840, train loss-2.0649, acc-0.3600, valid loss-2.0231, acc-0.4424, test loss-2.0239, acc-0.4337\n",
      "Iter-26850, train loss-2.0315, acc-0.3800, valid loss-2.0230, acc-0.4424, test loss-2.0238, acc-0.4336\n",
      "Iter-26860, train loss-1.9369, acc-0.5200, valid loss-2.0229, acc-0.4426, test loss-2.0237, acc-0.4336\n",
      "Iter-26870, train loss-2.0896, acc-0.3000, valid loss-2.0228, acc-0.4428, test loss-2.0236, acc-0.4336\n",
      "Iter-26880, train loss-2.0432, acc-0.4000, valid loss-2.0227, acc-0.4424, test loss-2.0235, acc-0.4337\n",
      "Iter-26890, train loss-1.9837, acc-0.5000, valid loss-2.0226, acc-0.4428, test loss-2.0234, acc-0.4338\n",
      "Iter-26900, train loss-2.0584, acc-0.4000, valid loss-2.0225, acc-0.4428, test loss-2.0233, acc-0.4338\n",
      "Iter-26910, train loss-2.0540, acc-0.4200, valid loss-2.0225, acc-0.4428, test loss-2.0233, acc-0.4341\n",
      "Iter-26920, train loss-2.0849, acc-0.3400, valid loss-2.0224, acc-0.4422, test loss-2.0232, acc-0.4338\n",
      "Iter-26930, train loss-2.0458, acc-0.4000, valid loss-2.0223, acc-0.4428, test loss-2.0231, acc-0.4339\n",
      "Iter-26940, train loss-2.0258, acc-0.3800, valid loss-2.0222, acc-0.4424, test loss-2.0230, acc-0.4338\n",
      "Iter-26950, train loss-2.0335, acc-0.3800, valid loss-2.0221, acc-0.4426, test loss-2.0229, acc-0.4336\n",
      "Iter-26960, train loss-2.0457, acc-0.4200, valid loss-2.0220, acc-0.4428, test loss-2.0228, acc-0.4338\n",
      "Iter-26970, train loss-2.0400, acc-0.4000, valid loss-2.0220, acc-0.4430, test loss-2.0227, acc-0.4340\n",
      "Iter-26980, train loss-2.0182, acc-0.5200, valid loss-2.0219, acc-0.4428, test loss-2.0227, acc-0.4338\n",
      "Iter-26990, train loss-2.0139, acc-0.5200, valid loss-2.0218, acc-0.4428, test loss-2.0226, acc-0.4339\n",
      "Iter-27000, train loss-2.0144, acc-0.5000, valid loss-2.0217, acc-0.4428, test loss-2.0225, acc-0.4338\n",
      "Iter-27010, train loss-2.0305, acc-0.4200, valid loss-2.0216, acc-0.4426, test loss-2.0224, acc-0.4340\n",
      "Iter-27020, train loss-2.0658, acc-0.3600, valid loss-2.0215, acc-0.4426, test loss-2.0223, acc-0.4340\n",
      "Iter-27030, train loss-1.9907, acc-0.4600, valid loss-2.0214, acc-0.4426, test loss-2.0222, acc-0.4340\n",
      "Iter-27040, train loss-2.0138, acc-0.4000, valid loss-2.0213, acc-0.4426, test loss-2.0221, acc-0.4340\n",
      "Iter-27050, train loss-2.0561, acc-0.4000, valid loss-2.0213, acc-0.4426, test loss-2.0221, acc-0.4339\n",
      "Iter-27060, train loss-2.0372, acc-0.4000, valid loss-2.0212, acc-0.4428, test loss-2.0220, acc-0.4341\n",
      "Iter-27070, train loss-2.0230, acc-0.4800, valid loss-2.0211, acc-0.4430, test loss-2.0219, acc-0.4341\n",
      "Iter-27080, train loss-2.0874, acc-0.3400, valid loss-2.0210, acc-0.4430, test loss-2.0218, acc-0.4341\n",
      "Iter-27090, train loss-1.9796, acc-0.4800, valid loss-2.0209, acc-0.4432, test loss-2.0217, acc-0.4341\n",
      "Iter-27100, train loss-2.0127, acc-0.5000, valid loss-2.0208, acc-0.4432, test loss-2.0216, acc-0.4341\n",
      "Iter-27110, train loss-2.0259, acc-0.3600, valid loss-2.0207, acc-0.4428, test loss-2.0215, acc-0.4343\n",
      "Iter-27120, train loss-2.0561, acc-0.4400, valid loss-2.0207, acc-0.4432, test loss-2.0215, acc-0.4344\n",
      "Iter-27130, train loss-1.9637, acc-0.5600, valid loss-2.0206, acc-0.4434, test loss-2.0214, acc-0.4343\n",
      "Iter-27140, train loss-2.0282, acc-0.4000, valid loss-2.0205, acc-0.4430, test loss-2.0213, acc-0.4342\n",
      "Iter-27150, train loss-2.0878, acc-0.2800, valid loss-2.0204, acc-0.4434, test loss-2.0212, acc-0.4344\n",
      "Iter-27160, train loss-2.1302, acc-0.2800, valid loss-2.0203, acc-0.4436, test loss-2.0211, acc-0.4344\n",
      "Iter-27170, train loss-2.0320, acc-0.4400, valid loss-2.0202, acc-0.4438, test loss-2.0210, acc-0.4346\n",
      "Iter-27180, train loss-2.0076, acc-0.4200, valid loss-2.0202, acc-0.4440, test loss-2.0210, acc-0.4345\n",
      "Iter-27190, train loss-2.0753, acc-0.4000, valid loss-2.0201, acc-0.4438, test loss-2.0209, acc-0.4345\n",
      "Iter-27200, train loss-2.0089, acc-0.4800, valid loss-2.0200, acc-0.4438, test loss-2.0208, acc-0.4346\n",
      "Iter-27210, train loss-1.9636, acc-0.5400, valid loss-2.0199, acc-0.4436, test loss-2.0207, acc-0.4343\n",
      "Iter-27220, train loss-2.0818, acc-0.4400, valid loss-2.0198, acc-0.4438, test loss-2.0206, acc-0.4345\n",
      "Iter-27230, train loss-2.1017, acc-0.3400, valid loss-2.0198, acc-0.4436, test loss-2.0206, acc-0.4344\n",
      "Iter-27240, train loss-1.9791, acc-0.4200, valid loss-2.0197, acc-0.4436, test loss-2.0205, acc-0.4345\n",
      "Iter-27250, train loss-1.9649, acc-0.5000, valid loss-2.0196, acc-0.4434, test loss-2.0204, acc-0.4346\n",
      "Iter-27260, train loss-2.0345, acc-0.4400, valid loss-2.0195, acc-0.4434, test loss-2.0203, acc-0.4344\n",
      "Iter-27270, train loss-2.0759, acc-0.2600, valid loss-2.0194, acc-0.4430, test loss-2.0202, acc-0.4344\n",
      "Iter-27280, train loss-2.0637, acc-0.3600, valid loss-2.0193, acc-0.4430, test loss-2.0201, acc-0.4345\n",
      "Iter-27290, train loss-2.0009, acc-0.4800, valid loss-2.0192, acc-0.4434, test loss-2.0201, acc-0.4345\n",
      "Iter-27300, train loss-2.0458, acc-0.4000, valid loss-2.0192, acc-0.4432, test loss-2.0200, acc-0.4346\n",
      "Iter-27310, train loss-2.0075, acc-0.4600, valid loss-2.0191, acc-0.4432, test loss-2.0199, acc-0.4345\n",
      "Iter-27320, train loss-2.0560, acc-0.4400, valid loss-2.0190, acc-0.4432, test loss-2.0198, acc-0.4347\n",
      "Iter-27330, train loss-2.0121, acc-0.4600, valid loss-2.0189, acc-0.4436, test loss-2.0197, acc-0.4346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-27340, train loss-2.0412, acc-0.4000, valid loss-2.0188, acc-0.4434, test loss-2.0196, acc-0.4349\n",
      "Iter-27350, train loss-2.0019, acc-0.5000, valid loss-2.0187, acc-0.4434, test loss-2.0196, acc-0.4352\n",
      "Iter-27360, train loss-2.0205, acc-0.4200, valid loss-2.0187, acc-0.4436, test loss-2.0195, acc-0.4352\n",
      "Iter-27370, train loss-1.9833, acc-0.4800, valid loss-2.0186, acc-0.4436, test loss-2.0194, acc-0.4352\n",
      "Iter-27380, train loss-2.1363, acc-0.2600, valid loss-2.0185, acc-0.4438, test loss-2.0193, acc-0.4351\n",
      "Iter-27390, train loss-1.9889, acc-0.5200, valid loss-2.0184, acc-0.4438, test loss-2.0192, acc-0.4350\n",
      "Iter-27400, train loss-2.1035, acc-0.3200, valid loss-2.0183, acc-0.4440, test loss-2.0191, acc-0.4350\n",
      "Iter-27410, train loss-2.0441, acc-0.3800, valid loss-2.0182, acc-0.4442, test loss-2.0191, acc-0.4350\n",
      "Iter-27420, train loss-2.0448, acc-0.4000, valid loss-2.0181, acc-0.4442, test loss-2.0190, acc-0.4350\n",
      "Iter-27430, train loss-2.0952, acc-0.3600, valid loss-2.0181, acc-0.4436, test loss-2.0189, acc-0.4352\n",
      "Iter-27440, train loss-1.9728, acc-0.4200, valid loss-2.0180, acc-0.4436, test loss-2.0188, acc-0.4352\n",
      "Iter-27450, train loss-2.0669, acc-0.3000, valid loss-2.0179, acc-0.4436, test loss-2.0187, acc-0.4352\n",
      "Iter-27460, train loss-2.0267, acc-0.4000, valid loss-2.0178, acc-0.4434, test loss-2.0186, acc-0.4352\n",
      "Iter-27470, train loss-1.9911, acc-0.4800, valid loss-2.0177, acc-0.4436, test loss-2.0186, acc-0.4351\n",
      "Iter-27480, train loss-2.0104, acc-0.5200, valid loss-2.0176, acc-0.4434, test loss-2.0185, acc-0.4355\n",
      "Iter-27490, train loss-1.9908, acc-0.4800, valid loss-2.0176, acc-0.4442, test loss-2.0184, acc-0.4353\n",
      "Iter-27500, train loss-2.0076, acc-0.4400, valid loss-2.0175, acc-0.4442, test loss-2.0183, acc-0.4353\n",
      "Iter-27510, train loss-2.0563, acc-0.4000, valid loss-2.0174, acc-0.4440, test loss-2.0182, acc-0.4353\n",
      "Iter-27520, train loss-2.0189, acc-0.5000, valid loss-2.0173, acc-0.4440, test loss-2.0181, acc-0.4350\n",
      "Iter-27530, train loss-1.9829, acc-0.4200, valid loss-2.0172, acc-0.4438, test loss-2.0180, acc-0.4350\n",
      "Iter-27540, train loss-2.0515, acc-0.3400, valid loss-2.0171, acc-0.4438, test loss-2.0180, acc-0.4351\n",
      "Iter-27550, train loss-2.0410, acc-0.3200, valid loss-2.0170, acc-0.4436, test loss-2.0179, acc-0.4349\n",
      "Iter-27560, train loss-1.9781, acc-0.5800, valid loss-2.0170, acc-0.4440, test loss-2.0178, acc-0.4351\n",
      "Iter-27570, train loss-2.0287, acc-0.4400, valid loss-2.0169, acc-0.4436, test loss-2.0177, acc-0.4353\n",
      "Iter-27580, train loss-2.0722, acc-0.3000, valid loss-2.0168, acc-0.4436, test loss-2.0176, acc-0.4353\n",
      "Iter-27590, train loss-2.0471, acc-0.3600, valid loss-2.0167, acc-0.4436, test loss-2.0175, acc-0.4354\n",
      "Iter-27600, train loss-2.0773, acc-0.3400, valid loss-2.0166, acc-0.4436, test loss-2.0175, acc-0.4356\n",
      "Iter-27610, train loss-2.0818, acc-0.3600, valid loss-2.0166, acc-0.4440, test loss-2.0174, acc-0.4355\n",
      "Iter-27620, train loss-2.0454, acc-0.4200, valid loss-2.0165, acc-0.4440, test loss-2.0173, acc-0.4354\n",
      "Iter-27630, train loss-2.0390, acc-0.4600, valid loss-2.0164, acc-0.4440, test loss-2.0172, acc-0.4353\n",
      "Iter-27640, train loss-1.9896, acc-0.4400, valid loss-2.0163, acc-0.4438, test loss-2.0171, acc-0.4353\n",
      "Iter-27650, train loss-2.0392, acc-0.4400, valid loss-2.0162, acc-0.4442, test loss-2.0170, acc-0.4356\n",
      "Iter-27660, train loss-2.0191, acc-0.5000, valid loss-2.0161, acc-0.4440, test loss-2.0170, acc-0.4357\n",
      "Iter-27670, train loss-1.9801, acc-0.4000, valid loss-2.0161, acc-0.4442, test loss-2.0169, acc-0.4357\n",
      "Iter-27680, train loss-1.9890, acc-0.5000, valid loss-2.0160, acc-0.4444, test loss-2.0168, acc-0.4359\n",
      "Iter-27690, train loss-2.0932, acc-0.3800, valid loss-2.0159, acc-0.4444, test loss-2.0167, acc-0.4358\n",
      "Iter-27700, train loss-2.0384, acc-0.3600, valid loss-2.0158, acc-0.4450, test loss-2.0166, acc-0.4358\n",
      "Iter-27710, train loss-2.0031, acc-0.4400, valid loss-2.0157, acc-0.4446, test loss-2.0165, acc-0.4359\n",
      "Iter-27720, train loss-2.0571, acc-0.4200, valid loss-2.0156, acc-0.4444, test loss-2.0165, acc-0.4358\n",
      "Iter-27730, train loss-2.0607, acc-0.4600, valid loss-2.0156, acc-0.4444, test loss-2.0164, acc-0.4358\n",
      "Iter-27740, train loss-1.9938, acc-0.4000, valid loss-2.0155, acc-0.4444, test loss-2.0163, acc-0.4359\n",
      "Iter-27750, train loss-2.0254, acc-0.4000, valid loss-2.0154, acc-0.4444, test loss-2.0162, acc-0.4357\n",
      "Iter-27760, train loss-2.0158, acc-0.5000, valid loss-2.0153, acc-0.4444, test loss-2.0161, acc-0.4357\n",
      "Iter-27770, train loss-2.0126, acc-0.4000, valid loss-2.0152, acc-0.4444, test loss-2.0161, acc-0.4354\n",
      "Iter-27780, train loss-2.0053, acc-0.4400, valid loss-2.0151, acc-0.4446, test loss-2.0160, acc-0.4355\n",
      "Iter-27790, train loss-1.9265, acc-0.6000, valid loss-2.0151, acc-0.4446, test loss-2.0159, acc-0.4355\n",
      "Iter-27800, train loss-1.9849, acc-0.4400, valid loss-2.0150, acc-0.4444, test loss-2.0158, acc-0.4355\n",
      "Iter-27810, train loss-2.0198, acc-0.4400, valid loss-2.0149, acc-0.4446, test loss-2.0157, acc-0.4355\n",
      "Iter-27820, train loss-2.0524, acc-0.4400, valid loss-2.0148, acc-0.4446, test loss-2.0156, acc-0.4354\n",
      "Iter-27830, train loss-2.0161, acc-0.4400, valid loss-2.0147, acc-0.4446, test loss-2.0155, acc-0.4356\n",
      "Iter-27840, train loss-2.0307, acc-0.4000, valid loss-2.0146, acc-0.4446, test loss-2.0155, acc-0.4354\n",
      "Iter-27850, train loss-1.9859, acc-0.4600, valid loss-2.0146, acc-0.4444, test loss-2.0154, acc-0.4354\n",
      "Iter-27860, train loss-1.9933, acc-0.4800, valid loss-2.0145, acc-0.4444, test loss-2.0153, acc-0.4353\n",
      "Iter-27870, train loss-1.9925, acc-0.5400, valid loss-2.0144, acc-0.4446, test loss-2.0152, acc-0.4354\n",
      "Iter-27880, train loss-1.9696, acc-0.5200, valid loss-2.0143, acc-0.4446, test loss-2.0151, acc-0.4353\n",
      "Iter-27890, train loss-1.9646, acc-0.5000, valid loss-2.0142, acc-0.4446, test loss-2.0151, acc-0.4354\n",
      "Iter-27900, train loss-1.9751, acc-0.5000, valid loss-2.0141, acc-0.4446, test loss-2.0150, acc-0.4352\n",
      "Iter-27910, train loss-2.0362, acc-0.3800, valid loss-2.0141, acc-0.4446, test loss-2.0149, acc-0.4353\n",
      "Iter-27920, train loss-2.0427, acc-0.3400, valid loss-2.0140, acc-0.4446, test loss-2.0148, acc-0.4354\n",
      "Iter-27930, train loss-2.0085, acc-0.4600, valid loss-2.0139, acc-0.4442, test loss-2.0147, acc-0.4356\n",
      "Iter-27940, train loss-2.0035, acc-0.5600, valid loss-2.0138, acc-0.4444, test loss-2.0146, acc-0.4356\n",
      "Iter-27950, train loss-1.9805, acc-0.4000, valid loss-2.0137, acc-0.4444, test loss-2.0146, acc-0.4357\n",
      "Iter-27960, train loss-2.0076, acc-0.4600, valid loss-2.0137, acc-0.4446, test loss-2.0145, acc-0.4355\n",
      "Iter-27970, train loss-1.9593, acc-0.5200, valid loss-2.0136, acc-0.4444, test loss-2.0144, acc-0.4356\n",
      "Iter-27980, train loss-1.9018, acc-0.5600, valid loss-2.0135, acc-0.4444, test loss-2.0143, acc-0.4357\n",
      "Iter-27990, train loss-2.0095, acc-0.4000, valid loss-2.0134, acc-0.4448, test loss-2.0142, acc-0.4358\n",
      "Iter-28000, train loss-2.0320, acc-0.4200, valid loss-2.0133, acc-0.4448, test loss-2.0141, acc-0.4358\n",
      "Iter-28010, train loss-2.0500, acc-0.4000, valid loss-2.0132, acc-0.4448, test loss-2.0141, acc-0.4359\n",
      "Iter-28020, train loss-2.0324, acc-0.4400, valid loss-2.0132, acc-0.4446, test loss-2.0140, acc-0.4361\n",
      "Iter-28030, train loss-1.9826, acc-0.5200, valid loss-2.0131, acc-0.4446, test loss-2.0139, acc-0.4359\n",
      "Iter-28040, train loss-1.9977, acc-0.4400, valid loss-2.0130, acc-0.4446, test loss-2.0138, acc-0.4362\n",
      "Iter-28050, train loss-2.0453, acc-0.3800, valid loss-2.0129, acc-0.4448, test loss-2.0137, acc-0.4359\n",
      "Iter-28060, train loss-2.0763, acc-0.3600, valid loss-2.0128, acc-0.4448, test loss-2.0136, acc-0.4360\n",
      "Iter-28070, train loss-1.9929, acc-0.5000, valid loss-2.0127, acc-0.4446, test loss-2.0136, acc-0.4365\n",
      "Iter-28080, train loss-2.0021, acc-0.4800, valid loss-2.0127, acc-0.4448, test loss-2.0135, acc-0.4362\n",
      "Iter-28090, train loss-2.0375, acc-0.4200, valid loss-2.0126, acc-0.4446, test loss-2.0134, acc-0.4363\n",
      "Iter-28100, train loss-2.0568, acc-0.3800, valid loss-2.0125, acc-0.4446, test loss-2.0133, acc-0.4367\n",
      "Iter-28110, train loss-2.0043, acc-0.5000, valid loss-2.0124, acc-0.4444, test loss-2.0132, acc-0.4366\n",
      "Iter-28120, train loss-2.0416, acc-0.3200, valid loss-2.0123, acc-0.4444, test loss-2.0132, acc-0.4365\n",
      "Iter-28130, train loss-2.0540, acc-0.4200, valid loss-2.0123, acc-0.4444, test loss-2.0131, acc-0.4366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-28140, train loss-1.9990, acc-0.4800, valid loss-2.0122, acc-0.4444, test loss-2.0130, acc-0.4365\n",
      "Iter-28150, train loss-2.0610, acc-0.4000, valid loss-2.0121, acc-0.4442, test loss-2.0129, acc-0.4365\n",
      "Iter-28160, train loss-2.0001, acc-0.4800, valid loss-2.0120, acc-0.4446, test loss-2.0128, acc-0.4363\n",
      "Iter-28170, train loss-2.0419, acc-0.4400, valid loss-2.0119, acc-0.4448, test loss-2.0127, acc-0.4362\n",
      "Iter-28180, train loss-2.0590, acc-0.3000, valid loss-2.0118, acc-0.4446, test loss-2.0127, acc-0.4364\n",
      "Iter-28190, train loss-1.9973, acc-0.3800, valid loss-2.0118, acc-0.4444, test loss-2.0126, acc-0.4364\n",
      "Iter-28200, train loss-2.0321, acc-0.4200, valid loss-2.0117, acc-0.4444, test loss-2.0125, acc-0.4365\n",
      "Iter-28210, train loss-2.0138, acc-0.5000, valid loss-2.0116, acc-0.4444, test loss-2.0124, acc-0.4361\n",
      "Iter-28220, train loss-2.0094, acc-0.4000, valid loss-2.0115, acc-0.4444, test loss-2.0123, acc-0.4360\n",
      "Iter-28230, train loss-2.0642, acc-0.4000, valid loss-2.0114, acc-0.4444, test loss-2.0122, acc-0.4358\n",
      "Iter-28240, train loss-2.0361, acc-0.3200, valid loss-2.0113, acc-0.4446, test loss-2.0122, acc-0.4358\n",
      "Iter-28250, train loss-2.0374, acc-0.4800, valid loss-2.0113, acc-0.4448, test loss-2.0121, acc-0.4356\n",
      "Iter-28260, train loss-2.0606, acc-0.3800, valid loss-2.0112, acc-0.4448, test loss-2.0120, acc-0.4356\n",
      "Iter-28270, train loss-2.0464, acc-0.4200, valid loss-2.0111, acc-0.4450, test loss-2.0119, acc-0.4356\n",
      "Iter-28280, train loss-1.9891, acc-0.3800, valid loss-2.0110, acc-0.4448, test loss-2.0118, acc-0.4356\n",
      "Iter-28290, train loss-2.0399, acc-0.4000, valid loss-2.0109, acc-0.4448, test loss-2.0118, acc-0.4357\n",
      "Iter-28300, train loss-2.0175, acc-0.4200, valid loss-2.0109, acc-0.4448, test loss-2.0117, acc-0.4354\n",
      "Iter-28310, train loss-2.0200, acc-0.3400, valid loss-2.0108, acc-0.4450, test loss-2.0116, acc-0.4356\n",
      "Iter-28320, train loss-1.9846, acc-0.5800, valid loss-2.0107, acc-0.4448, test loss-2.0115, acc-0.4357\n",
      "Iter-28330, train loss-1.9677, acc-0.5000, valid loss-2.0106, acc-0.4446, test loss-2.0114, acc-0.4361\n",
      "Iter-28340, train loss-1.9897, acc-0.4200, valid loss-2.0105, acc-0.4448, test loss-2.0113, acc-0.4362\n",
      "Iter-28350, train loss-2.0242, acc-0.3400, valid loss-2.0104, acc-0.4446, test loss-2.0113, acc-0.4362\n",
      "Iter-28360, train loss-1.9631, acc-0.5000, valid loss-2.0103, acc-0.4450, test loss-2.0112, acc-0.4361\n",
      "Iter-28370, train loss-2.0215, acc-0.4400, valid loss-2.0103, acc-0.4450, test loss-2.0111, acc-0.4361\n",
      "Iter-28380, train loss-2.0378, acc-0.3800, valid loss-2.0102, acc-0.4450, test loss-2.0110, acc-0.4363\n",
      "Iter-28390, train loss-2.0669, acc-0.3400, valid loss-2.0101, acc-0.4456, test loss-2.0109, acc-0.4364\n",
      "Iter-28400, train loss-2.0933, acc-0.3400, valid loss-2.0100, acc-0.4454, test loss-2.0108, acc-0.4365\n",
      "Iter-28410, train loss-1.9956, acc-0.5400, valid loss-2.0099, acc-0.4450, test loss-2.0108, acc-0.4364\n",
      "Iter-28420, train loss-2.0375, acc-0.4200, valid loss-2.0099, acc-0.4452, test loss-2.0107, acc-0.4363\n",
      "Iter-28430, train loss-1.9805, acc-0.6200, valid loss-2.0098, acc-0.4450, test loss-2.0106, acc-0.4362\n",
      "Iter-28440, train loss-1.9897, acc-0.4000, valid loss-2.0097, acc-0.4450, test loss-2.0105, acc-0.4363\n",
      "Iter-28450, train loss-1.9979, acc-0.4400, valid loss-2.0096, acc-0.4452, test loss-2.0104, acc-0.4365\n",
      "Iter-28460, train loss-1.9911, acc-0.4800, valid loss-2.0095, acc-0.4454, test loss-2.0104, acc-0.4364\n",
      "Iter-28470, train loss-2.0030, acc-0.4000, valid loss-2.0095, acc-0.4454, test loss-2.0103, acc-0.4365\n",
      "Iter-28480, train loss-2.0279, acc-0.4000, valid loss-2.0094, acc-0.4456, test loss-2.0102, acc-0.4366\n",
      "Iter-28490, train loss-2.0550, acc-0.3600, valid loss-2.0093, acc-0.4456, test loss-2.0101, acc-0.4366\n",
      "Iter-28500, train loss-2.0025, acc-0.5400, valid loss-2.0092, acc-0.4454, test loss-2.0100, acc-0.4364\n",
      "Iter-28510, train loss-2.0355, acc-0.3000, valid loss-2.0091, acc-0.4454, test loss-2.0099, acc-0.4366\n",
      "Iter-28520, train loss-2.0668, acc-0.4000, valid loss-2.0090, acc-0.4454, test loss-2.0099, acc-0.4365\n",
      "Iter-28530, train loss-1.9087, acc-0.5200, valid loss-2.0090, acc-0.4454, test loss-2.0098, acc-0.4366\n",
      "Iter-28540, train loss-1.9987, acc-0.4800, valid loss-2.0089, acc-0.4452, test loss-2.0097, acc-0.4367\n",
      "Iter-28550, train loss-2.0123, acc-0.4400, valid loss-2.0088, acc-0.4452, test loss-2.0096, acc-0.4367\n",
      "Iter-28560, train loss-2.0445, acc-0.4400, valid loss-2.0087, acc-0.4454, test loss-2.0095, acc-0.4368\n",
      "Iter-28570, train loss-1.9338, acc-0.5600, valid loss-2.0086, acc-0.4454, test loss-2.0095, acc-0.4365\n",
      "Iter-28580, train loss-2.0076, acc-0.3600, valid loss-2.0086, acc-0.4458, test loss-2.0094, acc-0.4368\n",
      "Iter-28590, train loss-2.0039, acc-0.4600, valid loss-2.0085, acc-0.4456, test loss-2.0093, acc-0.4365\n",
      "Iter-28600, train loss-1.9912, acc-0.4600, valid loss-2.0084, acc-0.4456, test loss-2.0092, acc-0.4364\n",
      "Iter-28610, train loss-2.0240, acc-0.4400, valid loss-2.0083, acc-0.4460, test loss-2.0091, acc-0.4366\n",
      "Iter-28620, train loss-2.0311, acc-0.5000, valid loss-2.0082, acc-0.4462, test loss-2.0091, acc-0.4365\n",
      "Iter-28630, train loss-2.0392, acc-0.3400, valid loss-2.0082, acc-0.4466, test loss-2.0090, acc-0.4366\n",
      "Iter-28640, train loss-1.9749, acc-0.5200, valid loss-2.0081, acc-0.4464, test loss-2.0089, acc-0.4367\n",
      "Iter-28650, train loss-2.0165, acc-0.3600, valid loss-2.0080, acc-0.4460, test loss-2.0088, acc-0.4368\n",
      "Iter-28660, train loss-2.0492, acc-0.3400, valid loss-2.0079, acc-0.4460, test loss-2.0087, acc-0.4369\n",
      "Iter-28670, train loss-1.9878, acc-0.4200, valid loss-2.0078, acc-0.4460, test loss-2.0087, acc-0.4368\n",
      "Iter-28680, train loss-2.0445, acc-0.4000, valid loss-2.0078, acc-0.4462, test loss-2.0086, acc-0.4370\n",
      "Iter-28690, train loss-2.0401, acc-0.4200, valid loss-2.0077, acc-0.4460, test loss-2.0085, acc-0.4370\n",
      "Iter-28700, train loss-2.0091, acc-0.3400, valid loss-2.0076, acc-0.4460, test loss-2.0084, acc-0.4370\n",
      "Iter-28710, train loss-2.0130, acc-0.4600, valid loss-2.0075, acc-0.4462, test loss-2.0083, acc-0.4370\n",
      "Iter-28720, train loss-2.0538, acc-0.3200, valid loss-2.0074, acc-0.4462, test loss-2.0082, acc-0.4371\n",
      "Iter-28730, train loss-2.0180, acc-0.4800, valid loss-2.0074, acc-0.4464, test loss-2.0082, acc-0.4372\n",
      "Iter-28740, train loss-2.0044, acc-0.4200, valid loss-2.0073, acc-0.4464, test loss-2.0081, acc-0.4371\n",
      "Iter-28750, train loss-2.0036, acc-0.4400, valid loss-2.0072, acc-0.4466, test loss-2.0080, acc-0.4371\n",
      "Iter-28760, train loss-1.9613, acc-0.5200, valid loss-2.0071, acc-0.4466, test loss-2.0079, acc-0.4371\n",
      "Iter-28770, train loss-2.0881, acc-0.3000, valid loss-2.0070, acc-0.4464, test loss-2.0078, acc-0.4372\n",
      "Iter-28780, train loss-2.0682, acc-0.5000, valid loss-2.0070, acc-0.4466, test loss-2.0078, acc-0.4371\n",
      "Iter-28790, train loss-1.9565, acc-0.4400, valid loss-2.0069, acc-0.4466, test loss-2.0077, acc-0.4375\n",
      "Iter-28800, train loss-2.0886, acc-0.3400, valid loss-2.0068, acc-0.4466, test loss-2.0076, acc-0.4375\n",
      "Iter-28810, train loss-2.0481, acc-0.3600, valid loss-2.0067, acc-0.4466, test loss-2.0075, acc-0.4374\n",
      "Iter-28820, train loss-2.0417, acc-0.4000, valid loss-2.0066, acc-0.4468, test loss-2.0074, acc-0.4376\n",
      "Iter-28830, train loss-2.0784, acc-0.3800, valid loss-2.0066, acc-0.4468, test loss-2.0074, acc-0.4375\n",
      "Iter-28840, train loss-2.0395, acc-0.3800, valid loss-2.0065, acc-0.4470, test loss-2.0073, acc-0.4378\n",
      "Iter-28850, train loss-2.0558, acc-0.3400, valid loss-2.0064, acc-0.4468, test loss-2.0072, acc-0.4376\n",
      "Iter-28860, train loss-2.0510, acc-0.3600, valid loss-2.0063, acc-0.4472, test loss-2.0071, acc-0.4376\n",
      "Iter-28870, train loss-2.0092, acc-0.5400, valid loss-2.0062, acc-0.4470, test loss-2.0070, acc-0.4379\n",
      "Iter-28880, train loss-2.0544, acc-0.4800, valid loss-2.0062, acc-0.4474, test loss-2.0070, acc-0.4376\n",
      "Iter-28890, train loss-2.0275, acc-0.4600, valid loss-2.0061, acc-0.4472, test loss-2.0069, acc-0.4375\n",
      "Iter-28900, train loss-2.0348, acc-0.4400, valid loss-2.0060, acc-0.4470, test loss-2.0068, acc-0.4376\n",
      "Iter-28910, train loss-2.0876, acc-0.3200, valid loss-2.0059, acc-0.4470, test loss-2.0067, acc-0.4376\n",
      "Iter-28920, train loss-1.9729, acc-0.5200, valid loss-2.0058, acc-0.4470, test loss-2.0066, acc-0.4377\n",
      "Iter-28930, train loss-2.1257, acc-0.2200, valid loss-2.0058, acc-0.4470, test loss-2.0066, acc-0.4376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-28940, train loss-2.0117, acc-0.5000, valid loss-2.0057, acc-0.4470, test loss-2.0065, acc-0.4377\n",
      "Iter-28950, train loss-2.1181, acc-0.2800, valid loss-2.0056, acc-0.4472, test loss-2.0064, acc-0.4376\n",
      "Iter-28960, train loss-2.0071, acc-0.3800, valid loss-2.0055, acc-0.4472, test loss-2.0063, acc-0.4376\n",
      "Iter-28970, train loss-2.0231, acc-0.4800, valid loss-2.0054, acc-0.4472, test loss-2.0062, acc-0.4376\n",
      "Iter-28980, train loss-1.9952, acc-0.4800, valid loss-2.0054, acc-0.4472, test loss-2.0062, acc-0.4377\n",
      "Iter-28990, train loss-2.0063, acc-0.4600, valid loss-2.0053, acc-0.4472, test loss-2.0061, acc-0.4378\n",
      "Iter-29000, train loss-2.0277, acc-0.3800, valid loss-2.0052, acc-0.4472, test loss-2.0060, acc-0.4376\n",
      "Iter-29010, train loss-1.9856, acc-0.5000, valid loss-2.0051, acc-0.4472, test loss-2.0059, acc-0.4376\n",
      "Iter-29020, train loss-2.0213, acc-0.4200, valid loss-2.0050, acc-0.4474, test loss-2.0058, acc-0.4374\n",
      "Iter-29030, train loss-2.0003, acc-0.4200, valid loss-2.0050, acc-0.4474, test loss-2.0058, acc-0.4371\n",
      "Iter-29040, train loss-2.0066, acc-0.4400, valid loss-2.0049, acc-0.4474, test loss-2.0057, acc-0.4375\n",
      "Iter-29050, train loss-2.0583, acc-0.4600, valid loss-2.0048, acc-0.4476, test loss-2.0056, acc-0.4376\n",
      "Iter-29060, train loss-2.0024, acc-0.4200, valid loss-2.0047, acc-0.4474, test loss-2.0055, acc-0.4375\n",
      "Iter-29070, train loss-2.0144, acc-0.3800, valid loss-2.0046, acc-0.4474, test loss-2.0054, acc-0.4374\n",
      "Iter-29080, train loss-2.0445, acc-0.2800, valid loss-2.0046, acc-0.4474, test loss-2.0054, acc-0.4374\n",
      "Iter-29090, train loss-1.9545, acc-0.5200, valid loss-2.0045, acc-0.4474, test loss-2.0053, acc-0.4374\n",
      "Iter-29100, train loss-2.0871, acc-0.3400, valid loss-2.0044, acc-0.4478, test loss-2.0052, acc-0.4375\n",
      "Iter-29110, train loss-1.9362, acc-0.4600, valid loss-2.0043, acc-0.4476, test loss-2.0051, acc-0.4377\n",
      "Iter-29120, train loss-2.0192, acc-0.3600, valid loss-2.0042, acc-0.4476, test loss-2.0050, acc-0.4376\n",
      "Iter-29130, train loss-2.0474, acc-0.4200, valid loss-2.0041, acc-0.4476, test loss-2.0050, acc-0.4377\n",
      "Iter-29140, train loss-2.0144, acc-0.4600, valid loss-2.0041, acc-0.4476, test loss-2.0049, acc-0.4378\n",
      "Iter-29150, train loss-2.0031, acc-0.4400, valid loss-2.0040, acc-0.4478, test loss-2.0048, acc-0.4379\n",
      "Iter-29160, train loss-1.9618, acc-0.5200, valid loss-2.0039, acc-0.4476, test loss-2.0047, acc-0.4378\n",
      "Iter-29170, train loss-2.0471, acc-0.3600, valid loss-2.0038, acc-0.4478, test loss-2.0046, acc-0.4379\n",
      "Iter-29180, train loss-2.0176, acc-0.5000, valid loss-2.0037, acc-0.4474, test loss-2.0045, acc-0.4382\n",
      "Iter-29190, train loss-1.9707, acc-0.4400, valid loss-2.0037, acc-0.4474, test loss-2.0045, acc-0.4380\n",
      "Iter-29200, train loss-2.0344, acc-0.4000, valid loss-2.0036, acc-0.4472, test loss-2.0044, acc-0.4378\n",
      "Iter-29210, train loss-2.0234, acc-0.4800, valid loss-2.0035, acc-0.4472, test loss-2.0043, acc-0.4381\n",
      "Iter-29220, train loss-2.0561, acc-0.4200, valid loss-2.0034, acc-0.4472, test loss-2.0042, acc-0.4378\n",
      "Iter-29230, train loss-1.9761, acc-0.4400, valid loss-2.0033, acc-0.4474, test loss-2.0041, acc-0.4378\n",
      "Iter-29240, train loss-2.0244, acc-0.4000, valid loss-2.0032, acc-0.4476, test loss-2.0041, acc-0.4377\n",
      "Iter-29250, train loss-1.9911, acc-0.4000, valid loss-2.0032, acc-0.4478, test loss-2.0040, acc-0.4378\n",
      "Iter-29260, train loss-1.9668, acc-0.5600, valid loss-2.0031, acc-0.4480, test loss-2.0039, acc-0.4381\n",
      "Iter-29270, train loss-1.9681, acc-0.5200, valid loss-2.0030, acc-0.4480, test loss-2.0038, acc-0.4380\n",
      "Iter-29280, train loss-2.0342, acc-0.4200, valid loss-2.0029, acc-0.4478, test loss-2.0037, acc-0.4379\n",
      "Iter-29290, train loss-2.0029, acc-0.4400, valid loss-2.0028, acc-0.4480, test loss-2.0036, acc-0.4382\n",
      "Iter-29300, train loss-2.0086, acc-0.3200, valid loss-2.0028, acc-0.4484, test loss-2.0036, acc-0.4385\n",
      "Iter-29310, train loss-2.0075, acc-0.4600, valid loss-2.0027, acc-0.4486, test loss-2.0035, acc-0.4386\n",
      "Iter-29320, train loss-2.0605, acc-0.3800, valid loss-2.0026, acc-0.4490, test loss-2.0034, acc-0.4386\n",
      "Iter-29330, train loss-2.0694, acc-0.4000, valid loss-2.0025, acc-0.4484, test loss-2.0033, acc-0.4388\n",
      "Iter-29340, train loss-2.0314, acc-0.4800, valid loss-2.0024, acc-0.4490, test loss-2.0032, acc-0.4389\n",
      "Iter-29350, train loss-2.0402, acc-0.4400, valid loss-2.0024, acc-0.4486, test loss-2.0032, acc-0.4389\n",
      "Iter-29360, train loss-2.0303, acc-0.3800, valid loss-2.0023, acc-0.4488, test loss-2.0031, acc-0.4389\n",
      "Iter-29370, train loss-2.0017, acc-0.4400, valid loss-2.0022, acc-0.4490, test loss-2.0030, acc-0.4388\n",
      "Iter-29380, train loss-2.0261, acc-0.4400, valid loss-2.0021, acc-0.4492, test loss-2.0029, acc-0.4386\n",
      "Iter-29390, train loss-2.0682, acc-0.3400, valid loss-2.0020, acc-0.4492, test loss-2.0028, acc-0.4385\n",
      "Iter-29400, train loss-1.9666, acc-0.5800, valid loss-2.0020, acc-0.4492, test loss-2.0028, acc-0.4385\n",
      "Iter-29410, train loss-1.9862, acc-0.4600, valid loss-2.0019, acc-0.4490, test loss-2.0027, acc-0.4388\n",
      "Iter-29420, train loss-1.9739, acc-0.5200, valid loss-2.0018, acc-0.4494, test loss-2.0026, acc-0.4391\n",
      "Iter-29430, train loss-2.0454, acc-0.4000, valid loss-2.0017, acc-0.4490, test loss-2.0025, acc-0.4393\n",
      "Iter-29440, train loss-1.9515, acc-0.5400, valid loss-2.0016, acc-0.4494, test loss-2.0024, acc-0.4393\n",
      "Iter-29450, train loss-2.0924, acc-0.3200, valid loss-2.0016, acc-0.4496, test loss-2.0024, acc-0.4395\n",
      "Iter-29460, train loss-1.9825, acc-0.4800, valid loss-2.0015, acc-0.4496, test loss-2.0023, acc-0.4393\n",
      "Iter-29470, train loss-2.0248, acc-0.3800, valid loss-2.0014, acc-0.4496, test loss-2.0022, acc-0.4396\n",
      "Iter-29480, train loss-1.9661, acc-0.5000, valid loss-2.0013, acc-0.4490, test loss-2.0021, acc-0.4395\n",
      "Iter-29490, train loss-1.9519, acc-0.5400, valid loss-2.0012, acc-0.4492, test loss-2.0020, acc-0.4393\n",
      "Iter-29500, train loss-1.9556, acc-0.4800, valid loss-2.0012, acc-0.4494, test loss-2.0020, acc-0.4393\n",
      "Iter-29510, train loss-2.0497, acc-0.3800, valid loss-2.0011, acc-0.4498, test loss-2.0019, acc-0.4392\n",
      "Iter-29520, train loss-2.0229, acc-0.4200, valid loss-2.0010, acc-0.4496, test loss-2.0018, acc-0.4392\n",
      "Iter-29530, train loss-2.0213, acc-0.4200, valid loss-2.0009, acc-0.4498, test loss-2.0017, acc-0.4395\n",
      "Iter-29540, train loss-1.9666, acc-0.4400, valid loss-2.0008, acc-0.4498, test loss-2.0017, acc-0.4394\n",
      "Iter-29550, train loss-2.0303, acc-0.4200, valid loss-2.0007, acc-0.4496, test loss-2.0016, acc-0.4393\n",
      "Iter-29560, train loss-2.0702, acc-0.3400, valid loss-2.0007, acc-0.4500, test loss-2.0015, acc-0.4390\n",
      "Iter-29570, train loss-1.9827, acc-0.5200, valid loss-2.0006, acc-0.4502, test loss-2.0014, acc-0.4389\n",
      "Iter-29580, train loss-2.0052, acc-0.4400, valid loss-2.0005, acc-0.4504, test loss-2.0013, acc-0.4388\n",
      "Iter-29590, train loss-1.9890, acc-0.4600, valid loss-2.0004, acc-0.4500, test loss-2.0012, acc-0.4389\n",
      "Iter-29600, train loss-2.0735, acc-0.3800, valid loss-2.0003, acc-0.4504, test loss-2.0012, acc-0.4386\n",
      "Iter-29610, train loss-2.0332, acc-0.4400, valid loss-2.0002, acc-0.4498, test loss-2.0011, acc-0.4390\n",
      "Iter-29620, train loss-1.9921, acc-0.4600, valid loss-2.0002, acc-0.4502, test loss-2.0010, acc-0.4391\n",
      "Iter-29630, train loss-2.0274, acc-0.4400, valid loss-2.0001, acc-0.4502, test loss-2.0009, acc-0.4390\n",
      "Iter-29640, train loss-1.9881, acc-0.5000, valid loss-2.0000, acc-0.4500, test loss-2.0008, acc-0.4392\n",
      "Iter-29650, train loss-2.0782, acc-0.3000, valid loss-1.9999, acc-0.4502, test loss-2.0008, acc-0.4393\n",
      "Iter-29660, train loss-2.0031, acc-0.4600, valid loss-1.9998, acc-0.4504, test loss-2.0007, acc-0.4392\n",
      "Iter-29670, train loss-1.9057, acc-0.6800, valid loss-1.9998, acc-0.4506, test loss-2.0006, acc-0.4393\n",
      "Iter-29680, train loss-1.9567, acc-0.5400, valid loss-1.9997, acc-0.4506, test loss-2.0005, acc-0.4393\n",
      "Iter-29690, train loss-2.0292, acc-0.4800, valid loss-1.9996, acc-0.4506, test loss-2.0004, acc-0.4393\n",
      "Iter-29700, train loss-1.9431, acc-0.6000, valid loss-1.9995, acc-0.4502, test loss-2.0004, acc-0.4395\n",
      "Iter-29710, train loss-2.0958, acc-0.4000, valid loss-1.9995, acc-0.4506, test loss-2.0003, acc-0.4397\n",
      "Iter-29720, train loss-2.0289, acc-0.4600, valid loss-1.9994, acc-0.4508, test loss-2.0002, acc-0.4397\n",
      "Iter-29730, train loss-1.9858, acc-0.4600, valid loss-1.9993, acc-0.4506, test loss-2.0001, acc-0.4399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-29740, train loss-2.0405, acc-0.4000, valid loss-1.9992, acc-0.4506, test loss-2.0000, acc-0.4400\n",
      "Iter-29750, train loss-1.9966, acc-0.4600, valid loss-1.9991, acc-0.4506, test loss-1.9999, acc-0.4400\n",
      "Iter-29760, train loss-1.9712, acc-0.5400, valid loss-1.9990, acc-0.4506, test loss-1.9999, acc-0.4400\n",
      "Iter-29770, train loss-1.9961, acc-0.5000, valid loss-1.9990, acc-0.4506, test loss-1.9998, acc-0.4398\n",
      "Iter-29780, train loss-2.0175, acc-0.4400, valid loss-1.9989, acc-0.4504, test loss-1.9997, acc-0.4401\n",
      "Iter-29790, train loss-2.0182, acc-0.4200, valid loss-1.9988, acc-0.4510, test loss-1.9996, acc-0.4402\n",
      "Iter-29800, train loss-2.0769, acc-0.3200, valid loss-1.9987, acc-0.4506, test loss-1.9995, acc-0.4401\n",
      "Iter-29810, train loss-2.0621, acc-0.3600, valid loss-1.9986, acc-0.4504, test loss-1.9995, acc-0.4402\n",
      "Iter-29820, train loss-1.9924, acc-0.4000, valid loss-1.9985, acc-0.4504, test loss-1.9994, acc-0.4401\n",
      "Iter-29830, train loss-1.9713, acc-0.4400, valid loss-1.9985, acc-0.4504, test loss-1.9993, acc-0.4402\n",
      "Iter-29840, train loss-1.9729, acc-0.4400, valid loss-1.9984, acc-0.4506, test loss-1.9992, acc-0.4403\n",
      "Iter-29850, train loss-1.9447, acc-0.5000, valid loss-1.9983, acc-0.4508, test loss-1.9991, acc-0.4402\n",
      "Iter-29860, train loss-2.0336, acc-0.3800, valid loss-1.9982, acc-0.4508, test loss-1.9991, acc-0.4402\n",
      "Iter-29870, train loss-2.0173, acc-0.4000, valid loss-1.9982, acc-0.4510, test loss-1.9990, acc-0.4404\n",
      "Iter-29880, train loss-1.9806, acc-0.4800, valid loss-1.9981, acc-0.4508, test loss-1.9989, acc-0.4402\n",
      "Iter-29890, train loss-1.8939, acc-0.5600, valid loss-1.9980, acc-0.4504, test loss-1.9988, acc-0.4402\n",
      "Iter-29900, train loss-2.0433, acc-0.3800, valid loss-1.9979, acc-0.4504, test loss-1.9987, acc-0.4401\n",
      "Iter-29910, train loss-2.0032, acc-0.5000, valid loss-1.9978, acc-0.4504, test loss-1.9987, acc-0.4399\n",
      "Iter-29920, train loss-2.0061, acc-0.4400, valid loss-1.9977, acc-0.4504, test loss-1.9986, acc-0.4397\n",
      "Iter-29930, train loss-2.0445, acc-0.4000, valid loss-1.9977, acc-0.4508, test loss-1.9985, acc-0.4397\n",
      "Iter-29940, train loss-2.0521, acc-0.3200, valid loss-1.9976, acc-0.4508, test loss-1.9984, acc-0.4399\n",
      "Iter-29950, train loss-2.0445, acc-0.4200, valid loss-1.9975, acc-0.4510, test loss-1.9983, acc-0.4396\n",
      "Iter-29960, train loss-1.9867, acc-0.5200, valid loss-1.9974, acc-0.4512, test loss-1.9983, acc-0.4395\n",
      "Iter-29970, train loss-2.0030, acc-0.4600, valid loss-1.9973, acc-0.4512, test loss-1.9982, acc-0.4395\n",
      "Iter-29980, train loss-1.9921, acc-0.3800, valid loss-1.9973, acc-0.4512, test loss-1.9981, acc-0.4399\n",
      "Iter-29990, train loss-2.0468, acc-0.4200, valid loss-1.9972, acc-0.4512, test loss-1.9980, acc-0.4399\n",
      "Iter-30000, train loss-1.9679, acc-0.4800, valid loss-1.9971, acc-0.4512, test loss-1.9979, acc-0.4398\n",
      "Iter-30010, train loss-1.9994, acc-0.4200, valid loss-1.9970, acc-0.4512, test loss-1.9979, acc-0.4399\n",
      "Iter-30020, train loss-2.0568, acc-0.4200, valid loss-1.9969, acc-0.4512, test loss-1.9978, acc-0.4401\n",
      "Iter-30030, train loss-1.9854, acc-0.5000, valid loss-1.9969, acc-0.4514, test loss-1.9977, acc-0.4401\n",
      "Iter-30040, train loss-1.9650, acc-0.5000, valid loss-1.9968, acc-0.4514, test loss-1.9976, acc-0.4400\n",
      "Iter-30050, train loss-2.0803, acc-0.3800, valid loss-1.9967, acc-0.4514, test loss-1.9975, acc-0.4397\n",
      "Iter-30060, train loss-1.9922, acc-0.4600, valid loss-1.9966, acc-0.4514, test loss-1.9975, acc-0.4396\n",
      "Iter-30070, train loss-2.0206, acc-0.5200, valid loss-1.9965, acc-0.4516, test loss-1.9974, acc-0.4396\n",
      "Iter-30080, train loss-1.9989, acc-0.4800, valid loss-1.9965, acc-0.4516, test loss-1.9973, acc-0.4395\n",
      "Iter-30090, train loss-1.9787, acc-0.4800, valid loss-1.9964, acc-0.4512, test loss-1.9972, acc-0.4394\n",
      "Iter-30100, train loss-2.0800, acc-0.2400, valid loss-1.9963, acc-0.4514, test loss-1.9971, acc-0.4395\n",
      "Iter-30110, train loss-1.9782, acc-0.5000, valid loss-1.9962, acc-0.4514, test loss-1.9971, acc-0.4395\n",
      "Iter-30120, train loss-2.0044, acc-0.5200, valid loss-1.9961, acc-0.4514, test loss-1.9970, acc-0.4397\n",
      "Iter-30130, train loss-1.9604, acc-0.5000, valid loss-1.9961, acc-0.4512, test loss-1.9969, acc-0.4397\n",
      "Iter-30140, train loss-1.9592, acc-0.4600, valid loss-1.9960, acc-0.4514, test loss-1.9968, acc-0.4397\n",
      "Iter-30150, train loss-2.0096, acc-0.4000, valid loss-1.9959, acc-0.4514, test loss-1.9967, acc-0.4397\n",
      "Iter-30160, train loss-2.0406, acc-0.4400, valid loss-1.9958, acc-0.4512, test loss-1.9967, acc-0.4396\n",
      "Iter-30170, train loss-2.0171, acc-0.3400, valid loss-1.9957, acc-0.4508, test loss-1.9966, acc-0.4397\n",
      "Iter-30180, train loss-2.0569, acc-0.4000, valid loss-1.9957, acc-0.4512, test loss-1.9965, acc-0.4398\n",
      "Iter-30190, train loss-2.0587, acc-0.4800, valid loss-1.9956, acc-0.4506, test loss-1.9964, acc-0.4394\n",
      "Iter-30200, train loss-1.9936, acc-0.4400, valid loss-1.9955, acc-0.4508, test loss-1.9963, acc-0.4395\n",
      "Iter-30210, train loss-1.9837, acc-0.4600, valid loss-1.9954, acc-0.4510, test loss-1.9963, acc-0.4398\n",
      "Iter-30220, train loss-2.0676, acc-0.3000, valid loss-1.9953, acc-0.4516, test loss-1.9962, acc-0.4400\n",
      "Iter-30230, train loss-2.0152, acc-0.4600, valid loss-1.9953, acc-0.4510, test loss-1.9961, acc-0.4401\n",
      "Iter-30240, train loss-2.0283, acc-0.4000, valid loss-1.9952, acc-0.4512, test loss-1.9960, acc-0.4399\n",
      "Iter-30250, train loss-2.0101, acc-0.4400, valid loss-1.9951, acc-0.4516, test loss-1.9959, acc-0.4402\n",
      "Iter-30260, train loss-1.9376, acc-0.5400, valid loss-1.9950, acc-0.4516, test loss-1.9959, acc-0.4403\n",
      "Iter-30270, train loss-2.0501, acc-0.3600, valid loss-1.9949, acc-0.4516, test loss-1.9958, acc-0.4402\n",
      "Iter-30280, train loss-1.9440, acc-0.5400, valid loss-1.9949, acc-0.4514, test loss-1.9957, acc-0.4403\n",
      "Iter-30290, train loss-2.0069, acc-0.5000, valid loss-1.9948, acc-0.4510, test loss-1.9956, acc-0.4400\n",
      "Iter-30300, train loss-1.9464, acc-0.4200, valid loss-1.9947, acc-0.4510, test loss-1.9955, acc-0.4399\n",
      "Iter-30310, train loss-1.9660, acc-0.4000, valid loss-1.9946, acc-0.4512, test loss-1.9954, acc-0.4400\n",
      "Iter-30320, train loss-2.0700, acc-0.3600, valid loss-1.9945, acc-0.4510, test loss-1.9954, acc-0.4402\n",
      "Iter-30330, train loss-2.0587, acc-0.3400, valid loss-1.9945, acc-0.4514, test loss-1.9953, acc-0.4400\n",
      "Iter-30340, train loss-1.9980, acc-0.5800, valid loss-1.9944, acc-0.4514, test loss-1.9952, acc-0.4400\n",
      "Iter-30350, train loss-2.0555, acc-0.3800, valid loss-1.9943, acc-0.4514, test loss-1.9951, acc-0.4399\n",
      "Iter-30360, train loss-1.9906, acc-0.4200, valid loss-1.9942, acc-0.4512, test loss-1.9950, acc-0.4403\n",
      "Iter-30370, train loss-1.9270, acc-0.5800, valid loss-1.9941, acc-0.4512, test loss-1.9950, acc-0.4403\n",
      "Iter-30380, train loss-2.0230, acc-0.4200, valid loss-1.9940, acc-0.4512, test loss-1.9949, acc-0.4403\n",
      "Iter-30390, train loss-1.9767, acc-0.4600, valid loss-1.9940, acc-0.4512, test loss-1.9948, acc-0.4403\n",
      "Iter-30400, train loss-2.0251, acc-0.5000, valid loss-1.9939, acc-0.4514, test loss-1.9947, acc-0.4404\n",
      "Iter-30410, train loss-2.0792, acc-0.3200, valid loss-1.9938, acc-0.4512, test loss-1.9947, acc-0.4404\n",
      "Iter-30420, train loss-2.0127, acc-0.4800, valid loss-1.9937, acc-0.4512, test loss-1.9946, acc-0.4403\n",
      "Iter-30430, train loss-1.9513, acc-0.5200, valid loss-1.9937, acc-0.4512, test loss-1.9945, acc-0.4403\n",
      "Iter-30440, train loss-2.0570, acc-0.3600, valid loss-1.9936, acc-0.4512, test loss-1.9944, acc-0.4405\n",
      "Iter-30450, train loss-2.0061, acc-0.4600, valid loss-1.9935, acc-0.4516, test loss-1.9943, acc-0.4406\n",
      "Iter-30460, train loss-2.0072, acc-0.4800, valid loss-1.9934, acc-0.4512, test loss-1.9943, acc-0.4404\n",
      "Iter-30470, train loss-2.0045, acc-0.4800, valid loss-1.9933, acc-0.4514, test loss-1.9942, acc-0.4405\n",
      "Iter-30480, train loss-1.9988, acc-0.5800, valid loss-1.9933, acc-0.4514, test loss-1.9941, acc-0.4407\n",
      "Iter-30490, train loss-1.9800, acc-0.5000, valid loss-1.9932, acc-0.4518, test loss-1.9940, acc-0.4407\n",
      "Iter-30500, train loss-1.9496, acc-0.5000, valid loss-1.9931, acc-0.4518, test loss-1.9939, acc-0.4405\n",
      "Iter-30510, train loss-2.0476, acc-0.3400, valid loss-1.9930, acc-0.4516, test loss-1.9939, acc-0.4404\n",
      "Iter-30520, train loss-2.0105, acc-0.5400, valid loss-1.9929, acc-0.4518, test loss-1.9938, acc-0.4404\n",
      "Iter-30530, train loss-2.0106, acc-0.4200, valid loss-1.9929, acc-0.4516, test loss-1.9937, acc-0.4406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-30540, train loss-2.0017, acc-0.4400, valid loss-1.9928, acc-0.4516, test loss-1.9936, acc-0.4405\n",
      "Iter-30550, train loss-2.0598, acc-0.3600, valid loss-1.9927, acc-0.4518, test loss-1.9936, acc-0.4405\n",
      "Iter-30560, train loss-1.9205, acc-0.4600, valid loss-1.9926, acc-0.4524, test loss-1.9935, acc-0.4405\n",
      "Iter-30570, train loss-2.0068, acc-0.3800, valid loss-1.9926, acc-0.4528, test loss-1.9934, acc-0.4407\n",
      "Iter-30580, train loss-1.9896, acc-0.4400, valid loss-1.9925, acc-0.4528, test loss-1.9933, acc-0.4406\n",
      "Iter-30590, train loss-2.0142, acc-0.4800, valid loss-1.9924, acc-0.4528, test loss-1.9932, acc-0.4408\n",
      "Iter-30600, train loss-2.0716, acc-0.3800, valid loss-1.9923, acc-0.4526, test loss-1.9932, acc-0.4406\n",
      "Iter-30610, train loss-2.0232, acc-0.4800, valid loss-1.9922, acc-0.4526, test loss-1.9931, acc-0.4405\n",
      "Iter-30620, train loss-1.9852, acc-0.4200, valid loss-1.9922, acc-0.4524, test loss-1.9930, acc-0.4409\n",
      "Iter-30630, train loss-2.0826, acc-0.3000, valid loss-1.9921, acc-0.4524, test loss-1.9929, acc-0.4411\n",
      "Iter-30640, train loss-2.0395, acc-0.4000, valid loss-1.9920, acc-0.4524, test loss-1.9928, acc-0.4409\n",
      "Iter-30650, train loss-1.9716, acc-0.4600, valid loss-1.9919, acc-0.4524, test loss-1.9928, acc-0.4409\n",
      "Iter-30660, train loss-2.0214, acc-0.3600, valid loss-1.9919, acc-0.4526, test loss-1.9927, acc-0.4410\n",
      "Iter-30670, train loss-2.0111, acc-0.3600, valid loss-1.9918, acc-0.4526, test loss-1.9926, acc-0.4411\n",
      "Iter-30680, train loss-1.9897, acc-0.4600, valid loss-1.9917, acc-0.4524, test loss-1.9925, acc-0.4411\n",
      "Iter-30690, train loss-2.0322, acc-0.4000, valid loss-1.9916, acc-0.4522, test loss-1.9924, acc-0.4410\n",
      "Iter-30700, train loss-1.9027, acc-0.5600, valid loss-1.9915, acc-0.4522, test loss-1.9924, acc-0.4412\n",
      "Iter-30710, train loss-1.9907, acc-0.4200, valid loss-1.9915, acc-0.4522, test loss-1.9923, acc-0.4415\n",
      "Iter-30720, train loss-2.0140, acc-0.4200, valid loss-1.9914, acc-0.4522, test loss-1.9922, acc-0.4413\n",
      "Iter-30730, train loss-1.9993, acc-0.3800, valid loss-1.9913, acc-0.4524, test loss-1.9921, acc-0.4413\n",
      "Iter-30740, train loss-2.0825, acc-0.3400, valid loss-1.9912, acc-0.4522, test loss-1.9920, acc-0.4413\n",
      "Iter-30750, train loss-2.0217, acc-0.3400, valid loss-1.9911, acc-0.4522, test loss-1.9919, acc-0.4411\n",
      "Iter-30760, train loss-1.9417, acc-0.5000, valid loss-1.9910, acc-0.4522, test loss-1.9919, acc-0.4410\n",
      "Iter-30770, train loss-2.0073, acc-0.4400, valid loss-1.9910, acc-0.4520, test loss-1.9918, acc-0.4413\n",
      "Iter-30780, train loss-1.9786, acc-0.5200, valid loss-1.9909, acc-0.4522, test loss-1.9917, acc-0.4415\n",
      "Iter-30790, train loss-2.0634, acc-0.3600, valid loss-1.9908, acc-0.4522, test loss-1.9916, acc-0.4412\n",
      "Iter-30800, train loss-2.0910, acc-0.3200, valid loss-1.9907, acc-0.4520, test loss-1.9916, acc-0.4415\n",
      "Iter-30810, train loss-1.9889, acc-0.5400, valid loss-1.9906, acc-0.4522, test loss-1.9915, acc-0.4415\n",
      "Iter-30820, train loss-1.9771, acc-0.4600, valid loss-1.9906, acc-0.4522, test loss-1.9914, acc-0.4415\n",
      "Iter-30830, train loss-1.9979, acc-0.4400, valid loss-1.9905, acc-0.4520, test loss-1.9913, acc-0.4416\n",
      "Iter-30840, train loss-1.9460, acc-0.5000, valid loss-1.9904, acc-0.4524, test loss-1.9912, acc-0.4418\n",
      "Iter-30850, train loss-1.9820, acc-0.4400, valid loss-1.9903, acc-0.4524, test loss-1.9912, acc-0.4416\n",
      "Iter-30860, train loss-1.9707, acc-0.5000, valid loss-1.9902, acc-0.4526, test loss-1.9911, acc-0.4416\n",
      "Iter-30870, train loss-2.0285, acc-0.4000, valid loss-1.9902, acc-0.4526, test loss-1.9910, acc-0.4416\n",
      "Iter-30880, train loss-2.0095, acc-0.3600, valid loss-1.9901, acc-0.4526, test loss-1.9909, acc-0.4416\n",
      "Iter-30890, train loss-1.9784, acc-0.4600, valid loss-1.9900, acc-0.4528, test loss-1.9908, acc-0.4418\n",
      "Iter-30900, train loss-2.0322, acc-0.4400, valid loss-1.9899, acc-0.4528, test loss-1.9908, acc-0.4419\n",
      "Iter-30910, train loss-1.9710, acc-0.4600, valid loss-1.9898, acc-0.4528, test loss-1.9907, acc-0.4419\n",
      "Iter-30920, train loss-2.0261, acc-0.5200, valid loss-1.9898, acc-0.4528, test loss-1.9906, acc-0.4420\n",
      "Iter-30930, train loss-2.0125, acc-0.4200, valid loss-1.9897, acc-0.4530, test loss-1.9905, acc-0.4420\n",
      "Iter-30940, train loss-1.9661, acc-0.5400, valid loss-1.9896, acc-0.4528, test loss-1.9905, acc-0.4419\n",
      "Iter-30950, train loss-1.9309, acc-0.4800, valid loss-1.9895, acc-0.4530, test loss-1.9904, acc-0.4421\n",
      "Iter-30960, train loss-1.9564, acc-0.4600, valid loss-1.9894, acc-0.4530, test loss-1.9903, acc-0.4422\n",
      "Iter-30970, train loss-2.0145, acc-0.4400, valid loss-1.9894, acc-0.4530, test loss-1.9902, acc-0.4421\n",
      "Iter-30980, train loss-1.9897, acc-0.4000, valid loss-1.9893, acc-0.4532, test loss-1.9901, acc-0.4421\n",
      "Iter-30990, train loss-2.0190, acc-0.4200, valid loss-1.9892, acc-0.4530, test loss-1.9901, acc-0.4420\n",
      "Iter-31000, train loss-2.0333, acc-0.4600, valid loss-1.9891, acc-0.4528, test loss-1.9900, acc-0.4422\n",
      "Iter-31010, train loss-1.9524, acc-0.5000, valid loss-1.9891, acc-0.4528, test loss-1.9899, acc-0.4423\n",
      "Iter-31020, train loss-1.9892, acc-0.4400, valid loss-1.9890, acc-0.4530, test loss-1.9898, acc-0.4423\n",
      "Iter-31030, train loss-1.9863, acc-0.4600, valid loss-1.9889, acc-0.4532, test loss-1.9897, acc-0.4423\n",
      "Iter-31040, train loss-1.9205, acc-0.5600, valid loss-1.9888, acc-0.4532, test loss-1.9897, acc-0.4422\n",
      "Iter-31050, train loss-1.9812, acc-0.4000, valid loss-1.9887, acc-0.4534, test loss-1.9896, acc-0.4424\n",
      "Iter-31060, train loss-2.0156, acc-0.4200, valid loss-1.9886, acc-0.4536, test loss-1.9895, acc-0.4425\n",
      "Iter-31070, train loss-1.9472, acc-0.4200, valid loss-1.9886, acc-0.4538, test loss-1.9894, acc-0.4424\n",
      "Iter-31080, train loss-1.9521, acc-0.5000, valid loss-1.9885, acc-0.4538, test loss-1.9893, acc-0.4425\n",
      "Iter-31090, train loss-1.9760, acc-0.5200, valid loss-1.9884, acc-0.4536, test loss-1.9893, acc-0.4425\n",
      "Iter-31100, train loss-2.0248, acc-0.4200, valid loss-1.9883, acc-0.4536, test loss-1.9892, acc-0.4424\n",
      "Iter-31110, train loss-1.9863, acc-0.4400, valid loss-1.9883, acc-0.4538, test loss-1.9891, acc-0.4425\n",
      "Iter-31120, train loss-1.9083, acc-0.6000, valid loss-1.9882, acc-0.4536, test loss-1.9890, acc-0.4423\n",
      "Iter-31130, train loss-2.0330, acc-0.4200, valid loss-1.9881, acc-0.4532, test loss-1.9890, acc-0.4425\n",
      "Iter-31140, train loss-1.9780, acc-0.5000, valid loss-1.9880, acc-0.4530, test loss-1.9889, acc-0.4422\n",
      "Iter-31150, train loss-2.0057, acc-0.4600, valid loss-1.9879, acc-0.4536, test loss-1.9888, acc-0.4426\n",
      "Iter-31160, train loss-1.9949, acc-0.4600, valid loss-1.9879, acc-0.4536, test loss-1.9887, acc-0.4427\n",
      "Iter-31170, train loss-2.0484, acc-0.4400, valid loss-1.9878, acc-0.4530, test loss-1.9886, acc-0.4424\n",
      "Iter-31180, train loss-1.9645, acc-0.5200, valid loss-1.9877, acc-0.4530, test loss-1.9886, acc-0.4427\n",
      "Iter-31190, train loss-2.0055, acc-0.4200, valid loss-1.9876, acc-0.4528, test loss-1.9885, acc-0.4428\n",
      "Iter-31200, train loss-1.9396, acc-0.5000, valid loss-1.9875, acc-0.4528, test loss-1.9884, acc-0.4427\n",
      "Iter-31210, train loss-2.0805, acc-0.2600, valid loss-1.9875, acc-0.4534, test loss-1.9883, acc-0.4427\n",
      "Iter-31220, train loss-1.9834, acc-0.5000, valid loss-1.9874, acc-0.4534, test loss-1.9882, acc-0.4426\n",
      "Iter-31230, train loss-1.9971, acc-0.5000, valid loss-1.9873, acc-0.4534, test loss-1.9882, acc-0.4424\n",
      "Iter-31240, train loss-2.0061, acc-0.4200, valid loss-1.9872, acc-0.4534, test loss-1.9881, acc-0.4424\n",
      "Iter-31250, train loss-2.0067, acc-0.4800, valid loss-1.9872, acc-0.4536, test loss-1.9880, acc-0.4426\n",
      "Iter-31260, train loss-2.0610, acc-0.2800, valid loss-1.9871, acc-0.4534, test loss-1.9879, acc-0.4426\n",
      "Iter-31270, train loss-2.0127, acc-0.4200, valid loss-1.9870, acc-0.4534, test loss-1.9879, acc-0.4430\n",
      "Iter-31280, train loss-2.0337, acc-0.3400, valid loss-1.9869, acc-0.4540, test loss-1.9878, acc-0.4430\n",
      "Iter-31290, train loss-2.0029, acc-0.3600, valid loss-1.9868, acc-0.4538, test loss-1.9877, acc-0.4431\n",
      "Iter-31300, train loss-1.9519, acc-0.5200, valid loss-1.9868, acc-0.4540, test loss-1.9876, acc-0.4430\n",
      "Iter-31310, train loss-1.9690, acc-0.5200, valid loss-1.9867, acc-0.4538, test loss-1.9875, acc-0.4429\n",
      "Iter-31320, train loss-2.0691, acc-0.3600, valid loss-1.9866, acc-0.4540, test loss-1.9875, acc-0.4432\n",
      "Iter-31330, train loss-2.0170, acc-0.4000, valid loss-1.9865, acc-0.4540, test loss-1.9874, acc-0.4432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-31340, train loss-2.0213, acc-0.3400, valid loss-1.9864, acc-0.4540, test loss-1.9873, acc-0.4434\n",
      "Iter-31350, train loss-2.0000, acc-0.4200, valid loss-1.9864, acc-0.4540, test loss-1.9872, acc-0.4431\n",
      "Iter-31360, train loss-2.0441, acc-0.4800, valid loss-1.9863, acc-0.4542, test loss-1.9872, acc-0.4434\n",
      "Iter-31370, train loss-1.9665, acc-0.5600, valid loss-1.9862, acc-0.4542, test loss-1.9871, acc-0.4433\n",
      "Iter-31380, train loss-1.9987, acc-0.3400, valid loss-1.9861, acc-0.4544, test loss-1.9870, acc-0.4433\n",
      "Iter-31390, train loss-2.0167, acc-0.3200, valid loss-1.9861, acc-0.4544, test loss-1.9869, acc-0.4434\n",
      "Iter-31400, train loss-1.9928, acc-0.4200, valid loss-1.9860, acc-0.4548, test loss-1.9868, acc-0.4435\n",
      "Iter-31410, train loss-1.9883, acc-0.4200, valid loss-1.9859, acc-0.4548, test loss-1.9868, acc-0.4436\n",
      "Iter-31420, train loss-2.0357, acc-0.4400, valid loss-1.9858, acc-0.4544, test loss-1.9867, acc-0.4436\n",
      "Iter-31430, train loss-2.0256, acc-0.3800, valid loss-1.9857, acc-0.4544, test loss-1.9866, acc-0.4436\n",
      "Iter-31440, train loss-1.9548, acc-0.5400, valid loss-1.9857, acc-0.4542, test loss-1.9865, acc-0.4434\n",
      "Iter-31450, train loss-1.9805, acc-0.4400, valid loss-1.9856, acc-0.4544, test loss-1.9865, acc-0.4434\n",
      "Iter-31460, train loss-1.9226, acc-0.5000, valid loss-1.9855, acc-0.4544, test loss-1.9864, acc-0.4433\n",
      "Iter-31470, train loss-1.9764, acc-0.4600, valid loss-1.9854, acc-0.4542, test loss-1.9863, acc-0.4435\n",
      "Iter-31480, train loss-2.0025, acc-0.4600, valid loss-1.9853, acc-0.4546, test loss-1.9862, acc-0.4435\n",
      "Iter-31490, train loss-1.9334, acc-0.5800, valid loss-1.9853, acc-0.4542, test loss-1.9861, acc-0.4439\n",
      "Iter-31500, train loss-2.0005, acc-0.4000, valid loss-1.9852, acc-0.4542, test loss-1.9861, acc-0.4434\n",
      "Iter-31510, train loss-1.9835, acc-0.4600, valid loss-1.9851, acc-0.4538, test loss-1.9860, acc-0.4436\n",
      "Iter-31520, train loss-1.9650, acc-0.4800, valid loss-1.9850, acc-0.4538, test loss-1.9859, acc-0.4439\n",
      "Iter-31530, train loss-2.0520, acc-0.3200, valid loss-1.9849, acc-0.4536, test loss-1.9858, acc-0.4438\n",
      "Iter-31540, train loss-2.0072, acc-0.4400, valid loss-1.9849, acc-0.4536, test loss-1.9857, acc-0.4438\n",
      "Iter-31550, train loss-1.9571, acc-0.4200, valid loss-1.9848, acc-0.4536, test loss-1.9857, acc-0.4437\n",
      "Iter-31560, train loss-2.0199, acc-0.4600, valid loss-1.9847, acc-0.4534, test loss-1.9856, acc-0.4435\n",
      "Iter-31570, train loss-2.0123, acc-0.4200, valid loss-1.9846, acc-0.4538, test loss-1.9855, acc-0.4438\n",
      "Iter-31580, train loss-2.0557, acc-0.3000, valid loss-1.9846, acc-0.4538, test loss-1.9854, acc-0.4439\n",
      "Iter-31590, train loss-2.0107, acc-0.4200, valid loss-1.9845, acc-0.4540, test loss-1.9854, acc-0.4441\n",
      "Iter-31600, train loss-1.8820, acc-0.6200, valid loss-1.9844, acc-0.4538, test loss-1.9853, acc-0.4440\n",
      "Iter-31610, train loss-1.9471, acc-0.4600, valid loss-1.9843, acc-0.4540, test loss-1.9852, acc-0.4445\n",
      "Iter-31620, train loss-1.9400, acc-0.4400, valid loss-1.9842, acc-0.4538, test loss-1.9851, acc-0.4441\n",
      "Iter-31630, train loss-2.0295, acc-0.4800, valid loss-1.9842, acc-0.4538, test loss-1.9850, acc-0.4442\n",
      "Iter-31640, train loss-2.0523, acc-0.3800, valid loss-1.9841, acc-0.4540, test loss-1.9850, acc-0.4442\n",
      "Iter-31650, train loss-2.0454, acc-0.4800, valid loss-1.9840, acc-0.4540, test loss-1.9849, acc-0.4443\n",
      "Iter-31660, train loss-1.9328, acc-0.4600, valid loss-1.9839, acc-0.4542, test loss-1.9848, acc-0.4446\n",
      "Iter-31670, train loss-1.9460, acc-0.4000, valid loss-1.9839, acc-0.4542, test loss-1.9847, acc-0.4446\n",
      "Iter-31680, train loss-2.0031, acc-0.4400, valid loss-1.9838, acc-0.4542, test loss-1.9847, acc-0.4446\n",
      "Iter-31690, train loss-1.9552, acc-0.4600, valid loss-1.9837, acc-0.4546, test loss-1.9846, acc-0.4446\n",
      "Iter-31700, train loss-2.0075, acc-0.4800, valid loss-1.9836, acc-0.4544, test loss-1.9845, acc-0.4447\n",
      "Iter-31710, train loss-1.9476, acc-0.4800, valid loss-1.9835, acc-0.4546, test loss-1.9844, acc-0.4448\n",
      "Iter-31720, train loss-1.9943, acc-0.4200, valid loss-1.9835, acc-0.4544, test loss-1.9843, acc-0.4450\n",
      "Iter-31730, train loss-1.8922, acc-0.5200, valid loss-1.9834, acc-0.4542, test loss-1.9843, acc-0.4445\n",
      "Iter-31740, train loss-1.9687, acc-0.4600, valid loss-1.9833, acc-0.4542, test loss-1.9842, acc-0.4448\n",
      "Iter-31750, train loss-2.0176, acc-0.4400, valid loss-1.9832, acc-0.4544, test loss-1.9841, acc-0.4448\n",
      "Iter-31760, train loss-2.0192, acc-0.4000, valid loss-1.9831, acc-0.4538, test loss-1.9840, acc-0.4448\n",
      "Iter-31770, train loss-2.0285, acc-0.4000, valid loss-1.9831, acc-0.4540, test loss-1.9840, acc-0.4447\n",
      "Iter-31780, train loss-1.9264, acc-0.5000, valid loss-1.9830, acc-0.4540, test loss-1.9839, acc-0.4446\n",
      "Iter-31790, train loss-1.9924, acc-0.4400, valid loss-1.9829, acc-0.4540, test loss-1.9838, acc-0.4447\n",
      "Iter-31800, train loss-2.0693, acc-0.3000, valid loss-1.9828, acc-0.4542, test loss-1.9837, acc-0.4447\n",
      "Iter-31810, train loss-1.9967, acc-0.4400, valid loss-1.9828, acc-0.4542, test loss-1.9836, acc-0.4446\n",
      "Iter-31820, train loss-1.9838, acc-0.4000, valid loss-1.9827, acc-0.4540, test loss-1.9836, acc-0.4447\n",
      "Iter-31830, train loss-1.9420, acc-0.4400, valid loss-1.9826, acc-0.4542, test loss-1.9835, acc-0.4445\n",
      "Iter-31840, train loss-1.9974, acc-0.5200, valid loss-1.9825, acc-0.4542, test loss-1.9834, acc-0.4447\n",
      "Iter-31850, train loss-1.9510, acc-0.5200, valid loss-1.9824, acc-0.4540, test loss-1.9833, acc-0.4448\n",
      "Iter-31860, train loss-1.8978, acc-0.5600, valid loss-1.9824, acc-0.4540, test loss-1.9833, acc-0.4448\n",
      "Iter-31870, train loss-2.0182, acc-0.4200, valid loss-1.9823, acc-0.4542, test loss-1.9832, acc-0.4447\n",
      "Iter-31880, train loss-1.9178, acc-0.4000, valid loss-1.9822, acc-0.4542, test loss-1.9831, acc-0.4447\n",
      "Iter-31890, train loss-1.9907, acc-0.5400, valid loss-1.9821, acc-0.4538, test loss-1.9830, acc-0.4449\n",
      "Iter-31900, train loss-1.8957, acc-0.5800, valid loss-1.9820, acc-0.4540, test loss-1.9829, acc-0.4448\n",
      "Iter-31910, train loss-1.9970, acc-0.4200, valid loss-1.9820, acc-0.4544, test loss-1.9829, acc-0.4448\n",
      "Iter-31920, train loss-2.0006, acc-0.4400, valid loss-1.9819, acc-0.4544, test loss-1.9828, acc-0.4450\n",
      "Iter-31930, train loss-2.0646, acc-0.3200, valid loss-1.9818, acc-0.4544, test loss-1.9827, acc-0.4450\n",
      "Iter-31940, train loss-2.0140, acc-0.3600, valid loss-1.9817, acc-0.4544, test loss-1.9826, acc-0.4449\n",
      "Iter-31950, train loss-1.9660, acc-0.4200, valid loss-1.9817, acc-0.4542, test loss-1.9826, acc-0.4448\n",
      "Iter-31960, train loss-1.9419, acc-0.5600, valid loss-1.9816, acc-0.4542, test loss-1.9825, acc-0.4448\n",
      "Iter-31970, train loss-1.9790, acc-0.5000, valid loss-1.9815, acc-0.4542, test loss-1.9824, acc-0.4448\n",
      "Iter-31980, train loss-2.0351, acc-0.3800, valid loss-1.9814, acc-0.4540, test loss-1.9823, acc-0.4448\n",
      "Iter-31990, train loss-2.0003, acc-0.4200, valid loss-1.9813, acc-0.4538, test loss-1.9822, acc-0.4446\n",
      "Iter-32000, train loss-1.9287, acc-0.5400, valid loss-1.9813, acc-0.4538, test loss-1.9822, acc-0.4448\n",
      "Iter-32010, train loss-1.9383, acc-0.5200, valid loss-1.9812, acc-0.4538, test loss-1.9821, acc-0.4448\n",
      "Iter-32020, train loss-1.9590, acc-0.4400, valid loss-1.9811, acc-0.4538, test loss-1.9820, acc-0.4448\n",
      "Iter-32030, train loss-2.0215, acc-0.4000, valid loss-1.9810, acc-0.4540, test loss-1.9819, acc-0.4445\n",
      "Iter-32040, train loss-1.9943, acc-0.5200, valid loss-1.9809, acc-0.4538, test loss-1.9819, acc-0.4445\n",
      "Iter-32050, train loss-2.0681, acc-0.3400, valid loss-1.9809, acc-0.4538, test loss-1.9818, acc-0.4448\n",
      "Iter-32060, train loss-1.9616, acc-0.4800, valid loss-1.9808, acc-0.4540, test loss-1.9817, acc-0.4446\n",
      "Iter-32070, train loss-1.9679, acc-0.4600, valid loss-1.9807, acc-0.4540, test loss-1.9816, acc-0.4446\n",
      "Iter-32080, train loss-2.0153, acc-0.3800, valid loss-1.9806, acc-0.4540, test loss-1.9815, acc-0.4446\n",
      "Iter-32090, train loss-1.9508, acc-0.5200, valid loss-1.9806, acc-0.4542, test loss-1.9815, acc-0.4446\n",
      "Iter-32100, train loss-2.0407, acc-0.3600, valid loss-1.9805, acc-0.4540, test loss-1.9814, acc-0.4446\n",
      "Iter-32110, train loss-2.0221, acc-0.3600, valid loss-1.9804, acc-0.4542, test loss-1.9813, acc-0.4448\n",
      "Iter-32120, train loss-1.9284, acc-0.5400, valid loss-1.9803, acc-0.4536, test loss-1.9812, acc-0.4444\n",
      "Iter-32130, train loss-1.9212, acc-0.5400, valid loss-1.9803, acc-0.4536, test loss-1.9812, acc-0.4445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-32140, train loss-1.9730, acc-0.4800, valid loss-1.9802, acc-0.4536, test loss-1.9811, acc-0.4446\n",
      "Iter-32150, train loss-1.9794, acc-0.5000, valid loss-1.9801, acc-0.4536, test loss-1.9810, acc-0.4446\n",
      "Iter-32160, train loss-1.9894, acc-0.4200, valid loss-1.9800, acc-0.4538, test loss-1.9809, acc-0.4448\n",
      "Iter-32170, train loss-1.9442, acc-0.5600, valid loss-1.9799, acc-0.4536, test loss-1.9808, acc-0.4449\n",
      "Iter-32180, train loss-1.9931, acc-0.4200, valid loss-1.9799, acc-0.4536, test loss-1.9808, acc-0.4448\n",
      "Iter-32190, train loss-2.0211, acc-0.3600, valid loss-1.9798, acc-0.4538, test loss-1.9807, acc-0.4452\n",
      "Iter-32200, train loss-1.9360, acc-0.5400, valid loss-1.9797, acc-0.4542, test loss-1.9806, acc-0.4450\n",
      "Iter-32210, train loss-1.9515, acc-0.4200, valid loss-1.9796, acc-0.4540, test loss-1.9805, acc-0.4450\n",
      "Iter-32220, train loss-1.9897, acc-0.3800, valid loss-1.9795, acc-0.4544, test loss-1.9804, acc-0.4452\n",
      "Iter-32230, train loss-1.9899, acc-0.5000, valid loss-1.9795, acc-0.4544, test loss-1.9804, acc-0.4452\n",
      "Iter-32240, train loss-2.0490, acc-0.3600, valid loss-1.9794, acc-0.4544, test loss-1.9803, acc-0.4452\n",
      "Iter-32250, train loss-1.9375, acc-0.5000, valid loss-1.9793, acc-0.4544, test loss-1.9802, acc-0.4453\n",
      "Iter-32260, train loss-1.9764, acc-0.4200, valid loss-1.9792, acc-0.4542, test loss-1.9801, acc-0.4453\n",
      "Iter-32270, train loss-2.0477, acc-0.3800, valid loss-1.9792, acc-0.4546, test loss-1.9801, acc-0.4455\n",
      "Iter-32280, train loss-2.0211, acc-0.3800, valid loss-1.9791, acc-0.4548, test loss-1.9800, acc-0.4456\n",
      "Iter-32290, train loss-1.9893, acc-0.4400, valid loss-1.9790, acc-0.4548, test loss-1.9799, acc-0.4454\n",
      "Iter-32300, train loss-2.0443, acc-0.3600, valid loss-1.9789, acc-0.4554, test loss-1.9798, acc-0.4455\n",
      "Iter-32310, train loss-2.0177, acc-0.4400, valid loss-1.9789, acc-0.4556, test loss-1.9798, acc-0.4454\n",
      "Iter-32320, train loss-1.9907, acc-0.4400, valid loss-1.9788, acc-0.4554, test loss-1.9797, acc-0.4457\n",
      "Iter-32330, train loss-1.9678, acc-0.5000, valid loss-1.9787, acc-0.4554, test loss-1.9796, acc-0.4455\n",
      "Iter-32340, train loss-1.9741, acc-0.3400, valid loss-1.9786, acc-0.4552, test loss-1.9795, acc-0.4456\n",
      "Iter-32350, train loss-2.0160, acc-0.4000, valid loss-1.9785, acc-0.4554, test loss-1.9794, acc-0.4456\n",
      "Iter-32360, train loss-1.9762, acc-0.4200, valid loss-1.9785, acc-0.4552, test loss-1.9794, acc-0.4454\n",
      "Iter-32370, train loss-1.9488, acc-0.4400, valid loss-1.9784, acc-0.4554, test loss-1.9793, acc-0.4456\n",
      "Iter-32380, train loss-1.9871, acc-0.4600, valid loss-1.9783, acc-0.4548, test loss-1.9792, acc-0.4457\n",
      "Iter-32390, train loss-2.0271, acc-0.3400, valid loss-1.9782, acc-0.4550, test loss-1.9791, acc-0.4455\n",
      "Iter-32400, train loss-1.9129, acc-0.4800, valid loss-1.9782, acc-0.4550, test loss-1.9791, acc-0.4455\n",
      "Iter-32410, train loss-1.9713, acc-0.5000, valid loss-1.9781, acc-0.4554, test loss-1.9790, acc-0.4457\n",
      "Iter-32420, train loss-1.9775, acc-0.4600, valid loss-1.9780, acc-0.4556, test loss-1.9789, acc-0.4456\n",
      "Iter-32430, train loss-1.9856, acc-0.4600, valid loss-1.9779, acc-0.4556, test loss-1.9788, acc-0.4458\n",
      "Iter-32440, train loss-2.0017, acc-0.4400, valid loss-1.9778, acc-0.4556, test loss-1.9787, acc-0.4461\n",
      "Iter-32450, train loss-1.9077, acc-0.5200, valid loss-1.9778, acc-0.4558, test loss-1.9787, acc-0.4461\n",
      "Iter-32460, train loss-2.0262, acc-0.4600, valid loss-1.9777, acc-0.4562, test loss-1.9786, acc-0.4462\n",
      "Iter-32470, train loss-2.0351, acc-0.3400, valid loss-1.9776, acc-0.4564, test loss-1.9785, acc-0.4461\n",
      "Iter-32480, train loss-1.9805, acc-0.5200, valid loss-1.9775, acc-0.4564, test loss-1.9784, acc-0.4460\n",
      "Iter-32490, train loss-1.9983, acc-0.4600, valid loss-1.9775, acc-0.4560, test loss-1.9784, acc-0.4462\n",
      "Iter-32500, train loss-2.1083, acc-0.3200, valid loss-1.9774, acc-0.4558, test loss-1.9783, acc-0.4462\n",
      "Iter-32510, train loss-2.0389, acc-0.4600, valid loss-1.9773, acc-0.4562, test loss-1.9782, acc-0.4460\n",
      "Iter-32520, train loss-2.0268, acc-0.2400, valid loss-1.9772, acc-0.4562, test loss-1.9781, acc-0.4460\n",
      "Iter-32530, train loss-1.9210, acc-0.4600, valid loss-1.9772, acc-0.4562, test loss-1.9781, acc-0.4459\n",
      "Iter-32540, train loss-2.0703, acc-0.3600, valid loss-1.9771, acc-0.4564, test loss-1.9780, acc-0.4460\n",
      "Iter-32550, train loss-1.9614, acc-0.4200, valid loss-1.9770, acc-0.4562, test loss-1.9779, acc-0.4461\n",
      "Iter-32560, train loss-1.9661, acc-0.4600, valid loss-1.9769, acc-0.4560, test loss-1.9778, acc-0.4464\n",
      "Iter-32570, train loss-2.0110, acc-0.4000, valid loss-1.9768, acc-0.4562, test loss-1.9777, acc-0.4461\n",
      "Iter-32580, train loss-1.9839, acc-0.4200, valid loss-1.9768, acc-0.4562, test loss-1.9777, acc-0.4462\n",
      "Iter-32590, train loss-2.0147, acc-0.4400, valid loss-1.9767, acc-0.4566, test loss-1.9776, acc-0.4462\n",
      "Iter-32600, train loss-1.9709, acc-0.5200, valid loss-1.9766, acc-0.4566, test loss-1.9775, acc-0.4462\n",
      "Iter-32610, train loss-1.9639, acc-0.5200, valid loss-1.9765, acc-0.4566, test loss-1.9774, acc-0.4463\n",
      "Iter-32620, train loss-2.0517, acc-0.3200, valid loss-1.9765, acc-0.4566, test loss-1.9774, acc-0.4464\n",
      "Iter-32630, train loss-2.0353, acc-0.3800, valid loss-1.9764, acc-0.4566, test loss-1.9773, acc-0.4464\n",
      "Iter-32640, train loss-2.0601, acc-0.3200, valid loss-1.9763, acc-0.4566, test loss-1.9772, acc-0.4462\n",
      "Iter-32650, train loss-1.9727, acc-0.4200, valid loss-1.9762, acc-0.4564, test loss-1.9771, acc-0.4463\n",
      "Iter-32660, train loss-1.9618, acc-0.4400, valid loss-1.9762, acc-0.4570, test loss-1.9771, acc-0.4463\n",
      "Iter-32670, train loss-2.0466, acc-0.3000, valid loss-1.9761, acc-0.4572, test loss-1.9770, acc-0.4464\n",
      "Iter-32680, train loss-1.9930, acc-0.4000, valid loss-1.9760, acc-0.4570, test loss-1.9769, acc-0.4465\n",
      "Iter-32690, train loss-2.0156, acc-0.3400, valid loss-1.9759, acc-0.4570, test loss-1.9768, acc-0.4464\n",
      "Iter-32700, train loss-2.0004, acc-0.3600, valid loss-1.9758, acc-0.4572, test loss-1.9767, acc-0.4465\n",
      "Iter-32710, train loss-2.0764, acc-0.3200, valid loss-1.9758, acc-0.4572, test loss-1.9767, acc-0.4464\n",
      "Iter-32720, train loss-2.0448, acc-0.3600, valid loss-1.9757, acc-0.4572, test loss-1.9766, acc-0.4465\n",
      "Iter-32730, train loss-1.9536, acc-0.5800, valid loss-1.9756, acc-0.4570, test loss-1.9765, acc-0.4464\n",
      "Iter-32740, train loss-1.9629, acc-0.3600, valid loss-1.9755, acc-0.4570, test loss-1.9765, acc-0.4466\n",
      "Iter-32750, train loss-1.9599, acc-0.5000, valid loss-1.9755, acc-0.4570, test loss-1.9764, acc-0.4466\n",
      "Iter-32760, train loss-1.9600, acc-0.5400, valid loss-1.9754, acc-0.4570, test loss-1.9763, acc-0.4466\n",
      "Iter-32770, train loss-2.0049, acc-0.3400, valid loss-1.9753, acc-0.4574, test loss-1.9762, acc-0.4470\n",
      "Iter-32780, train loss-1.8499, acc-0.6200, valid loss-1.9752, acc-0.4572, test loss-1.9762, acc-0.4467\n",
      "Iter-32790, train loss-2.0080, acc-0.4000, valid loss-1.9752, acc-0.4574, test loss-1.9761, acc-0.4466\n",
      "Iter-32800, train loss-2.0810, acc-0.3200, valid loss-1.9751, acc-0.4574, test loss-1.9760, acc-0.4467\n",
      "Iter-32810, train loss-1.9721, acc-0.4800, valid loss-1.9750, acc-0.4574, test loss-1.9759, acc-0.4467\n",
      "Iter-32820, train loss-2.0369, acc-0.4000, valid loss-1.9749, acc-0.4574, test loss-1.9758, acc-0.4467\n",
      "Iter-32830, train loss-2.0029, acc-0.3200, valid loss-1.9749, acc-0.4574, test loss-1.9758, acc-0.4467\n",
      "Iter-32840, train loss-1.9208, acc-0.5000, valid loss-1.9748, acc-0.4568, test loss-1.9757, acc-0.4468\n",
      "Iter-32850, train loss-1.9950, acc-0.4000, valid loss-1.9747, acc-0.4568, test loss-1.9756, acc-0.4467\n",
      "Iter-32860, train loss-2.0007, acc-0.4400, valid loss-1.9746, acc-0.4568, test loss-1.9755, acc-0.4467\n",
      "Iter-32870, train loss-2.0143, acc-0.4800, valid loss-1.9746, acc-0.4568, test loss-1.9755, acc-0.4468\n",
      "Iter-32880, train loss-2.0205, acc-0.4200, valid loss-1.9745, acc-0.4574, test loss-1.9754, acc-0.4470\n",
      "Iter-32890, train loss-1.9239, acc-0.5600, valid loss-1.9744, acc-0.4574, test loss-1.9753, acc-0.4470\n",
      "Iter-32900, train loss-1.8826, acc-0.5600, valid loss-1.9743, acc-0.4574, test loss-1.9753, acc-0.4471\n",
      "Iter-32910, train loss-2.0218, acc-0.4000, valid loss-1.9743, acc-0.4574, test loss-1.9752, acc-0.4469\n",
      "Iter-32920, train loss-1.9866, acc-0.4800, valid loss-1.9742, acc-0.4574, test loss-1.9751, acc-0.4469\n",
      "Iter-32930, train loss-2.0120, acc-0.4400, valid loss-1.9741, acc-0.4574, test loss-1.9750, acc-0.4469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-32940, train loss-2.0230, acc-0.4400, valid loss-1.9740, acc-0.4572, test loss-1.9749, acc-0.4468\n",
      "Iter-32950, train loss-1.9740, acc-0.4600, valid loss-1.9739, acc-0.4572, test loss-1.9749, acc-0.4469\n",
      "Iter-32960, train loss-2.0191, acc-0.4600, valid loss-1.9739, acc-0.4572, test loss-1.9748, acc-0.4468\n",
      "Iter-32970, train loss-1.8528, acc-0.6200, valid loss-1.9738, acc-0.4574, test loss-1.9747, acc-0.4468\n",
      "Iter-32980, train loss-2.0536, acc-0.3400, valid loss-1.9737, acc-0.4574, test loss-1.9746, acc-0.4469\n",
      "Iter-32990, train loss-2.0183, acc-0.3400, valid loss-1.9736, acc-0.4578, test loss-1.9746, acc-0.4468\n",
      "Iter-33000, train loss-2.0199, acc-0.3800, valid loss-1.9736, acc-0.4576, test loss-1.9745, acc-0.4468\n",
      "Iter-33010, train loss-1.9729, acc-0.5200, valid loss-1.9735, acc-0.4580, test loss-1.9744, acc-0.4468\n",
      "Iter-33020, train loss-1.9442, acc-0.5200, valid loss-1.9734, acc-0.4578, test loss-1.9743, acc-0.4470\n",
      "Iter-33030, train loss-1.9530, acc-0.5200, valid loss-1.9733, acc-0.4578, test loss-1.9743, acc-0.4470\n",
      "Iter-33040, train loss-1.9938, acc-0.4600, valid loss-1.9733, acc-0.4578, test loss-1.9742, acc-0.4470\n",
      "Iter-33050, train loss-1.9890, acc-0.4200, valid loss-1.9732, acc-0.4578, test loss-1.9741, acc-0.4469\n",
      "Iter-33060, train loss-2.0536, acc-0.3200, valid loss-1.9731, acc-0.4574, test loss-1.9740, acc-0.4470\n",
      "Iter-33070, train loss-2.0217, acc-0.3800, valid loss-1.9730, acc-0.4574, test loss-1.9740, acc-0.4469\n",
      "Iter-33080, train loss-1.9329, acc-0.4800, valid loss-1.9730, acc-0.4578, test loss-1.9739, acc-0.4469\n",
      "Iter-33090, train loss-1.9909, acc-0.4200, valid loss-1.9729, acc-0.4574, test loss-1.9738, acc-0.4469\n",
      "Iter-33100, train loss-1.9810, acc-0.4000, valid loss-1.9728, acc-0.4578, test loss-1.9737, acc-0.4471\n",
      "Iter-33110, train loss-1.9141, acc-0.5200, valid loss-1.9727, acc-0.4574, test loss-1.9736, acc-0.4470\n",
      "Iter-33120, train loss-2.0018, acc-0.3800, valid loss-1.9727, acc-0.4576, test loss-1.9736, acc-0.4471\n",
      "Iter-33130, train loss-1.9353, acc-0.4800, valid loss-1.9726, acc-0.4578, test loss-1.9735, acc-0.4472\n",
      "Iter-33140, train loss-2.0101, acc-0.3200, valid loss-1.9725, acc-0.4576, test loss-1.9734, acc-0.4470\n",
      "Iter-33150, train loss-1.9510, acc-0.4800, valid loss-1.9724, acc-0.4574, test loss-1.9733, acc-0.4471\n",
      "Iter-33160, train loss-2.0359, acc-0.4400, valid loss-1.9724, acc-0.4574, test loss-1.9733, acc-0.4472\n",
      "Iter-33170, train loss-2.0115, acc-0.4400, valid loss-1.9723, acc-0.4572, test loss-1.9732, acc-0.4472\n",
      "Iter-33180, train loss-1.9599, acc-0.5000, valid loss-1.9722, acc-0.4576, test loss-1.9731, acc-0.4473\n",
      "Iter-33190, train loss-1.9932, acc-0.4000, valid loss-1.9721, acc-0.4578, test loss-1.9730, acc-0.4474\n",
      "Iter-33200, train loss-2.0094, acc-0.4200, valid loss-1.9720, acc-0.4574, test loss-1.9730, acc-0.4472\n",
      "Iter-33210, train loss-1.9647, acc-0.5000, valid loss-1.9720, acc-0.4574, test loss-1.9729, acc-0.4471\n",
      "Iter-33220, train loss-2.0360, acc-0.4400, valid loss-1.9719, acc-0.4576, test loss-1.9728, acc-0.4472\n",
      "Iter-33230, train loss-2.0868, acc-0.3200, valid loss-1.9718, acc-0.4576, test loss-1.9727, acc-0.4471\n",
      "Iter-33240, train loss-2.0029, acc-0.4200, valid loss-1.9717, acc-0.4578, test loss-1.9727, acc-0.4472\n",
      "Iter-33250, train loss-2.0953, acc-0.2600, valid loss-1.9717, acc-0.4576, test loss-1.9726, acc-0.4473\n",
      "Iter-33260, train loss-2.0090, acc-0.4000, valid loss-1.9716, acc-0.4576, test loss-1.9725, acc-0.4473\n",
      "Iter-33270, train loss-1.9568, acc-0.4800, valid loss-1.9715, acc-0.4576, test loss-1.9724, acc-0.4473\n",
      "Iter-33280, train loss-2.0655, acc-0.3400, valid loss-1.9714, acc-0.4576, test loss-1.9724, acc-0.4471\n",
      "Iter-33290, train loss-2.0112, acc-0.3400, valid loss-1.9714, acc-0.4576, test loss-1.9723, acc-0.4472\n",
      "Iter-33300, train loss-1.9884, acc-0.4400, valid loss-1.9713, acc-0.4574, test loss-1.9722, acc-0.4473\n",
      "Iter-33310, train loss-1.9462, acc-0.5200, valid loss-1.9712, acc-0.4578, test loss-1.9721, acc-0.4474\n",
      "Iter-33320, train loss-1.8967, acc-0.4800, valid loss-1.9711, acc-0.4572, test loss-1.9720, acc-0.4473\n",
      "Iter-33330, train loss-2.0447, acc-0.3200, valid loss-1.9711, acc-0.4574, test loss-1.9720, acc-0.4472\n",
      "Iter-33340, train loss-2.0204, acc-0.3600, valid loss-1.9710, acc-0.4576, test loss-1.9719, acc-0.4472\n",
      "Iter-33350, train loss-1.9886, acc-0.4800, valid loss-1.9709, acc-0.4576, test loss-1.9718, acc-0.4470\n",
      "Iter-33360, train loss-2.0045, acc-0.4600, valid loss-1.9708, acc-0.4576, test loss-1.9717, acc-0.4471\n",
      "Iter-33370, train loss-1.9555, acc-0.5200, valid loss-1.9708, acc-0.4576, test loss-1.9717, acc-0.4471\n",
      "Iter-33380, train loss-1.9191, acc-0.5800, valid loss-1.9707, acc-0.4580, test loss-1.9716, acc-0.4472\n",
      "Iter-33390, train loss-1.9940, acc-0.4400, valid loss-1.9706, acc-0.4580, test loss-1.9715, acc-0.4472\n",
      "Iter-33400, train loss-2.0064, acc-0.3600, valid loss-1.9705, acc-0.4578, test loss-1.9714, acc-0.4472\n",
      "Iter-33410, train loss-1.9553, acc-0.5200, valid loss-1.9704, acc-0.4576, test loss-1.9714, acc-0.4471\n",
      "Iter-33420, train loss-1.9952, acc-0.3800, valid loss-1.9704, acc-0.4578, test loss-1.9713, acc-0.4473\n",
      "Iter-33430, train loss-2.0291, acc-0.4000, valid loss-1.9703, acc-0.4576, test loss-1.9712, acc-0.4473\n",
      "Iter-33440, train loss-1.9250, acc-0.4000, valid loss-1.9702, acc-0.4578, test loss-1.9711, acc-0.4472\n",
      "Iter-33450, train loss-1.9408, acc-0.4800, valid loss-1.9701, acc-0.4576, test loss-1.9711, acc-0.4474\n",
      "Iter-33460, train loss-2.0568, acc-0.4000, valid loss-1.9701, acc-0.4578, test loss-1.9710, acc-0.4473\n",
      "Iter-33470, train loss-1.9488, acc-0.5200, valid loss-1.9700, acc-0.4578, test loss-1.9709, acc-0.4473\n",
      "Iter-33480, train loss-1.9514, acc-0.4600, valid loss-1.9699, acc-0.4576, test loss-1.9708, acc-0.4471\n",
      "Iter-33490, train loss-2.0509, acc-0.2200, valid loss-1.9699, acc-0.4576, test loss-1.9708, acc-0.4471\n",
      "Iter-33500, train loss-2.0415, acc-0.3800, valid loss-1.9698, acc-0.4576, test loss-1.9707, acc-0.4473\n",
      "Iter-33510, train loss-1.9986, acc-0.4400, valid loss-1.9697, acc-0.4578, test loss-1.9706, acc-0.4473\n",
      "Iter-33520, train loss-2.0361, acc-0.2800, valid loss-1.9696, acc-0.4576, test loss-1.9706, acc-0.4474\n",
      "Iter-33530, train loss-1.9500, acc-0.5400, valid loss-1.9696, acc-0.4574, test loss-1.9705, acc-0.4474\n",
      "Iter-33540, train loss-1.9441, acc-0.4600, valid loss-1.9695, acc-0.4574, test loss-1.9704, acc-0.4475\n",
      "Iter-33550, train loss-2.0039, acc-0.4800, valid loss-1.9694, acc-0.4574, test loss-1.9703, acc-0.4474\n",
      "Iter-33560, train loss-1.9845, acc-0.4400, valid loss-1.9693, acc-0.4578, test loss-1.9703, acc-0.4475\n",
      "Iter-33570, train loss-1.9647, acc-0.5800, valid loss-1.9693, acc-0.4578, test loss-1.9702, acc-0.4475\n",
      "Iter-33580, train loss-1.9901, acc-0.4600, valid loss-1.9692, acc-0.4576, test loss-1.9701, acc-0.4474\n",
      "Iter-33590, train loss-1.9663, acc-0.4600, valid loss-1.9691, acc-0.4580, test loss-1.9700, acc-0.4474\n",
      "Iter-33600, train loss-1.9712, acc-0.4200, valid loss-1.9690, acc-0.4578, test loss-1.9700, acc-0.4476\n",
      "Iter-33610, train loss-1.9552, acc-0.4400, valid loss-1.9690, acc-0.4576, test loss-1.9699, acc-0.4476\n",
      "Iter-33620, train loss-1.9954, acc-0.4600, valid loss-1.9689, acc-0.4576, test loss-1.9698, acc-0.4475\n",
      "Iter-33630, train loss-1.9997, acc-0.5200, valid loss-1.9688, acc-0.4580, test loss-1.9697, acc-0.4477\n",
      "Iter-33640, train loss-2.0578, acc-0.4400, valid loss-1.9688, acc-0.4578, test loss-1.9697, acc-0.4477\n",
      "Iter-33650, train loss-1.9401, acc-0.5200, valid loss-1.9687, acc-0.4578, test loss-1.9696, acc-0.4477\n",
      "Iter-33660, train loss-1.9829, acc-0.4000, valid loss-1.9686, acc-0.4578, test loss-1.9695, acc-0.4476\n",
      "Iter-33670, train loss-1.9177, acc-0.4200, valid loss-1.9685, acc-0.4582, test loss-1.9694, acc-0.4479\n",
      "Iter-33680, train loss-2.0360, acc-0.3800, valid loss-1.9684, acc-0.4582, test loss-1.9694, acc-0.4479\n",
      "Iter-33690, train loss-2.0770, acc-0.3600, valid loss-1.9684, acc-0.4582, test loss-1.9693, acc-0.4478\n",
      "Iter-33700, train loss-1.9656, acc-0.5000, valid loss-1.9683, acc-0.4576, test loss-1.9692, acc-0.4478\n",
      "Iter-33710, train loss-1.9993, acc-0.5000, valid loss-1.9682, acc-0.4580, test loss-1.9691, acc-0.4479\n",
      "Iter-33720, train loss-1.9510, acc-0.5200, valid loss-1.9681, acc-0.4576, test loss-1.9691, acc-0.4479\n",
      "Iter-33730, train loss-2.0062, acc-0.4000, valid loss-1.9681, acc-0.4580, test loss-1.9690, acc-0.4479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-33740, train loss-1.9784, acc-0.4400, valid loss-1.9680, acc-0.4582, test loss-1.9689, acc-0.4479\n",
      "Iter-33750, train loss-1.9311, acc-0.5800, valid loss-1.9679, acc-0.4582, test loss-1.9688, acc-0.4478\n",
      "Iter-33760, train loss-1.9203, acc-0.5000, valid loss-1.9678, acc-0.4580, test loss-1.9688, acc-0.4479\n",
      "Iter-33770, train loss-1.9975, acc-0.4200, valid loss-1.9678, acc-0.4578, test loss-1.9687, acc-0.4478\n",
      "Iter-33780, train loss-1.9629, acc-0.4400, valid loss-1.9677, acc-0.4582, test loss-1.9686, acc-0.4479\n",
      "Iter-33790, train loss-1.9150, acc-0.5200, valid loss-1.9676, acc-0.4580, test loss-1.9685, acc-0.4478\n",
      "Iter-33800, train loss-1.9438, acc-0.4800, valid loss-1.9675, acc-0.4580, test loss-1.9685, acc-0.4476\n",
      "Iter-33810, train loss-1.9620, acc-0.5200, valid loss-1.9675, acc-0.4582, test loss-1.9684, acc-0.4479\n",
      "Iter-33820, train loss-1.9803, acc-0.4800, valid loss-1.9674, acc-0.4580, test loss-1.9683, acc-0.4479\n",
      "Iter-33830, train loss-2.0883, acc-0.3600, valid loss-1.9673, acc-0.4578, test loss-1.9682, acc-0.4479\n",
      "Iter-33840, train loss-2.0506, acc-0.3800, valid loss-1.9672, acc-0.4580, test loss-1.9682, acc-0.4478\n",
      "Iter-33850, train loss-1.9907, acc-0.4600, valid loss-1.9672, acc-0.4582, test loss-1.9681, acc-0.4478\n",
      "Iter-33860, train loss-1.9143, acc-0.5200, valid loss-1.9671, acc-0.4580, test loss-1.9680, acc-0.4478\n",
      "Iter-33870, train loss-1.9730, acc-0.4600, valid loss-1.9670, acc-0.4578, test loss-1.9679, acc-0.4478\n",
      "Iter-33880, train loss-1.8472, acc-0.4800, valid loss-1.9669, acc-0.4580, test loss-1.9679, acc-0.4478\n",
      "Iter-33890, train loss-1.9175, acc-0.5200, valid loss-1.9669, acc-0.4580, test loss-1.9678, acc-0.4478\n",
      "Iter-33900, train loss-2.0295, acc-0.4000, valid loss-1.9668, acc-0.4578, test loss-1.9677, acc-0.4480\n",
      "Iter-33910, train loss-1.9736, acc-0.4800, valid loss-1.9667, acc-0.4580, test loss-1.9676, acc-0.4476\n",
      "Iter-33920, train loss-1.9605, acc-0.3600, valid loss-1.9666, acc-0.4584, test loss-1.9675, acc-0.4475\n",
      "Iter-33930, train loss-2.0282, acc-0.3400, valid loss-1.9666, acc-0.4582, test loss-1.9675, acc-0.4475\n",
      "Iter-33940, train loss-2.0471, acc-0.4000, valid loss-1.9665, acc-0.4586, test loss-1.9674, acc-0.4476\n",
      "Iter-33950, train loss-1.9731, acc-0.4600, valid loss-1.9664, acc-0.4584, test loss-1.9673, acc-0.4477\n",
      "Iter-33960, train loss-1.9893, acc-0.4400, valid loss-1.9663, acc-0.4586, test loss-1.9672, acc-0.4476\n",
      "Iter-33970, train loss-1.8961, acc-0.5400, valid loss-1.9663, acc-0.4586, test loss-1.9672, acc-0.4480\n",
      "Iter-33980, train loss-1.9472, acc-0.5000, valid loss-1.9662, acc-0.4588, test loss-1.9671, acc-0.4482\n",
      "Iter-33990, train loss-2.0302, acc-0.4200, valid loss-1.9661, acc-0.4588, test loss-1.9670, acc-0.4481\n",
      "Iter-34000, train loss-1.9752, acc-0.4600, valid loss-1.9660, acc-0.4588, test loss-1.9669, acc-0.4481\n",
      "Iter-34010, train loss-2.0291, acc-0.4200, valid loss-1.9660, acc-0.4586, test loss-1.9669, acc-0.4481\n",
      "Iter-34020, train loss-1.9136, acc-0.4600, valid loss-1.9659, acc-0.4586, test loss-1.9668, acc-0.4480\n",
      "Iter-34030, train loss-1.9142, acc-0.5000, valid loss-1.9658, acc-0.4586, test loss-1.9667, acc-0.4480\n",
      "Iter-34040, train loss-2.0868, acc-0.3400, valid loss-1.9657, acc-0.4586, test loss-1.9666, acc-0.4480\n",
      "Iter-34050, train loss-1.9517, acc-0.4600, valid loss-1.9657, acc-0.4586, test loss-1.9666, acc-0.4482\n",
      "Iter-34060, train loss-2.0027, acc-0.4600, valid loss-1.9656, acc-0.4586, test loss-1.9665, acc-0.4481\n",
      "Iter-34070, train loss-1.9512, acc-0.5200, valid loss-1.9655, acc-0.4586, test loss-1.9664, acc-0.4482\n",
      "Iter-34080, train loss-2.0173, acc-0.4000, valid loss-1.9654, acc-0.4588, test loss-1.9663, acc-0.4482\n",
      "Iter-34090, train loss-1.9671, acc-0.4600, valid loss-1.9654, acc-0.4588, test loss-1.9663, acc-0.4481\n",
      "Iter-34100, train loss-1.9922, acc-0.4200, valid loss-1.9653, acc-0.4588, test loss-1.9662, acc-0.4480\n",
      "Iter-34110, train loss-2.0346, acc-0.3200, valid loss-1.9652, acc-0.4588, test loss-1.9661, acc-0.4479\n",
      "Iter-34120, train loss-1.9894, acc-0.3400, valid loss-1.9651, acc-0.4586, test loss-1.9660, acc-0.4479\n",
      "Iter-34130, train loss-1.9408, acc-0.5400, valid loss-1.9651, acc-0.4586, test loss-1.9660, acc-0.4479\n",
      "Iter-34140, train loss-1.9623, acc-0.5000, valid loss-1.9650, acc-0.4586, test loss-1.9659, acc-0.4482\n",
      "Iter-34150, train loss-1.9440, acc-0.4400, valid loss-1.9649, acc-0.4590, test loss-1.9658, acc-0.4483\n",
      "Iter-34160, train loss-1.9566, acc-0.5000, valid loss-1.9648, acc-0.4590, test loss-1.9657, acc-0.4484\n",
      "Iter-34170, train loss-1.8988, acc-0.6000, valid loss-1.9648, acc-0.4588, test loss-1.9657, acc-0.4485\n",
      "Iter-34180, train loss-1.9754, acc-0.4800, valid loss-1.9647, acc-0.4588, test loss-1.9656, acc-0.4486\n",
      "Iter-34190, train loss-1.9950, acc-0.4000, valid loss-1.9646, acc-0.4592, test loss-1.9655, acc-0.4486\n",
      "Iter-34200, train loss-1.9291, acc-0.5600, valid loss-1.9646, acc-0.4592, test loss-1.9654, acc-0.4485\n",
      "Iter-34210, train loss-1.9437, acc-0.4400, valid loss-1.9645, acc-0.4590, test loss-1.9654, acc-0.4485\n",
      "Iter-34220, train loss-2.0093, acc-0.4200, valid loss-1.9644, acc-0.4590, test loss-1.9653, acc-0.4483\n",
      "Iter-34230, train loss-1.9587, acc-0.4800, valid loss-1.9643, acc-0.4592, test loss-1.9652, acc-0.4485\n",
      "Iter-34240, train loss-2.0161, acc-0.4000, valid loss-1.9643, acc-0.4592, test loss-1.9651, acc-0.4485\n",
      "Iter-34250, train loss-1.9899, acc-0.5000, valid loss-1.9642, acc-0.4594, test loss-1.9651, acc-0.4486\n",
      "Iter-34260, train loss-1.9760, acc-0.4600, valid loss-1.9641, acc-0.4592, test loss-1.9650, acc-0.4487\n",
      "Iter-34270, train loss-2.0215, acc-0.4000, valid loss-1.9640, acc-0.4592, test loss-1.9649, acc-0.4487\n",
      "Iter-34280, train loss-1.9733, acc-0.4200, valid loss-1.9640, acc-0.4594, test loss-1.9649, acc-0.4487\n",
      "Iter-34290, train loss-1.9749, acc-0.3400, valid loss-1.9639, acc-0.4592, test loss-1.9648, acc-0.4487\n",
      "Iter-34300, train loss-1.9732, acc-0.5200, valid loss-1.9638, acc-0.4596, test loss-1.9647, acc-0.4488\n",
      "Iter-34310, train loss-1.9126, acc-0.5000, valid loss-1.9637, acc-0.4594, test loss-1.9646, acc-0.4487\n",
      "Iter-34320, train loss-1.9816, acc-0.4400, valid loss-1.9637, acc-0.4594, test loss-1.9645, acc-0.4487\n",
      "Iter-34330, train loss-1.9463, acc-0.4200, valid loss-1.9636, acc-0.4596, test loss-1.9645, acc-0.4487\n",
      "Iter-34340, train loss-1.9801, acc-0.5400, valid loss-1.9635, acc-0.4592, test loss-1.9644, acc-0.4486\n",
      "Iter-34350, train loss-1.9857, acc-0.5200, valid loss-1.9634, acc-0.4592, test loss-1.9643, acc-0.4486\n",
      "Iter-34360, train loss-1.9862, acc-0.4000, valid loss-1.9634, acc-0.4590, test loss-1.9642, acc-0.4486\n",
      "Iter-34370, train loss-1.9509, acc-0.5000, valid loss-1.9633, acc-0.4592, test loss-1.9642, acc-0.4488\n",
      "Iter-34380, train loss-1.9521, acc-0.4600, valid loss-1.9632, acc-0.4592, test loss-1.9641, acc-0.4486\n",
      "Iter-34390, train loss-1.8886, acc-0.5800, valid loss-1.9631, acc-0.4596, test loss-1.9640, acc-0.4484\n",
      "Iter-34400, train loss-2.0227, acc-0.3800, valid loss-1.9631, acc-0.4594, test loss-1.9639, acc-0.4485\n",
      "Iter-34410, train loss-1.9736, acc-0.4600, valid loss-1.9630, acc-0.4594, test loss-1.9639, acc-0.4484\n",
      "Iter-34420, train loss-1.9192, acc-0.4800, valid loss-1.9629, acc-0.4596, test loss-1.9638, acc-0.4481\n",
      "Iter-34430, train loss-1.9442, acc-0.5200, valid loss-1.9628, acc-0.4594, test loss-1.9637, acc-0.4482\n",
      "Iter-34440, train loss-1.9759, acc-0.4800, valid loss-1.9628, acc-0.4594, test loss-1.9636, acc-0.4485\n",
      "Iter-34450, train loss-2.0074, acc-0.5400, valid loss-1.9627, acc-0.4594, test loss-1.9636, acc-0.4487\n",
      "Iter-34460, train loss-1.9708, acc-0.3600, valid loss-1.9626, acc-0.4592, test loss-1.9635, acc-0.4482\n",
      "Iter-34470, train loss-1.9952, acc-0.5000, valid loss-1.9625, acc-0.4592, test loss-1.9634, acc-0.4483\n",
      "Iter-34480, train loss-1.9621, acc-0.4400, valid loss-1.9625, acc-0.4594, test loss-1.9633, acc-0.4483\n",
      "Iter-34490, train loss-1.9885, acc-0.4400, valid loss-1.9624, acc-0.4596, test loss-1.9633, acc-0.4481\n",
      "Iter-34500, train loss-1.9945, acc-0.3800, valid loss-1.9623, acc-0.4600, test loss-1.9632, acc-0.4484\n",
      "Iter-34510, train loss-1.9681, acc-0.4400, valid loss-1.9623, acc-0.4598, test loss-1.9631, acc-0.4484\n",
      "Iter-34520, train loss-1.8795, acc-0.6000, valid loss-1.9622, acc-0.4598, test loss-1.9631, acc-0.4485\n",
      "Iter-34530, train loss-1.9842, acc-0.4200, valid loss-1.9621, acc-0.4600, test loss-1.9630, acc-0.4483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34540, train loss-1.8955, acc-0.6000, valid loss-1.9620, acc-0.4602, test loss-1.9629, acc-0.4485\n",
      "Iter-34550, train loss-1.9164, acc-0.4400, valid loss-1.9619, acc-0.4604, test loss-1.9628, acc-0.4486\n",
      "Iter-34560, train loss-1.9938, acc-0.4600, valid loss-1.9619, acc-0.4604, test loss-1.9628, acc-0.4487\n",
      "Iter-34570, train loss-1.9456, acc-0.5400, valid loss-1.9618, acc-0.4604, test loss-1.9627, acc-0.4487\n",
      "Iter-34580, train loss-1.9394, acc-0.5200, valid loss-1.9617, acc-0.4606, test loss-1.9626, acc-0.4487\n",
      "Iter-34590, train loss-1.9217, acc-0.5600, valid loss-1.9617, acc-0.4602, test loss-1.9625, acc-0.4486\n",
      "Iter-34600, train loss-2.0340, acc-0.3400, valid loss-1.9616, acc-0.4600, test loss-1.9625, acc-0.4485\n",
      "Iter-34610, train loss-1.9741, acc-0.4400, valid loss-1.9615, acc-0.4602, test loss-1.9624, acc-0.4487\n",
      "Iter-34620, train loss-1.9895, acc-0.5000, valid loss-1.9614, acc-0.4602, test loss-1.9623, acc-0.4485\n",
      "Iter-34630, train loss-1.9924, acc-0.4400, valid loss-1.9614, acc-0.4606, test loss-1.9623, acc-0.4488\n",
      "Iter-34640, train loss-1.9823, acc-0.4200, valid loss-1.9613, acc-0.4602, test loss-1.9622, acc-0.4489\n",
      "Iter-34650, train loss-1.9247, acc-0.5200, valid loss-1.9612, acc-0.4604, test loss-1.9621, acc-0.4491\n",
      "Iter-34660, train loss-1.9996, acc-0.4200, valid loss-1.9611, acc-0.4608, test loss-1.9620, acc-0.4490\n",
      "Iter-34670, train loss-1.9388, acc-0.3600, valid loss-1.9611, acc-0.4604, test loss-1.9620, acc-0.4491\n",
      "Iter-34680, train loss-2.0296, acc-0.4000, valid loss-1.9610, acc-0.4604, test loss-1.9619, acc-0.4492\n",
      "Iter-34690, train loss-1.9457, acc-0.5200, valid loss-1.9609, acc-0.4602, test loss-1.9618, acc-0.4491\n",
      "Iter-34700, train loss-1.9768, acc-0.4400, valid loss-1.9608, acc-0.4604, test loss-1.9617, acc-0.4493\n",
      "Iter-34710, train loss-2.0004, acc-0.4200, valid loss-1.9608, acc-0.4604, test loss-1.9617, acc-0.4493\n",
      "Iter-34720, train loss-1.9627, acc-0.4000, valid loss-1.9607, acc-0.4604, test loss-1.9616, acc-0.4494\n",
      "Iter-34730, train loss-1.9948, acc-0.3600, valid loss-1.9606, acc-0.4602, test loss-1.9615, acc-0.4494\n",
      "Iter-34740, train loss-2.0197, acc-0.4200, valid loss-1.9605, acc-0.4608, test loss-1.9614, acc-0.4494\n",
      "Iter-34750, train loss-1.8965, acc-0.4800, valid loss-1.9605, acc-0.4602, test loss-1.9614, acc-0.4495\n",
      "Iter-34760, train loss-1.9936, acc-0.4600, valid loss-1.9604, acc-0.4600, test loss-1.9613, acc-0.4493\n",
      "Iter-34770, train loss-2.0154, acc-0.3400, valid loss-1.9603, acc-0.4600, test loss-1.9612, acc-0.4493\n",
      "Iter-34780, train loss-2.0226, acc-0.4000, valid loss-1.9602, acc-0.4604, test loss-1.9612, acc-0.4493\n",
      "Iter-34790, train loss-1.9377, acc-0.6000, valid loss-1.9602, acc-0.4602, test loss-1.9611, acc-0.4493\n",
      "Iter-34800, train loss-1.8884, acc-0.5600, valid loss-1.9601, acc-0.4602, test loss-1.9610, acc-0.4492\n",
      "Iter-34810, train loss-2.0084, acc-0.3800, valid loss-1.9600, acc-0.4600, test loss-1.9609, acc-0.4492\n",
      "Iter-34820, train loss-1.9368, acc-0.5200, valid loss-1.9599, acc-0.4600, test loss-1.9608, acc-0.4492\n",
      "Iter-34830, train loss-1.8981, acc-0.5600, valid loss-1.9599, acc-0.4602, test loss-1.9608, acc-0.4490\n",
      "Iter-34840, train loss-1.9959, acc-0.4600, valid loss-1.9598, acc-0.4602, test loss-1.9607, acc-0.4491\n",
      "Iter-34850, train loss-1.9451, acc-0.4600, valid loss-1.9597, acc-0.4602, test loss-1.9606, acc-0.4489\n",
      "Iter-34860, train loss-1.9341, acc-0.4600, valid loss-1.9596, acc-0.4602, test loss-1.9606, acc-0.4490\n",
      "Iter-34870, train loss-1.9041, acc-0.5200, valid loss-1.9596, acc-0.4602, test loss-1.9605, acc-0.4491\n",
      "Iter-34880, train loss-1.9488, acc-0.4800, valid loss-1.9595, acc-0.4600, test loss-1.9604, acc-0.4489\n",
      "Iter-34890, train loss-2.0354, acc-0.3200, valid loss-1.9594, acc-0.4602, test loss-1.9603, acc-0.4490\n",
      "Iter-34900, train loss-2.0101, acc-0.4600, valid loss-1.9593, acc-0.4602, test loss-1.9603, acc-0.4490\n",
      "Iter-34910, train loss-1.9016, acc-0.5200, valid loss-1.9593, acc-0.4598, test loss-1.9602, acc-0.4491\n",
      "Iter-34920, train loss-1.9133, acc-0.4600, valid loss-1.9592, acc-0.4598, test loss-1.9601, acc-0.4491\n",
      "Iter-34930, train loss-1.9759, acc-0.4400, valid loss-1.9591, acc-0.4600, test loss-1.9600, acc-0.4491\n",
      "Iter-34940, train loss-1.9829, acc-0.4600, valid loss-1.9590, acc-0.4604, test loss-1.9600, acc-0.4495\n",
      "Iter-34950, train loss-1.9634, acc-0.5000, valid loss-1.9590, acc-0.4602, test loss-1.9599, acc-0.4491\n",
      "Iter-34960, train loss-1.8942, acc-0.4600, valid loss-1.9589, acc-0.4602, test loss-1.9598, acc-0.4492\n",
      "Iter-34970, train loss-1.9273, acc-0.5800, valid loss-1.9588, acc-0.4600, test loss-1.9597, acc-0.4494\n",
      "Iter-34980, train loss-1.9582, acc-0.3400, valid loss-1.9587, acc-0.4598, test loss-1.9597, acc-0.4496\n",
      "Iter-34990, train loss-1.9998, acc-0.4200, valid loss-1.9587, acc-0.4600, test loss-1.9596, acc-0.4498\n",
      "Iter-35000, train loss-1.9517, acc-0.4800, valid loss-1.9586, acc-0.4602, test loss-1.9595, acc-0.4498\n",
      "Iter-35010, train loss-1.9775, acc-0.4000, valid loss-1.9585, acc-0.4604, test loss-1.9594, acc-0.4498\n",
      "Iter-35020, train loss-1.9974, acc-0.4400, valid loss-1.9584, acc-0.4602, test loss-1.9594, acc-0.4499\n",
      "Iter-35030, train loss-1.9703, acc-0.5400, valid loss-1.9584, acc-0.4602, test loss-1.9593, acc-0.4498\n",
      "Iter-35040, train loss-1.9756, acc-0.4000, valid loss-1.9583, acc-0.4604, test loss-1.9592, acc-0.4497\n",
      "Iter-35050, train loss-1.9599, acc-0.4400, valid loss-1.9582, acc-0.4602, test loss-1.9591, acc-0.4500\n",
      "Iter-35060, train loss-1.9817, acc-0.3800, valid loss-1.9582, acc-0.4602, test loss-1.9591, acc-0.4501\n",
      "Iter-35070, train loss-2.0790, acc-0.3200, valid loss-1.9581, acc-0.4602, test loss-1.9590, acc-0.4501\n",
      "Iter-35080, train loss-1.9921, acc-0.3600, valid loss-1.9580, acc-0.4604, test loss-1.9589, acc-0.4501\n",
      "Iter-35090, train loss-1.9958, acc-0.3600, valid loss-1.9579, acc-0.4606, test loss-1.9589, acc-0.4501\n",
      "Iter-35100, train loss-1.9075, acc-0.4400, valid loss-1.9579, acc-0.4604, test loss-1.9588, acc-0.4502\n",
      "Iter-35110, train loss-1.9410, acc-0.4600, valid loss-1.9578, acc-0.4602, test loss-1.9587, acc-0.4502\n",
      "Iter-35120, train loss-1.9278, acc-0.5000, valid loss-1.9577, acc-0.4604, test loss-1.9586, acc-0.4504\n",
      "Iter-35130, train loss-1.9617, acc-0.4400, valid loss-1.9576, acc-0.4606, test loss-1.9586, acc-0.4502\n",
      "Iter-35140, train loss-1.8795, acc-0.6200, valid loss-1.9575, acc-0.4604, test loss-1.9585, acc-0.4502\n",
      "Iter-35150, train loss-1.9441, acc-0.4400, valid loss-1.9575, acc-0.4604, test loss-1.9584, acc-0.4500\n",
      "Iter-35160, train loss-1.9685, acc-0.5200, valid loss-1.9574, acc-0.4606, test loss-1.9583, acc-0.4499\n",
      "Iter-35170, train loss-1.9949, acc-0.5000, valid loss-1.9573, acc-0.4604, test loss-1.9583, acc-0.4501\n",
      "Iter-35180, train loss-1.9919, acc-0.3800, valid loss-1.9573, acc-0.4608, test loss-1.9582, acc-0.4501\n",
      "Iter-35190, train loss-1.9286, acc-0.4800, valid loss-1.9572, acc-0.4608, test loss-1.9581, acc-0.4503\n",
      "Iter-35200, train loss-2.0206, acc-0.3400, valid loss-1.9571, acc-0.4608, test loss-1.9580, acc-0.4504\n",
      "Iter-35210, train loss-1.9904, acc-0.3800, valid loss-1.9570, acc-0.4610, test loss-1.9580, acc-0.4504\n",
      "Iter-35220, train loss-1.9721, acc-0.5400, valid loss-1.9570, acc-0.4610, test loss-1.9579, acc-0.4505\n",
      "Iter-35230, train loss-2.0255, acc-0.3200, valid loss-1.9569, acc-0.4612, test loss-1.9578, acc-0.4505\n",
      "Iter-35240, train loss-1.8916, acc-0.5600, valid loss-1.9568, acc-0.4614, test loss-1.9578, acc-0.4505\n",
      "Iter-35250, train loss-1.9985, acc-0.3400, valid loss-1.9568, acc-0.4612, test loss-1.9577, acc-0.4507\n",
      "Iter-35260, train loss-1.9683, acc-0.4200, valid loss-1.9567, acc-0.4614, test loss-1.9576, acc-0.4506\n",
      "Iter-35270, train loss-1.9026, acc-0.5400, valid loss-1.9566, acc-0.4614, test loss-1.9575, acc-0.4507\n",
      "Iter-35280, train loss-1.9376, acc-0.5000, valid loss-1.9565, acc-0.4614, test loss-1.9575, acc-0.4506\n",
      "Iter-35290, train loss-1.9353, acc-0.5400, valid loss-1.9565, acc-0.4614, test loss-1.9574, acc-0.4504\n",
      "Iter-35300, train loss-1.8684, acc-0.5800, valid loss-1.9564, acc-0.4612, test loss-1.9573, acc-0.4501\n",
      "Iter-35310, train loss-1.9122, acc-0.5200, valid loss-1.9563, acc-0.4610, test loss-1.9572, acc-0.4503\n",
      "Iter-35320, train loss-1.9557, acc-0.5000, valid loss-1.9562, acc-0.4610, test loss-1.9572, acc-0.4502\n",
      "Iter-35330, train loss-1.9748, acc-0.4400, valid loss-1.9562, acc-0.4610, test loss-1.9571, acc-0.4503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-35340, train loss-1.9864, acc-0.4000, valid loss-1.9561, acc-0.4610, test loss-1.9570, acc-0.4503\n",
      "Iter-35350, train loss-1.9355, acc-0.4600, valid loss-1.9560, acc-0.4610, test loss-1.9569, acc-0.4503\n",
      "Iter-35360, train loss-2.0078, acc-0.4600, valid loss-1.9559, acc-0.4612, test loss-1.9569, acc-0.4502\n",
      "Iter-35370, train loss-1.9937, acc-0.3200, valid loss-1.9559, acc-0.4612, test loss-1.9568, acc-0.4502\n",
      "Iter-35380, train loss-2.0729, acc-0.3800, valid loss-1.9558, acc-0.4610, test loss-1.9567, acc-0.4499\n",
      "Iter-35390, train loss-1.9414, acc-0.4400, valid loss-1.9557, acc-0.4612, test loss-1.9567, acc-0.4499\n",
      "Iter-35400, train loss-2.0121, acc-0.3600, valid loss-1.9557, acc-0.4610, test loss-1.9566, acc-0.4499\n",
      "Iter-35410, train loss-1.9475, acc-0.5000, valid loss-1.9556, acc-0.4610, test loss-1.9565, acc-0.4500\n",
      "Iter-35420, train loss-1.9930, acc-0.5200, valid loss-1.9555, acc-0.4612, test loss-1.9564, acc-0.4502\n",
      "Iter-35430, train loss-1.9961, acc-0.4600, valid loss-1.9555, acc-0.4614, test loss-1.9564, acc-0.4502\n",
      "Iter-35440, train loss-2.0030, acc-0.4000, valid loss-1.9554, acc-0.4614, test loss-1.9563, acc-0.4501\n",
      "Iter-35450, train loss-1.9078, acc-0.5200, valid loss-1.9553, acc-0.4614, test loss-1.9562, acc-0.4501\n",
      "Iter-35460, train loss-1.9334, acc-0.5000, valid loss-1.9552, acc-0.4614, test loss-1.9562, acc-0.4502\n",
      "Iter-35470, train loss-2.0325, acc-0.4000, valid loss-1.9552, acc-0.4614, test loss-1.9561, acc-0.4502\n",
      "Iter-35480, train loss-2.0562, acc-0.4200, valid loss-1.9551, acc-0.4616, test loss-1.9560, acc-0.4501\n",
      "Iter-35490, train loss-1.9001, acc-0.5400, valid loss-1.9550, acc-0.4614, test loss-1.9559, acc-0.4502\n",
      "Iter-35500, train loss-1.8942, acc-0.5000, valid loss-1.9549, acc-0.4616, test loss-1.9559, acc-0.4502\n",
      "Iter-35510, train loss-1.9907, acc-0.3200, valid loss-1.9549, acc-0.4614, test loss-1.9558, acc-0.4504\n",
      "Iter-35520, train loss-1.9359, acc-0.5000, valid loss-1.9548, acc-0.4614, test loss-1.9557, acc-0.4506\n",
      "Iter-35530, train loss-1.9926, acc-0.3800, valid loss-1.9547, acc-0.4612, test loss-1.9556, acc-0.4505\n",
      "Iter-35540, train loss-2.0161, acc-0.2800, valid loss-1.9546, acc-0.4614, test loss-1.9556, acc-0.4505\n",
      "Iter-35550, train loss-1.9576, acc-0.4600, valid loss-1.9546, acc-0.4612, test loss-1.9555, acc-0.4504\n",
      "Iter-35560, train loss-1.9030, acc-0.5200, valid loss-1.9545, acc-0.4612, test loss-1.9554, acc-0.4506\n",
      "Iter-35570, train loss-2.0100, acc-0.3600, valid loss-1.9544, acc-0.4612, test loss-1.9554, acc-0.4503\n",
      "Iter-35580, train loss-1.9618, acc-0.3600, valid loss-1.9544, acc-0.4614, test loss-1.9553, acc-0.4504\n",
      "Iter-35590, train loss-1.9425, acc-0.4600, valid loss-1.9543, acc-0.4614, test loss-1.9552, acc-0.4505\n",
      "Iter-35600, train loss-1.9660, acc-0.4600, valid loss-1.9542, acc-0.4614, test loss-1.9551, acc-0.4507\n",
      "Iter-35610, train loss-1.9849, acc-0.5200, valid loss-1.9541, acc-0.4614, test loss-1.9551, acc-0.4507\n",
      "Iter-35620, train loss-1.8869, acc-0.5400, valid loss-1.9541, acc-0.4616, test loss-1.9550, acc-0.4507\n",
      "Iter-35630, train loss-1.8944, acc-0.5400, valid loss-1.9540, acc-0.4610, test loss-1.9549, acc-0.4505\n",
      "Iter-35640, train loss-1.9471, acc-0.5000, valid loss-1.9539, acc-0.4614, test loss-1.9548, acc-0.4506\n",
      "Iter-35650, train loss-2.0137, acc-0.3000, valid loss-1.9539, acc-0.4618, test loss-1.9548, acc-0.4507\n",
      "Iter-35660, train loss-1.9788, acc-0.4000, valid loss-1.9538, acc-0.4616, test loss-1.9547, acc-0.4505\n",
      "Iter-35670, train loss-1.9437, acc-0.5000, valid loss-1.9537, acc-0.4614, test loss-1.9546, acc-0.4507\n",
      "Iter-35680, train loss-1.9772, acc-0.4000, valid loss-1.9536, acc-0.4618, test loss-1.9546, acc-0.4508\n",
      "Iter-35690, train loss-2.0660, acc-0.3400, valid loss-1.9536, acc-0.4618, test loss-1.9545, acc-0.4508\n",
      "Iter-35700, train loss-1.9856, acc-0.3400, valid loss-1.9535, acc-0.4618, test loss-1.9544, acc-0.4508\n",
      "Iter-35710, train loss-1.9846, acc-0.5000, valid loss-1.9534, acc-0.4618, test loss-1.9543, acc-0.4507\n",
      "Iter-35720, train loss-2.0058, acc-0.3400, valid loss-1.9534, acc-0.4618, test loss-1.9543, acc-0.4507\n",
      "Iter-35730, train loss-2.0046, acc-0.4200, valid loss-1.9533, acc-0.4618, test loss-1.9542, acc-0.4508\n",
      "Iter-35740, train loss-2.0328, acc-0.3600, valid loss-1.9532, acc-0.4620, test loss-1.9541, acc-0.4509\n",
      "Iter-35750, train loss-2.0008, acc-0.5000, valid loss-1.9532, acc-0.4622, test loss-1.9541, acc-0.4509\n",
      "Iter-35760, train loss-1.9281, acc-0.5000, valid loss-1.9531, acc-0.4620, test loss-1.9540, acc-0.4512\n",
      "Iter-35770, train loss-1.9296, acc-0.4400, valid loss-1.9530, acc-0.4616, test loss-1.9539, acc-0.4512\n",
      "Iter-35780, train loss-2.0036, acc-0.4400, valid loss-1.9529, acc-0.4616, test loss-1.9538, acc-0.4512\n",
      "Iter-35790, train loss-1.9495, acc-0.4600, valid loss-1.9529, acc-0.4616, test loss-1.9538, acc-0.4513\n",
      "Iter-35800, train loss-1.9390, acc-0.4800, valid loss-1.9528, acc-0.4616, test loss-1.9537, acc-0.4513\n",
      "Iter-35810, train loss-1.9294, acc-0.4600, valid loss-1.9527, acc-0.4622, test loss-1.9536, acc-0.4514\n",
      "Iter-35820, train loss-1.9788, acc-0.4600, valid loss-1.9526, acc-0.4622, test loss-1.9535, acc-0.4514\n",
      "Iter-35830, train loss-2.0143, acc-0.3800, valid loss-1.9526, acc-0.4622, test loss-1.9535, acc-0.4512\n",
      "Iter-35840, train loss-2.0666, acc-0.2800, valid loss-1.9525, acc-0.4622, test loss-1.9534, acc-0.4513\n",
      "Iter-35850, train loss-1.9648, acc-0.4200, valid loss-1.9524, acc-0.4622, test loss-1.9533, acc-0.4512\n",
      "Iter-35860, train loss-1.9268, acc-0.5200, valid loss-1.9523, acc-0.4622, test loss-1.9532, acc-0.4511\n",
      "Iter-35870, train loss-1.9503, acc-0.4400, valid loss-1.9523, acc-0.4622, test loss-1.9532, acc-0.4509\n",
      "Iter-35880, train loss-2.0459, acc-0.3200, valid loss-1.9522, acc-0.4622, test loss-1.9531, acc-0.4512\n",
      "Iter-35890, train loss-2.0191, acc-0.4000, valid loss-1.9521, acc-0.4622, test loss-1.9530, acc-0.4510\n",
      "Iter-35900, train loss-1.9301, acc-0.4800, valid loss-1.9520, acc-0.4622, test loss-1.9530, acc-0.4511\n",
      "Iter-35910, train loss-2.0078, acc-0.2800, valid loss-1.9520, acc-0.4622, test loss-1.9529, acc-0.4511\n",
      "Iter-35920, train loss-1.8678, acc-0.6200, valid loss-1.9519, acc-0.4624, test loss-1.9528, acc-0.4512\n",
      "Iter-35930, train loss-1.8802, acc-0.5800, valid loss-1.9518, acc-0.4628, test loss-1.9527, acc-0.4514\n",
      "Iter-35940, train loss-1.9284, acc-0.4800, valid loss-1.9518, acc-0.4626, test loss-1.9527, acc-0.4513\n",
      "Iter-35950, train loss-2.0090, acc-0.4400, valid loss-1.9517, acc-0.4628, test loss-1.9526, acc-0.4513\n",
      "Iter-35960, train loss-1.8903, acc-0.4400, valid loss-1.9516, acc-0.4626, test loss-1.9525, acc-0.4515\n",
      "Iter-35970, train loss-1.9386, acc-0.4800, valid loss-1.9515, acc-0.4626, test loss-1.9524, acc-0.4516\n",
      "Iter-35980, train loss-1.8893, acc-0.4600, valid loss-1.9515, acc-0.4622, test loss-1.9524, acc-0.4517\n",
      "Iter-35990, train loss-1.9598, acc-0.5400, valid loss-1.9514, acc-0.4622, test loss-1.9523, acc-0.4517\n",
      "Iter-36000, train loss-1.9771, acc-0.4000, valid loss-1.9513, acc-0.4624, test loss-1.9522, acc-0.4517\n",
      "Iter-36010, train loss-1.9704, acc-0.4200, valid loss-1.9512, acc-0.4624, test loss-1.9521, acc-0.4517\n",
      "Iter-36020, train loss-1.9719, acc-0.5000, valid loss-1.9512, acc-0.4622, test loss-1.9521, acc-0.4517\n",
      "Iter-36030, train loss-1.9779, acc-0.4400, valid loss-1.9511, acc-0.4626, test loss-1.9520, acc-0.4517\n",
      "Iter-36040, train loss-1.9335, acc-0.4400, valid loss-1.9510, acc-0.4626, test loss-1.9519, acc-0.4517\n",
      "Iter-36050, train loss-1.9697, acc-0.4200, valid loss-1.9510, acc-0.4626, test loss-1.9519, acc-0.4517\n",
      "Iter-36060, train loss-1.9717, acc-0.4600, valid loss-1.9509, acc-0.4624, test loss-1.9518, acc-0.4520\n",
      "Iter-36070, train loss-1.8236, acc-0.6200, valid loss-1.9508, acc-0.4624, test loss-1.9517, acc-0.4519\n",
      "Iter-36080, train loss-1.9821, acc-0.5200, valid loss-1.9507, acc-0.4624, test loss-1.9516, acc-0.4519\n",
      "Iter-36090, train loss-1.9727, acc-0.4600, valid loss-1.9507, acc-0.4624, test loss-1.9516, acc-0.4519\n",
      "Iter-36100, train loss-2.0038, acc-0.4200, valid loss-1.9506, acc-0.4624, test loss-1.9515, acc-0.4519\n",
      "Iter-36110, train loss-1.9519, acc-0.4200, valid loss-1.9505, acc-0.4628, test loss-1.9514, acc-0.4519\n",
      "Iter-36120, train loss-1.9478, acc-0.5000, valid loss-1.9505, acc-0.4626, test loss-1.9513, acc-0.4519\n",
      "Iter-36130, train loss-2.0152, acc-0.4200, valid loss-1.9504, acc-0.4628, test loss-1.9513, acc-0.4519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-36140, train loss-1.9499, acc-0.4600, valid loss-1.9503, acc-0.4628, test loss-1.9512, acc-0.4519\n",
      "Iter-36150, train loss-1.9488, acc-0.4400, valid loss-1.9502, acc-0.4628, test loss-1.9511, acc-0.4519\n",
      "Iter-36160, train loss-2.0171, acc-0.3600, valid loss-1.9502, acc-0.4630, test loss-1.9511, acc-0.4520\n",
      "Iter-36170, train loss-1.9904, acc-0.3800, valid loss-1.9501, acc-0.4634, test loss-1.9510, acc-0.4522\n",
      "Iter-36180, train loss-1.9675, acc-0.4600, valid loss-1.9500, acc-0.4632, test loss-1.9509, acc-0.4523\n",
      "Iter-36190, train loss-2.0007, acc-0.3800, valid loss-1.9500, acc-0.4636, test loss-1.9508, acc-0.4524\n",
      "Iter-36200, train loss-1.9860, acc-0.4800, valid loss-1.9499, acc-0.4636, test loss-1.9508, acc-0.4524\n",
      "Iter-36210, train loss-2.0451, acc-0.2800, valid loss-1.9498, acc-0.4630, test loss-1.9507, acc-0.4523\n",
      "Iter-36220, train loss-1.9820, acc-0.5600, valid loss-1.9497, acc-0.4632, test loss-1.9506, acc-0.4522\n",
      "Iter-36230, train loss-1.9449, acc-0.4400, valid loss-1.9497, acc-0.4630, test loss-1.9506, acc-0.4521\n",
      "Iter-36240, train loss-1.9507, acc-0.4600, valid loss-1.9496, acc-0.4630, test loss-1.9505, acc-0.4523\n",
      "Iter-36250, train loss-2.0586, acc-0.2600, valid loss-1.9495, acc-0.4628, test loss-1.9504, acc-0.4523\n",
      "Iter-36260, train loss-1.9884, acc-0.3800, valid loss-1.9495, acc-0.4630, test loss-1.9503, acc-0.4522\n",
      "Iter-36270, train loss-1.9358, acc-0.4600, valid loss-1.9494, acc-0.4632, test loss-1.9503, acc-0.4521\n",
      "Iter-36280, train loss-2.0493, acc-0.3600, valid loss-1.9493, acc-0.4630, test loss-1.9502, acc-0.4521\n",
      "Iter-36290, train loss-1.9639, acc-0.4200, valid loss-1.9493, acc-0.4628, test loss-1.9501, acc-0.4521\n",
      "Iter-36300, train loss-1.9616, acc-0.5000, valid loss-1.9492, acc-0.4630, test loss-1.9501, acc-0.4522\n",
      "Iter-36310, train loss-1.9416, acc-0.5400, valid loss-1.9491, acc-0.4628, test loss-1.9500, acc-0.4521\n",
      "Iter-36320, train loss-2.0452, acc-0.4000, valid loss-1.9490, acc-0.4628, test loss-1.9499, acc-0.4521\n",
      "Iter-36330, train loss-1.9167, acc-0.4600, valid loss-1.9490, acc-0.4628, test loss-1.9499, acc-0.4520\n",
      "Iter-36340, train loss-1.9862, acc-0.5000, valid loss-1.9489, acc-0.4628, test loss-1.9498, acc-0.4521\n",
      "Iter-36350, train loss-1.9432, acc-0.4800, valid loss-1.9488, acc-0.4628, test loss-1.9497, acc-0.4521\n",
      "Iter-36360, train loss-1.8794, acc-0.5600, valid loss-1.9488, acc-0.4628, test loss-1.9496, acc-0.4522\n",
      "Iter-36370, train loss-1.9126, acc-0.5800, valid loss-1.9487, acc-0.4630, test loss-1.9496, acc-0.4522\n",
      "Iter-36380, train loss-1.9583, acc-0.5400, valid loss-1.9486, acc-0.4632, test loss-1.9495, acc-0.4524\n",
      "Iter-36390, train loss-2.0276, acc-0.4600, valid loss-1.9485, acc-0.4630, test loss-1.9494, acc-0.4524\n",
      "Iter-36400, train loss-1.9112, acc-0.5400, valid loss-1.9485, acc-0.4632, test loss-1.9494, acc-0.4524\n",
      "Iter-36410, train loss-1.9435, acc-0.4400, valid loss-1.9484, acc-0.4630, test loss-1.9493, acc-0.4524\n",
      "Iter-36420, train loss-1.9400, acc-0.4400, valid loss-1.9483, acc-0.4630, test loss-1.9492, acc-0.4524\n",
      "Iter-36430, train loss-1.9673, acc-0.4200, valid loss-1.9483, acc-0.4632, test loss-1.9491, acc-0.4524\n",
      "Iter-36440, train loss-1.9496, acc-0.4400, valid loss-1.9482, acc-0.4634, test loss-1.9491, acc-0.4525\n",
      "Iter-36450, train loss-1.9153, acc-0.5000, valid loss-1.9481, acc-0.4634, test loss-1.9490, acc-0.4526\n",
      "Iter-36460, train loss-1.9922, acc-0.4800, valid loss-1.9480, acc-0.4632, test loss-1.9489, acc-0.4525\n",
      "Iter-36470, train loss-1.9046, acc-0.4600, valid loss-1.9480, acc-0.4632, test loss-1.9489, acc-0.4526\n",
      "Iter-36480, train loss-1.9409, acc-0.5000, valid loss-1.9479, acc-0.4632, test loss-1.9488, acc-0.4525\n",
      "Iter-36490, train loss-1.9735, acc-0.4600, valid loss-1.9478, acc-0.4634, test loss-1.9487, acc-0.4525\n",
      "Iter-36500, train loss-2.0541, acc-0.3400, valid loss-1.9478, acc-0.4636, test loss-1.9486, acc-0.4527\n",
      "Iter-36510, train loss-1.9273, acc-0.4800, valid loss-1.9477, acc-0.4638, test loss-1.9486, acc-0.4526\n",
      "Iter-36520, train loss-1.8940, acc-0.5400, valid loss-1.9476, acc-0.4636, test loss-1.9485, acc-0.4524\n",
      "Iter-36530, train loss-1.9661, acc-0.4200, valid loss-1.9475, acc-0.4638, test loss-1.9484, acc-0.4527\n",
      "Iter-36540, train loss-1.9325, acc-0.4600, valid loss-1.9475, acc-0.4638, test loss-1.9483, acc-0.4527\n",
      "Iter-36550, train loss-1.9834, acc-0.3400, valid loss-1.9474, acc-0.4636, test loss-1.9483, acc-0.4526\n",
      "Iter-36560, train loss-1.9369, acc-0.5600, valid loss-1.9473, acc-0.4638, test loss-1.9482, acc-0.4525\n",
      "Iter-36570, train loss-1.9587, acc-0.4800, valid loss-1.9473, acc-0.4638, test loss-1.9481, acc-0.4527\n",
      "Iter-36580, train loss-1.9420, acc-0.4200, valid loss-1.9472, acc-0.4638, test loss-1.9481, acc-0.4527\n",
      "Iter-36590, train loss-1.8922, acc-0.4600, valid loss-1.9471, acc-0.4638, test loss-1.9480, acc-0.4526\n",
      "Iter-36600, train loss-1.9568, acc-0.3800, valid loss-1.9470, acc-0.4636, test loss-1.9479, acc-0.4526\n",
      "Iter-36610, train loss-1.9326, acc-0.5200, valid loss-1.9470, acc-0.4638, test loss-1.9478, acc-0.4526\n",
      "Iter-36620, train loss-1.9497, acc-0.5400, valid loss-1.9469, acc-0.4638, test loss-1.9478, acc-0.4527\n",
      "Iter-36630, train loss-1.9505, acc-0.4200, valid loss-1.9468, acc-0.4638, test loss-1.9477, acc-0.4527\n",
      "Iter-36640, train loss-1.9920, acc-0.4200, valid loss-1.9467, acc-0.4642, test loss-1.9476, acc-0.4527\n",
      "Iter-36650, train loss-1.9449, acc-0.4800, valid loss-1.9467, acc-0.4638, test loss-1.9475, acc-0.4527\n",
      "Iter-36660, train loss-1.9272, acc-0.4200, valid loss-1.9466, acc-0.4638, test loss-1.9475, acc-0.4529\n",
      "Iter-36670, train loss-1.8936, acc-0.4600, valid loss-1.9465, acc-0.4638, test loss-1.9474, acc-0.4530\n",
      "Iter-36680, train loss-1.9632, acc-0.5200, valid loss-1.9465, acc-0.4638, test loss-1.9473, acc-0.4531\n",
      "Iter-36690, train loss-2.0111, acc-0.4400, valid loss-1.9464, acc-0.4638, test loss-1.9473, acc-0.4530\n",
      "Iter-36700, train loss-1.8450, acc-0.5800, valid loss-1.9463, acc-0.4638, test loss-1.9472, acc-0.4529\n",
      "Iter-36710, train loss-1.9818, acc-0.4800, valid loss-1.9462, acc-0.4638, test loss-1.9471, acc-0.4529\n",
      "Iter-36720, train loss-1.9301, acc-0.5400, valid loss-1.9462, acc-0.4636, test loss-1.9471, acc-0.4530\n",
      "Iter-36730, train loss-2.0118, acc-0.3400, valid loss-1.9461, acc-0.4632, test loss-1.9470, acc-0.4530\n",
      "Iter-36740, train loss-1.9189, acc-0.6000, valid loss-1.9460, acc-0.4636, test loss-1.9469, acc-0.4530\n",
      "Iter-36750, train loss-2.0354, acc-0.3800, valid loss-1.9460, acc-0.4640, test loss-1.9468, acc-0.4530\n",
      "Iter-36760, train loss-1.9646, acc-0.4400, valid loss-1.9459, acc-0.4636, test loss-1.9468, acc-0.4533\n",
      "Iter-36770, train loss-2.0267, acc-0.3200, valid loss-1.9458, acc-0.4640, test loss-1.9467, acc-0.4529\n",
      "Iter-36780, train loss-1.9046, acc-0.4200, valid loss-1.9458, acc-0.4638, test loss-1.9466, acc-0.4531\n",
      "Iter-36790, train loss-1.9295, acc-0.5200, valid loss-1.9457, acc-0.4636, test loss-1.9466, acc-0.4534\n",
      "Iter-36800, train loss-1.9388, acc-0.5800, valid loss-1.9456, acc-0.4636, test loss-1.9465, acc-0.4534\n",
      "Iter-36810, train loss-1.9299, acc-0.5200, valid loss-1.9456, acc-0.4638, test loss-1.9464, acc-0.4535\n",
      "Iter-36820, train loss-1.9218, acc-0.4400, valid loss-1.9455, acc-0.4636, test loss-1.9464, acc-0.4534\n",
      "Iter-36830, train loss-1.9406, acc-0.4800, valid loss-1.9454, acc-0.4638, test loss-1.9463, acc-0.4535\n",
      "Iter-36840, train loss-1.9087, acc-0.5600, valid loss-1.9453, acc-0.4638, test loss-1.9462, acc-0.4535\n",
      "Iter-36850, train loss-1.9165, acc-0.4600, valid loss-1.9453, acc-0.4638, test loss-1.9461, acc-0.4535\n",
      "Iter-36860, train loss-2.0064, acc-0.4200, valid loss-1.9452, acc-0.4638, test loss-1.9461, acc-0.4533\n",
      "Iter-36870, train loss-2.0211, acc-0.4600, valid loss-1.9451, acc-0.4638, test loss-1.9460, acc-0.4533\n",
      "Iter-36880, train loss-1.9388, acc-0.4800, valid loss-1.9451, acc-0.4638, test loss-1.9459, acc-0.4533\n",
      "Iter-36890, train loss-2.0455, acc-0.3600, valid loss-1.9450, acc-0.4638, test loss-1.9459, acc-0.4535\n",
      "Iter-36900, train loss-1.9041, acc-0.5600, valid loss-1.9449, acc-0.4638, test loss-1.9458, acc-0.4533\n",
      "Iter-36910, train loss-1.9967, acc-0.4400, valid loss-1.9448, acc-0.4640, test loss-1.9457, acc-0.4535\n",
      "Iter-36920, train loss-1.9713, acc-0.3800, valid loss-1.9448, acc-0.4638, test loss-1.9456, acc-0.4535\n",
      "Iter-36930, train loss-1.9785, acc-0.4000, valid loss-1.9447, acc-0.4640, test loss-1.9456, acc-0.4534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-36940, train loss-2.0178, acc-0.3600, valid loss-1.9446, acc-0.4642, test loss-1.9455, acc-0.4528\n",
      "Iter-36950, train loss-2.0734, acc-0.3200, valid loss-1.9446, acc-0.4640, test loss-1.9454, acc-0.4534\n",
      "Iter-36960, train loss-1.9046, acc-0.4800, valid loss-1.9445, acc-0.4640, test loss-1.9454, acc-0.4534\n",
      "Iter-36970, train loss-1.9770, acc-0.4200, valid loss-1.9444, acc-0.4640, test loss-1.9453, acc-0.4537\n",
      "Iter-36980, train loss-1.9650, acc-0.4200, valid loss-1.9444, acc-0.4640, test loss-1.9452, acc-0.4539\n",
      "Iter-36990, train loss-1.9161, acc-0.5200, valid loss-1.9443, acc-0.4640, test loss-1.9452, acc-0.4537\n",
      "Iter-37000, train loss-2.0076, acc-0.4200, valid loss-1.9442, acc-0.4640, test loss-1.9451, acc-0.4538\n",
      "Iter-37010, train loss-1.8994, acc-0.4600, valid loss-1.9441, acc-0.4640, test loss-1.9450, acc-0.4540\n",
      "Iter-37020, train loss-1.9373, acc-0.4200, valid loss-1.9441, acc-0.4640, test loss-1.9450, acc-0.4540\n",
      "Iter-37030, train loss-1.9928, acc-0.3600, valid loss-1.9440, acc-0.4644, test loss-1.9449, acc-0.4537\n",
      "Iter-37040, train loss-2.0415, acc-0.3000, valid loss-1.9439, acc-0.4646, test loss-1.9448, acc-0.4538\n",
      "Iter-37050, train loss-1.8494, acc-0.5400, valid loss-1.9439, acc-0.4644, test loss-1.9447, acc-0.4538\n",
      "Iter-37060, train loss-2.0278, acc-0.3000, valid loss-1.9438, acc-0.4642, test loss-1.9447, acc-0.4538\n",
      "Iter-37070, train loss-1.9347, acc-0.4200, valid loss-1.9437, acc-0.4638, test loss-1.9446, acc-0.4540\n",
      "Iter-37080, train loss-1.9029, acc-0.5200, valid loss-1.9436, acc-0.4640, test loss-1.9445, acc-0.4540\n",
      "Iter-37090, train loss-1.9125, acc-0.4800, valid loss-1.9436, acc-0.4638, test loss-1.9445, acc-0.4539\n",
      "Iter-37100, train loss-1.8572, acc-0.5600, valid loss-1.9435, acc-0.4640, test loss-1.9444, acc-0.4538\n",
      "Iter-37110, train loss-1.9696, acc-0.4400, valid loss-1.9434, acc-0.4642, test loss-1.9443, acc-0.4538\n",
      "Iter-37120, train loss-1.9151, acc-0.4400, valid loss-1.9433, acc-0.4640, test loss-1.9442, acc-0.4536\n",
      "Iter-37130, train loss-1.9930, acc-0.3600, valid loss-1.9433, acc-0.4642, test loss-1.9442, acc-0.4537\n",
      "Iter-37140, train loss-1.9438, acc-0.4400, valid loss-1.9432, acc-0.4642, test loss-1.9441, acc-0.4536\n",
      "Iter-37150, train loss-1.9706, acc-0.3800, valid loss-1.9431, acc-0.4642, test loss-1.9440, acc-0.4537\n",
      "Iter-37160, train loss-1.9730, acc-0.4000, valid loss-1.9431, acc-0.4642, test loss-1.9439, acc-0.4537\n",
      "Iter-37170, train loss-1.9590, acc-0.4200, valid loss-1.9430, acc-0.4642, test loss-1.9439, acc-0.4538\n",
      "Iter-37180, train loss-1.9142, acc-0.4200, valid loss-1.9429, acc-0.4640, test loss-1.9438, acc-0.4536\n",
      "Iter-37190, train loss-1.9371, acc-0.4800, valid loss-1.9428, acc-0.4640, test loss-1.9437, acc-0.4539\n",
      "Iter-37200, train loss-2.0404, acc-0.3800, valid loss-1.9428, acc-0.4640, test loss-1.9437, acc-0.4539\n",
      "Iter-37210, train loss-1.9564, acc-0.5200, valid loss-1.9427, acc-0.4642, test loss-1.9436, acc-0.4538\n",
      "Iter-37220, train loss-1.9917, acc-0.3400, valid loss-1.9426, acc-0.4642, test loss-1.9435, acc-0.4540\n",
      "Iter-37230, train loss-1.9638, acc-0.4800, valid loss-1.9426, acc-0.4644, test loss-1.9435, acc-0.4540\n",
      "Iter-37240, train loss-1.8957, acc-0.5000, valid loss-1.9425, acc-0.4644, test loss-1.9434, acc-0.4540\n",
      "Iter-37250, train loss-1.9529, acc-0.4000, valid loss-1.9424, acc-0.4644, test loss-1.9433, acc-0.4541\n",
      "Iter-37260, train loss-1.9773, acc-0.3200, valid loss-1.9424, acc-0.4646, test loss-1.9432, acc-0.4542\n",
      "Iter-37270, train loss-1.9777, acc-0.4000, valid loss-1.9423, acc-0.4646, test loss-1.9432, acc-0.4544\n",
      "Iter-37280, train loss-2.0235, acc-0.3400, valid loss-1.9422, acc-0.4646, test loss-1.9431, acc-0.4544\n",
      "Iter-37290, train loss-1.9737, acc-0.5000, valid loss-1.9421, acc-0.4646, test loss-1.9430, acc-0.4545\n",
      "Iter-37300, train loss-1.9906, acc-0.3800, valid loss-1.9421, acc-0.4646, test loss-1.9430, acc-0.4544\n",
      "Iter-37310, train loss-1.8836, acc-0.5400, valid loss-1.9420, acc-0.4646, test loss-1.9429, acc-0.4546\n",
      "Iter-37320, train loss-1.9963, acc-0.3800, valid loss-1.9419, acc-0.4646, test loss-1.9428, acc-0.4548\n",
      "Iter-37330, train loss-1.9606, acc-0.4600, valid loss-1.9419, acc-0.4648, test loss-1.9427, acc-0.4547\n",
      "Iter-37340, train loss-1.9418, acc-0.4800, valid loss-1.9418, acc-0.4646, test loss-1.9427, acc-0.4547\n",
      "Iter-37350, train loss-1.9392, acc-0.4200, valid loss-1.9417, acc-0.4646, test loss-1.9426, acc-0.4550\n",
      "Iter-37360, train loss-1.9300, acc-0.3800, valid loss-1.9416, acc-0.4646, test loss-1.9425, acc-0.4550\n",
      "Iter-37370, train loss-1.9440, acc-0.4800, valid loss-1.9416, acc-0.4648, test loss-1.9425, acc-0.4550\n",
      "Iter-37380, train loss-1.8734, acc-0.5200, valid loss-1.9415, acc-0.4648, test loss-1.9424, acc-0.4547\n",
      "Iter-37390, train loss-1.9696, acc-0.5200, valid loss-1.9414, acc-0.4648, test loss-1.9423, acc-0.4549\n",
      "Iter-37400, train loss-1.9371, acc-0.3800, valid loss-1.9414, acc-0.4648, test loss-1.9423, acc-0.4549\n",
      "Iter-37410, train loss-1.9112, acc-0.4400, valid loss-1.9413, acc-0.4648, test loss-1.9422, acc-0.4549\n",
      "Iter-37420, train loss-2.0167, acc-0.3800, valid loss-1.9412, acc-0.4648, test loss-1.9421, acc-0.4551\n",
      "Iter-37430, train loss-2.0485, acc-0.2800, valid loss-1.9412, acc-0.4648, test loss-1.9420, acc-0.4550\n",
      "Iter-37440, train loss-1.9945, acc-0.4000, valid loss-1.9411, acc-0.4648, test loss-1.9420, acc-0.4552\n",
      "Iter-37450, train loss-1.9067, acc-0.4800, valid loss-1.9410, acc-0.4648, test loss-1.9419, acc-0.4553\n",
      "Iter-37460, train loss-1.9515, acc-0.4200, valid loss-1.9409, acc-0.4648, test loss-1.9418, acc-0.4554\n",
      "Iter-37470, train loss-1.9590, acc-0.4200, valid loss-1.9409, acc-0.4648, test loss-1.9418, acc-0.4555\n",
      "Iter-37480, train loss-1.9341, acc-0.5000, valid loss-1.9408, acc-0.4646, test loss-1.9417, acc-0.4554\n",
      "Iter-37490, train loss-1.9032, acc-0.4800, valid loss-1.9407, acc-0.4646, test loss-1.9416, acc-0.4556\n",
      "Iter-37500, train loss-1.8488, acc-0.5400, valid loss-1.9407, acc-0.4648, test loss-1.9415, acc-0.4556\n",
      "Iter-37510, train loss-1.9227, acc-0.4800, valid loss-1.9406, acc-0.4646, test loss-1.9415, acc-0.4556\n",
      "Iter-37520, train loss-1.9293, acc-0.4400, valid loss-1.9405, acc-0.4646, test loss-1.9414, acc-0.4558\n",
      "Iter-37530, train loss-1.9766, acc-0.5000, valid loss-1.9404, acc-0.4646, test loss-1.9413, acc-0.4560\n",
      "Iter-37540, train loss-1.8921, acc-0.5200, valid loss-1.9404, acc-0.4650, test loss-1.9413, acc-0.4557\n",
      "Iter-37550, train loss-1.9354, acc-0.4400, valid loss-1.9403, acc-0.4652, test loss-1.9412, acc-0.4558\n",
      "Iter-37560, train loss-1.9056, acc-0.6000, valid loss-1.9402, acc-0.4652, test loss-1.9411, acc-0.4559\n",
      "Iter-37570, train loss-1.9835, acc-0.3600, valid loss-1.9402, acc-0.4650, test loss-1.9411, acc-0.4559\n",
      "Iter-37580, train loss-1.8922, acc-0.5200, valid loss-1.9401, acc-0.4650, test loss-1.9410, acc-0.4558\n",
      "Iter-37590, train loss-1.9044, acc-0.4600, valid loss-1.9400, acc-0.4650, test loss-1.9409, acc-0.4557\n",
      "Iter-37600, train loss-1.9754, acc-0.4400, valid loss-1.9400, acc-0.4650, test loss-1.9408, acc-0.4559\n",
      "Iter-37610, train loss-1.9793, acc-0.3200, valid loss-1.9399, acc-0.4648, test loss-1.9408, acc-0.4558\n",
      "Iter-37620, train loss-1.9027, acc-0.5600, valid loss-1.9398, acc-0.4648, test loss-1.9407, acc-0.4559\n",
      "Iter-37630, train loss-1.9517, acc-0.5000, valid loss-1.9398, acc-0.4646, test loss-1.9406, acc-0.4559\n",
      "Iter-37640, train loss-1.8986, acc-0.4600, valid loss-1.9397, acc-0.4648, test loss-1.9406, acc-0.4559\n",
      "Iter-37650, train loss-1.9436, acc-0.5000, valid loss-1.9396, acc-0.4652, test loss-1.9405, acc-0.4559\n",
      "Iter-37660, train loss-2.0429, acc-0.4200, valid loss-1.9395, acc-0.4648, test loss-1.9404, acc-0.4559\n",
      "Iter-37670, train loss-1.9442, acc-0.4600, valid loss-1.9395, acc-0.4648, test loss-1.9404, acc-0.4555\n",
      "Iter-37680, train loss-1.9222, acc-0.4800, valid loss-1.9394, acc-0.4648, test loss-1.9403, acc-0.4557\n",
      "Iter-37690, train loss-1.9659, acc-0.4200, valid loss-1.9393, acc-0.4648, test loss-1.9402, acc-0.4558\n",
      "Iter-37700, train loss-1.9396, acc-0.4200, valid loss-1.9393, acc-0.4648, test loss-1.9401, acc-0.4559\n",
      "Iter-37710, train loss-1.8930, acc-0.4800, valid loss-1.9392, acc-0.4648, test loss-1.9401, acc-0.4559\n",
      "Iter-37720, train loss-1.9554, acc-0.4400, valid loss-1.9391, acc-0.4650, test loss-1.9400, acc-0.4558\n",
      "Iter-37730, train loss-1.9133, acc-0.5000, valid loss-1.9390, acc-0.4648, test loss-1.9399, acc-0.4559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-37740, train loss-1.9580, acc-0.5400, valid loss-1.9390, acc-0.4650, test loss-1.9399, acc-0.4559\n",
      "Iter-37750, train loss-1.8555, acc-0.6600, valid loss-1.9389, acc-0.4650, test loss-1.9398, acc-0.4560\n",
      "Iter-37760, train loss-2.0212, acc-0.3600, valid loss-1.9388, acc-0.4654, test loss-1.9397, acc-0.4560\n",
      "Iter-37770, train loss-2.0246, acc-0.4000, valid loss-1.9388, acc-0.4654, test loss-1.9396, acc-0.4560\n",
      "Iter-37780, train loss-1.9817, acc-0.3800, valid loss-1.9387, acc-0.4652, test loss-1.9396, acc-0.4560\n",
      "Iter-37790, train loss-2.0673, acc-0.3000, valid loss-1.9386, acc-0.4652, test loss-1.9395, acc-0.4558\n",
      "Iter-37800, train loss-2.0664, acc-0.3400, valid loss-1.9386, acc-0.4652, test loss-1.9394, acc-0.4559\n",
      "Iter-37810, train loss-1.9196, acc-0.5600, valid loss-1.9385, acc-0.4654, test loss-1.9394, acc-0.4557\n",
      "Iter-37820, train loss-1.9517, acc-0.4200, valid loss-1.9384, acc-0.4654, test loss-1.9393, acc-0.4557\n",
      "Iter-37830, train loss-1.9886, acc-0.4200, valid loss-1.9383, acc-0.4656, test loss-1.9392, acc-0.4557\n",
      "Iter-37840, train loss-1.9797, acc-0.4200, valid loss-1.9383, acc-0.4656, test loss-1.9392, acc-0.4558\n",
      "Iter-37850, train loss-1.9495, acc-0.4400, valid loss-1.9382, acc-0.4660, test loss-1.9391, acc-0.4558\n",
      "Iter-37860, train loss-1.9803, acc-0.3400, valid loss-1.9381, acc-0.4658, test loss-1.9390, acc-0.4557\n",
      "Iter-37870, train loss-1.9715, acc-0.3600, valid loss-1.9381, acc-0.4658, test loss-1.9390, acc-0.4558\n",
      "Iter-37880, train loss-2.0443, acc-0.3600, valid loss-1.9380, acc-0.4662, test loss-1.9389, acc-0.4560\n",
      "Iter-37890, train loss-1.9683, acc-0.4400, valid loss-1.9379, acc-0.4662, test loss-1.9388, acc-0.4562\n",
      "Iter-37900, train loss-2.0102, acc-0.4000, valid loss-1.9379, acc-0.4664, test loss-1.9387, acc-0.4564\n",
      "Iter-37910, train loss-1.9571, acc-0.4000, valid loss-1.9378, acc-0.4664, test loss-1.9387, acc-0.4564\n",
      "Iter-37920, train loss-2.0296, acc-0.3000, valid loss-1.9377, acc-0.4660, test loss-1.9386, acc-0.4562\n",
      "Iter-37930, train loss-1.9264, acc-0.4600, valid loss-1.9376, acc-0.4658, test loss-1.9385, acc-0.4561\n",
      "Iter-37940, train loss-2.0341, acc-0.3800, valid loss-1.9376, acc-0.4660, test loss-1.9385, acc-0.4562\n",
      "Iter-37950, train loss-1.9742, acc-0.4000, valid loss-1.9375, acc-0.4660, test loss-1.9384, acc-0.4562\n",
      "Iter-37960, train loss-1.9475, acc-0.5000, valid loss-1.9374, acc-0.4658, test loss-1.9383, acc-0.4563\n",
      "Iter-37970, train loss-1.9630, acc-0.5200, valid loss-1.9374, acc-0.4662, test loss-1.9383, acc-0.4561\n",
      "Iter-37980, train loss-2.0160, acc-0.3800, valid loss-1.9373, acc-0.4662, test loss-1.9382, acc-0.4560\n",
      "Iter-37990, train loss-1.9590, acc-0.4800, valid loss-1.9372, acc-0.4662, test loss-1.9381, acc-0.4560\n",
      "Iter-38000, train loss-1.9276, acc-0.4600, valid loss-1.9372, acc-0.4664, test loss-1.9380, acc-0.4560\n",
      "Iter-38010, train loss-1.8942, acc-0.4400, valid loss-1.9371, acc-0.4660, test loss-1.9380, acc-0.4560\n",
      "Iter-38020, train loss-1.9585, acc-0.5200, valid loss-1.9370, acc-0.4662, test loss-1.9379, acc-0.4560\n",
      "Iter-38030, train loss-1.9642, acc-0.3400, valid loss-1.9369, acc-0.4666, test loss-1.9378, acc-0.4560\n",
      "Iter-38040, train loss-1.9263, acc-0.4200, valid loss-1.9369, acc-0.4668, test loss-1.9377, acc-0.4558\n",
      "Iter-38050, train loss-1.9319, acc-0.5200, valid loss-1.9368, acc-0.4670, test loss-1.9377, acc-0.4560\n",
      "Iter-38060, train loss-1.9516, acc-0.4800, valid loss-1.9367, acc-0.4670, test loss-1.9376, acc-0.4561\n",
      "Iter-38070, train loss-1.8731, acc-0.4800, valid loss-1.9367, acc-0.4670, test loss-1.9375, acc-0.4561\n",
      "Iter-38080, train loss-1.9352, acc-0.5200, valid loss-1.9366, acc-0.4666, test loss-1.9375, acc-0.4563\n",
      "Iter-38090, train loss-1.8898, acc-0.6400, valid loss-1.9365, acc-0.4668, test loss-1.9374, acc-0.4563\n",
      "Iter-38100, train loss-1.9351, acc-0.4200, valid loss-1.9365, acc-0.4668, test loss-1.9373, acc-0.4561\n",
      "Iter-38110, train loss-1.9648, acc-0.3200, valid loss-1.9364, acc-0.4668, test loss-1.9373, acc-0.4560\n",
      "Iter-38120, train loss-1.9407, acc-0.4800, valid loss-1.9363, acc-0.4666, test loss-1.9372, acc-0.4563\n",
      "Iter-38130, train loss-1.9217, acc-0.5200, valid loss-1.9363, acc-0.4666, test loss-1.9371, acc-0.4562\n",
      "Iter-38140, train loss-1.9753, acc-0.4400, valid loss-1.9362, acc-0.4670, test loss-1.9371, acc-0.4561\n",
      "Iter-38150, train loss-1.8978, acc-0.5400, valid loss-1.9361, acc-0.4668, test loss-1.9370, acc-0.4563\n",
      "Iter-38160, train loss-1.8973, acc-0.5200, valid loss-1.9361, acc-0.4666, test loss-1.9369, acc-0.4563\n",
      "Iter-38170, train loss-1.9295, acc-0.4600, valid loss-1.9360, acc-0.4668, test loss-1.9369, acc-0.4565\n",
      "Iter-38180, train loss-1.8886, acc-0.4600, valid loss-1.9359, acc-0.4666, test loss-1.9368, acc-0.4565\n",
      "Iter-38190, train loss-1.9994, acc-0.4000, valid loss-1.9358, acc-0.4668, test loss-1.9367, acc-0.4562\n",
      "Iter-38200, train loss-1.8688, acc-0.5800, valid loss-1.9358, acc-0.4666, test loss-1.9366, acc-0.4561\n",
      "Iter-38210, train loss-1.9341, acc-0.4600, valid loss-1.9357, acc-0.4670, test loss-1.9366, acc-0.4561\n",
      "Iter-38220, train loss-1.8886, acc-0.5000, valid loss-1.9356, acc-0.4668, test loss-1.9365, acc-0.4563\n",
      "Iter-38230, train loss-1.9747, acc-0.4000, valid loss-1.9356, acc-0.4668, test loss-1.9364, acc-0.4567\n",
      "Iter-38240, train loss-1.9177, acc-0.4000, valid loss-1.9355, acc-0.4666, test loss-1.9364, acc-0.4568\n",
      "Iter-38250, train loss-2.0058, acc-0.4200, valid loss-1.9354, acc-0.4668, test loss-1.9363, acc-0.4566\n",
      "Iter-38260, train loss-1.9220, acc-0.4800, valid loss-1.9354, acc-0.4668, test loss-1.9362, acc-0.4566\n",
      "Iter-38270, train loss-1.9400, acc-0.4400, valid loss-1.9353, acc-0.4668, test loss-1.9361, acc-0.4567\n",
      "Iter-38280, train loss-1.9285, acc-0.5400, valid loss-1.9352, acc-0.4666, test loss-1.9361, acc-0.4567\n",
      "Iter-38290, train loss-2.0279, acc-0.3800, valid loss-1.9351, acc-0.4666, test loss-1.9360, acc-0.4566\n",
      "Iter-38300, train loss-1.9219, acc-0.5000, valid loss-1.9351, acc-0.4666, test loss-1.9359, acc-0.4565\n",
      "Iter-38310, train loss-2.0028, acc-0.3000, valid loss-1.9350, acc-0.4666, test loss-1.9359, acc-0.4566\n",
      "Iter-38320, train loss-1.9171, acc-0.4000, valid loss-1.9349, acc-0.4668, test loss-1.9358, acc-0.4565\n",
      "Iter-38330, train loss-1.9303, acc-0.4800, valid loss-1.9349, acc-0.4666, test loss-1.9357, acc-0.4567\n",
      "Iter-38340, train loss-1.9535, acc-0.4400, valid loss-1.9348, acc-0.4666, test loss-1.9357, acc-0.4568\n",
      "Iter-38350, train loss-1.9630, acc-0.4800, valid loss-1.9347, acc-0.4666, test loss-1.9356, acc-0.4568\n",
      "Iter-38360, train loss-1.8637, acc-0.5600, valid loss-1.9347, acc-0.4666, test loss-1.9355, acc-0.4568\n",
      "Iter-38370, train loss-1.9773, acc-0.4400, valid loss-1.9346, acc-0.4668, test loss-1.9355, acc-0.4571\n",
      "Iter-38380, train loss-1.9088, acc-0.5200, valid loss-1.9345, acc-0.4668, test loss-1.9354, acc-0.4570\n",
      "Iter-38390, train loss-2.0130, acc-0.3800, valid loss-1.9345, acc-0.4672, test loss-1.9353, acc-0.4570\n",
      "Iter-38400, train loss-2.0120, acc-0.3400, valid loss-1.9344, acc-0.4668, test loss-1.9352, acc-0.4570\n",
      "Iter-38410, train loss-1.9150, acc-0.4400, valid loss-1.9343, acc-0.4670, test loss-1.9352, acc-0.4571\n",
      "Iter-38420, train loss-1.8962, acc-0.5000, valid loss-1.9343, acc-0.4672, test loss-1.9351, acc-0.4571\n",
      "Iter-38430, train loss-1.9288, acc-0.4800, valid loss-1.9342, acc-0.4674, test loss-1.9350, acc-0.4571\n",
      "Iter-38440, train loss-1.9898, acc-0.3800, valid loss-1.9341, acc-0.4674, test loss-1.9350, acc-0.4573\n",
      "Iter-38450, train loss-1.9594, acc-0.3600, valid loss-1.9341, acc-0.4674, test loss-1.9349, acc-0.4575\n",
      "Iter-38460, train loss-1.9311, acc-0.5200, valid loss-1.9340, acc-0.4676, test loss-1.9348, acc-0.4575\n",
      "Iter-38470, train loss-1.8929, acc-0.4400, valid loss-1.9339, acc-0.4676, test loss-1.9348, acc-0.4574\n",
      "Iter-38480, train loss-1.9907, acc-0.3400, valid loss-1.9339, acc-0.4676, test loss-1.9347, acc-0.4573\n",
      "Iter-38490, train loss-1.9266, acc-0.5000, valid loss-1.9338, acc-0.4674, test loss-1.9346, acc-0.4574\n",
      "Iter-38500, train loss-1.8794, acc-0.4800, valid loss-1.9337, acc-0.4676, test loss-1.9346, acc-0.4574\n",
      "Iter-38510, train loss-1.8848, acc-0.5000, valid loss-1.9336, acc-0.4672, test loss-1.9345, acc-0.4573\n",
      "Iter-38520, train loss-1.9928, acc-0.3600, valid loss-1.9336, acc-0.4676, test loss-1.9344, acc-0.4573\n",
      "Iter-38530, train loss-1.9154, acc-0.5000, valid loss-1.9335, acc-0.4670, test loss-1.9343, acc-0.4576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-38540, train loss-1.8870, acc-0.4800, valid loss-1.9334, acc-0.4676, test loss-1.9343, acc-0.4579\n",
      "Iter-38550, train loss-1.8770, acc-0.5400, valid loss-1.9334, acc-0.4672, test loss-1.9342, acc-0.4578\n",
      "Iter-38560, train loss-1.8994, acc-0.4800, valid loss-1.9333, acc-0.4672, test loss-1.9341, acc-0.4576\n",
      "Iter-38570, train loss-1.8945, acc-0.5200, valid loss-1.9332, acc-0.4674, test loss-1.9341, acc-0.4578\n",
      "Iter-38580, train loss-1.9502, acc-0.5000, valid loss-1.9332, acc-0.4674, test loss-1.9340, acc-0.4578\n",
      "Iter-38590, train loss-1.9552, acc-0.4400, valid loss-1.9331, acc-0.4676, test loss-1.9339, acc-0.4580\n",
      "Iter-38600, train loss-1.8737, acc-0.5200, valid loss-1.9330, acc-0.4676, test loss-1.9339, acc-0.4578\n",
      "Iter-38610, train loss-1.8895, acc-0.4000, valid loss-1.9330, acc-0.4678, test loss-1.9338, acc-0.4578\n",
      "Iter-38620, train loss-1.8849, acc-0.6000, valid loss-1.9329, acc-0.4676, test loss-1.9337, acc-0.4577\n",
      "Iter-38630, train loss-1.9252, acc-0.4200, valid loss-1.9328, acc-0.4678, test loss-1.9336, acc-0.4576\n",
      "Iter-38640, train loss-1.9627, acc-0.4000, valid loss-1.9327, acc-0.4678, test loss-1.9336, acc-0.4577\n",
      "Iter-38650, train loss-1.9809, acc-0.5000, valid loss-1.9327, acc-0.4676, test loss-1.9335, acc-0.4579\n",
      "Iter-38660, train loss-1.9249, acc-0.4400, valid loss-1.9326, acc-0.4678, test loss-1.9334, acc-0.4580\n",
      "Iter-38670, train loss-1.9606, acc-0.4800, valid loss-1.9325, acc-0.4678, test loss-1.9334, acc-0.4581\n",
      "Iter-38680, train loss-1.9431, acc-0.4600, valid loss-1.9325, acc-0.4676, test loss-1.9333, acc-0.4583\n",
      "Iter-38690, train loss-1.9467, acc-0.4600, valid loss-1.9324, acc-0.4676, test loss-1.9333, acc-0.4581\n",
      "Iter-38700, train loss-1.9372, acc-0.4600, valid loss-1.9323, acc-0.4676, test loss-1.9332, acc-0.4583\n",
      "Iter-38710, train loss-1.9573, acc-0.5000, valid loss-1.9323, acc-0.4676, test loss-1.9331, acc-0.4581\n",
      "Iter-38720, train loss-1.9855, acc-0.4200, valid loss-1.9322, acc-0.4678, test loss-1.9330, acc-0.4581\n",
      "Iter-38730, train loss-1.8716, acc-0.5400, valid loss-1.9321, acc-0.4676, test loss-1.9330, acc-0.4584\n",
      "Iter-38740, train loss-1.9088, acc-0.5800, valid loss-1.9321, acc-0.4674, test loss-1.9329, acc-0.4583\n",
      "Iter-38750, train loss-1.9299, acc-0.4800, valid loss-1.9320, acc-0.4674, test loss-1.9328, acc-0.4583\n",
      "Iter-38760, train loss-1.8999, acc-0.4800, valid loss-1.9319, acc-0.4678, test loss-1.9328, acc-0.4584\n",
      "Iter-38770, train loss-2.0177, acc-0.3800, valid loss-1.9319, acc-0.4676, test loss-1.9327, acc-0.4584\n",
      "Iter-38780, train loss-1.9027, acc-0.5000, valid loss-1.9318, acc-0.4676, test loss-1.9326, acc-0.4583\n",
      "Iter-38790, train loss-1.9184, acc-0.4400, valid loss-1.9317, acc-0.4674, test loss-1.9326, acc-0.4582\n",
      "Iter-38800, train loss-2.0161, acc-0.4000, valid loss-1.9317, acc-0.4678, test loss-1.9325, acc-0.4583\n",
      "Iter-38810, train loss-1.9636, acc-0.3400, valid loss-1.9316, acc-0.4674, test loss-1.9324, acc-0.4584\n",
      "Iter-38820, train loss-1.9104, acc-0.5400, valid loss-1.9315, acc-0.4674, test loss-1.9323, acc-0.4583\n",
      "Iter-38830, train loss-1.9092, acc-0.5800, valid loss-1.9315, acc-0.4676, test loss-1.9323, acc-0.4585\n",
      "Iter-38840, train loss-1.9202, acc-0.5800, valid loss-1.9314, acc-0.4678, test loss-1.9322, acc-0.4584\n",
      "Iter-38850, train loss-1.8653, acc-0.6000, valid loss-1.9313, acc-0.4680, test loss-1.9321, acc-0.4585\n",
      "Iter-38860, train loss-1.9420, acc-0.4600, valid loss-1.9312, acc-0.4680, test loss-1.9321, acc-0.4584\n",
      "Iter-38870, train loss-1.9158, acc-0.4800, valid loss-1.9312, acc-0.4682, test loss-1.9320, acc-0.4584\n",
      "Iter-38880, train loss-1.9609, acc-0.4600, valid loss-1.9311, acc-0.4684, test loss-1.9319, acc-0.4586\n",
      "Iter-38890, train loss-2.0601, acc-0.2800, valid loss-1.9310, acc-0.4682, test loss-1.9319, acc-0.4588\n",
      "Iter-38900, train loss-2.0520, acc-0.3200, valid loss-1.9310, acc-0.4684, test loss-1.9318, acc-0.4588\n",
      "Iter-38910, train loss-1.9907, acc-0.4200, valid loss-1.9309, acc-0.4686, test loss-1.9317, acc-0.4589\n",
      "Iter-38920, train loss-1.9344, acc-0.4400, valid loss-1.9308, acc-0.4686, test loss-1.9317, acc-0.4591\n",
      "Iter-38930, train loss-1.9903, acc-0.4600, valid loss-1.9308, acc-0.4684, test loss-1.9316, acc-0.4590\n",
      "Iter-38940, train loss-1.9634, acc-0.4000, valid loss-1.9307, acc-0.4682, test loss-1.9315, acc-0.4589\n",
      "Iter-38950, train loss-1.9184, acc-0.5200, valid loss-1.9306, acc-0.4680, test loss-1.9315, acc-0.4590\n",
      "Iter-38960, train loss-1.9177, acc-0.5000, valid loss-1.9306, acc-0.4678, test loss-1.9314, acc-0.4589\n",
      "Iter-38970, train loss-1.8552, acc-0.5400, valid loss-1.9305, acc-0.4684, test loss-1.9313, acc-0.4588\n",
      "Iter-38980, train loss-1.8984, acc-0.5200, valid loss-1.9304, acc-0.4682, test loss-1.9312, acc-0.4589\n",
      "Iter-38990, train loss-1.9711, acc-0.4400, valid loss-1.9303, acc-0.4682, test loss-1.9312, acc-0.4591\n",
      "Iter-39000, train loss-1.9519, acc-0.5200, valid loss-1.9303, acc-0.4682, test loss-1.9311, acc-0.4589\n",
      "Iter-39010, train loss-1.9045, acc-0.4800, valid loss-1.9302, acc-0.4682, test loss-1.9310, acc-0.4587\n",
      "Iter-39020, train loss-1.9427, acc-0.4200, valid loss-1.9301, acc-0.4682, test loss-1.9310, acc-0.4590\n",
      "Iter-39030, train loss-1.8954, acc-0.4400, valid loss-1.9301, acc-0.4682, test loss-1.9309, acc-0.4588\n",
      "Iter-39040, train loss-1.9814, acc-0.3800, valid loss-1.9300, acc-0.4684, test loss-1.9308, acc-0.4586\n",
      "Iter-39050, train loss-1.9176, acc-0.4400, valid loss-1.9299, acc-0.4682, test loss-1.9308, acc-0.4586\n",
      "Iter-39060, train loss-1.8891, acc-0.5000, valid loss-1.9299, acc-0.4684, test loss-1.9307, acc-0.4588\n",
      "Iter-39070, train loss-2.0054, acc-0.3400, valid loss-1.9298, acc-0.4682, test loss-1.9306, acc-0.4588\n",
      "Iter-39080, train loss-1.9676, acc-0.4400, valid loss-1.9297, acc-0.4684, test loss-1.9306, acc-0.4586\n",
      "Iter-39090, train loss-1.9945, acc-0.3800, valid loss-1.9297, acc-0.4684, test loss-1.9305, acc-0.4586\n",
      "Iter-39100, train loss-1.9515, acc-0.4800, valid loss-1.9296, acc-0.4686, test loss-1.9304, acc-0.4586\n",
      "Iter-39110, train loss-1.9235, acc-0.4800, valid loss-1.9295, acc-0.4686, test loss-1.9304, acc-0.4588\n",
      "Iter-39120, train loss-1.9899, acc-0.4000, valid loss-1.9294, acc-0.4686, test loss-1.9303, acc-0.4590\n",
      "Iter-39130, train loss-1.9553, acc-0.5000, valid loss-1.9294, acc-0.4686, test loss-1.9302, acc-0.4587\n",
      "Iter-39140, train loss-2.0056, acc-0.3800, valid loss-1.9293, acc-0.4686, test loss-1.9301, acc-0.4588\n",
      "Iter-39150, train loss-1.9981, acc-0.4600, valid loss-1.9292, acc-0.4684, test loss-1.9301, acc-0.4590\n",
      "Iter-39160, train loss-1.8880, acc-0.5400, valid loss-1.9292, acc-0.4686, test loss-1.9300, acc-0.4591\n",
      "Iter-39170, train loss-1.9415, acc-0.4200, valid loss-1.9291, acc-0.4684, test loss-1.9299, acc-0.4590\n",
      "Iter-39180, train loss-1.9091, acc-0.4400, valid loss-1.9290, acc-0.4684, test loss-1.9299, acc-0.4591\n",
      "Iter-39190, train loss-1.9253, acc-0.3600, valid loss-1.9290, acc-0.4684, test loss-1.9298, acc-0.4589\n",
      "Iter-39200, train loss-1.9373, acc-0.5600, valid loss-1.9289, acc-0.4684, test loss-1.9297, acc-0.4590\n",
      "Iter-39210, train loss-1.9347, acc-0.4800, valid loss-1.9288, acc-0.4684, test loss-1.9297, acc-0.4591\n",
      "Iter-39220, train loss-1.9415, acc-0.4400, valid loss-1.9287, acc-0.4684, test loss-1.9296, acc-0.4589\n",
      "Iter-39230, train loss-1.9719, acc-0.3800, valid loss-1.9287, acc-0.4684, test loss-1.9295, acc-0.4592\n",
      "Iter-39240, train loss-1.9839, acc-0.4200, valid loss-1.9286, acc-0.4684, test loss-1.9295, acc-0.4590\n",
      "Iter-39250, train loss-1.9076, acc-0.4800, valid loss-1.9285, acc-0.4684, test loss-1.9294, acc-0.4589\n",
      "Iter-39260, train loss-1.9660, acc-0.4600, valid loss-1.9285, acc-0.4684, test loss-1.9293, acc-0.4589\n",
      "Iter-39270, train loss-1.9396, acc-0.4600, valid loss-1.9284, acc-0.4684, test loss-1.9293, acc-0.4591\n",
      "Iter-39280, train loss-1.9080, acc-0.4600, valid loss-1.9283, acc-0.4684, test loss-1.9292, acc-0.4587\n",
      "Iter-39290, train loss-1.9872, acc-0.4600, valid loss-1.9283, acc-0.4684, test loss-1.9291, acc-0.4585\n",
      "Iter-39300, train loss-1.8790, acc-0.5200, valid loss-1.9282, acc-0.4684, test loss-1.9291, acc-0.4583\n",
      "Iter-39310, train loss-1.9980, acc-0.4000, valid loss-1.9281, acc-0.4690, test loss-1.9290, acc-0.4587\n",
      "Iter-39320, train loss-1.9431, acc-0.3400, valid loss-1.9281, acc-0.4692, test loss-1.9289, acc-0.4586\n",
      "Iter-39330, train loss-1.9576, acc-0.3200, valid loss-1.9280, acc-0.4688, test loss-1.9289, acc-0.4588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-39340, train loss-2.0054, acc-0.4000, valid loss-1.9279, acc-0.4694, test loss-1.9288, acc-0.4590\n",
      "Iter-39350, train loss-1.9574, acc-0.3400, valid loss-1.9279, acc-0.4694, test loss-1.9287, acc-0.4587\n",
      "Iter-39360, train loss-1.8852, acc-0.4800, valid loss-1.9278, acc-0.4694, test loss-1.9287, acc-0.4588\n",
      "Iter-39370, train loss-1.9959, acc-0.3800, valid loss-1.9277, acc-0.4694, test loss-1.9286, acc-0.4588\n",
      "Iter-39380, train loss-1.9081, acc-0.5600, valid loss-1.9277, acc-0.4694, test loss-1.9285, acc-0.4588\n",
      "Iter-39390, train loss-1.9193, acc-0.5200, valid loss-1.9276, acc-0.4694, test loss-1.9285, acc-0.4589\n",
      "Iter-39400, train loss-1.9063, acc-0.4400, valid loss-1.9275, acc-0.4698, test loss-1.9284, acc-0.4587\n",
      "Iter-39410, train loss-1.8957, acc-0.4800, valid loss-1.9274, acc-0.4696, test loss-1.9283, acc-0.4586\n",
      "Iter-39420, train loss-1.9223, acc-0.4600, valid loss-1.9274, acc-0.4696, test loss-1.9282, acc-0.4586\n",
      "Iter-39430, train loss-1.9494, acc-0.4800, valid loss-1.9273, acc-0.4698, test loss-1.9282, acc-0.4586\n",
      "Iter-39440, train loss-1.8893, acc-0.5200, valid loss-1.9272, acc-0.4696, test loss-1.9281, acc-0.4586\n",
      "Iter-39450, train loss-1.8140, acc-0.5200, valid loss-1.9272, acc-0.4696, test loss-1.9280, acc-0.4587\n",
      "Iter-39460, train loss-1.9199, acc-0.5200, valid loss-1.9271, acc-0.4698, test loss-1.9280, acc-0.4588\n",
      "Iter-39470, train loss-1.9164, acc-0.4800, valid loss-1.9270, acc-0.4698, test loss-1.9279, acc-0.4591\n",
      "Iter-39480, train loss-1.9370, acc-0.5000, valid loss-1.9270, acc-0.4698, test loss-1.9278, acc-0.4592\n",
      "Iter-39490, train loss-1.9509, acc-0.3800, valid loss-1.9269, acc-0.4698, test loss-1.9278, acc-0.4588\n",
      "Iter-39500, train loss-1.8350, acc-0.5200, valid loss-1.9268, acc-0.4700, test loss-1.9277, acc-0.4587\n",
      "Iter-39510, train loss-2.0143, acc-0.3800, valid loss-1.9268, acc-0.4700, test loss-1.9276, acc-0.4589\n",
      "Iter-39520, train loss-1.8916, acc-0.5200, valid loss-1.9267, acc-0.4700, test loss-1.9276, acc-0.4588\n",
      "Iter-39530, train loss-1.9847, acc-0.4000, valid loss-1.9266, acc-0.4696, test loss-1.9275, acc-0.4590\n",
      "Iter-39540, train loss-1.9631, acc-0.3800, valid loss-1.9266, acc-0.4702, test loss-1.9274, acc-0.4590\n",
      "Iter-39550, train loss-1.9289, acc-0.5200, valid loss-1.9265, acc-0.4700, test loss-1.9274, acc-0.4589\n",
      "Iter-39560, train loss-1.9954, acc-0.3600, valid loss-1.9264, acc-0.4700, test loss-1.9273, acc-0.4589\n",
      "Iter-39570, train loss-1.9769, acc-0.4000, valid loss-1.9264, acc-0.4702, test loss-1.9272, acc-0.4588\n",
      "Iter-39580, train loss-1.9591, acc-0.4200, valid loss-1.9263, acc-0.4702, test loss-1.9272, acc-0.4587\n",
      "Iter-39590, train loss-1.9079, acc-0.5200, valid loss-1.9262, acc-0.4704, test loss-1.9271, acc-0.4588\n",
      "Iter-39600, train loss-1.9202, acc-0.4800, valid loss-1.9261, acc-0.4700, test loss-1.9270, acc-0.4587\n",
      "Iter-39610, train loss-2.0295, acc-0.3200, valid loss-1.9261, acc-0.4706, test loss-1.9270, acc-0.4587\n",
      "Iter-39620, train loss-1.9862, acc-0.4000, valid loss-1.9260, acc-0.4706, test loss-1.9269, acc-0.4588\n",
      "Iter-39630, train loss-1.9860, acc-0.3600, valid loss-1.9259, acc-0.4706, test loss-1.9268, acc-0.4589\n",
      "Iter-39640, train loss-1.8783, acc-0.5400, valid loss-1.9259, acc-0.4706, test loss-1.9268, acc-0.4588\n",
      "Iter-39650, train loss-1.9530, acc-0.4800, valid loss-1.9258, acc-0.4704, test loss-1.9267, acc-0.4589\n",
      "Iter-39660, train loss-1.9933, acc-0.3600, valid loss-1.9257, acc-0.4708, test loss-1.9266, acc-0.4589\n",
      "Iter-39670, train loss-1.9067, acc-0.5600, valid loss-1.9257, acc-0.4704, test loss-1.9265, acc-0.4589\n",
      "Iter-39680, train loss-1.8695, acc-0.4800, valid loss-1.9256, acc-0.4700, test loss-1.9265, acc-0.4590\n",
      "Iter-39690, train loss-2.0062, acc-0.3800, valid loss-1.9255, acc-0.4702, test loss-1.9264, acc-0.4588\n",
      "Iter-39700, train loss-1.8831, acc-0.4400, valid loss-1.9255, acc-0.4704, test loss-1.9263, acc-0.4588\n",
      "Iter-39710, train loss-1.9507, acc-0.4400, valid loss-1.9254, acc-0.4704, test loss-1.9263, acc-0.4589\n",
      "Iter-39720, train loss-1.8749, acc-0.5000, valid loss-1.9253, acc-0.4706, test loss-1.9262, acc-0.4591\n",
      "Iter-39730, train loss-1.9523, acc-0.3600, valid loss-1.9253, acc-0.4706, test loss-1.9261, acc-0.4591\n",
      "Iter-39740, train loss-1.9335, acc-0.3800, valid loss-1.9252, acc-0.4708, test loss-1.9261, acc-0.4591\n",
      "Iter-39750, train loss-2.0120, acc-0.4200, valid loss-1.9251, acc-0.4708, test loss-1.9260, acc-0.4592\n",
      "Iter-39760, train loss-1.9191, acc-0.4200, valid loss-1.9251, acc-0.4706, test loss-1.9259, acc-0.4590\n",
      "Iter-39770, train loss-1.9846, acc-0.4600, valid loss-1.9250, acc-0.4708, test loss-1.9259, acc-0.4593\n",
      "Iter-39780, train loss-1.9128, acc-0.4200, valid loss-1.9249, acc-0.4708, test loss-1.9258, acc-0.4593\n",
      "Iter-39790, train loss-1.9010, acc-0.5200, valid loss-1.9249, acc-0.4710, test loss-1.9257, acc-0.4593\n",
      "Iter-39800, train loss-1.9782, acc-0.3000, valid loss-1.9248, acc-0.4708, test loss-1.9257, acc-0.4595\n",
      "Iter-39810, train loss-1.9580, acc-0.5000, valid loss-1.9247, acc-0.4712, test loss-1.9256, acc-0.4593\n",
      "Iter-39820, train loss-1.9266, acc-0.4600, valid loss-1.9247, acc-0.4712, test loss-1.9255, acc-0.4594\n",
      "Iter-39830, train loss-2.0094, acc-0.4200, valid loss-1.9246, acc-0.4712, test loss-1.9255, acc-0.4593\n",
      "Iter-39840, train loss-1.9950, acc-0.4000, valid loss-1.9245, acc-0.4710, test loss-1.9254, acc-0.4597\n",
      "Iter-39850, train loss-1.9779, acc-0.3400, valid loss-1.9245, acc-0.4710, test loss-1.9253, acc-0.4595\n",
      "Iter-39860, train loss-1.8942, acc-0.4400, valid loss-1.9244, acc-0.4708, test loss-1.9253, acc-0.4595\n",
      "Iter-39870, train loss-1.9621, acc-0.4200, valid loss-1.9243, acc-0.4710, test loss-1.9252, acc-0.4595\n",
      "Iter-39880, train loss-1.8897, acc-0.5000, valid loss-1.9242, acc-0.4710, test loss-1.9251, acc-0.4593\n",
      "Iter-39890, train loss-1.9059, acc-0.5200, valid loss-1.9242, acc-0.4710, test loss-1.9251, acc-0.4597\n",
      "Iter-39900, train loss-1.9462, acc-0.4600, valid loss-1.9241, acc-0.4710, test loss-1.9250, acc-0.4597\n",
      "Iter-39910, train loss-1.9265, acc-0.5200, valid loss-1.9240, acc-0.4712, test loss-1.9249, acc-0.4597\n",
      "Iter-39920, train loss-2.0058, acc-0.3400, valid loss-1.9240, acc-0.4710, test loss-1.9249, acc-0.4599\n",
      "Iter-39930, train loss-1.9045, acc-0.4200, valid loss-1.9239, acc-0.4712, test loss-1.9248, acc-0.4601\n",
      "Iter-39940, train loss-1.9066, acc-0.4400, valid loss-1.9238, acc-0.4714, test loss-1.9247, acc-0.4599\n",
      "Iter-39950, train loss-1.8992, acc-0.5200, valid loss-1.9238, acc-0.4714, test loss-1.9247, acc-0.4600\n",
      "Iter-39960, train loss-1.9859, acc-0.3800, valid loss-1.9237, acc-0.4714, test loss-1.9246, acc-0.4600\n",
      "Iter-39970, train loss-1.8922, acc-0.4400, valid loss-1.9236, acc-0.4714, test loss-1.9245, acc-0.4601\n",
      "Iter-39980, train loss-1.9325, acc-0.4200, valid loss-1.9236, acc-0.4714, test loss-1.9245, acc-0.4600\n",
      "Iter-39990, train loss-1.8954, acc-0.5600, valid loss-1.9235, acc-0.4714, test loss-1.9244, acc-0.4599\n",
      "Iter-40000, train loss-1.9085, acc-0.5400, valid loss-1.9234, acc-0.4714, test loss-1.9243, acc-0.4600\n",
      "Iter-40010, train loss-1.8894, acc-0.4600, valid loss-1.9234, acc-0.4712, test loss-1.9243, acc-0.4600\n",
      "Iter-40020, train loss-1.9480, acc-0.4400, valid loss-1.9233, acc-0.4714, test loss-1.9242, acc-0.4601\n",
      "Iter-40030, train loss-1.9046, acc-0.5200, valid loss-1.9232, acc-0.4712, test loss-1.9241, acc-0.4601\n",
      "Iter-40040, train loss-1.9385, acc-0.5400, valid loss-1.9232, acc-0.4712, test loss-1.9241, acc-0.4603\n",
      "Iter-40050, train loss-1.9930, acc-0.3800, valid loss-1.9231, acc-0.4712, test loss-1.9240, acc-0.4602\n",
      "Iter-40060, train loss-1.8930, acc-0.5000, valid loss-1.9230, acc-0.4714, test loss-1.9239, acc-0.4604\n",
      "Iter-40070, train loss-1.9142, acc-0.5000, valid loss-1.9230, acc-0.4714, test loss-1.9239, acc-0.4603\n",
      "Iter-40080, train loss-1.9649, acc-0.5000, valid loss-1.9229, acc-0.4714, test loss-1.9238, acc-0.4603\n",
      "Iter-40090, train loss-1.9654, acc-0.4600, valid loss-1.9228, acc-0.4716, test loss-1.9237, acc-0.4604\n",
      "Iter-40100, train loss-1.9578, acc-0.4600, valid loss-1.9228, acc-0.4716, test loss-1.9237, acc-0.4603\n",
      "Iter-40110, train loss-1.8623, acc-0.5400, valid loss-1.9227, acc-0.4716, test loss-1.9236, acc-0.4604\n",
      "Iter-40120, train loss-1.9404, acc-0.5400, valid loss-1.9226, acc-0.4718, test loss-1.9235, acc-0.4605\n",
      "Iter-40130, train loss-1.9257, acc-0.5000, valid loss-1.9226, acc-0.4714, test loss-1.9234, acc-0.4608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-40140, train loss-1.8686, acc-0.5600, valid loss-1.9225, acc-0.4718, test loss-1.9234, acc-0.4606\n",
      "Iter-40150, train loss-1.8957, acc-0.5000, valid loss-1.9224, acc-0.4716, test loss-1.9233, acc-0.4605\n",
      "Iter-40160, train loss-1.8544, acc-0.5400, valid loss-1.9224, acc-0.4716, test loss-1.9232, acc-0.4606\n",
      "Iter-40170, train loss-1.8820, acc-0.6400, valid loss-1.9223, acc-0.4716, test loss-1.9232, acc-0.4608\n",
      "Iter-40180, train loss-1.9878, acc-0.3200, valid loss-1.9222, acc-0.4716, test loss-1.9231, acc-0.4605\n",
      "Iter-40190, train loss-1.9289, acc-0.4200, valid loss-1.9222, acc-0.4716, test loss-1.9230, acc-0.4603\n",
      "Iter-40200, train loss-1.9053, acc-0.4800, valid loss-1.9221, acc-0.4716, test loss-1.9230, acc-0.4604\n",
      "Iter-40210, train loss-1.9420, acc-0.5200, valid loss-1.9220, acc-0.4718, test loss-1.9229, acc-0.4609\n",
      "Iter-40220, train loss-1.8893, acc-0.4800, valid loss-1.9220, acc-0.4718, test loss-1.9228, acc-0.4608\n",
      "Iter-40230, train loss-1.9316, acc-0.5600, valid loss-1.9219, acc-0.4718, test loss-1.9228, acc-0.4607\n",
      "Iter-40240, train loss-1.9572, acc-0.4400, valid loss-1.9218, acc-0.4720, test loss-1.9227, acc-0.4608\n",
      "Iter-40250, train loss-1.9740, acc-0.3800, valid loss-1.9217, acc-0.4720, test loss-1.9226, acc-0.4610\n",
      "Iter-40260, train loss-1.9325, acc-0.4400, valid loss-1.9217, acc-0.4720, test loss-1.9226, acc-0.4609\n",
      "Iter-40270, train loss-1.9350, acc-0.4600, valid loss-1.9216, acc-0.4720, test loss-1.9225, acc-0.4610\n",
      "Iter-40280, train loss-1.9128, acc-0.5000, valid loss-1.9215, acc-0.4720, test loss-1.9224, acc-0.4611\n",
      "Iter-40290, train loss-1.9566, acc-0.3600, valid loss-1.9215, acc-0.4720, test loss-1.9224, acc-0.4611\n",
      "Iter-40300, train loss-1.8846, acc-0.5800, valid loss-1.9214, acc-0.4718, test loss-1.9223, acc-0.4609\n",
      "Iter-40310, train loss-1.8876, acc-0.5000, valid loss-1.9213, acc-0.4720, test loss-1.9222, acc-0.4610\n",
      "Iter-40320, train loss-1.8893, acc-0.5000, valid loss-1.9213, acc-0.4722, test loss-1.9222, acc-0.4609\n",
      "Iter-40330, train loss-1.8805, acc-0.5400, valid loss-1.9212, acc-0.4720, test loss-1.9221, acc-0.4610\n",
      "Iter-40340, train loss-1.9249, acc-0.4200, valid loss-1.9211, acc-0.4720, test loss-1.9220, acc-0.4612\n",
      "Iter-40350, train loss-1.9580, acc-0.4400, valid loss-1.9211, acc-0.4718, test loss-1.9220, acc-0.4615\n",
      "Iter-40360, train loss-1.9674, acc-0.4800, valid loss-1.9210, acc-0.4716, test loss-1.9219, acc-0.4612\n",
      "Iter-40370, train loss-1.9361, acc-0.4000, valid loss-1.9209, acc-0.4720, test loss-1.9218, acc-0.4614\n",
      "Iter-40380, train loss-1.9967, acc-0.4800, valid loss-1.9209, acc-0.4720, test loss-1.9218, acc-0.4613\n",
      "Iter-40390, train loss-2.0055, acc-0.3800, valid loss-1.9208, acc-0.4718, test loss-1.9217, acc-0.4612\n",
      "Iter-40400, train loss-1.9025, acc-0.4800, valid loss-1.9207, acc-0.4718, test loss-1.9216, acc-0.4613\n",
      "Iter-40410, train loss-1.8837, acc-0.4600, valid loss-1.9207, acc-0.4720, test loss-1.9216, acc-0.4614\n",
      "Iter-40420, train loss-1.9780, acc-0.3600, valid loss-1.9206, acc-0.4718, test loss-1.9215, acc-0.4617\n",
      "Iter-40430, train loss-1.9226, acc-0.4600, valid loss-1.9205, acc-0.4722, test loss-1.9214, acc-0.4618\n",
      "Iter-40440, train loss-1.9336, acc-0.4800, valid loss-1.9205, acc-0.4720, test loss-1.9214, acc-0.4618\n",
      "Iter-40450, train loss-1.8989, acc-0.4000, valid loss-1.9204, acc-0.4722, test loss-1.9213, acc-0.4616\n",
      "Iter-40460, train loss-1.9542, acc-0.4600, valid loss-1.9203, acc-0.4720, test loss-1.9212, acc-0.4616\n",
      "Iter-40470, train loss-2.0434, acc-0.3800, valid loss-1.9203, acc-0.4720, test loss-1.9212, acc-0.4617\n",
      "Iter-40480, train loss-1.9172, acc-0.5400, valid loss-1.9202, acc-0.4720, test loss-1.9211, acc-0.4616\n",
      "Iter-40490, train loss-1.8650, acc-0.5000, valid loss-1.9201, acc-0.4720, test loss-1.9210, acc-0.4615\n",
      "Iter-40500, train loss-1.9316, acc-0.4600, valid loss-1.9201, acc-0.4720, test loss-1.9210, acc-0.4615\n",
      "Iter-40510, train loss-1.9742, acc-0.4600, valid loss-1.9200, acc-0.4720, test loss-1.9209, acc-0.4617\n",
      "Iter-40520, train loss-1.9591, acc-0.3800, valid loss-1.9199, acc-0.4720, test loss-1.9208, acc-0.4619\n",
      "Iter-40530, train loss-1.9846, acc-0.3400, valid loss-1.9199, acc-0.4720, test loss-1.9208, acc-0.4620\n",
      "Iter-40540, train loss-1.8443, acc-0.5800, valid loss-1.9198, acc-0.4722, test loss-1.9207, acc-0.4620\n",
      "Iter-40550, train loss-1.9922, acc-0.3000, valid loss-1.9197, acc-0.4720, test loss-1.9206, acc-0.4620\n",
      "Iter-40560, train loss-1.8364, acc-0.5600, valid loss-1.9197, acc-0.4722, test loss-1.9206, acc-0.4619\n",
      "Iter-40570, train loss-1.9232, acc-0.5200, valid loss-1.9196, acc-0.4720, test loss-1.9205, acc-0.4618\n",
      "Iter-40580, train loss-1.9175, acc-0.5000, valid loss-1.9195, acc-0.4720, test loss-1.9204, acc-0.4619\n",
      "Iter-40590, train loss-1.9553, acc-0.4400, valid loss-1.9195, acc-0.4720, test loss-1.9204, acc-0.4623\n",
      "Iter-40600, train loss-1.8035, acc-0.6600, valid loss-1.9194, acc-0.4720, test loss-1.9203, acc-0.4622\n",
      "Iter-40610, train loss-1.9045, acc-0.6200, valid loss-1.9193, acc-0.4720, test loss-1.9202, acc-0.4620\n",
      "Iter-40620, train loss-1.9531, acc-0.4600, valid loss-1.9193, acc-0.4720, test loss-1.9202, acc-0.4622\n",
      "Iter-40630, train loss-1.9121, acc-0.5800, valid loss-1.9192, acc-0.4720, test loss-1.9201, acc-0.4622\n",
      "Iter-40640, train loss-1.9879, acc-0.4000, valid loss-1.9191, acc-0.4722, test loss-1.9200, acc-0.4620\n",
      "Iter-40650, train loss-1.9612, acc-0.4000, valid loss-1.9191, acc-0.4720, test loss-1.9200, acc-0.4624\n",
      "Iter-40660, train loss-1.9562, acc-0.3400, valid loss-1.9190, acc-0.4720, test loss-1.9199, acc-0.4622\n",
      "Iter-40670, train loss-1.9404, acc-0.3600, valid loss-1.9189, acc-0.4722, test loss-1.9198, acc-0.4623\n",
      "Iter-40680, train loss-1.9473, acc-0.5600, valid loss-1.9189, acc-0.4722, test loss-1.9198, acc-0.4622\n",
      "Iter-40690, train loss-1.8242, acc-0.5600, valid loss-1.9188, acc-0.4722, test loss-1.9197, acc-0.4626\n",
      "Iter-40700, train loss-1.9955, acc-0.3000, valid loss-1.9187, acc-0.4722, test loss-1.9196, acc-0.4622\n",
      "Iter-40710, train loss-1.9525, acc-0.5000, valid loss-1.9187, acc-0.4722, test loss-1.9196, acc-0.4623\n",
      "Iter-40720, train loss-1.8956, acc-0.5000, valid loss-1.9186, acc-0.4720, test loss-1.9195, acc-0.4622\n",
      "Iter-40730, train loss-1.9585, acc-0.4400, valid loss-1.9185, acc-0.4720, test loss-1.9194, acc-0.4624\n",
      "Iter-40740, train loss-1.9536, acc-0.5400, valid loss-1.9185, acc-0.4722, test loss-1.9194, acc-0.4629\n",
      "Iter-40750, train loss-1.9632, acc-0.4200, valid loss-1.9184, acc-0.4720, test loss-1.9193, acc-0.4628\n",
      "Iter-40760, train loss-1.9798, acc-0.3800, valid loss-1.9183, acc-0.4724, test loss-1.9192, acc-0.4627\n",
      "Iter-40770, train loss-1.9111, acc-0.3800, valid loss-1.9183, acc-0.4724, test loss-1.9192, acc-0.4628\n",
      "Iter-40780, train loss-1.9521, acc-0.4600, valid loss-1.9182, acc-0.4722, test loss-1.9191, acc-0.4631\n",
      "Iter-40790, train loss-1.9451, acc-0.4600, valid loss-1.9181, acc-0.4724, test loss-1.9190, acc-0.4630\n",
      "Iter-40800, train loss-1.9381, acc-0.4400, valid loss-1.9181, acc-0.4722, test loss-1.9190, acc-0.4631\n",
      "Iter-40810, train loss-2.0042, acc-0.4000, valid loss-1.9180, acc-0.4720, test loss-1.9189, acc-0.4630\n",
      "Iter-40820, train loss-1.8604, acc-0.5000, valid loss-1.9179, acc-0.4724, test loss-1.9188, acc-0.4630\n",
      "Iter-40830, train loss-1.8547, acc-0.4400, valid loss-1.9179, acc-0.4726, test loss-1.9188, acc-0.4628\n",
      "Iter-40840, train loss-1.9660, acc-0.4400, valid loss-1.9178, acc-0.4726, test loss-1.9187, acc-0.4627\n",
      "Iter-40850, train loss-2.0262, acc-0.3600, valid loss-1.9177, acc-0.4726, test loss-1.9186, acc-0.4626\n",
      "Iter-40860, train loss-1.9082, acc-0.4600, valid loss-1.9177, acc-0.4724, test loss-1.9186, acc-0.4629\n",
      "Iter-40870, train loss-1.9095, acc-0.4400, valid loss-1.9176, acc-0.4726, test loss-1.9185, acc-0.4628\n",
      "Iter-40880, train loss-1.9067, acc-0.4600, valid loss-1.9175, acc-0.4730, test loss-1.9184, acc-0.4630\n",
      "Iter-40890, train loss-1.8933, acc-0.4000, valid loss-1.9175, acc-0.4730, test loss-1.9184, acc-0.4629\n",
      "Iter-40900, train loss-1.9072, acc-0.4800, valid loss-1.9174, acc-0.4728, test loss-1.9183, acc-0.4627\n",
      "Iter-40910, train loss-1.8913, acc-0.4200, valid loss-1.9173, acc-0.4726, test loss-1.9182, acc-0.4629\n",
      "Iter-40920, train loss-1.8454, acc-0.5200, valid loss-1.9173, acc-0.4730, test loss-1.9182, acc-0.4628\n",
      "Iter-40930, train loss-1.8973, acc-0.4600, valid loss-1.9172, acc-0.4732, test loss-1.9181, acc-0.4627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-40940, train loss-1.8572, acc-0.5600, valid loss-1.9171, acc-0.4734, test loss-1.9180, acc-0.4628\n",
      "Iter-40950, train loss-1.9556, acc-0.4800, valid loss-1.9171, acc-0.4732, test loss-1.9180, acc-0.4628\n",
      "Iter-40960, train loss-1.8660, acc-0.5200, valid loss-1.9170, acc-0.4730, test loss-1.9179, acc-0.4628\n",
      "Iter-40970, train loss-1.8464, acc-0.5400, valid loss-1.9169, acc-0.4728, test loss-1.9178, acc-0.4628\n",
      "Iter-40980, train loss-1.8948, acc-0.5400, valid loss-1.9169, acc-0.4730, test loss-1.9178, acc-0.4628\n",
      "Iter-40990, train loss-1.9215, acc-0.4200, valid loss-1.9168, acc-0.4730, test loss-1.9177, acc-0.4629\n",
      "Iter-41000, train loss-1.9442, acc-0.4600, valid loss-1.9167, acc-0.4730, test loss-1.9176, acc-0.4628\n",
      "Iter-41010, train loss-1.8533, acc-0.5000, valid loss-1.9167, acc-0.4730, test loss-1.9176, acc-0.4629\n",
      "Iter-41020, train loss-1.9759, acc-0.4000, valid loss-1.9166, acc-0.4730, test loss-1.9175, acc-0.4629\n",
      "Iter-41030, train loss-1.9381, acc-0.4400, valid loss-1.9165, acc-0.4728, test loss-1.9174, acc-0.4627\n",
      "Iter-41040, train loss-1.9233, acc-0.4400, valid loss-1.9165, acc-0.4728, test loss-1.9174, acc-0.4630\n",
      "Iter-41050, train loss-1.9488, acc-0.4400, valid loss-1.9164, acc-0.4732, test loss-1.9173, acc-0.4628\n",
      "Iter-41060, train loss-1.9275, acc-0.4600, valid loss-1.9163, acc-0.4728, test loss-1.9172, acc-0.4628\n",
      "Iter-41070, train loss-1.8911, acc-0.5200, valid loss-1.9163, acc-0.4730, test loss-1.9172, acc-0.4629\n",
      "Iter-41080, train loss-1.9766, acc-0.5000, valid loss-1.9162, acc-0.4730, test loss-1.9171, acc-0.4629\n",
      "Iter-41090, train loss-1.9369, acc-0.4600, valid loss-1.9161, acc-0.4730, test loss-1.9170, acc-0.4629\n",
      "Iter-41100, train loss-2.0241, acc-0.3600, valid loss-1.9161, acc-0.4732, test loss-1.9170, acc-0.4631\n",
      "Iter-41110, train loss-1.8755, acc-0.5400, valid loss-1.9160, acc-0.4730, test loss-1.9169, acc-0.4631\n",
      "Iter-41120, train loss-1.9377, acc-0.5000, valid loss-1.9159, acc-0.4732, test loss-1.9168, acc-0.4628\n",
      "Iter-41130, train loss-1.9647, acc-0.4400, valid loss-1.9159, acc-0.4732, test loss-1.9168, acc-0.4629\n",
      "Iter-41140, train loss-1.8652, acc-0.5200, valid loss-1.9158, acc-0.4732, test loss-1.9167, acc-0.4629\n",
      "Iter-41150, train loss-1.9725, acc-0.3800, valid loss-1.9157, acc-0.4732, test loss-1.9166, acc-0.4630\n",
      "Iter-41160, train loss-1.9630, acc-0.4200, valid loss-1.9157, acc-0.4732, test loss-1.9166, acc-0.4629\n",
      "Iter-41170, train loss-1.9565, acc-0.4200, valid loss-1.9156, acc-0.4734, test loss-1.9165, acc-0.4630\n",
      "Iter-41180, train loss-1.8045, acc-0.6000, valid loss-1.9155, acc-0.4734, test loss-1.9164, acc-0.4630\n",
      "Iter-41190, train loss-1.9608, acc-0.4600, valid loss-1.9155, acc-0.4734, test loss-1.9164, acc-0.4631\n",
      "Iter-41200, train loss-1.9077, acc-0.5200, valid loss-1.9154, acc-0.4734, test loss-1.9163, acc-0.4631\n",
      "Iter-41210, train loss-1.9492, acc-0.3400, valid loss-1.9153, acc-0.4732, test loss-1.9162, acc-0.4632\n",
      "Iter-41220, train loss-1.9031, acc-0.5600, valid loss-1.9153, acc-0.4734, test loss-1.9162, acc-0.4633\n",
      "Iter-41230, train loss-1.8603, acc-0.4400, valid loss-1.9152, acc-0.4730, test loss-1.9161, acc-0.4633\n",
      "Iter-41240, train loss-1.9390, acc-0.3800, valid loss-1.9151, acc-0.4730, test loss-1.9160, acc-0.4633\n",
      "Iter-41250, train loss-1.9327, acc-0.4400, valid loss-1.9151, acc-0.4732, test loss-1.9160, acc-0.4635\n",
      "Iter-41260, train loss-1.9836, acc-0.3600, valid loss-1.9150, acc-0.4732, test loss-1.9159, acc-0.4635\n",
      "Iter-41270, train loss-1.9090, acc-0.4400, valid loss-1.9149, acc-0.4732, test loss-1.9158, acc-0.4637\n",
      "Iter-41280, train loss-1.9046, acc-0.4400, valid loss-1.9149, acc-0.4730, test loss-1.9158, acc-0.4636\n",
      "Iter-41290, train loss-1.9507, acc-0.5000, valid loss-1.9148, acc-0.4732, test loss-1.9157, acc-0.4634\n",
      "Iter-41300, train loss-1.9184, acc-0.5400, valid loss-1.9148, acc-0.4730, test loss-1.9156, acc-0.4635\n",
      "Iter-41310, train loss-1.9708, acc-0.3400, valid loss-1.9147, acc-0.4730, test loss-1.9156, acc-0.4638\n",
      "Iter-41320, train loss-1.9453, acc-0.4800, valid loss-1.9146, acc-0.4730, test loss-1.9155, acc-0.4638\n",
      "Iter-41330, train loss-1.8910, acc-0.4200, valid loss-1.9146, acc-0.4730, test loss-1.9154, acc-0.4637\n",
      "Iter-41340, train loss-1.9259, acc-0.4000, valid loss-1.9145, acc-0.4730, test loss-1.9154, acc-0.4639\n",
      "Iter-41350, train loss-1.8929, acc-0.4400, valid loss-1.9144, acc-0.4732, test loss-1.9153, acc-0.4636\n",
      "Iter-41360, train loss-2.0034, acc-0.5000, valid loss-1.9144, acc-0.4732, test loss-1.9152, acc-0.4636\n",
      "Iter-41370, train loss-1.9191, acc-0.3800, valid loss-1.9143, acc-0.4732, test loss-1.9152, acc-0.4635\n",
      "Iter-41380, train loss-1.8558, acc-0.5200, valid loss-1.9142, acc-0.4730, test loss-1.9151, acc-0.4637\n",
      "Iter-41390, train loss-1.9867, acc-0.4000, valid loss-1.9142, acc-0.4730, test loss-1.9151, acc-0.4637\n",
      "Iter-41400, train loss-1.9359, acc-0.5000, valid loss-1.9141, acc-0.4730, test loss-1.9150, acc-0.4636\n",
      "Iter-41410, train loss-1.9150, acc-0.4800, valid loss-1.9140, acc-0.4730, test loss-1.9149, acc-0.4638\n",
      "Iter-41420, train loss-1.9295, acc-0.4400, valid loss-1.9140, acc-0.4730, test loss-1.9148, acc-0.4635\n",
      "Iter-41430, train loss-1.9399, acc-0.4800, valid loss-1.9139, acc-0.4732, test loss-1.9148, acc-0.4636\n",
      "Iter-41440, train loss-1.9300, acc-0.4800, valid loss-1.9138, acc-0.4730, test loss-1.9147, acc-0.4637\n",
      "Iter-41450, train loss-1.9114, acc-0.5400, valid loss-1.9138, acc-0.4734, test loss-1.9147, acc-0.4639\n",
      "Iter-41460, train loss-1.8880, acc-0.5600, valid loss-1.9137, acc-0.4732, test loss-1.9146, acc-0.4638\n",
      "Iter-41470, train loss-1.9051, acc-0.5000, valid loss-1.9136, acc-0.4734, test loss-1.9145, acc-0.4638\n",
      "Iter-41480, train loss-1.9090, acc-0.4200, valid loss-1.9135, acc-0.4734, test loss-1.9144, acc-0.4638\n",
      "Iter-41490, train loss-1.9052, acc-0.4000, valid loss-1.9135, acc-0.4734, test loss-1.9144, acc-0.4639\n",
      "Iter-41500, train loss-1.9823, acc-0.4200, valid loss-1.9134, acc-0.4732, test loss-1.9143, acc-0.4637\n",
      "Iter-41510, train loss-1.9332, acc-0.3400, valid loss-1.9134, acc-0.4732, test loss-1.9143, acc-0.4637\n",
      "Iter-41520, train loss-1.8748, acc-0.5400, valid loss-1.9133, acc-0.4732, test loss-1.9142, acc-0.4641\n",
      "Iter-41530, train loss-1.9665, acc-0.4600, valid loss-1.9132, acc-0.4734, test loss-1.9141, acc-0.4640\n",
      "Iter-41540, train loss-1.8903, acc-0.5800, valid loss-1.9132, acc-0.4734, test loss-1.9141, acc-0.4640\n",
      "Iter-41550, train loss-1.8467, acc-0.6000, valid loss-1.9131, acc-0.4732, test loss-1.9140, acc-0.4643\n",
      "Iter-41560, train loss-1.8928, acc-0.4600, valid loss-1.9130, acc-0.4734, test loss-1.9139, acc-0.4642\n",
      "Iter-41570, train loss-1.8411, acc-0.5400, valid loss-1.9130, acc-0.4732, test loss-1.9139, acc-0.4640\n",
      "Iter-41580, train loss-1.9709, acc-0.3800, valid loss-1.9129, acc-0.4732, test loss-1.9138, acc-0.4641\n",
      "Iter-41590, train loss-1.8156, acc-0.5400, valid loss-1.9128, acc-0.4734, test loss-1.9137, acc-0.4643\n",
      "Iter-41600, train loss-1.8982, acc-0.4000, valid loss-1.9128, acc-0.4730, test loss-1.9137, acc-0.4644\n",
      "Iter-41610, train loss-1.8966, acc-0.4800, valid loss-1.9127, acc-0.4728, test loss-1.9136, acc-0.4642\n",
      "Iter-41620, train loss-1.8882, acc-0.4800, valid loss-1.9126, acc-0.4730, test loss-1.9135, acc-0.4643\n",
      "Iter-41630, train loss-1.8940, acc-0.4800, valid loss-1.9126, acc-0.4732, test loss-1.9135, acc-0.4642\n",
      "Iter-41640, train loss-1.9337, acc-0.5000, valid loss-1.9125, acc-0.4732, test loss-1.9134, acc-0.4642\n",
      "Iter-41650, train loss-1.9361, acc-0.4600, valid loss-1.9124, acc-0.4732, test loss-1.9133, acc-0.4643\n",
      "Iter-41660, train loss-1.9352, acc-0.5200, valid loss-1.9124, acc-0.4734, test loss-1.9133, acc-0.4643\n",
      "Iter-41670, train loss-1.8716, acc-0.5000, valid loss-1.9123, acc-0.4732, test loss-1.9132, acc-0.4644\n",
      "Iter-41680, train loss-1.9164, acc-0.5000, valid loss-1.9122, acc-0.4732, test loss-1.9131, acc-0.4644\n",
      "Iter-41690, train loss-1.8915, acc-0.5400, valid loss-1.9122, acc-0.4732, test loss-1.9131, acc-0.4643\n",
      "Iter-41700, train loss-1.9170, acc-0.5000, valid loss-1.9121, acc-0.4734, test loss-1.9130, acc-0.4642\n",
      "Iter-41710, train loss-1.9364, acc-0.4800, valid loss-1.9120, acc-0.4732, test loss-1.9129, acc-0.4642\n",
      "Iter-41720, train loss-1.9749, acc-0.4200, valid loss-1.9120, acc-0.4732, test loss-1.9129, acc-0.4642\n",
      "Iter-41730, train loss-1.8745, acc-0.5200, valid loss-1.9119, acc-0.4732, test loss-1.9128, acc-0.4643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-41740, train loss-1.8951, acc-0.5200, valid loss-1.9118, acc-0.4730, test loss-1.9127, acc-0.4643\n",
      "Iter-41750, train loss-1.8654, acc-0.4400, valid loss-1.9118, acc-0.4732, test loss-1.9127, acc-0.4643\n",
      "Iter-41760, train loss-1.8963, acc-0.4600, valid loss-1.9117, acc-0.4732, test loss-1.9126, acc-0.4647\n",
      "Iter-41770, train loss-1.8669, acc-0.5000, valid loss-1.9116, acc-0.4734, test loss-1.9125, acc-0.4647\n",
      "Iter-41780, train loss-1.9233, acc-0.4800, valid loss-1.9116, acc-0.4732, test loss-1.9125, acc-0.4646\n",
      "Iter-41790, train loss-1.8801, acc-0.4600, valid loss-1.9115, acc-0.4734, test loss-1.9124, acc-0.4647\n",
      "Iter-41800, train loss-1.9343, acc-0.5000, valid loss-1.9114, acc-0.4734, test loss-1.9123, acc-0.4647\n",
      "Iter-41810, train loss-1.9044, acc-0.4600, valid loss-1.9114, acc-0.4734, test loss-1.9123, acc-0.4648\n",
      "Iter-41820, train loss-1.9529, acc-0.3200, valid loss-1.9113, acc-0.4734, test loss-1.9122, acc-0.4648\n",
      "Iter-41830, train loss-1.8581, acc-0.5400, valid loss-1.9112, acc-0.4734, test loss-1.9121, acc-0.4648\n",
      "Iter-41840, train loss-1.9396, acc-0.4000, valid loss-1.9112, acc-0.4736, test loss-1.9121, acc-0.4648\n",
      "Iter-41850, train loss-1.8532, acc-0.5000, valid loss-1.9111, acc-0.4736, test loss-1.9120, acc-0.4649\n",
      "Iter-41860, train loss-1.9863, acc-0.3200, valid loss-1.9111, acc-0.4736, test loss-1.9119, acc-0.4647\n",
      "Iter-41870, train loss-1.9153, acc-0.4000, valid loss-1.9110, acc-0.4736, test loss-1.9119, acc-0.4648\n",
      "Iter-41880, train loss-1.8714, acc-0.6200, valid loss-1.9109, acc-0.4734, test loss-1.9118, acc-0.4648\n",
      "Iter-41890, train loss-1.9649, acc-0.4000, valid loss-1.9108, acc-0.4734, test loss-1.9117, acc-0.4650\n",
      "Iter-41900, train loss-1.8964, acc-0.5200, valid loss-1.9108, acc-0.4734, test loss-1.9117, acc-0.4648\n",
      "Iter-41910, train loss-1.8591, acc-0.5800, valid loss-1.9107, acc-0.4734, test loss-1.9116, acc-0.4649\n",
      "Iter-41920, train loss-1.9326, acc-0.3800, valid loss-1.9106, acc-0.4736, test loss-1.9116, acc-0.4649\n",
      "Iter-41930, train loss-1.9717, acc-0.4000, valid loss-1.9106, acc-0.4736, test loss-1.9115, acc-0.4648\n",
      "Iter-41940, train loss-1.9400, acc-0.3800, valid loss-1.9105, acc-0.4736, test loss-1.9114, acc-0.4651\n",
      "Iter-41950, train loss-2.0266, acc-0.4200, valid loss-1.9104, acc-0.4736, test loss-1.9114, acc-0.4648\n",
      "Iter-41960, train loss-1.9016, acc-0.5800, valid loss-1.9104, acc-0.4736, test loss-1.9113, acc-0.4651\n",
      "Iter-41970, train loss-1.8748, acc-0.5000, valid loss-1.9103, acc-0.4736, test loss-1.9112, acc-0.4651\n",
      "Iter-41980, train loss-1.9360, acc-0.4600, valid loss-1.9102, acc-0.4736, test loss-1.9112, acc-0.4650\n",
      "Iter-41990, train loss-1.8956, acc-0.4200, valid loss-1.9102, acc-0.4736, test loss-1.9111, acc-0.4652\n",
      "Iter-42000, train loss-1.8859, acc-0.4400, valid loss-1.9101, acc-0.4736, test loss-1.9110, acc-0.4653\n",
      "Iter-42010, train loss-1.8541, acc-0.5800, valid loss-1.9101, acc-0.4736, test loss-1.9110, acc-0.4654\n",
      "Iter-42020, train loss-1.9500, acc-0.4200, valid loss-1.9100, acc-0.4736, test loss-1.9109, acc-0.4653\n",
      "Iter-42030, train loss-1.9167, acc-0.4400, valid loss-1.9099, acc-0.4736, test loss-1.9108, acc-0.4656\n",
      "Iter-42040, train loss-1.8937, acc-0.4600, valid loss-1.9099, acc-0.4736, test loss-1.9108, acc-0.4656\n",
      "Iter-42050, train loss-1.9141, acc-0.4000, valid loss-1.9098, acc-0.4736, test loss-1.9107, acc-0.4658\n",
      "Iter-42060, train loss-1.8861, acc-0.4400, valid loss-1.9097, acc-0.4736, test loss-1.9107, acc-0.4654\n",
      "Iter-42070, train loss-1.8730, acc-0.5200, valid loss-1.9097, acc-0.4734, test loss-1.9106, acc-0.4655\n",
      "Iter-42080, train loss-1.9513, acc-0.4600, valid loss-1.9096, acc-0.4736, test loss-1.9105, acc-0.4652\n",
      "Iter-42090, train loss-1.9493, acc-0.5400, valid loss-1.9095, acc-0.4736, test loss-1.9105, acc-0.4653\n",
      "Iter-42100, train loss-1.9025, acc-0.5000, valid loss-1.9095, acc-0.4734, test loss-1.9104, acc-0.4655\n",
      "Iter-42110, train loss-1.8663, acc-0.4400, valid loss-1.9094, acc-0.4734, test loss-1.9103, acc-0.4656\n",
      "Iter-42120, train loss-1.9660, acc-0.4400, valid loss-1.9093, acc-0.4734, test loss-1.9103, acc-0.4653\n",
      "Iter-42130, train loss-1.9074, acc-0.4400, valid loss-1.9093, acc-0.4736, test loss-1.9102, acc-0.4654\n",
      "Iter-42140, train loss-1.9525, acc-0.3600, valid loss-1.9092, acc-0.4736, test loss-1.9101, acc-0.4655\n",
      "Iter-42150, train loss-1.9023, acc-0.5400, valid loss-1.9091, acc-0.4736, test loss-1.9101, acc-0.4652\n",
      "Iter-42160, train loss-1.8495, acc-0.6200, valid loss-1.9091, acc-0.4736, test loss-1.9100, acc-0.4651\n",
      "Iter-42170, train loss-1.8439, acc-0.5600, valid loss-1.9090, acc-0.4734, test loss-1.9099, acc-0.4651\n",
      "Iter-42180, train loss-1.9862, acc-0.3400, valid loss-1.9090, acc-0.4734, test loss-1.9099, acc-0.4652\n",
      "Iter-42190, train loss-1.8482, acc-0.5800, valid loss-1.9089, acc-0.4734, test loss-1.9098, acc-0.4653\n",
      "Iter-42200, train loss-1.9458, acc-0.3800, valid loss-1.9088, acc-0.4734, test loss-1.9097, acc-0.4655\n",
      "Iter-42210, train loss-1.9562, acc-0.3800, valid loss-1.9088, acc-0.4734, test loss-1.9097, acc-0.4654\n",
      "Iter-42220, train loss-1.9895, acc-0.4000, valid loss-1.9087, acc-0.4734, test loss-1.9096, acc-0.4656\n",
      "Iter-42230, train loss-1.9220, acc-0.4600, valid loss-1.9086, acc-0.4732, test loss-1.9095, acc-0.4658\n",
      "Iter-42240, train loss-1.8228, acc-0.6000, valid loss-1.9086, acc-0.4732, test loss-1.9095, acc-0.4656\n",
      "Iter-42250, train loss-1.9492, acc-0.4600, valid loss-1.9085, acc-0.4732, test loss-1.9094, acc-0.4656\n",
      "Iter-42260, train loss-1.8781, acc-0.4000, valid loss-1.9084, acc-0.4732, test loss-1.9093, acc-0.4657\n",
      "Iter-42270, train loss-1.9472, acc-0.3600, valid loss-1.9084, acc-0.4732, test loss-1.9093, acc-0.4655\n",
      "Iter-42280, train loss-1.8791, acc-0.4800, valid loss-1.9083, acc-0.4736, test loss-1.9092, acc-0.4657\n",
      "Iter-42290, train loss-1.8703, acc-0.4800, valid loss-1.9082, acc-0.4736, test loss-1.9091, acc-0.4658\n",
      "Iter-42300, train loss-1.8994, acc-0.4600, valid loss-1.9082, acc-0.4736, test loss-1.9091, acc-0.4656\n",
      "Iter-42310, train loss-1.8867, acc-0.5400, valid loss-1.9081, acc-0.4736, test loss-1.9090, acc-0.4655\n",
      "Iter-42320, train loss-1.9482, acc-0.4800, valid loss-1.9081, acc-0.4736, test loss-1.9089, acc-0.4657\n",
      "Iter-42330, train loss-1.7944, acc-0.6000, valid loss-1.9080, acc-0.4734, test loss-1.9089, acc-0.4655\n",
      "Iter-42340, train loss-1.9327, acc-0.4600, valid loss-1.9079, acc-0.4738, test loss-1.9088, acc-0.4655\n",
      "Iter-42350, train loss-1.9513, acc-0.4200, valid loss-1.9078, acc-0.4738, test loss-1.9087, acc-0.4654\n",
      "Iter-42360, train loss-1.9843, acc-0.4800, valid loss-1.9078, acc-0.4738, test loss-1.9087, acc-0.4654\n",
      "Iter-42370, train loss-1.8935, acc-0.4200, valid loss-1.9077, acc-0.4736, test loss-1.9086, acc-0.4657\n",
      "Iter-42380, train loss-2.0016, acc-0.4200, valid loss-1.9077, acc-0.4738, test loss-1.9085, acc-0.4657\n",
      "Iter-42390, train loss-1.8575, acc-0.5400, valid loss-1.9076, acc-0.4738, test loss-1.9085, acc-0.4658\n",
      "Iter-42400, train loss-1.8905, acc-0.4600, valid loss-1.9075, acc-0.4738, test loss-1.9084, acc-0.4659\n",
      "Iter-42410, train loss-1.8711, acc-0.5600, valid loss-1.9075, acc-0.4738, test loss-1.9084, acc-0.4660\n",
      "Iter-42420, train loss-1.9659, acc-0.4800, valid loss-1.9074, acc-0.4738, test loss-1.9083, acc-0.4656\n",
      "Iter-42430, train loss-1.9162, acc-0.4200, valid loss-1.9073, acc-0.4736, test loss-1.9082, acc-0.4658\n",
      "Iter-42440, train loss-1.9252, acc-0.4600, valid loss-1.9073, acc-0.4736, test loss-1.9082, acc-0.4659\n",
      "Iter-42450, train loss-1.9009, acc-0.4800, valid loss-1.9072, acc-0.4736, test loss-1.9081, acc-0.4659\n",
      "Iter-42460, train loss-1.9927, acc-0.3800, valid loss-1.9071, acc-0.4736, test loss-1.9080, acc-0.4659\n",
      "Iter-42470, train loss-1.8760, acc-0.5200, valid loss-1.9071, acc-0.4736, test loss-1.9080, acc-0.4660\n",
      "Iter-42480, train loss-1.9076, acc-0.4000, valid loss-1.9070, acc-0.4736, test loss-1.9079, acc-0.4660\n",
      "Iter-42490, train loss-1.9409, acc-0.5200, valid loss-1.9069, acc-0.4736, test loss-1.9078, acc-0.4658\n",
      "Iter-42500, train loss-1.9152, acc-0.4800, valid loss-1.9069, acc-0.4736, test loss-1.9078, acc-0.4661\n",
      "Iter-42510, train loss-1.8947, acc-0.5000, valid loss-1.9068, acc-0.4736, test loss-1.9077, acc-0.4663\n",
      "Iter-42520, train loss-1.9777, acc-0.3200, valid loss-1.9067, acc-0.4736, test loss-1.9076, acc-0.4663\n",
      "Iter-42530, train loss-1.9622, acc-0.3800, valid loss-1.9067, acc-0.4736, test loss-1.9076, acc-0.4661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-42540, train loss-1.9073, acc-0.4600, valid loss-1.9066, acc-0.4736, test loss-1.9075, acc-0.4665\n",
      "Iter-42550, train loss-1.8776, acc-0.4800, valid loss-1.9065, acc-0.4736, test loss-1.9074, acc-0.4663\n",
      "Iter-42560, train loss-1.9368, acc-0.4400, valid loss-1.9065, acc-0.4736, test loss-1.9074, acc-0.4663\n",
      "Iter-42570, train loss-1.9119, acc-0.4400, valid loss-1.9064, acc-0.4736, test loss-1.9073, acc-0.4662\n",
      "Iter-42580, train loss-1.9777, acc-0.3000, valid loss-1.9064, acc-0.4736, test loss-1.9072, acc-0.4663\n",
      "Iter-42590, train loss-1.9049, acc-0.5000, valid loss-1.9063, acc-0.4736, test loss-1.9072, acc-0.4663\n",
      "Iter-42600, train loss-1.9034, acc-0.4200, valid loss-1.9062, acc-0.4736, test loss-1.9071, acc-0.4665\n",
      "Iter-42610, train loss-1.8620, acc-0.5000, valid loss-1.9062, acc-0.4736, test loss-1.9070, acc-0.4666\n",
      "Iter-42620, train loss-1.8646, acc-0.4800, valid loss-1.9061, acc-0.4734, test loss-1.9070, acc-0.4668\n",
      "Iter-42630, train loss-1.8687, acc-0.5000, valid loss-1.9060, acc-0.4736, test loss-1.9069, acc-0.4668\n",
      "Iter-42640, train loss-1.9878, acc-0.4600, valid loss-1.9060, acc-0.4736, test loss-1.9069, acc-0.4670\n",
      "Iter-42650, train loss-1.9307, acc-0.4400, valid loss-1.9059, acc-0.4736, test loss-1.9068, acc-0.4672\n",
      "Iter-42660, train loss-1.9735, acc-0.3600, valid loss-1.9058, acc-0.4738, test loss-1.9067, acc-0.4672\n",
      "Iter-42670, train loss-1.8891, acc-0.5800, valid loss-1.9058, acc-0.4738, test loss-1.9067, acc-0.4670\n",
      "Iter-42680, train loss-1.9228, acc-0.5200, valid loss-1.9057, acc-0.4738, test loss-1.9066, acc-0.4672\n",
      "Iter-42690, train loss-1.8927, acc-0.4600, valid loss-1.9056, acc-0.4740, test loss-1.9065, acc-0.4673\n",
      "Iter-42700, train loss-1.8590, acc-0.5400, valid loss-1.9056, acc-0.4740, test loss-1.9065, acc-0.4674\n",
      "Iter-42710, train loss-2.0088, acc-0.4600, valid loss-1.9055, acc-0.4740, test loss-1.9064, acc-0.4673\n",
      "Iter-42720, train loss-1.9767, acc-0.4000, valid loss-1.9055, acc-0.4740, test loss-1.9063, acc-0.4672\n",
      "Iter-42730, train loss-1.9275, acc-0.4800, valid loss-1.9054, acc-0.4738, test loss-1.9063, acc-0.4672\n",
      "Iter-42740, train loss-1.8916, acc-0.4600, valid loss-1.9053, acc-0.4740, test loss-1.9062, acc-0.4672\n",
      "Iter-42750, train loss-1.9263, acc-0.4200, valid loss-1.9053, acc-0.4740, test loss-1.9061, acc-0.4674\n",
      "Iter-42760, train loss-1.9346, acc-0.5200, valid loss-1.9052, acc-0.4740, test loss-1.9061, acc-0.4674\n",
      "Iter-42770, train loss-1.9466, acc-0.4400, valid loss-1.9051, acc-0.4740, test loss-1.9060, acc-0.4673\n",
      "Iter-42780, train loss-1.9195, acc-0.4200, valid loss-1.9051, acc-0.4740, test loss-1.9060, acc-0.4674\n",
      "Iter-42790, train loss-1.9713, acc-0.4600, valid loss-1.9050, acc-0.4740, test loss-1.9059, acc-0.4674\n",
      "Iter-42800, train loss-1.9174, acc-0.5200, valid loss-1.9049, acc-0.4740, test loss-1.9058, acc-0.4674\n",
      "Iter-42810, train loss-1.9660, acc-0.3400, valid loss-1.9049, acc-0.4740, test loss-1.9058, acc-0.4674\n",
      "Iter-42820, train loss-1.9791, acc-0.3400, valid loss-1.9048, acc-0.4742, test loss-1.9057, acc-0.4674\n",
      "Iter-42830, train loss-1.8225, acc-0.5800, valid loss-1.9047, acc-0.4744, test loss-1.9056, acc-0.4676\n",
      "Iter-42840, train loss-1.9164, acc-0.5000, valid loss-1.9047, acc-0.4744, test loss-1.9056, acc-0.4677\n",
      "Iter-42850, train loss-1.9214, acc-0.4400, valid loss-1.9046, acc-0.4742, test loss-1.9055, acc-0.4676\n",
      "Iter-42860, train loss-1.8878, acc-0.5000, valid loss-1.9046, acc-0.4742, test loss-1.9054, acc-0.4677\n",
      "Iter-42870, train loss-1.9614, acc-0.4600, valid loss-1.9045, acc-0.4740, test loss-1.9054, acc-0.4677\n",
      "Iter-42880, train loss-1.8459, acc-0.5200, valid loss-1.9044, acc-0.4740, test loss-1.9053, acc-0.4677\n",
      "Iter-42890, train loss-1.8939, acc-0.5400, valid loss-1.9044, acc-0.4738, test loss-1.9052, acc-0.4676\n",
      "Iter-42900, train loss-1.9427, acc-0.4000, valid loss-1.9043, acc-0.4740, test loss-1.9052, acc-0.4677\n",
      "Iter-42910, train loss-1.9768, acc-0.4200, valid loss-1.9042, acc-0.4742, test loss-1.9051, acc-0.4675\n",
      "Iter-42920, train loss-1.8552, acc-0.6400, valid loss-1.9042, acc-0.4740, test loss-1.9051, acc-0.4676\n",
      "Iter-42930, train loss-1.8696, acc-0.5800, valid loss-1.9041, acc-0.4742, test loss-1.9050, acc-0.4675\n",
      "Iter-42940, train loss-1.9383, acc-0.5200, valid loss-1.9040, acc-0.4740, test loss-1.9049, acc-0.4675\n",
      "Iter-42950, train loss-1.8648, acc-0.5200, valid loss-1.9039, acc-0.4740, test loss-1.9049, acc-0.4674\n",
      "Iter-42960, train loss-1.8841, acc-0.3800, valid loss-1.9039, acc-0.4740, test loss-1.9048, acc-0.4674\n",
      "Iter-42970, train loss-1.9225, acc-0.4800, valid loss-1.9038, acc-0.4742, test loss-1.9047, acc-0.4673\n",
      "Iter-42980, train loss-1.9418, acc-0.4600, valid loss-1.9038, acc-0.4742, test loss-1.9047, acc-0.4673\n",
      "Iter-42990, train loss-1.8776, acc-0.4800, valid loss-1.9037, acc-0.4744, test loss-1.9046, acc-0.4676\n",
      "Iter-43000, train loss-1.8096, acc-0.5400, valid loss-1.9036, acc-0.4742, test loss-1.9045, acc-0.4676\n",
      "Iter-43010, train loss-1.9091, acc-0.4200, valid loss-1.9036, acc-0.4742, test loss-1.9045, acc-0.4674\n",
      "Iter-43020, train loss-1.8953, acc-0.4600, valid loss-1.9035, acc-0.4744, test loss-1.9044, acc-0.4676\n",
      "Iter-43030, train loss-1.9348, acc-0.4400, valid loss-1.9034, acc-0.4742, test loss-1.9043, acc-0.4676\n",
      "Iter-43040, train loss-1.8574, acc-0.4600, valid loss-1.9034, acc-0.4742, test loss-1.9043, acc-0.4675\n",
      "Iter-43050, train loss-1.9532, acc-0.3800, valid loss-1.9033, acc-0.4742, test loss-1.9042, acc-0.4677\n",
      "Iter-43060, train loss-1.9296, acc-0.4600, valid loss-1.9032, acc-0.4742, test loss-1.9042, acc-0.4677\n",
      "Iter-43070, train loss-1.8853, acc-0.6200, valid loss-1.9032, acc-0.4744, test loss-1.9041, acc-0.4677\n",
      "Iter-43080, train loss-1.8477, acc-0.5600, valid loss-1.9031, acc-0.4744, test loss-1.9040, acc-0.4679\n",
      "Iter-43090, train loss-1.9780, acc-0.3000, valid loss-1.9031, acc-0.4744, test loss-1.9040, acc-0.4678\n",
      "Iter-43100, train loss-1.9467, acc-0.4600, valid loss-1.9030, acc-0.4742, test loss-1.9039, acc-0.4677\n",
      "Iter-43110, train loss-1.9439, acc-0.4200, valid loss-1.9029, acc-0.4744, test loss-1.9038, acc-0.4677\n",
      "Iter-43120, train loss-1.9442, acc-0.4200, valid loss-1.9029, acc-0.4742, test loss-1.9038, acc-0.4678\n",
      "Iter-43130, train loss-1.9795, acc-0.4200, valid loss-1.9028, acc-0.4742, test loss-1.9037, acc-0.4678\n",
      "Iter-43140, train loss-1.8353, acc-0.5200, valid loss-1.9027, acc-0.4744, test loss-1.9036, acc-0.4679\n",
      "Iter-43150, train loss-1.8880, acc-0.4800, valid loss-1.9027, acc-0.4742, test loss-1.9036, acc-0.4680\n",
      "Iter-43160, train loss-1.8802, acc-0.3800, valid loss-1.9026, acc-0.4742, test loss-1.9035, acc-0.4681\n",
      "Iter-43170, train loss-1.8883, acc-0.4400, valid loss-1.9025, acc-0.4742, test loss-1.9035, acc-0.4681\n",
      "Iter-43180, train loss-1.8998, acc-0.5000, valid loss-1.9025, acc-0.4742, test loss-1.9034, acc-0.4681\n",
      "Iter-43190, train loss-1.8713, acc-0.4800, valid loss-1.9024, acc-0.4742, test loss-1.9033, acc-0.4680\n",
      "Iter-43200, train loss-1.8637, acc-0.4200, valid loss-1.9024, acc-0.4742, test loss-1.9033, acc-0.4680\n",
      "Iter-43210, train loss-1.9255, acc-0.4400, valid loss-1.9023, acc-0.4742, test loss-1.9032, acc-0.4680\n",
      "Iter-43220, train loss-1.8471, acc-0.5000, valid loss-1.9022, acc-0.4742, test loss-1.9031, acc-0.4681\n",
      "Iter-43230, train loss-1.8660, acc-0.4200, valid loss-1.9022, acc-0.4742, test loss-1.9031, acc-0.4681\n",
      "Iter-43240, train loss-1.8643, acc-0.5400, valid loss-1.9021, acc-0.4742, test loss-1.9030, acc-0.4681\n",
      "Iter-43250, train loss-1.9474, acc-0.4200, valid loss-1.9020, acc-0.4742, test loss-1.9029, acc-0.4681\n",
      "Iter-43260, train loss-1.9284, acc-0.4800, valid loss-1.9020, acc-0.4740, test loss-1.9029, acc-0.4681\n",
      "Iter-43270, train loss-1.9299, acc-0.4200, valid loss-1.9019, acc-0.4742, test loss-1.9028, acc-0.4681\n",
      "Iter-43280, train loss-1.9813, acc-0.3800, valid loss-1.9018, acc-0.4740, test loss-1.9027, acc-0.4681\n",
      "Iter-43290, train loss-1.9469, acc-0.4000, valid loss-1.9018, acc-0.4740, test loss-1.9027, acc-0.4681\n",
      "Iter-43300, train loss-1.9098, acc-0.4000, valid loss-1.9017, acc-0.4740, test loss-1.9026, acc-0.4681\n",
      "Iter-43310, train loss-1.9190, acc-0.4000, valid loss-1.9016, acc-0.4740, test loss-1.9026, acc-0.4681\n",
      "Iter-43320, train loss-1.8921, acc-0.5000, valid loss-1.9016, acc-0.4740, test loss-1.9025, acc-0.4680\n",
      "Iter-43330, train loss-1.9348, acc-0.3800, valid loss-1.9015, acc-0.4740, test loss-1.9024, acc-0.4682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-43340, train loss-1.9468, acc-0.5400, valid loss-1.9015, acc-0.4740, test loss-1.9024, acc-0.4682\n",
      "Iter-43350, train loss-1.9530, acc-0.3800, valid loss-1.9014, acc-0.4742, test loss-1.9023, acc-0.4683\n",
      "Iter-43360, train loss-1.9940, acc-0.3800, valid loss-1.9013, acc-0.4740, test loss-1.9022, acc-0.4684\n",
      "Iter-43370, train loss-1.8927, acc-0.4200, valid loss-1.9013, acc-0.4738, test loss-1.9022, acc-0.4688\n",
      "Iter-43380, train loss-1.8734, acc-0.5400, valid loss-1.9012, acc-0.4736, test loss-1.9021, acc-0.4685\n",
      "Iter-43390, train loss-1.8903, acc-0.5800, valid loss-1.9011, acc-0.4740, test loss-1.9020, acc-0.4686\n",
      "Iter-43400, train loss-1.9820, acc-0.3600, valid loss-1.9011, acc-0.4738, test loss-1.9020, acc-0.4686\n",
      "Iter-43410, train loss-1.8893, acc-0.4400, valid loss-1.9010, acc-0.4740, test loss-1.9019, acc-0.4686\n",
      "Iter-43420, train loss-1.9778, acc-0.4000, valid loss-1.9010, acc-0.4742, test loss-1.9018, acc-0.4686\n",
      "Iter-43430, train loss-1.9612, acc-0.3800, valid loss-1.9009, acc-0.4742, test loss-1.9018, acc-0.4689\n",
      "Iter-43440, train loss-1.9656, acc-0.3800, valid loss-1.9008, acc-0.4742, test loss-1.9017, acc-0.4687\n",
      "Iter-43450, train loss-1.8659, acc-0.5000, valid loss-1.9008, acc-0.4742, test loss-1.9017, acc-0.4687\n",
      "Iter-43460, train loss-1.9053, acc-0.4000, valid loss-1.9007, acc-0.4744, test loss-1.9016, acc-0.4690\n",
      "Iter-43470, train loss-1.8779, acc-0.6000, valid loss-1.9006, acc-0.4744, test loss-1.9015, acc-0.4689\n",
      "Iter-43480, train loss-1.9123, acc-0.5000, valid loss-1.9006, acc-0.4740, test loss-1.9015, acc-0.4690\n",
      "Iter-43490, train loss-1.8417, acc-0.5800, valid loss-1.9005, acc-0.4738, test loss-1.9014, acc-0.4690\n",
      "Iter-43500, train loss-1.8664, acc-0.4600, valid loss-1.9004, acc-0.4738, test loss-1.9013, acc-0.4690\n",
      "Iter-43510, train loss-1.9408, acc-0.5200, valid loss-1.9004, acc-0.4738, test loss-1.9013, acc-0.4690\n",
      "Iter-43520, train loss-1.9425, acc-0.3600, valid loss-1.9003, acc-0.4740, test loss-1.9012, acc-0.4689\n",
      "Iter-43530, train loss-1.9632, acc-0.4000, valid loss-1.9003, acc-0.4736, test loss-1.9012, acc-0.4691\n",
      "Iter-43540, train loss-1.8547, acc-0.5400, valid loss-1.9002, acc-0.4736, test loss-1.9011, acc-0.4690\n",
      "Iter-43550, train loss-1.9370, acc-0.4000, valid loss-1.9001, acc-0.4740, test loss-1.9010, acc-0.4689\n",
      "Iter-43560, train loss-1.9597, acc-0.4200, valid loss-1.9001, acc-0.4736, test loss-1.9010, acc-0.4690\n",
      "Iter-43570, train loss-1.9680, acc-0.3800, valid loss-1.9000, acc-0.4738, test loss-1.9009, acc-0.4693\n",
      "Iter-43580, train loss-2.0147, acc-0.3000, valid loss-1.8999, acc-0.4736, test loss-1.9008, acc-0.4692\n",
      "Iter-43590, train loss-1.8662, acc-0.5200, valid loss-1.8999, acc-0.4738, test loss-1.9008, acc-0.4693\n",
      "Iter-43600, train loss-1.9344, acc-0.4000, valid loss-1.8998, acc-0.4738, test loss-1.9007, acc-0.4691\n",
      "Iter-43610, train loss-1.8578, acc-0.5200, valid loss-1.8997, acc-0.4738, test loss-1.9007, acc-0.4692\n",
      "Iter-43620, train loss-1.9273, acc-0.4000, valid loss-1.8997, acc-0.4738, test loss-1.9006, acc-0.4692\n",
      "Iter-43630, train loss-1.8806, acc-0.4800, valid loss-1.8996, acc-0.4738, test loss-1.9005, acc-0.4691\n",
      "Iter-43640, train loss-1.8971, acc-0.4800, valid loss-1.8995, acc-0.4738, test loss-1.9005, acc-0.4693\n",
      "Iter-43650, train loss-1.9045, acc-0.4400, valid loss-1.8995, acc-0.4740, test loss-1.9004, acc-0.4695\n",
      "Iter-43660, train loss-1.9014, acc-0.5200, valid loss-1.8994, acc-0.4740, test loss-1.9003, acc-0.4694\n",
      "Iter-43670, train loss-1.8711, acc-0.5000, valid loss-1.8994, acc-0.4738, test loss-1.9003, acc-0.4695\n",
      "Iter-43680, train loss-1.9331, acc-0.4800, valid loss-1.8993, acc-0.4740, test loss-1.9002, acc-0.4695\n",
      "Iter-43690, train loss-1.8230, acc-0.5600, valid loss-1.8992, acc-0.4740, test loss-1.9001, acc-0.4695\n",
      "Iter-43700, train loss-1.8917, acc-0.5800, valid loss-1.8992, acc-0.4742, test loss-1.9001, acc-0.4697\n",
      "Iter-43710, train loss-1.9030, acc-0.5400, valid loss-1.8991, acc-0.4740, test loss-1.9000, acc-0.4695\n",
      "Iter-43720, train loss-1.9405, acc-0.4400, valid loss-1.8991, acc-0.4738, test loss-1.8999, acc-0.4695\n",
      "Iter-43730, train loss-1.9455, acc-0.4600, valid loss-1.8990, acc-0.4736, test loss-1.8999, acc-0.4694\n",
      "Iter-43740, train loss-1.8873, acc-0.5200, valid loss-1.8989, acc-0.4738, test loss-1.8998, acc-0.4694\n",
      "Iter-43750, train loss-1.8550, acc-0.5400, valid loss-1.8989, acc-0.4738, test loss-1.8997, acc-0.4695\n",
      "Iter-43760, train loss-1.9565, acc-0.4000, valid loss-1.8988, acc-0.4740, test loss-1.8997, acc-0.4695\n",
      "Iter-43770, train loss-2.0101, acc-0.3400, valid loss-1.8987, acc-0.4738, test loss-1.8996, acc-0.4694\n",
      "Iter-43780, train loss-1.8644, acc-0.5000, valid loss-1.8987, acc-0.4736, test loss-1.8996, acc-0.4694\n",
      "Iter-43790, train loss-1.9229, acc-0.4600, valid loss-1.8986, acc-0.4736, test loss-1.8995, acc-0.4694\n",
      "Iter-43800, train loss-1.9613, acc-0.4600, valid loss-1.8985, acc-0.4736, test loss-1.8994, acc-0.4694\n",
      "Iter-43810, train loss-1.8447, acc-0.5600, valid loss-1.8985, acc-0.4736, test loss-1.8994, acc-0.4692\n",
      "Iter-43820, train loss-1.8842, acc-0.5400, valid loss-1.8984, acc-0.4738, test loss-1.8993, acc-0.4694\n",
      "Iter-43830, train loss-1.9355, acc-0.4200, valid loss-1.8984, acc-0.4736, test loss-1.8993, acc-0.4695\n",
      "Iter-43840, train loss-1.9434, acc-0.4000, valid loss-1.8983, acc-0.4738, test loss-1.8992, acc-0.4696\n",
      "Iter-43850, train loss-1.9254, acc-0.4800, valid loss-1.8982, acc-0.4736, test loss-1.8991, acc-0.4693\n",
      "Iter-43860, train loss-1.8782, acc-0.6000, valid loss-1.8982, acc-0.4736, test loss-1.8991, acc-0.4695\n",
      "Iter-43870, train loss-1.8255, acc-0.5800, valid loss-1.8981, acc-0.4742, test loss-1.8990, acc-0.4693\n",
      "Iter-43880, train loss-1.8753, acc-0.5600, valid loss-1.8980, acc-0.4742, test loss-1.8989, acc-0.4696\n",
      "Iter-43890, train loss-2.0260, acc-0.4000, valid loss-1.8980, acc-0.4740, test loss-1.8989, acc-0.4696\n",
      "Iter-43900, train loss-1.9540, acc-0.4200, valid loss-1.8979, acc-0.4740, test loss-1.8988, acc-0.4696\n",
      "Iter-43910, train loss-1.8923, acc-0.5000, valid loss-1.8979, acc-0.4744, test loss-1.8987, acc-0.4696\n",
      "Iter-43920, train loss-1.8712, acc-0.4800, valid loss-1.8978, acc-0.4742, test loss-1.8987, acc-0.4696\n",
      "Iter-43930, train loss-1.8474, acc-0.4800, valid loss-1.8977, acc-0.4742, test loss-1.8986, acc-0.4697\n",
      "Iter-43940, train loss-1.8708, acc-0.4600, valid loss-1.8977, acc-0.4746, test loss-1.8986, acc-0.4697\n",
      "Iter-43950, train loss-1.8620, acc-0.4200, valid loss-1.8976, acc-0.4742, test loss-1.8985, acc-0.4697\n",
      "Iter-43960, train loss-1.8410, acc-0.4800, valid loss-1.8975, acc-0.4748, test loss-1.8984, acc-0.4693\n",
      "Iter-43970, train loss-1.9075, acc-0.3800, valid loss-1.8975, acc-0.4750, test loss-1.8984, acc-0.4692\n",
      "Iter-43980, train loss-1.8947, acc-0.5000, valid loss-1.8974, acc-0.4744, test loss-1.8983, acc-0.4695\n",
      "Iter-43990, train loss-1.7232, acc-0.5800, valid loss-1.8973, acc-0.4750, test loss-1.8982, acc-0.4692\n",
      "Iter-44000, train loss-1.9455, acc-0.3800, valid loss-1.8973, acc-0.4748, test loss-1.8982, acc-0.4693\n",
      "Iter-44010, train loss-1.9328, acc-0.4400, valid loss-1.8972, acc-0.4746, test loss-1.8981, acc-0.4696\n",
      "Iter-44020, train loss-1.9457, acc-0.3400, valid loss-1.8972, acc-0.4744, test loss-1.8980, acc-0.4697\n",
      "Iter-44030, train loss-1.9658, acc-0.4600, valid loss-1.8971, acc-0.4748, test loss-1.8980, acc-0.4697\n",
      "Iter-44040, train loss-1.9517, acc-0.4600, valid loss-1.8970, acc-0.4752, test loss-1.8979, acc-0.4697\n",
      "Iter-44050, train loss-1.9672, acc-0.3800, valid loss-1.8970, acc-0.4752, test loss-1.8979, acc-0.4698\n",
      "Iter-44060, train loss-1.8421, acc-0.5000, valid loss-1.8969, acc-0.4752, test loss-1.8978, acc-0.4698\n",
      "Iter-44070, train loss-1.9265, acc-0.4200, valid loss-1.8968, acc-0.4752, test loss-1.8977, acc-0.4696\n",
      "Iter-44080, train loss-1.9168, acc-0.5200, valid loss-1.8968, acc-0.4752, test loss-1.8977, acc-0.4699\n",
      "Iter-44090, train loss-1.9100, acc-0.4200, valid loss-1.8967, acc-0.4752, test loss-1.8976, acc-0.4698\n",
      "Iter-44100, train loss-1.8632, acc-0.5200, valid loss-1.8967, acc-0.4752, test loss-1.8975, acc-0.4700\n",
      "Iter-44110, train loss-1.8663, acc-0.5200, valid loss-1.8966, acc-0.4752, test loss-1.8975, acc-0.4701\n",
      "Iter-44120, train loss-1.8771, acc-0.5000, valid loss-1.8965, acc-0.4752, test loss-1.8974, acc-0.4701\n",
      "Iter-44130, train loss-1.8359, acc-0.5400, valid loss-1.8965, acc-0.4752, test loss-1.8974, acc-0.4701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-44140, train loss-1.9263, acc-0.4400, valid loss-1.8964, acc-0.4752, test loss-1.8973, acc-0.4700\n",
      "Iter-44150, train loss-1.8612, acc-0.5200, valid loss-1.8964, acc-0.4748, test loss-1.8972, acc-0.4701\n",
      "Iter-44160, train loss-1.8357, acc-0.5200, valid loss-1.8963, acc-0.4752, test loss-1.8972, acc-0.4701\n",
      "Iter-44170, train loss-1.8239, acc-0.5800, valid loss-1.8962, acc-0.4752, test loss-1.8971, acc-0.4701\n",
      "Iter-44180, train loss-1.9480, acc-0.4400, valid loss-1.8962, acc-0.4752, test loss-1.8971, acc-0.4699\n",
      "Iter-44190, train loss-1.9401, acc-0.4600, valid loss-1.8961, acc-0.4752, test loss-1.8970, acc-0.4700\n",
      "Iter-44200, train loss-1.8133, acc-0.5800, valid loss-1.8960, acc-0.4752, test loss-1.8969, acc-0.4701\n",
      "Iter-44210, train loss-1.9209, acc-0.4600, valid loss-1.8960, acc-0.4752, test loss-1.8969, acc-0.4701\n",
      "Iter-44220, train loss-1.9228, acc-0.3800, valid loss-1.8959, acc-0.4756, test loss-1.8968, acc-0.4699\n",
      "Iter-44230, train loss-1.9156, acc-0.5000, valid loss-1.8958, acc-0.4756, test loss-1.8967, acc-0.4700\n",
      "Iter-44240, train loss-1.9171, acc-0.4400, valid loss-1.8958, acc-0.4756, test loss-1.8967, acc-0.4704\n",
      "Iter-44250, train loss-1.8264, acc-0.5800, valid loss-1.8957, acc-0.4754, test loss-1.8966, acc-0.4704\n",
      "Iter-44260, train loss-1.7765, acc-0.5600, valid loss-1.8957, acc-0.4752, test loss-1.8965, acc-0.4703\n",
      "Iter-44270, train loss-1.7846, acc-0.5200, valid loss-1.8956, acc-0.4754, test loss-1.8965, acc-0.4705\n",
      "Iter-44280, train loss-1.8909, acc-0.5200, valid loss-1.8955, acc-0.4754, test loss-1.8964, acc-0.4703\n",
      "Iter-44290, train loss-1.8023, acc-0.5600, valid loss-1.8955, acc-0.4754, test loss-1.8964, acc-0.4701\n",
      "Iter-44300, train loss-1.8984, acc-0.4600, valid loss-1.8954, acc-0.4756, test loss-1.8963, acc-0.4702\n",
      "Iter-44310, train loss-1.8516, acc-0.5200, valid loss-1.8954, acc-0.4756, test loss-1.8962, acc-0.4703\n",
      "Iter-44320, train loss-1.9250, acc-0.4200, valid loss-1.8953, acc-0.4756, test loss-1.8962, acc-0.4702\n",
      "Iter-44330, train loss-1.7491, acc-0.6200, valid loss-1.8952, acc-0.4754, test loss-1.8961, acc-0.4703\n",
      "Iter-44340, train loss-1.9672, acc-0.5400, valid loss-1.8952, acc-0.4754, test loss-1.8960, acc-0.4702\n",
      "Iter-44350, train loss-1.9889, acc-0.3600, valid loss-1.8951, acc-0.4754, test loss-1.8960, acc-0.4704\n",
      "Iter-44360, train loss-1.9327, acc-0.4800, valid loss-1.8950, acc-0.4754, test loss-1.8959, acc-0.4705\n",
      "Iter-44370, train loss-1.9801, acc-0.3800, valid loss-1.8950, acc-0.4752, test loss-1.8959, acc-0.4705\n",
      "Iter-44380, train loss-1.8801, acc-0.5200, valid loss-1.8949, acc-0.4752, test loss-1.8958, acc-0.4705\n",
      "Iter-44390, train loss-1.9471, acc-0.5200, valid loss-1.8949, acc-0.4754, test loss-1.8957, acc-0.4705\n",
      "Iter-44400, train loss-1.8483, acc-0.5400, valid loss-1.8948, acc-0.4754, test loss-1.8957, acc-0.4703\n",
      "Iter-44410, train loss-1.9662, acc-0.4800, valid loss-1.8947, acc-0.4754, test loss-1.8956, acc-0.4705\n",
      "Iter-44420, train loss-1.9332, acc-0.4000, valid loss-1.8947, acc-0.4754, test loss-1.8955, acc-0.4704\n",
      "Iter-44430, train loss-1.8959, acc-0.4400, valid loss-1.8946, acc-0.4754, test loss-1.8955, acc-0.4703\n",
      "Iter-44440, train loss-1.8977, acc-0.5200, valid loss-1.8946, acc-0.4754, test loss-1.8954, acc-0.4703\n",
      "Iter-44450, train loss-1.9260, acc-0.4200, valid loss-1.8945, acc-0.4756, test loss-1.8954, acc-0.4705\n",
      "Iter-44460, train loss-1.8067, acc-0.5400, valid loss-1.8944, acc-0.4756, test loss-1.8953, acc-0.4705\n",
      "Iter-44470, train loss-1.9044, acc-0.4600, valid loss-1.8944, acc-0.4756, test loss-1.8952, acc-0.4704\n",
      "Iter-44480, train loss-1.9108, acc-0.5000, valid loss-1.8943, acc-0.4752, test loss-1.8952, acc-0.4705\n",
      "Iter-44490, train loss-1.8348, acc-0.5200, valid loss-1.8942, acc-0.4752, test loss-1.8951, acc-0.4704\n",
      "Iter-44500, train loss-1.8780, acc-0.5400, valid loss-1.8942, acc-0.4754, test loss-1.8950, acc-0.4705\n",
      "Iter-44510, train loss-1.9833, acc-0.3400, valid loss-1.8941, acc-0.4756, test loss-1.8950, acc-0.4704\n",
      "Iter-44520, train loss-1.9145, acc-0.4400, valid loss-1.8940, acc-0.4756, test loss-1.8949, acc-0.4704\n",
      "Iter-44530, train loss-1.9592, acc-0.4000, valid loss-1.8940, acc-0.4758, test loss-1.8948, acc-0.4705\n",
      "Iter-44540, train loss-1.9511, acc-0.5600, valid loss-1.8939, acc-0.4758, test loss-1.8948, acc-0.4705\n",
      "Iter-44550, train loss-1.8954, acc-0.4400, valid loss-1.8939, acc-0.4762, test loss-1.8947, acc-0.4705\n",
      "Iter-44560, train loss-1.8064, acc-0.5400, valid loss-1.8938, acc-0.4762, test loss-1.8947, acc-0.4703\n",
      "Iter-44570, train loss-1.9048, acc-0.4400, valid loss-1.8937, acc-0.4758, test loss-1.8946, acc-0.4705\n",
      "Iter-44580, train loss-1.9984, acc-0.4200, valid loss-1.8937, acc-0.4758, test loss-1.8945, acc-0.4704\n",
      "Iter-44590, train loss-1.9201, acc-0.5000, valid loss-1.8936, acc-0.4758, test loss-1.8945, acc-0.4705\n",
      "Iter-44600, train loss-1.9096, acc-0.4200, valid loss-1.8936, acc-0.4758, test loss-1.8944, acc-0.4705\n",
      "Iter-44610, train loss-1.8998, acc-0.4800, valid loss-1.8935, acc-0.4756, test loss-1.8944, acc-0.4705\n",
      "Iter-44620, train loss-1.8864, acc-0.4600, valid loss-1.8934, acc-0.4758, test loss-1.8943, acc-0.4705\n",
      "Iter-44630, train loss-1.8968, acc-0.4000, valid loss-1.8934, acc-0.4758, test loss-1.8942, acc-0.4705\n",
      "Iter-44640, train loss-1.8999, acc-0.5200, valid loss-1.8933, acc-0.4758, test loss-1.8942, acc-0.4705\n",
      "Iter-44650, train loss-1.9462, acc-0.3600, valid loss-1.8932, acc-0.4758, test loss-1.8941, acc-0.4704\n",
      "Iter-44660, train loss-1.9074, acc-0.4600, valid loss-1.8932, acc-0.4760, test loss-1.8940, acc-0.4706\n",
      "Iter-44670, train loss-1.9496, acc-0.3600, valid loss-1.8931, acc-0.4758, test loss-1.8940, acc-0.4705\n",
      "Iter-44680, train loss-1.8720, acc-0.5600, valid loss-1.8931, acc-0.4758, test loss-1.8939, acc-0.4703\n",
      "Iter-44690, train loss-1.8671, acc-0.5600, valid loss-1.8930, acc-0.4756, test loss-1.8939, acc-0.4705\n",
      "Iter-44700, train loss-1.8938, acc-0.5000, valid loss-1.8929, acc-0.4756, test loss-1.8938, acc-0.4705\n",
      "Iter-44710, train loss-1.9588, acc-0.3800, valid loss-1.8929, acc-0.4756, test loss-1.8937, acc-0.4703\n",
      "Iter-44720, train loss-1.9152, acc-0.3800, valid loss-1.8928, acc-0.4758, test loss-1.8937, acc-0.4705\n",
      "Iter-44730, train loss-1.9446, acc-0.4600, valid loss-1.8928, acc-0.4760, test loss-1.8936, acc-0.4706\n",
      "Iter-44740, train loss-1.8684, acc-0.4800, valid loss-1.8927, acc-0.4758, test loss-1.8936, acc-0.4704\n",
      "Iter-44750, train loss-1.9749, acc-0.3600, valid loss-1.8926, acc-0.4762, test loss-1.8935, acc-0.4706\n",
      "Iter-44760, train loss-1.8642, acc-0.4400, valid loss-1.8926, acc-0.4762, test loss-1.8934, acc-0.4706\n",
      "Iter-44770, train loss-1.8278, acc-0.5200, valid loss-1.8925, acc-0.4762, test loss-1.8934, acc-0.4707\n",
      "Iter-44780, train loss-1.9040, acc-0.5000, valid loss-1.8924, acc-0.4760, test loss-1.8933, acc-0.4707\n",
      "Iter-44790, train loss-1.9270, acc-0.4800, valid loss-1.8924, acc-0.4762, test loss-1.8932, acc-0.4707\n",
      "Iter-44800, train loss-1.8904, acc-0.4800, valid loss-1.8923, acc-0.4760, test loss-1.8932, acc-0.4708\n",
      "Iter-44810, train loss-1.9170, acc-0.3800, valid loss-1.8923, acc-0.4760, test loss-1.8931, acc-0.4707\n",
      "Iter-44820, train loss-1.8655, acc-0.6200, valid loss-1.8922, acc-0.4762, test loss-1.8931, acc-0.4710\n",
      "Iter-44830, train loss-1.8075, acc-0.5200, valid loss-1.8921, acc-0.4764, test loss-1.8930, acc-0.4708\n",
      "Iter-44840, train loss-1.9139, acc-0.4600, valid loss-1.8921, acc-0.4762, test loss-1.8929, acc-0.4709\n",
      "Iter-44850, train loss-1.8280, acc-0.5400, valid loss-1.8920, acc-0.4762, test loss-1.8929, acc-0.4709\n",
      "Iter-44860, train loss-1.9242, acc-0.5000, valid loss-1.8919, acc-0.4760, test loss-1.8928, acc-0.4709\n",
      "Iter-44870, train loss-1.9030, acc-0.4400, valid loss-1.8919, acc-0.4762, test loss-1.8928, acc-0.4710\n",
      "Iter-44880, train loss-1.9224, acc-0.4800, valid loss-1.8918, acc-0.4762, test loss-1.8927, acc-0.4711\n",
      "Iter-44890, train loss-1.8230, acc-0.5000, valid loss-1.8918, acc-0.4762, test loss-1.8926, acc-0.4711\n",
      "Iter-44900, train loss-1.8710, acc-0.5600, valid loss-1.8917, acc-0.4764, test loss-1.8926, acc-0.4710\n",
      "Iter-44910, train loss-1.9079, acc-0.4800, valid loss-1.8916, acc-0.4764, test loss-1.8925, acc-0.4709\n",
      "Iter-44920, train loss-1.9338, acc-0.4200, valid loss-1.8916, acc-0.4764, test loss-1.8924, acc-0.4709\n",
      "Iter-44930, train loss-1.8769, acc-0.4600, valid loss-1.8915, acc-0.4764, test loss-1.8924, acc-0.4709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-44940, train loss-1.8663, acc-0.4400, valid loss-1.8914, acc-0.4764, test loss-1.8923, acc-0.4709\n",
      "Iter-44950, train loss-1.9034, acc-0.3600, valid loss-1.8914, acc-0.4764, test loss-1.8922, acc-0.4709\n",
      "Iter-44960, train loss-1.7633, acc-0.5800, valid loss-1.8913, acc-0.4764, test loss-1.8922, acc-0.4709\n",
      "Iter-44970, train loss-1.8978, acc-0.4400, valid loss-1.8912, acc-0.4764, test loss-1.8921, acc-0.4709\n",
      "Iter-44980, train loss-1.8248, acc-0.5600, valid loss-1.8912, acc-0.4764, test loss-1.8920, acc-0.4710\n",
      "Iter-44990, train loss-1.9761, acc-0.3600, valid loss-1.8911, acc-0.4760, test loss-1.8920, acc-0.4711\n",
      "Iter-45000, train loss-1.9272, acc-0.3600, valid loss-1.8910, acc-0.4760, test loss-1.8919, acc-0.4711\n",
      "Iter-45010, train loss-1.8681, acc-0.5200, valid loss-1.8910, acc-0.4762, test loss-1.8919, acc-0.4710\n",
      "Iter-45020, train loss-1.9039, acc-0.4800, valid loss-1.8909, acc-0.4766, test loss-1.8918, acc-0.4709\n",
      "Iter-45030, train loss-1.9387, acc-0.3400, valid loss-1.8909, acc-0.4766, test loss-1.8917, acc-0.4709\n",
      "Iter-45040, train loss-1.9599, acc-0.5000, valid loss-1.8908, acc-0.4766, test loss-1.8917, acc-0.4709\n",
      "Iter-45050, train loss-1.9265, acc-0.3600, valid loss-1.8907, acc-0.4766, test loss-1.8916, acc-0.4710\n",
      "Iter-45060, train loss-1.8536, acc-0.4800, valid loss-1.8907, acc-0.4768, test loss-1.8916, acc-0.4710\n",
      "Iter-45070, train loss-1.9156, acc-0.4400, valid loss-1.8906, acc-0.4768, test loss-1.8915, acc-0.4710\n",
      "Iter-45080, train loss-1.9352, acc-0.4600, valid loss-1.8906, acc-0.4772, test loss-1.8914, acc-0.4710\n",
      "Iter-45090, train loss-1.9374, acc-0.4600, valid loss-1.8905, acc-0.4770, test loss-1.8914, acc-0.4710\n",
      "Iter-45100, train loss-1.8253, acc-0.4600, valid loss-1.8904, acc-0.4770, test loss-1.8913, acc-0.4710\n",
      "Iter-45110, train loss-1.9338, acc-0.5000, valid loss-1.8904, acc-0.4768, test loss-1.8912, acc-0.4711\n",
      "Iter-45120, train loss-1.9189, acc-0.4600, valid loss-1.8903, acc-0.4772, test loss-1.8912, acc-0.4709\n",
      "Iter-45130, train loss-1.9528, acc-0.3800, valid loss-1.8902, acc-0.4772, test loss-1.8911, acc-0.4710\n",
      "Iter-45140, train loss-1.7662, acc-0.6200, valid loss-1.8902, acc-0.4772, test loss-1.8911, acc-0.4711\n",
      "Iter-45150, train loss-1.8441, acc-0.5800, valid loss-1.8901, acc-0.4772, test loss-1.8910, acc-0.4709\n",
      "Iter-45160, train loss-1.8413, acc-0.5800, valid loss-1.8900, acc-0.4772, test loss-1.8909, acc-0.4709\n",
      "Iter-45170, train loss-1.8551, acc-0.4200, valid loss-1.8900, acc-0.4772, test loss-1.8909, acc-0.4710\n",
      "Iter-45180, train loss-2.0214, acc-0.3200, valid loss-1.8899, acc-0.4772, test loss-1.8908, acc-0.4709\n",
      "Iter-45190, train loss-1.9011, acc-0.4400, valid loss-1.8899, acc-0.4772, test loss-1.8907, acc-0.4709\n",
      "Iter-45200, train loss-1.9099, acc-0.4400, valid loss-1.8898, acc-0.4772, test loss-1.8907, acc-0.4709\n",
      "Iter-45210, train loss-1.8666, acc-0.5200, valid loss-1.8897, acc-0.4774, test loss-1.8906, acc-0.4709\n",
      "Iter-45220, train loss-1.7743, acc-0.5800, valid loss-1.8897, acc-0.4774, test loss-1.8906, acc-0.4709\n",
      "Iter-45230, train loss-1.8584, acc-0.5000, valid loss-1.8896, acc-0.4774, test loss-1.8905, acc-0.4711\n",
      "Iter-45240, train loss-1.8867, acc-0.5200, valid loss-1.8895, acc-0.4774, test loss-1.8904, acc-0.4711\n",
      "Iter-45250, train loss-1.9373, acc-0.5000, valid loss-1.8895, acc-0.4774, test loss-1.8904, acc-0.4711\n",
      "Iter-45260, train loss-1.9038, acc-0.5400, valid loss-1.8894, acc-0.4770, test loss-1.8903, acc-0.4710\n",
      "Iter-45270, train loss-1.9289, acc-0.4000, valid loss-1.8894, acc-0.4770, test loss-1.8902, acc-0.4710\n",
      "Iter-45280, train loss-1.9546, acc-0.3600, valid loss-1.8893, acc-0.4770, test loss-1.8902, acc-0.4711\n",
      "Iter-45290, train loss-1.9043, acc-0.4400, valid loss-1.8892, acc-0.4774, test loss-1.8901, acc-0.4709\n",
      "Iter-45300, train loss-1.8328, acc-0.5000, valid loss-1.8892, acc-0.4770, test loss-1.8901, acc-0.4710\n",
      "Iter-45310, train loss-1.8914, acc-0.4400, valid loss-1.8891, acc-0.4770, test loss-1.8900, acc-0.4710\n",
      "Iter-45320, train loss-2.0221, acc-0.2800, valid loss-1.8890, acc-0.4772, test loss-1.8899, acc-0.4708\n",
      "Iter-45330, train loss-1.9115, acc-0.5000, valid loss-1.8890, acc-0.4770, test loss-1.8899, acc-0.4709\n",
      "Iter-45340, train loss-1.8768, acc-0.4400, valid loss-1.8889, acc-0.4768, test loss-1.8898, acc-0.4709\n",
      "Iter-45350, train loss-1.8262, acc-0.4600, valid loss-1.8888, acc-0.4768, test loss-1.8897, acc-0.4709\n",
      "Iter-45360, train loss-1.8721, acc-0.5400, valid loss-1.8888, acc-0.4770, test loss-1.8897, acc-0.4709\n",
      "Iter-45370, train loss-1.9245, acc-0.3800, valid loss-1.8887, acc-0.4770, test loss-1.8896, acc-0.4709\n",
      "Iter-45380, train loss-1.8224, acc-0.5600, valid loss-1.8887, acc-0.4766, test loss-1.8896, acc-0.4707\n",
      "Iter-45390, train loss-1.8683, acc-0.4000, valid loss-1.8886, acc-0.4768, test loss-1.8895, acc-0.4707\n",
      "Iter-45400, train loss-1.9784, acc-0.3600, valid loss-1.8885, acc-0.4766, test loss-1.8894, acc-0.4708\n",
      "Iter-45410, train loss-1.9660, acc-0.3800, valid loss-1.8885, acc-0.4766, test loss-1.8894, acc-0.4707\n",
      "Iter-45420, train loss-1.9666, acc-0.4000, valid loss-1.8884, acc-0.4768, test loss-1.8893, acc-0.4707\n",
      "Iter-45430, train loss-1.9064, acc-0.5600, valid loss-1.8884, acc-0.4768, test loss-1.8892, acc-0.4708\n",
      "Iter-45440, train loss-1.9549, acc-0.4200, valid loss-1.8883, acc-0.4770, test loss-1.8892, acc-0.4709\n",
      "Iter-45450, train loss-1.9489, acc-0.5000, valid loss-1.8882, acc-0.4770, test loss-1.8891, acc-0.4709\n",
      "Iter-45460, train loss-1.9646, acc-0.4600, valid loss-1.8882, acc-0.4770, test loss-1.8891, acc-0.4709\n",
      "Iter-45470, train loss-1.9126, acc-0.4200, valid loss-1.8881, acc-0.4770, test loss-1.8890, acc-0.4709\n",
      "Iter-45480, train loss-1.8447, acc-0.5400, valid loss-1.8880, acc-0.4768, test loss-1.8889, acc-0.4707\n",
      "Iter-45490, train loss-1.9410, acc-0.4400, valid loss-1.8880, acc-0.4768, test loss-1.8889, acc-0.4707\n",
      "Iter-45500, train loss-1.9424, acc-0.3800, valid loss-1.8879, acc-0.4768, test loss-1.8888, acc-0.4707\n",
      "Iter-45510, train loss-1.8865, acc-0.4800, valid loss-1.8879, acc-0.4768, test loss-1.8887, acc-0.4707\n",
      "Iter-45520, train loss-1.8255, acc-0.5800, valid loss-1.8878, acc-0.4768, test loss-1.8887, acc-0.4707\n",
      "Iter-45530, train loss-1.9123, acc-0.5000, valid loss-1.8877, acc-0.4770, test loss-1.8886, acc-0.4707\n",
      "Iter-45540, train loss-1.8878, acc-0.4800, valid loss-1.8877, acc-0.4770, test loss-1.8886, acc-0.4707\n",
      "Iter-45550, train loss-1.9104, acc-0.3600, valid loss-1.8876, acc-0.4768, test loss-1.8885, acc-0.4707\n",
      "Iter-45560, train loss-1.8741, acc-0.5400, valid loss-1.8875, acc-0.4768, test loss-1.8884, acc-0.4707\n",
      "Iter-45570, train loss-1.9950, acc-0.4000, valid loss-1.8875, acc-0.4768, test loss-1.8884, acc-0.4707\n",
      "Iter-45580, train loss-1.8742, acc-0.5200, valid loss-1.8874, acc-0.4770, test loss-1.8883, acc-0.4707\n",
      "Iter-45590, train loss-1.8816, acc-0.5000, valid loss-1.8874, acc-0.4770, test loss-1.8883, acc-0.4707\n",
      "Iter-45600, train loss-1.9250, acc-0.4000, valid loss-1.8873, acc-0.4770, test loss-1.8882, acc-0.4707\n",
      "Iter-45610, train loss-1.8855, acc-0.4200, valid loss-1.8872, acc-0.4770, test loss-1.8881, acc-0.4707\n",
      "Iter-45620, train loss-1.9057, acc-0.4200, valid loss-1.8872, acc-0.4772, test loss-1.8881, acc-0.4708\n",
      "Iter-45630, train loss-1.8794, acc-0.4200, valid loss-1.8871, acc-0.4772, test loss-1.8880, acc-0.4709\n",
      "Iter-45640, train loss-1.8980, acc-0.5600, valid loss-1.8870, acc-0.4772, test loss-1.8879, acc-0.4710\n",
      "Iter-45650, train loss-1.8368, acc-0.4800, valid loss-1.8870, acc-0.4774, test loss-1.8879, acc-0.4710\n",
      "Iter-45660, train loss-1.9432, acc-0.4000, valid loss-1.8869, acc-0.4774, test loss-1.8878, acc-0.4710\n",
      "Iter-45670, train loss-1.9544, acc-0.3800, valid loss-1.8869, acc-0.4776, test loss-1.8878, acc-0.4710\n",
      "Iter-45680, train loss-1.8224, acc-0.6800, valid loss-1.8868, acc-0.4780, test loss-1.8877, acc-0.4710\n",
      "Iter-45690, train loss-1.9514, acc-0.4800, valid loss-1.8867, acc-0.4780, test loss-1.8876, acc-0.4710\n",
      "Iter-45700, train loss-1.8302, acc-0.5000, valid loss-1.8867, acc-0.4780, test loss-1.8876, acc-0.4710\n",
      "Iter-45710, train loss-1.8617, acc-0.4800, valid loss-1.8866, acc-0.4780, test loss-1.8875, acc-0.4710\n",
      "Iter-45720, train loss-1.8855, acc-0.5200, valid loss-1.8866, acc-0.4782, test loss-1.8874, acc-0.4711\n",
      "Iter-45730, train loss-1.9541, acc-0.3800, valid loss-1.8865, acc-0.4782, test loss-1.8874, acc-0.4711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45740, train loss-1.8579, acc-0.5400, valid loss-1.8864, acc-0.4784, test loss-1.8873, acc-0.4710\n",
      "Iter-45750, train loss-1.8073, acc-0.5400, valid loss-1.8864, acc-0.4786, test loss-1.8873, acc-0.4712\n",
      "Iter-45760, train loss-1.8509, acc-0.3800, valid loss-1.8863, acc-0.4786, test loss-1.8872, acc-0.4710\n",
      "Iter-45770, train loss-1.9269, acc-0.5000, valid loss-1.8862, acc-0.4782, test loss-1.8871, acc-0.4711\n",
      "Iter-45780, train loss-1.9778, acc-0.4000, valid loss-1.8862, acc-0.4784, test loss-1.8871, acc-0.4712\n",
      "Iter-45790, train loss-1.9478, acc-0.4400, valid loss-1.8861, acc-0.4786, test loss-1.8870, acc-0.4713\n",
      "Iter-45800, train loss-1.8612, acc-0.4600, valid loss-1.8861, acc-0.4786, test loss-1.8869, acc-0.4713\n",
      "Iter-45810, train loss-1.9716, acc-0.4200, valid loss-1.8860, acc-0.4786, test loss-1.8869, acc-0.4713\n",
      "Iter-45820, train loss-2.0306, acc-0.3600, valid loss-1.8859, acc-0.4786, test loss-1.8868, acc-0.4713\n",
      "Iter-45830, train loss-1.9443, acc-0.5600, valid loss-1.8859, acc-0.4784, test loss-1.8868, acc-0.4713\n",
      "Iter-45840, train loss-1.8721, acc-0.4800, valid loss-1.8858, acc-0.4784, test loss-1.8867, acc-0.4715\n",
      "Iter-45850, train loss-1.9304, acc-0.4000, valid loss-1.8858, acc-0.4784, test loss-1.8866, acc-0.4717\n",
      "Iter-45860, train loss-1.8939, acc-0.5400, valid loss-1.8857, acc-0.4782, test loss-1.8866, acc-0.4716\n",
      "Iter-45870, train loss-1.8431, acc-0.5400, valid loss-1.8856, acc-0.4782, test loss-1.8865, acc-0.4717\n",
      "Iter-45880, train loss-1.8050, acc-0.5400, valid loss-1.8856, acc-0.4782, test loss-1.8865, acc-0.4716\n",
      "Iter-45890, train loss-1.8580, acc-0.4600, valid loss-1.8855, acc-0.4780, test loss-1.8864, acc-0.4714\n",
      "Iter-45900, train loss-1.8988, acc-0.4400, valid loss-1.8854, acc-0.4784, test loss-1.8863, acc-0.4716\n",
      "Iter-45910, train loss-1.8377, acc-0.5800, valid loss-1.8854, acc-0.4782, test loss-1.8863, acc-0.4717\n",
      "Iter-45920, train loss-1.8616, acc-0.5800, valid loss-1.8853, acc-0.4784, test loss-1.8862, acc-0.4716\n",
      "Iter-45930, train loss-1.8843, acc-0.4600, valid loss-1.8853, acc-0.4786, test loss-1.8862, acc-0.4716\n",
      "Iter-45940, train loss-1.9718, acc-0.3600, valid loss-1.8852, acc-0.4786, test loss-1.8861, acc-0.4716\n",
      "Iter-45950, train loss-1.9258, acc-0.4600, valid loss-1.8852, acc-0.4786, test loss-1.8860, acc-0.4716\n",
      "Iter-45960, train loss-1.9094, acc-0.5000, valid loss-1.8851, acc-0.4786, test loss-1.8860, acc-0.4716\n",
      "Iter-45970, train loss-1.9739, acc-0.4400, valid loss-1.8850, acc-0.4782, test loss-1.8859, acc-0.4716\n",
      "Iter-45980, train loss-1.9248, acc-0.4800, valid loss-1.8850, acc-0.4782, test loss-1.8859, acc-0.4716\n",
      "Iter-45990, train loss-1.9150, acc-0.4000, valid loss-1.8849, acc-0.4782, test loss-1.8858, acc-0.4717\n",
      "Iter-46000, train loss-1.9634, acc-0.4200, valid loss-1.8848, acc-0.4780, test loss-1.8857, acc-0.4720\n",
      "Iter-46010, train loss-1.8768, acc-0.5200, valid loss-1.8848, acc-0.4780, test loss-1.8857, acc-0.4719\n",
      "Iter-46020, train loss-1.9149, acc-0.5200, valid loss-1.8847, acc-0.4782, test loss-1.8856, acc-0.4723\n",
      "Iter-46030, train loss-1.8639, acc-0.4400, valid loss-1.8847, acc-0.4780, test loss-1.8855, acc-0.4721\n",
      "Iter-46040, train loss-1.8795, acc-0.5200, valid loss-1.8846, acc-0.4780, test loss-1.8855, acc-0.4721\n",
      "Iter-46050, train loss-1.8273, acc-0.5800, valid loss-1.8845, acc-0.4782, test loss-1.8854, acc-0.4723\n",
      "Iter-46060, train loss-1.8962, acc-0.4600, valid loss-1.8845, acc-0.4784, test loss-1.8854, acc-0.4720\n",
      "Iter-46070, train loss-1.8468, acc-0.5800, valid loss-1.8844, acc-0.4784, test loss-1.8853, acc-0.4725\n",
      "Iter-46080, train loss-1.8492, acc-0.5600, valid loss-1.8844, acc-0.4784, test loss-1.8852, acc-0.4726\n",
      "Iter-46090, train loss-1.9363, acc-0.4600, valid loss-1.8843, acc-0.4784, test loss-1.8852, acc-0.4725\n",
      "Iter-46100, train loss-1.8722, acc-0.5600, valid loss-1.8842, acc-0.4784, test loss-1.8851, acc-0.4723\n",
      "Iter-46110, train loss-1.9316, acc-0.5800, valid loss-1.8842, acc-0.4784, test loss-1.8851, acc-0.4726\n",
      "Iter-46120, train loss-1.9165, acc-0.5200, valid loss-1.8841, acc-0.4784, test loss-1.8850, acc-0.4726\n",
      "Iter-46130, train loss-1.8751, acc-0.4600, valid loss-1.8841, acc-0.4784, test loss-1.8850, acc-0.4725\n",
      "Iter-46140, train loss-1.8693, acc-0.4200, valid loss-1.8840, acc-0.4784, test loss-1.8849, acc-0.4726\n",
      "Iter-46150, train loss-1.8949, acc-0.4800, valid loss-1.8839, acc-0.4784, test loss-1.8848, acc-0.4728\n",
      "Iter-46160, train loss-1.8526, acc-0.4800, valid loss-1.8839, acc-0.4784, test loss-1.8848, acc-0.4728\n",
      "Iter-46170, train loss-1.8650, acc-0.5400, valid loss-1.8838, acc-0.4786, test loss-1.8847, acc-0.4728\n",
      "Iter-46180, train loss-1.9723, acc-0.3600, valid loss-1.8838, acc-0.4786, test loss-1.8847, acc-0.4727\n",
      "Iter-46190, train loss-1.8740, acc-0.4000, valid loss-1.8837, acc-0.4786, test loss-1.8846, acc-0.4729\n",
      "Iter-46200, train loss-1.8836, acc-0.5000, valid loss-1.8836, acc-0.4786, test loss-1.8845, acc-0.4727\n",
      "Iter-46210, train loss-1.9284, acc-0.4200, valid loss-1.8836, acc-0.4786, test loss-1.8845, acc-0.4728\n",
      "Iter-46220, train loss-1.8846, acc-0.4000, valid loss-1.8835, acc-0.4784, test loss-1.8844, acc-0.4728\n",
      "Iter-46230, train loss-1.9346, acc-0.4200, valid loss-1.8835, acc-0.4784, test loss-1.8843, acc-0.4729\n",
      "Iter-46240, train loss-1.8501, acc-0.4800, valid loss-1.8834, acc-0.4786, test loss-1.8843, acc-0.4727\n",
      "Iter-46250, train loss-1.9182, acc-0.4200, valid loss-1.8833, acc-0.4790, test loss-1.8842, acc-0.4729\n",
      "Iter-46260, train loss-1.8843, acc-0.4800, valid loss-1.8833, acc-0.4794, test loss-1.8842, acc-0.4731\n",
      "Iter-46270, train loss-1.9073, acc-0.5400, valid loss-1.8832, acc-0.4794, test loss-1.8841, acc-0.4729\n",
      "Iter-46280, train loss-1.8493, acc-0.4800, valid loss-1.8832, acc-0.4794, test loss-1.8840, acc-0.4730\n",
      "Iter-46290, train loss-1.8472, acc-0.5200, valid loss-1.8831, acc-0.4792, test loss-1.8840, acc-0.4729\n",
      "Iter-46300, train loss-1.9352, acc-0.4200, valid loss-1.8830, acc-0.4786, test loss-1.8839, acc-0.4730\n",
      "Iter-46310, train loss-1.8580, acc-0.4800, valid loss-1.8830, acc-0.4790, test loss-1.8839, acc-0.4731\n",
      "Iter-46320, train loss-1.9334, acc-0.4000, valid loss-1.8829, acc-0.4790, test loss-1.8838, acc-0.4731\n",
      "Iter-46330, train loss-1.9207, acc-0.4400, valid loss-1.8828, acc-0.4794, test loss-1.8837, acc-0.4731\n",
      "Iter-46340, train loss-1.9642, acc-0.3800, valid loss-1.8828, acc-0.4794, test loss-1.8837, acc-0.4733\n",
      "Iter-46350, train loss-1.8069, acc-0.5200, valid loss-1.8827, acc-0.4794, test loss-1.8836, acc-0.4734\n",
      "Iter-46360, train loss-1.8716, acc-0.3200, valid loss-1.8827, acc-0.4794, test loss-1.8835, acc-0.4734\n",
      "Iter-46370, train loss-1.8948, acc-0.5400, valid loss-1.8826, acc-0.4794, test loss-1.8835, acc-0.4734\n",
      "Iter-46380, train loss-1.8780, acc-0.4600, valid loss-1.8825, acc-0.4794, test loss-1.8834, acc-0.4732\n",
      "Iter-46390, train loss-1.8297, acc-0.4800, valid loss-1.8825, acc-0.4792, test loss-1.8834, acc-0.4732\n",
      "Iter-46400, train loss-1.9043, acc-0.4800, valid loss-1.8824, acc-0.4790, test loss-1.8833, acc-0.4731\n",
      "Iter-46410, train loss-1.8549, acc-0.5400, valid loss-1.8823, acc-0.4792, test loss-1.8832, acc-0.4730\n",
      "Iter-46420, train loss-1.8370, acc-0.5600, valid loss-1.8823, acc-0.4792, test loss-1.8832, acc-0.4732\n",
      "Iter-46430, train loss-1.9171, acc-0.5200, valid loss-1.8822, acc-0.4792, test loss-1.8831, acc-0.4733\n",
      "Iter-46440, train loss-1.7942, acc-0.5000, valid loss-1.8821, acc-0.4790, test loss-1.8830, acc-0.4730\n",
      "Iter-46450, train loss-1.8491, acc-0.5200, valid loss-1.8821, acc-0.4790, test loss-1.8830, acc-0.4732\n",
      "Iter-46460, train loss-2.0002, acc-0.2800, valid loss-1.8820, acc-0.4790, test loss-1.8829, acc-0.4731\n",
      "Iter-46470, train loss-1.8701, acc-0.5200, valid loss-1.8820, acc-0.4790, test loss-1.8829, acc-0.4731\n",
      "Iter-46480, train loss-1.9672, acc-0.3200, valid loss-1.8819, acc-0.4794, test loss-1.8828, acc-0.4734\n",
      "Iter-46490, train loss-1.9847, acc-0.3000, valid loss-1.8818, acc-0.4794, test loss-1.8827, acc-0.4734\n",
      "Iter-46500, train loss-1.9430, acc-0.5200, valid loss-1.8818, acc-0.4794, test loss-1.8827, acc-0.4736\n",
      "Iter-46510, train loss-1.9657, acc-0.4000, valid loss-1.8817, acc-0.4794, test loss-1.8826, acc-0.4734\n",
      "Iter-46520, train loss-1.9664, acc-0.3600, valid loss-1.8817, acc-0.4792, test loss-1.8826, acc-0.4735\n",
      "Iter-46530, train loss-1.8281, acc-0.6000, valid loss-1.8816, acc-0.4794, test loss-1.8825, acc-0.4737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-46540, train loss-1.8977, acc-0.4000, valid loss-1.8815, acc-0.4792, test loss-1.8824, acc-0.4736\n",
      "Iter-46550, train loss-1.9611, acc-0.3800, valid loss-1.8815, acc-0.4794, test loss-1.8824, acc-0.4736\n",
      "Iter-46560, train loss-1.9764, acc-0.3400, valid loss-1.8814, acc-0.4794, test loss-1.8823, acc-0.4737\n",
      "Iter-46570, train loss-1.9528, acc-0.4200, valid loss-1.8814, acc-0.4794, test loss-1.8823, acc-0.4735\n",
      "Iter-46580, train loss-1.9368, acc-0.4200, valid loss-1.8813, acc-0.4792, test loss-1.8822, acc-0.4736\n",
      "Iter-46590, train loss-1.9116, acc-0.5200, valid loss-1.8812, acc-0.4792, test loss-1.8821, acc-0.4736\n",
      "Iter-46600, train loss-1.8523, acc-0.5200, valid loss-1.8812, acc-0.4792, test loss-1.8821, acc-0.4737\n",
      "Iter-46610, train loss-1.9410, acc-0.4400, valid loss-1.8811, acc-0.4792, test loss-1.8820, acc-0.4738\n",
      "Iter-46620, train loss-1.8880, acc-0.5400, valid loss-1.8811, acc-0.4794, test loss-1.8819, acc-0.4738\n",
      "Iter-46630, train loss-1.8586, acc-0.4800, valid loss-1.8810, acc-0.4794, test loss-1.8819, acc-0.4738\n",
      "Iter-46640, train loss-1.7280, acc-0.6400, valid loss-1.8809, acc-0.4792, test loss-1.8818, acc-0.4737\n",
      "Iter-46650, train loss-1.8504, acc-0.5800, valid loss-1.8809, acc-0.4794, test loss-1.8818, acc-0.4738\n",
      "Iter-46660, train loss-1.8433, acc-0.6000, valid loss-1.8808, acc-0.4794, test loss-1.8817, acc-0.4737\n",
      "Iter-46670, train loss-1.8165, acc-0.4800, valid loss-1.8808, acc-0.4794, test loss-1.8816, acc-0.4737\n",
      "Iter-46680, train loss-1.8379, acc-0.5000, valid loss-1.8807, acc-0.4796, test loss-1.8816, acc-0.4737\n",
      "Iter-46690, train loss-1.8941, acc-0.4400, valid loss-1.8806, acc-0.4796, test loss-1.8815, acc-0.4737\n",
      "Iter-46700, train loss-1.9542, acc-0.4200, valid loss-1.8806, acc-0.4796, test loss-1.8815, acc-0.4737\n",
      "Iter-46710, train loss-1.8939, acc-0.4800, valid loss-1.8805, acc-0.4796, test loss-1.8814, acc-0.4737\n",
      "Iter-46720, train loss-1.9563, acc-0.4400, valid loss-1.8805, acc-0.4798, test loss-1.8813, acc-0.4736\n",
      "Iter-46730, train loss-1.9490, acc-0.3800, valid loss-1.8804, acc-0.4796, test loss-1.8813, acc-0.4739\n",
      "Iter-46740, train loss-1.8610, acc-0.6000, valid loss-1.8803, acc-0.4796, test loss-1.8812, acc-0.4739\n",
      "Iter-46750, train loss-1.8793, acc-0.4800, valid loss-1.8803, acc-0.4796, test loss-1.8812, acc-0.4737\n",
      "Iter-46760, train loss-1.9664, acc-0.4400, valid loss-1.8802, acc-0.4796, test loss-1.8811, acc-0.4741\n",
      "Iter-46770, train loss-1.9941, acc-0.4400, valid loss-1.8802, acc-0.4796, test loss-1.8810, acc-0.4739\n",
      "Iter-46780, train loss-1.9077, acc-0.4600, valid loss-1.8801, acc-0.4794, test loss-1.8810, acc-0.4741\n",
      "Iter-46790, train loss-1.9592, acc-0.3600, valid loss-1.8800, acc-0.4794, test loss-1.8809, acc-0.4742\n",
      "Iter-46800, train loss-1.8487, acc-0.5200, valid loss-1.8800, acc-0.4796, test loss-1.8809, acc-0.4741\n",
      "Iter-46810, train loss-1.8256, acc-0.5200, valid loss-1.8799, acc-0.4794, test loss-1.8808, acc-0.4743\n",
      "Iter-46820, train loss-1.8846, acc-0.4800, valid loss-1.8799, acc-0.4794, test loss-1.8807, acc-0.4743\n",
      "Iter-46830, train loss-1.8526, acc-0.4600, valid loss-1.8798, acc-0.4794, test loss-1.8807, acc-0.4741\n",
      "Iter-46840, train loss-1.9404, acc-0.5000, valid loss-1.8797, acc-0.4794, test loss-1.8806, acc-0.4744\n",
      "Iter-46850, train loss-1.8710, acc-0.4600, valid loss-1.8797, acc-0.4798, test loss-1.8806, acc-0.4747\n",
      "Iter-46860, train loss-1.9100, acc-0.4400, valid loss-1.8796, acc-0.4798, test loss-1.8805, acc-0.4747\n",
      "Iter-46870, train loss-1.9704, acc-0.4400, valid loss-1.8796, acc-0.4802, test loss-1.8804, acc-0.4747\n",
      "Iter-46880, train loss-1.8909, acc-0.4400, valid loss-1.8795, acc-0.4804, test loss-1.8804, acc-0.4748\n",
      "Iter-46890, train loss-1.7774, acc-0.5400, valid loss-1.8794, acc-0.4804, test loss-1.8803, acc-0.4750\n",
      "Iter-46900, train loss-1.9359, acc-0.4400, valid loss-1.8794, acc-0.4804, test loss-1.8803, acc-0.4748\n",
      "Iter-46910, train loss-1.8845, acc-0.4800, valid loss-1.8793, acc-0.4802, test loss-1.8802, acc-0.4748\n",
      "Iter-46920, train loss-1.9808, acc-0.5400, valid loss-1.8793, acc-0.4800, test loss-1.8801, acc-0.4748\n",
      "Iter-46930, train loss-1.9140, acc-0.4800, valid loss-1.8792, acc-0.4802, test loss-1.8801, acc-0.4747\n",
      "Iter-46940, train loss-1.8428, acc-0.4400, valid loss-1.8791, acc-0.4798, test loss-1.8800, acc-0.4744\n",
      "Iter-46950, train loss-1.9088, acc-0.4800, valid loss-1.8791, acc-0.4802, test loss-1.8800, acc-0.4746\n",
      "Iter-46960, train loss-1.8963, acc-0.4400, valid loss-1.8790, acc-0.4798, test loss-1.8799, acc-0.4746\n",
      "Iter-46970, train loss-1.9625, acc-0.4400, valid loss-1.8790, acc-0.4798, test loss-1.8798, acc-0.4747\n",
      "Iter-46980, train loss-1.8853, acc-0.3800, valid loss-1.8789, acc-0.4800, test loss-1.8798, acc-0.4748\n",
      "Iter-46990, train loss-1.8724, acc-0.4600, valid loss-1.8789, acc-0.4802, test loss-1.8797, acc-0.4748\n",
      "Iter-47000, train loss-1.9012, acc-0.4400, valid loss-1.8788, acc-0.4804, test loss-1.8797, acc-0.4748\n",
      "Iter-47010, train loss-1.9037, acc-0.4200, valid loss-1.8787, acc-0.4802, test loss-1.8796, acc-0.4750\n",
      "Iter-47020, train loss-1.8246, acc-0.5400, valid loss-1.8787, acc-0.4802, test loss-1.8795, acc-0.4749\n",
      "Iter-47030, train loss-1.8833, acc-0.4600, valid loss-1.8786, acc-0.4802, test loss-1.8795, acc-0.4748\n",
      "Iter-47040, train loss-1.8576, acc-0.5000, valid loss-1.8786, acc-0.4802, test loss-1.8794, acc-0.4749\n",
      "Iter-47050, train loss-1.9136, acc-0.5000, valid loss-1.8785, acc-0.4802, test loss-1.8794, acc-0.4748\n",
      "Iter-47060, train loss-1.8832, acc-0.4800, valid loss-1.8784, acc-0.4802, test loss-1.8793, acc-0.4746\n",
      "Iter-47070, train loss-1.8360, acc-0.5400, valid loss-1.8784, acc-0.4804, test loss-1.8792, acc-0.4749\n",
      "Iter-47080, train loss-1.8476, acc-0.5200, valid loss-1.8783, acc-0.4804, test loss-1.8792, acc-0.4750\n",
      "Iter-47090, train loss-1.9529, acc-0.5200, valid loss-1.8783, acc-0.4804, test loss-1.8791, acc-0.4747\n",
      "Iter-47100, train loss-1.9211, acc-0.4200, valid loss-1.8782, acc-0.4804, test loss-1.8791, acc-0.4749\n",
      "Iter-47110, train loss-1.8145, acc-0.5400, valid loss-1.8781, acc-0.4804, test loss-1.8790, acc-0.4750\n",
      "Iter-47120, train loss-1.8337, acc-0.4600, valid loss-1.8781, acc-0.4804, test loss-1.8789, acc-0.4750\n",
      "Iter-47130, train loss-1.9006, acc-0.4000, valid loss-1.8780, acc-0.4806, test loss-1.8789, acc-0.4752\n",
      "Iter-47140, train loss-1.8982, acc-0.5800, valid loss-1.8780, acc-0.4804, test loss-1.8788, acc-0.4748\n",
      "Iter-47150, train loss-1.7816, acc-0.5800, valid loss-1.8779, acc-0.4804, test loss-1.8788, acc-0.4749\n",
      "Iter-47160, train loss-1.9124, acc-0.3800, valid loss-1.8778, acc-0.4804, test loss-1.8787, acc-0.4750\n",
      "Iter-47170, train loss-1.9071, acc-0.3400, valid loss-1.8778, acc-0.4802, test loss-1.8786, acc-0.4751\n",
      "Iter-47180, train loss-1.8941, acc-0.5200, valid loss-1.8777, acc-0.4802, test loss-1.8786, acc-0.4748\n",
      "Iter-47190, train loss-1.8582, acc-0.5200, valid loss-1.8777, acc-0.4804, test loss-1.8785, acc-0.4749\n",
      "Iter-47200, train loss-2.0266, acc-0.3400, valid loss-1.8776, acc-0.4806, test loss-1.8784, acc-0.4751\n",
      "Iter-47210, train loss-1.8486, acc-0.4800, valid loss-1.8775, acc-0.4806, test loss-1.8784, acc-0.4751\n",
      "Iter-47220, train loss-1.8604, acc-0.5000, valid loss-1.8775, acc-0.4808, test loss-1.8783, acc-0.4749\n",
      "Iter-47230, train loss-1.9116, acc-0.4400, valid loss-1.8774, acc-0.4806, test loss-1.8783, acc-0.4751\n",
      "Iter-47240, train loss-1.8314, acc-0.5200, valid loss-1.8774, acc-0.4806, test loss-1.8782, acc-0.4751\n",
      "Iter-47250, train loss-1.8983, acc-0.4000, valid loss-1.8773, acc-0.4806, test loss-1.8781, acc-0.4751\n",
      "Iter-47260, train loss-1.8353, acc-0.5800, valid loss-1.8772, acc-0.4806, test loss-1.8781, acc-0.4751\n",
      "Iter-47270, train loss-1.8742, acc-0.5200, valid loss-1.8772, acc-0.4806, test loss-1.8780, acc-0.4751\n",
      "Iter-47280, train loss-1.9251, acc-0.4200, valid loss-1.8771, acc-0.4806, test loss-1.8780, acc-0.4752\n",
      "Iter-47290, train loss-1.8908, acc-0.4200, valid loss-1.8771, acc-0.4808, test loss-1.8779, acc-0.4751\n",
      "Iter-47300, train loss-1.8987, acc-0.3600, valid loss-1.8770, acc-0.4810, test loss-1.8779, acc-0.4752\n",
      "Iter-47310, train loss-1.8351, acc-0.5600, valid loss-1.8769, acc-0.4808, test loss-1.8778, acc-0.4753\n",
      "Iter-47320, train loss-1.8549, acc-0.4800, valid loss-1.8769, acc-0.4810, test loss-1.8777, acc-0.4753\n",
      "Iter-47330, train loss-1.9417, acc-0.3600, valid loss-1.8768, acc-0.4810, test loss-1.8777, acc-0.4755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-47340, train loss-1.8081, acc-0.5200, valid loss-1.8768, acc-0.4810, test loss-1.8776, acc-0.4752\n",
      "Iter-47350, train loss-1.8584, acc-0.5000, valid loss-1.8767, acc-0.4812, test loss-1.8775, acc-0.4756\n",
      "Iter-47360, train loss-1.8235, acc-0.5200, valid loss-1.8767, acc-0.4812, test loss-1.8775, acc-0.4757\n",
      "Iter-47370, train loss-1.8320, acc-0.5600, valid loss-1.8766, acc-0.4812, test loss-1.8774, acc-0.4760\n",
      "Iter-47380, train loss-1.8922, acc-0.5000, valid loss-1.8765, acc-0.4812, test loss-1.8774, acc-0.4759\n",
      "Iter-47390, train loss-1.9981, acc-0.3400, valid loss-1.8765, acc-0.4812, test loss-1.8773, acc-0.4759\n",
      "Iter-47400, train loss-1.9210, acc-0.4600, valid loss-1.8764, acc-0.4812, test loss-1.8772, acc-0.4760\n",
      "Iter-47410, train loss-1.8882, acc-0.5000, valid loss-1.8764, acc-0.4812, test loss-1.8772, acc-0.4760\n",
      "Iter-47420, train loss-1.8844, acc-0.5600, valid loss-1.8763, acc-0.4814, test loss-1.8771, acc-0.4760\n",
      "Iter-47430, train loss-1.8258, acc-0.4600, valid loss-1.8762, acc-0.4814, test loss-1.8771, acc-0.4759\n",
      "Iter-47440, train loss-1.9060, acc-0.4000, valid loss-1.8762, acc-0.4814, test loss-1.8770, acc-0.4760\n",
      "Iter-47450, train loss-1.9344, acc-0.4600, valid loss-1.8761, acc-0.4814, test loss-1.8769, acc-0.4760\n",
      "Iter-47460, train loss-1.8221, acc-0.5800, valid loss-1.8761, acc-0.4814, test loss-1.8769, acc-0.4758\n",
      "Iter-47470, train loss-1.7825, acc-0.5600, valid loss-1.8760, acc-0.4814, test loss-1.8768, acc-0.4758\n",
      "Iter-47480, train loss-1.9821, acc-0.4000, valid loss-1.8759, acc-0.4814, test loss-1.8768, acc-0.4757\n",
      "Iter-47490, train loss-1.8769, acc-0.5200, valid loss-1.8759, acc-0.4814, test loss-1.8767, acc-0.4759\n",
      "Iter-47500, train loss-1.9302, acc-0.4000, valid loss-1.8758, acc-0.4816, test loss-1.8766, acc-0.4757\n",
      "Iter-47510, train loss-1.9469, acc-0.2800, valid loss-1.8758, acc-0.4816, test loss-1.8766, acc-0.4758\n",
      "Iter-47520, train loss-1.8771, acc-0.4600, valid loss-1.8757, acc-0.4814, test loss-1.8765, acc-0.4759\n",
      "Iter-47530, train loss-1.8927, acc-0.5000, valid loss-1.8756, acc-0.4816, test loss-1.8765, acc-0.4760\n",
      "Iter-47540, train loss-1.8519, acc-0.4200, valid loss-1.8756, acc-0.4814, test loss-1.8764, acc-0.4761\n",
      "Iter-47550, train loss-1.9633, acc-0.4400, valid loss-1.8755, acc-0.4814, test loss-1.8763, acc-0.4760\n",
      "Iter-47560, train loss-1.9611, acc-0.3600, valid loss-1.8754, acc-0.4816, test loss-1.8763, acc-0.4759\n",
      "Iter-47570, train loss-1.9966, acc-0.3200, valid loss-1.8754, acc-0.4814, test loss-1.8762, acc-0.4758\n",
      "Iter-47580, train loss-1.8739, acc-0.4400, valid loss-1.8753, acc-0.4816, test loss-1.8762, acc-0.4757\n",
      "Iter-47590, train loss-1.9426, acc-0.4000, valid loss-1.8753, acc-0.4816, test loss-1.8761, acc-0.4758\n",
      "Iter-47600, train loss-1.8313, acc-0.5600, valid loss-1.8752, acc-0.4818, test loss-1.8761, acc-0.4759\n",
      "Iter-47610, train loss-1.8944, acc-0.4800, valid loss-1.8752, acc-0.4816, test loss-1.8760, acc-0.4760\n",
      "Iter-47620, train loss-1.9037, acc-0.4600, valid loss-1.8751, acc-0.4818, test loss-1.8759, acc-0.4762\n",
      "Iter-47630, train loss-1.8611, acc-0.5400, valid loss-1.8751, acc-0.4818, test loss-1.8759, acc-0.4762\n",
      "Iter-47640, train loss-1.8378, acc-0.4600, valid loss-1.8750, acc-0.4818, test loss-1.8758, acc-0.4762\n",
      "Iter-47650, train loss-1.9267, acc-0.4400, valid loss-1.8749, acc-0.4824, test loss-1.8758, acc-0.4763\n",
      "Iter-47660, train loss-1.9255, acc-0.5000, valid loss-1.8749, acc-0.4824, test loss-1.8757, acc-0.4763\n",
      "Iter-47670, train loss-1.9134, acc-0.4800, valid loss-1.8748, acc-0.4820, test loss-1.8756, acc-0.4761\n",
      "Iter-47680, train loss-1.8539, acc-0.4600, valid loss-1.8748, acc-0.4824, test loss-1.8756, acc-0.4762\n",
      "Iter-47690, train loss-1.9755, acc-0.4000, valid loss-1.8747, acc-0.4826, test loss-1.8755, acc-0.4762\n",
      "Iter-47700, train loss-1.8685, acc-0.5600, valid loss-1.8746, acc-0.4824, test loss-1.8755, acc-0.4762\n",
      "Iter-47710, train loss-1.8688, acc-0.5200, valid loss-1.8746, acc-0.4824, test loss-1.8754, acc-0.4764\n",
      "Iter-47720, train loss-1.9162, acc-0.4800, valid loss-1.8745, acc-0.4826, test loss-1.8753, acc-0.4764\n",
      "Iter-47730, train loss-1.9798, acc-0.4400, valid loss-1.8745, acc-0.4824, test loss-1.8753, acc-0.4766\n",
      "Iter-47740, train loss-1.8747, acc-0.5000, valid loss-1.8744, acc-0.4824, test loss-1.8752, acc-0.4766\n",
      "Iter-47750, train loss-1.8485, acc-0.5000, valid loss-1.8743, acc-0.4822, test loss-1.8752, acc-0.4765\n",
      "Iter-47760, train loss-1.8836, acc-0.5400, valid loss-1.8743, acc-0.4822, test loss-1.8751, acc-0.4766\n",
      "Iter-47770, train loss-1.9160, acc-0.4600, valid loss-1.8742, acc-0.4824, test loss-1.8751, acc-0.4767\n",
      "Iter-47780, train loss-1.9571, acc-0.4000, valid loss-1.8742, acc-0.4824, test loss-1.8750, acc-0.4767\n",
      "Iter-47790, train loss-1.8097, acc-0.5400, valid loss-1.8741, acc-0.4824, test loss-1.8749, acc-0.4767\n",
      "Iter-47800, train loss-1.9366, acc-0.3400, valid loss-1.8740, acc-0.4824, test loss-1.8749, acc-0.4766\n",
      "Iter-47810, train loss-1.8292, acc-0.5400, valid loss-1.8740, acc-0.4824, test loss-1.8748, acc-0.4766\n",
      "Iter-47820, train loss-1.8628, acc-0.5600, valid loss-1.8739, acc-0.4824, test loss-1.8747, acc-0.4765\n",
      "Iter-47830, train loss-1.9511, acc-0.4000, valid loss-1.8739, acc-0.4822, test loss-1.8747, acc-0.4765\n",
      "Iter-47840, train loss-1.8693, acc-0.4600, valid loss-1.8738, acc-0.4824, test loss-1.8746, acc-0.4765\n",
      "Iter-47850, train loss-1.9465, acc-0.4600, valid loss-1.8737, acc-0.4824, test loss-1.8746, acc-0.4765\n",
      "Iter-47860, train loss-1.8357, acc-0.5600, valid loss-1.8737, acc-0.4820, test loss-1.8745, acc-0.4764\n",
      "Iter-47870, train loss-1.8413, acc-0.5200, valid loss-1.8736, acc-0.4822, test loss-1.8745, acc-0.4765\n",
      "Iter-47880, train loss-1.7780, acc-0.5800, valid loss-1.8736, acc-0.4822, test loss-1.8744, acc-0.4765\n",
      "Iter-47890, train loss-1.9004, acc-0.4600, valid loss-1.8735, acc-0.4822, test loss-1.8743, acc-0.4764\n",
      "Iter-47900, train loss-1.8978, acc-0.4200, valid loss-1.8734, acc-0.4822, test loss-1.8743, acc-0.4766\n",
      "Iter-47910, train loss-1.7670, acc-0.6000, valid loss-1.8734, acc-0.4822, test loss-1.8742, acc-0.4765\n",
      "Iter-47920, train loss-1.8684, acc-0.4200, valid loss-1.8733, acc-0.4822, test loss-1.8742, acc-0.4765\n",
      "Iter-47930, train loss-1.9299, acc-0.4000, valid loss-1.8733, acc-0.4822, test loss-1.8741, acc-0.4764\n",
      "Iter-47940, train loss-1.8907, acc-0.4000, valid loss-1.8732, acc-0.4822, test loss-1.8740, acc-0.4763\n",
      "Iter-47950, train loss-1.9321, acc-0.4400, valid loss-1.8731, acc-0.4822, test loss-1.8740, acc-0.4764\n",
      "Iter-47960, train loss-1.8453, acc-0.5400, valid loss-1.8731, acc-0.4820, test loss-1.8739, acc-0.4764\n",
      "Iter-47970, train loss-1.8234, acc-0.5000, valid loss-1.8730, acc-0.4822, test loss-1.8739, acc-0.4765\n",
      "Iter-47980, train loss-1.8479, acc-0.5600, valid loss-1.8730, acc-0.4824, test loss-1.8738, acc-0.4766\n",
      "Iter-47990, train loss-1.8733, acc-0.4800, valid loss-1.8729, acc-0.4824, test loss-1.8737, acc-0.4767\n",
      "Iter-48000, train loss-1.8268, acc-0.5600, valid loss-1.8728, acc-0.4824, test loss-1.8737, acc-0.4768\n",
      "Iter-48010, train loss-1.8500, acc-0.5200, valid loss-1.8728, acc-0.4824, test loss-1.8736, acc-0.4768\n",
      "Iter-48020, train loss-1.8624, acc-0.4600, valid loss-1.8727, acc-0.4820, test loss-1.8736, acc-0.4768\n",
      "Iter-48030, train loss-1.8124, acc-0.4600, valid loss-1.8727, acc-0.4820, test loss-1.8735, acc-0.4767\n",
      "Iter-48040, train loss-1.9083, acc-0.4400, valid loss-1.8726, acc-0.4820, test loss-1.8734, acc-0.4767\n",
      "Iter-48050, train loss-1.8800, acc-0.4200, valid loss-1.8725, acc-0.4820, test loss-1.8734, acc-0.4768\n",
      "Iter-48060, train loss-1.8668, acc-0.4800, valid loss-1.8725, acc-0.4822, test loss-1.8733, acc-0.4767\n",
      "Iter-48070, train loss-1.9207, acc-0.4600, valid loss-1.8724, acc-0.4824, test loss-1.8733, acc-0.4768\n",
      "Iter-48080, train loss-1.9617, acc-0.3600, valid loss-1.8724, acc-0.4822, test loss-1.8732, acc-0.4768\n",
      "Iter-48090, train loss-1.8443, acc-0.4800, valid loss-1.8723, acc-0.4824, test loss-1.8731, acc-0.4768\n",
      "Iter-48100, train loss-1.9732, acc-0.3800, valid loss-1.8722, acc-0.4824, test loss-1.8731, acc-0.4768\n",
      "Iter-48110, train loss-1.8236, acc-0.5000, valid loss-1.8722, acc-0.4824, test loss-1.8730, acc-0.4767\n",
      "Iter-48120, train loss-1.9022, acc-0.4400, valid loss-1.8721, acc-0.4822, test loss-1.8730, acc-0.4769\n",
      "Iter-48130, train loss-1.8420, acc-0.4200, valid loss-1.8721, acc-0.4824, test loss-1.8729, acc-0.4769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-48140, train loss-1.8678, acc-0.4600, valid loss-1.8720, acc-0.4824, test loss-1.8728, acc-0.4768\n",
      "Iter-48150, train loss-1.8546, acc-0.5600, valid loss-1.8719, acc-0.4824, test loss-1.8728, acc-0.4767\n",
      "Iter-48160, train loss-1.9589, acc-0.4600, valid loss-1.8719, acc-0.4824, test loss-1.8727, acc-0.4768\n",
      "Iter-48170, train loss-1.9493, acc-0.3800, valid loss-1.8718, acc-0.4824, test loss-1.8727, acc-0.4768\n",
      "Iter-48180, train loss-1.8734, acc-0.4400, valid loss-1.8718, acc-0.4828, test loss-1.8726, acc-0.4768\n",
      "Iter-48190, train loss-1.8729, acc-0.5600, valid loss-1.8717, acc-0.4826, test loss-1.8725, acc-0.4769\n",
      "Iter-48200, train loss-1.8670, acc-0.4400, valid loss-1.8716, acc-0.4826, test loss-1.8725, acc-0.4769\n",
      "Iter-48210, train loss-1.9608, acc-0.3600, valid loss-1.8716, acc-0.4828, test loss-1.8724, acc-0.4768\n",
      "Iter-48220, train loss-1.8537, acc-0.5200, valid loss-1.8715, acc-0.4828, test loss-1.8724, acc-0.4769\n",
      "Iter-48230, train loss-1.8099, acc-0.6400, valid loss-1.8715, acc-0.4828, test loss-1.8723, acc-0.4769\n",
      "Iter-48240, train loss-1.8863, acc-0.4600, valid loss-1.8714, acc-0.4826, test loss-1.8722, acc-0.4771\n",
      "Iter-48250, train loss-1.8740, acc-0.4800, valid loss-1.8714, acc-0.4826, test loss-1.8722, acc-0.4769\n",
      "Iter-48260, train loss-1.9056, acc-0.4800, valid loss-1.8713, acc-0.4826, test loss-1.8721, acc-0.4770\n",
      "Iter-48270, train loss-1.8995, acc-0.4800, valid loss-1.8712, acc-0.4826, test loss-1.8721, acc-0.4770\n",
      "Iter-48280, train loss-1.9375, acc-0.3800, valid loss-1.8712, acc-0.4826, test loss-1.8720, acc-0.4769\n",
      "Iter-48290, train loss-1.8543, acc-0.5400, valid loss-1.8711, acc-0.4828, test loss-1.8720, acc-0.4770\n",
      "Iter-48300, train loss-1.9031, acc-0.5000, valid loss-1.8711, acc-0.4828, test loss-1.8719, acc-0.4769\n",
      "Iter-48310, train loss-1.8966, acc-0.4600, valid loss-1.8710, acc-0.4826, test loss-1.8718, acc-0.4769\n",
      "Iter-48320, train loss-1.8096, acc-0.6000, valid loss-1.8710, acc-0.4832, test loss-1.8718, acc-0.4768\n",
      "Iter-48330, train loss-1.8897, acc-0.4000, valid loss-1.8709, acc-0.4828, test loss-1.8717, acc-0.4769\n",
      "Iter-48340, train loss-1.9652, acc-0.3600, valid loss-1.8708, acc-0.4828, test loss-1.8717, acc-0.4769\n",
      "Iter-48350, train loss-1.9541, acc-0.4800, valid loss-1.8708, acc-0.4830, test loss-1.8716, acc-0.4768\n",
      "Iter-48360, train loss-1.8402, acc-0.5200, valid loss-1.8707, acc-0.4830, test loss-1.8715, acc-0.4768\n",
      "Iter-48370, train loss-1.8583, acc-0.4600, valid loss-1.8707, acc-0.4824, test loss-1.8715, acc-0.4769\n",
      "Iter-48380, train loss-1.9146, acc-0.4000, valid loss-1.8706, acc-0.4826, test loss-1.8714, acc-0.4769\n",
      "Iter-48390, train loss-1.8976, acc-0.4400, valid loss-1.8705, acc-0.4830, test loss-1.8714, acc-0.4769\n",
      "Iter-48400, train loss-1.8869, acc-0.4600, valid loss-1.8705, acc-0.4826, test loss-1.8713, acc-0.4769\n",
      "Iter-48410, train loss-1.8644, acc-0.5000, valid loss-1.8704, acc-0.4826, test loss-1.8713, acc-0.4769\n",
      "Iter-48420, train loss-1.8431, acc-0.5600, valid loss-1.8704, acc-0.4828, test loss-1.8712, acc-0.4769\n",
      "Iter-48430, train loss-1.9971, acc-0.3000, valid loss-1.8703, acc-0.4830, test loss-1.8711, acc-0.4770\n",
      "Iter-48440, train loss-1.9417, acc-0.3400, valid loss-1.8702, acc-0.4832, test loss-1.8711, acc-0.4770\n",
      "Iter-48450, train loss-1.8903, acc-0.4600, valid loss-1.8702, acc-0.4832, test loss-1.8710, acc-0.4770\n",
      "Iter-48460, train loss-1.7249, acc-0.6600, valid loss-1.8701, acc-0.4832, test loss-1.8710, acc-0.4770\n",
      "Iter-48470, train loss-1.9024, acc-0.3800, valid loss-1.8701, acc-0.4832, test loss-1.8709, acc-0.4770\n",
      "Iter-48480, train loss-1.8586, acc-0.5400, valid loss-1.8700, acc-0.4830, test loss-1.8708, acc-0.4771\n",
      "Iter-48490, train loss-1.8909, acc-0.4200, valid loss-1.8700, acc-0.4832, test loss-1.8708, acc-0.4772\n",
      "Iter-48500, train loss-1.8915, acc-0.5000, valid loss-1.8699, acc-0.4832, test loss-1.8707, acc-0.4772\n",
      "Iter-48510, train loss-1.8988, acc-0.4600, valid loss-1.8698, acc-0.4832, test loss-1.8707, acc-0.4772\n",
      "Iter-48520, train loss-1.8624, acc-0.4800, valid loss-1.8698, acc-0.4832, test loss-1.8706, acc-0.4772\n",
      "Iter-48530, train loss-1.8703, acc-0.5400, valid loss-1.8697, acc-0.4830, test loss-1.8705, acc-0.4772\n",
      "Iter-48540, train loss-1.8600, acc-0.5200, valid loss-1.8697, acc-0.4830, test loss-1.8705, acc-0.4772\n",
      "Iter-48550, train loss-1.8798, acc-0.4600, valid loss-1.8696, acc-0.4834, test loss-1.8704, acc-0.4771\n",
      "Iter-48560, train loss-1.8621, acc-0.5400, valid loss-1.8695, acc-0.4838, test loss-1.8704, acc-0.4772\n",
      "Iter-48570, train loss-1.9222, acc-0.3600, valid loss-1.8695, acc-0.4838, test loss-1.8703, acc-0.4772\n",
      "Iter-48580, train loss-1.9268, acc-0.4800, valid loss-1.8694, acc-0.4836, test loss-1.8703, acc-0.4774\n",
      "Iter-48590, train loss-1.9089, acc-0.5400, valid loss-1.8694, acc-0.4836, test loss-1.8702, acc-0.4776\n",
      "Iter-48600, train loss-1.9661, acc-0.3200, valid loss-1.8693, acc-0.4836, test loss-1.8701, acc-0.4774\n",
      "Iter-48610, train loss-1.7731, acc-0.5400, valid loss-1.8693, acc-0.4834, test loss-1.8701, acc-0.4774\n",
      "Iter-48620, train loss-1.8773, acc-0.4600, valid loss-1.8692, acc-0.4834, test loss-1.8700, acc-0.4776\n",
      "Iter-48630, train loss-1.8778, acc-0.5600, valid loss-1.8691, acc-0.4834, test loss-1.8700, acc-0.4774\n",
      "Iter-48640, train loss-1.8615, acc-0.4200, valid loss-1.8691, acc-0.4834, test loss-1.8699, acc-0.4774\n",
      "Iter-48650, train loss-1.8676, acc-0.4800, valid loss-1.8690, acc-0.4834, test loss-1.8698, acc-0.4776\n",
      "Iter-48660, train loss-1.8691, acc-0.4800, valid loss-1.8690, acc-0.4834, test loss-1.8698, acc-0.4775\n",
      "Iter-48670, train loss-1.8164, acc-0.4400, valid loss-1.8689, acc-0.4834, test loss-1.8697, acc-0.4775\n",
      "Iter-48680, train loss-1.9067, acc-0.3800, valid loss-1.8688, acc-0.4834, test loss-1.8697, acc-0.4774\n",
      "Iter-48690, train loss-1.9085, acc-0.4200, valid loss-1.8688, acc-0.4834, test loss-1.8696, acc-0.4775\n",
      "Iter-48700, train loss-1.9242, acc-0.4600, valid loss-1.8687, acc-0.4834, test loss-1.8695, acc-0.4777\n",
      "Iter-48710, train loss-1.8475, acc-0.4800, valid loss-1.8687, acc-0.4834, test loss-1.8695, acc-0.4778\n",
      "Iter-48720, train loss-1.9646, acc-0.4000, valid loss-1.8686, acc-0.4834, test loss-1.8694, acc-0.4780\n",
      "Iter-48730, train loss-1.8860, acc-0.4800, valid loss-1.8686, acc-0.4834, test loss-1.8694, acc-0.4781\n",
      "Iter-48740, train loss-1.9974, acc-0.3200, valid loss-1.8685, acc-0.4836, test loss-1.8693, acc-0.4781\n",
      "Iter-48750, train loss-1.9543, acc-0.3600, valid loss-1.8684, acc-0.4838, test loss-1.8693, acc-0.4781\n",
      "Iter-48760, train loss-1.8975, acc-0.3200, valid loss-1.8684, acc-0.4838, test loss-1.8692, acc-0.4779\n",
      "Iter-48770, train loss-1.8538, acc-0.4600, valid loss-1.8683, acc-0.4838, test loss-1.8691, acc-0.4780\n",
      "Iter-48780, train loss-1.8150, acc-0.5000, valid loss-1.8683, acc-0.4836, test loss-1.8691, acc-0.4781\n",
      "Iter-48790, train loss-1.8083, acc-0.5000, valid loss-1.8682, acc-0.4838, test loss-1.8690, acc-0.4781\n",
      "Iter-48800, train loss-1.8712, acc-0.4600, valid loss-1.8682, acc-0.4840, test loss-1.8690, acc-0.4782\n",
      "Iter-48810, train loss-1.8651, acc-0.5000, valid loss-1.8681, acc-0.4842, test loss-1.8689, acc-0.4781\n",
      "Iter-48820, train loss-1.8747, acc-0.4600, valid loss-1.8680, acc-0.4842, test loss-1.8688, acc-0.4782\n",
      "Iter-48830, train loss-1.8616, acc-0.5000, valid loss-1.8680, acc-0.4842, test loss-1.8688, acc-0.4781\n",
      "Iter-48840, train loss-1.9383, acc-0.4200, valid loss-1.8679, acc-0.4844, test loss-1.8687, acc-0.4781\n",
      "Iter-48850, train loss-1.8943, acc-0.4400, valid loss-1.8678, acc-0.4844, test loss-1.8687, acc-0.4781\n",
      "Iter-48860, train loss-1.9268, acc-0.3000, valid loss-1.8678, acc-0.4844, test loss-1.8686, acc-0.4781\n",
      "Iter-48870, train loss-1.9155, acc-0.4800, valid loss-1.8677, acc-0.4842, test loss-1.8686, acc-0.4782\n",
      "Iter-48880, train loss-1.9366, acc-0.3800, valid loss-1.8677, acc-0.4842, test loss-1.8685, acc-0.4782\n",
      "Iter-48890, train loss-1.8895, acc-0.4600, valid loss-1.8676, acc-0.4842, test loss-1.8684, acc-0.4782\n",
      "Iter-48900, train loss-1.8699, acc-0.4400, valid loss-1.8676, acc-0.4838, test loss-1.8684, acc-0.4782\n",
      "Iter-48910, train loss-2.0292, acc-0.4000, valid loss-1.8675, acc-0.4840, test loss-1.8683, acc-0.4785\n",
      "Iter-48920, train loss-1.8563, acc-0.6400, valid loss-1.8674, acc-0.4842, test loss-1.8683, acc-0.4784\n",
      "Iter-48930, train loss-1.8392, acc-0.6200, valid loss-1.8674, acc-0.4842, test loss-1.8682, acc-0.4784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-48940, train loss-1.8071, acc-0.5400, valid loss-1.8673, acc-0.4842, test loss-1.8682, acc-0.4783\n",
      "Iter-48950, train loss-1.8985, acc-0.4400, valid loss-1.8673, acc-0.4842, test loss-1.8681, acc-0.4783\n",
      "Iter-48960, train loss-1.8286, acc-0.5000, valid loss-1.8672, acc-0.4842, test loss-1.8680, acc-0.4782\n",
      "Iter-48970, train loss-1.8411, acc-0.4800, valid loss-1.8671, acc-0.4842, test loss-1.8680, acc-0.4783\n",
      "Iter-48980, train loss-1.9615, acc-0.3400, valid loss-1.8671, acc-0.4840, test loss-1.8679, acc-0.4785\n",
      "Iter-48990, train loss-1.8500, acc-0.4200, valid loss-1.8670, acc-0.4840, test loss-1.8679, acc-0.4785\n",
      "Iter-49000, train loss-1.7913, acc-0.6400, valid loss-1.8670, acc-0.4836, test loss-1.8678, acc-0.4784\n",
      "Iter-49010, train loss-1.8411, acc-0.5000, valid loss-1.8669, acc-0.4838, test loss-1.8677, acc-0.4785\n",
      "Iter-49020, train loss-1.8534, acc-0.5200, valid loss-1.8668, acc-0.4838, test loss-1.8677, acc-0.4785\n",
      "Iter-49030, train loss-1.8668, acc-0.5400, valid loss-1.8668, acc-0.4838, test loss-1.8676, acc-0.4784\n",
      "Iter-49040, train loss-1.8572, acc-0.3800, valid loss-1.8667, acc-0.4838, test loss-1.8676, acc-0.4784\n",
      "Iter-49050, train loss-1.9290, acc-0.4400, valid loss-1.8667, acc-0.4840, test loss-1.8675, acc-0.4784\n",
      "Iter-49060, train loss-1.9391, acc-0.4200, valid loss-1.8666, acc-0.4840, test loss-1.8675, acc-0.4784\n",
      "Iter-49070, train loss-1.8726, acc-0.4600, valid loss-1.8666, acc-0.4838, test loss-1.8674, acc-0.4784\n",
      "Iter-49080, train loss-1.8831, acc-0.4000, valid loss-1.8665, acc-0.4840, test loss-1.8673, acc-0.4788\n",
      "Iter-49090, train loss-1.9398, acc-0.4000, valid loss-1.8664, acc-0.4840, test loss-1.8673, acc-0.4789\n",
      "Iter-49100, train loss-1.9037, acc-0.5000, valid loss-1.8664, acc-0.4840, test loss-1.8672, acc-0.4787\n",
      "Iter-49110, train loss-1.8364, acc-0.5400, valid loss-1.8663, acc-0.4840, test loss-1.8672, acc-0.4789\n",
      "Iter-49120, train loss-1.8282, acc-0.5200, valid loss-1.8663, acc-0.4842, test loss-1.8671, acc-0.4789\n",
      "Iter-49130, train loss-1.8542, acc-0.5400, valid loss-1.8662, acc-0.4840, test loss-1.8670, acc-0.4787\n",
      "Iter-49140, train loss-1.9178, acc-0.4200, valid loss-1.8661, acc-0.4842, test loss-1.8670, acc-0.4788\n",
      "Iter-49150, train loss-1.8593, acc-0.4800, valid loss-1.8661, acc-0.4840, test loss-1.8669, acc-0.4786\n",
      "Iter-49160, train loss-1.8574, acc-0.5000, valid loss-1.8660, acc-0.4840, test loss-1.8669, acc-0.4786\n",
      "Iter-49170, train loss-1.8223, acc-0.5000, valid loss-1.8660, acc-0.4842, test loss-1.8668, acc-0.4788\n",
      "Iter-49180, train loss-1.8262, acc-0.5000, valid loss-1.8659, acc-0.4842, test loss-1.8667, acc-0.4789\n",
      "Iter-49190, train loss-1.9118, acc-0.4800, valid loss-1.8658, acc-0.4844, test loss-1.8667, acc-0.4788\n",
      "Iter-49200, train loss-1.8444, acc-0.5000, valid loss-1.8658, acc-0.4844, test loss-1.8666, acc-0.4788\n",
      "Iter-49210, train loss-1.8189, acc-0.5400, valid loss-1.8657, acc-0.4844, test loss-1.8666, acc-0.4788\n",
      "Iter-49220, train loss-1.8269, acc-0.4800, valid loss-1.8657, acc-0.4844, test loss-1.8665, acc-0.4788\n",
      "Iter-49230, train loss-1.9993, acc-0.3600, valid loss-1.8656, acc-0.4844, test loss-1.8665, acc-0.4789\n",
      "Iter-49240, train loss-1.9025, acc-0.3800, valid loss-1.8656, acc-0.4844, test loss-1.8664, acc-0.4787\n",
      "Iter-49250, train loss-1.8046, acc-0.4200, valid loss-1.8655, acc-0.4844, test loss-1.8663, acc-0.4787\n",
      "Iter-49260, train loss-1.7990, acc-0.6200, valid loss-1.8654, acc-0.4844, test loss-1.8663, acc-0.4790\n",
      "Iter-49270, train loss-1.8391, acc-0.5200, valid loss-1.8654, acc-0.4844, test loss-1.8662, acc-0.4789\n",
      "Iter-49280, train loss-1.8234, acc-0.5600, valid loss-1.8653, acc-0.4844, test loss-1.8662, acc-0.4790\n",
      "Iter-49290, train loss-1.7741, acc-0.5400, valid loss-1.8653, acc-0.4844, test loss-1.8661, acc-0.4790\n",
      "Iter-49300, train loss-1.8579, acc-0.4400, valid loss-1.8652, acc-0.4844, test loss-1.8660, acc-0.4790\n",
      "Iter-49310, train loss-1.9187, acc-0.4400, valid loss-1.8652, acc-0.4844, test loss-1.8660, acc-0.4791\n",
      "Iter-49320, train loss-1.9051, acc-0.4600, valid loss-1.8651, acc-0.4844, test loss-1.8659, acc-0.4790\n",
      "Iter-49330, train loss-1.8191, acc-0.5200, valid loss-1.8650, acc-0.4842, test loss-1.8659, acc-0.4789\n",
      "Iter-49340, train loss-1.8917, acc-0.4600, valid loss-1.8650, acc-0.4842, test loss-1.8658, acc-0.4790\n",
      "Iter-49350, train loss-1.8446, acc-0.4800, valid loss-1.8649, acc-0.4842, test loss-1.8658, acc-0.4790\n",
      "Iter-49360, train loss-1.8412, acc-0.4600, valid loss-1.8649, acc-0.4844, test loss-1.8657, acc-0.4790\n",
      "Iter-49370, train loss-1.8151, acc-0.5000, valid loss-1.8648, acc-0.4844, test loss-1.8656, acc-0.4791\n",
      "Iter-49380, train loss-1.8601, acc-0.4800, valid loss-1.8647, acc-0.4844, test loss-1.8656, acc-0.4791\n",
      "Iter-49390, train loss-1.8851, acc-0.4200, valid loss-1.8647, acc-0.4844, test loss-1.8655, acc-0.4790\n",
      "Iter-49400, train loss-1.8667, acc-0.4200, valid loss-1.8646, acc-0.4846, test loss-1.8655, acc-0.4791\n",
      "Iter-49410, train loss-1.8997, acc-0.4400, valid loss-1.8646, acc-0.4846, test loss-1.8654, acc-0.4793\n",
      "Iter-49420, train loss-1.8812, acc-0.4800, valid loss-1.8645, acc-0.4846, test loss-1.8653, acc-0.4792\n",
      "Iter-49430, train loss-1.9026, acc-0.3600, valid loss-1.8645, acc-0.4846, test loss-1.8653, acc-0.4792\n",
      "Iter-49440, train loss-1.8637, acc-0.4800, valid loss-1.8644, acc-0.4846, test loss-1.8652, acc-0.4792\n",
      "Iter-49450, train loss-1.8491, acc-0.5200, valid loss-1.8643, acc-0.4846, test loss-1.8652, acc-0.4793\n",
      "Iter-49460, train loss-1.9708, acc-0.4600, valid loss-1.8643, acc-0.4846, test loss-1.8651, acc-0.4792\n",
      "Iter-49470, train loss-1.8837, acc-0.5200, valid loss-1.8642, acc-0.4848, test loss-1.8651, acc-0.4794\n",
      "Iter-49480, train loss-2.0113, acc-0.4000, valid loss-1.8642, acc-0.4850, test loss-1.8650, acc-0.4794\n",
      "Iter-49490, train loss-1.7956, acc-0.5400, valid loss-1.8641, acc-0.4850, test loss-1.8649, acc-0.4792\n",
      "Iter-49500, train loss-1.8802, acc-0.4200, valid loss-1.8641, acc-0.4852, test loss-1.8649, acc-0.4792\n",
      "Iter-49510, train loss-1.9326, acc-0.4400, valid loss-1.8640, acc-0.4848, test loss-1.8648, acc-0.4794\n",
      "Iter-49520, train loss-1.8116, acc-0.5200, valid loss-1.8640, acc-0.4848, test loss-1.8648, acc-0.4795\n",
      "Iter-49530, train loss-1.8763, acc-0.5400, valid loss-1.8639, acc-0.4848, test loss-1.8647, acc-0.4796\n",
      "Iter-49540, train loss-1.9844, acc-0.4400, valid loss-1.8638, acc-0.4848, test loss-1.8647, acc-0.4797\n",
      "Iter-49550, train loss-1.8460, acc-0.6000, valid loss-1.8638, acc-0.4848, test loss-1.8646, acc-0.4796\n",
      "Iter-49560, train loss-1.8384, acc-0.4600, valid loss-1.8637, acc-0.4850, test loss-1.8645, acc-0.4795\n",
      "Iter-49570, train loss-1.8131, acc-0.6000, valid loss-1.8637, acc-0.4850, test loss-1.8645, acc-0.4794\n",
      "Iter-49580, train loss-1.9093, acc-0.3800, valid loss-1.8636, acc-0.4848, test loss-1.8644, acc-0.4794\n",
      "Iter-49590, train loss-1.8514, acc-0.5000, valid loss-1.8636, acc-0.4848, test loss-1.8644, acc-0.4795\n",
      "Iter-49600, train loss-1.8114, acc-0.5200, valid loss-1.8635, acc-0.4846, test loss-1.8643, acc-0.4795\n",
      "Iter-49610, train loss-1.8204, acc-0.5000, valid loss-1.8634, acc-0.4846, test loss-1.8643, acc-0.4794\n",
      "Iter-49620, train loss-1.9432, acc-0.3800, valid loss-1.8634, acc-0.4846, test loss-1.8642, acc-0.4793\n",
      "Iter-49630, train loss-1.9273, acc-0.4600, valid loss-1.8633, acc-0.4846, test loss-1.8641, acc-0.4794\n",
      "Iter-49640, train loss-1.8988, acc-0.4600, valid loss-1.8633, acc-0.4846, test loss-1.8641, acc-0.4795\n",
      "Iter-49650, train loss-1.8318, acc-0.5400, valid loss-1.8632, acc-0.4846, test loss-1.8640, acc-0.4796\n",
      "Iter-49660, train loss-1.9706, acc-0.4400, valid loss-1.8631, acc-0.4846, test loss-1.8640, acc-0.4794\n",
      "Iter-49670, train loss-1.8947, acc-0.4800, valid loss-1.8631, acc-0.4844, test loss-1.8639, acc-0.4797\n",
      "Iter-49680, train loss-1.8767, acc-0.4000, valid loss-1.8630, acc-0.4844, test loss-1.8639, acc-0.4797\n",
      "Iter-49690, train loss-1.9151, acc-0.4000, valid loss-1.8630, acc-0.4844, test loss-1.8638, acc-0.4797\n",
      "Iter-49700, train loss-1.8192, acc-0.5200, valid loss-1.8629, acc-0.4844, test loss-1.8637, acc-0.4799\n",
      "Iter-49710, train loss-1.7732, acc-0.5200, valid loss-1.8629, acc-0.4844, test loss-1.8637, acc-0.4800\n",
      "Iter-49720, train loss-1.8360, acc-0.4400, valid loss-1.8628, acc-0.4846, test loss-1.8636, acc-0.4801\n",
      "Iter-49730, train loss-1.8959, acc-0.4000, valid loss-1.8627, acc-0.4846, test loss-1.8636, acc-0.4801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-49740, train loss-1.8633, acc-0.4200, valid loss-1.8627, acc-0.4848, test loss-1.8635, acc-0.4801\n",
      "Iter-49750, train loss-1.9640, acc-0.3800, valid loss-1.8626, acc-0.4848, test loss-1.8635, acc-0.4800\n",
      "Iter-49760, train loss-1.9523, acc-0.5000, valid loss-1.8626, acc-0.4848, test loss-1.8634, acc-0.4802\n",
      "Iter-49770, train loss-1.7736, acc-0.5600, valid loss-1.8625, acc-0.4848, test loss-1.8633, acc-0.4802\n",
      "Iter-49780, train loss-1.8902, acc-0.4800, valid loss-1.8625, acc-0.4846, test loss-1.8633, acc-0.4802\n",
      "Iter-49790, train loss-1.8325, acc-0.5200, valid loss-1.8624, acc-0.4848, test loss-1.8632, acc-0.4804\n",
      "Iter-49800, train loss-1.9176, acc-0.4000, valid loss-1.8623, acc-0.4848, test loss-1.8632, acc-0.4802\n",
      "Iter-49810, train loss-1.8622, acc-0.4400, valid loss-1.8623, acc-0.4848, test loss-1.8631, acc-0.4802\n",
      "Iter-49820, train loss-1.8706, acc-0.4200, valid loss-1.8622, acc-0.4850, test loss-1.8631, acc-0.4802\n",
      "Iter-49830, train loss-1.7959, acc-0.5400, valid loss-1.8622, acc-0.4850, test loss-1.8630, acc-0.4801\n",
      "Iter-49840, train loss-1.8837, acc-0.5600, valid loss-1.8621, acc-0.4848, test loss-1.8629, acc-0.4801\n",
      "Iter-49850, train loss-1.8754, acc-0.4600, valid loss-1.8620, acc-0.4848, test loss-1.8629, acc-0.4802\n",
      "Iter-49860, train loss-1.8777, acc-0.5200, valid loss-1.8620, acc-0.4850, test loss-1.8628, acc-0.4801\n",
      "Iter-49870, train loss-1.8575, acc-0.5000, valid loss-1.8619, acc-0.4850, test loss-1.8628, acc-0.4801\n",
      "Iter-49880, train loss-1.8623, acc-0.5600, valid loss-1.8619, acc-0.4852, test loss-1.8627, acc-0.4801\n",
      "Iter-49890, train loss-1.8355, acc-0.5400, valid loss-1.8618, acc-0.4852, test loss-1.8627, acc-0.4801\n",
      "Iter-49900, train loss-1.8292, acc-0.4600, valid loss-1.8617, acc-0.4852, test loss-1.8626, acc-0.4801\n",
      "Iter-49910, train loss-1.7955, acc-0.4600, valid loss-1.8617, acc-0.4854, test loss-1.8625, acc-0.4802\n",
      "Iter-49920, train loss-1.9687, acc-0.3800, valid loss-1.8616, acc-0.4854, test loss-1.8625, acc-0.4803\n",
      "Iter-49930, train loss-1.8651, acc-0.5000, valid loss-1.8616, acc-0.4854, test loss-1.8624, acc-0.4803\n",
      "Iter-49940, train loss-1.8467, acc-0.4600, valid loss-1.8615, acc-0.4854, test loss-1.8624, acc-0.4804\n",
      "Iter-49950, train loss-1.8965, acc-0.4400, valid loss-1.8615, acc-0.4856, test loss-1.8623, acc-0.4806\n",
      "Iter-49960, train loss-1.7958, acc-0.6200, valid loss-1.8614, acc-0.4856, test loss-1.8622, acc-0.4804\n",
      "Iter-49970, train loss-1.8031, acc-0.5400, valid loss-1.8613, acc-0.4858, test loss-1.8622, acc-0.4804\n",
      "Iter-49980, train loss-1.8654, acc-0.4600, valid loss-1.8613, acc-0.4860, test loss-1.8621, acc-0.4804\n",
      "Iter-49990, train loss-1.9674, acc-0.3000, valid loss-1.8612, acc-0.4858, test loss-1.8621, acc-0.4805\n",
      "Iter-50000, train loss-1.8472, acc-0.4600, valid loss-1.8612, acc-0.4856, test loss-1.8620, acc-0.4806\n",
      "Iter-50010, train loss-1.8545, acc-0.4600, valid loss-1.8611, acc-0.4856, test loss-1.8620, acc-0.4803\n",
      "Iter-50020, train loss-1.9748, acc-0.3400, valid loss-1.8611, acc-0.4854, test loss-1.8619, acc-0.4804\n",
      "Iter-50030, train loss-1.7848, acc-0.5400, valid loss-1.8610, acc-0.4854, test loss-1.8618, acc-0.4804\n",
      "Iter-50040, train loss-1.8400, acc-0.4800, valid loss-1.8609, acc-0.4854, test loss-1.8618, acc-0.4804\n",
      "Iter-50050, train loss-1.9063, acc-0.4400, valid loss-1.8609, acc-0.4854, test loss-1.8617, acc-0.4806\n",
      "Iter-50060, train loss-1.9164, acc-0.4600, valid loss-1.8608, acc-0.4854, test loss-1.8617, acc-0.4805\n",
      "Iter-50070, train loss-1.8312, acc-0.5400, valid loss-1.8608, acc-0.4854, test loss-1.8616, acc-0.4809\n",
      "Iter-50080, train loss-1.8206, acc-0.4600, valid loss-1.8607, acc-0.4854, test loss-1.8615, acc-0.4809\n",
      "Iter-50090, train loss-1.8592, acc-0.4800, valid loss-1.8607, acc-0.4854, test loss-1.8615, acc-0.4809\n",
      "Iter-50100, train loss-1.8691, acc-0.4400, valid loss-1.8606, acc-0.4854, test loss-1.8614, acc-0.4808\n",
      "Iter-50110, train loss-1.8909, acc-0.3200, valid loss-1.8605, acc-0.4858, test loss-1.8614, acc-0.4809\n",
      "Iter-50120, train loss-1.7856, acc-0.4600, valid loss-1.8605, acc-0.4858, test loss-1.8613, acc-0.4807\n",
      "Iter-50130, train loss-1.8851, acc-0.4400, valid loss-1.8604, acc-0.4858, test loss-1.8613, acc-0.4808\n",
      "Iter-50140, train loss-1.8647, acc-0.4400, valid loss-1.8604, acc-0.4858, test loss-1.8612, acc-0.4810\n",
      "Iter-50150, train loss-1.9109, acc-0.4400, valid loss-1.8603, acc-0.4858, test loss-1.8611, acc-0.4808\n",
      "Iter-50160, train loss-1.8586, acc-0.5000, valid loss-1.8603, acc-0.4856, test loss-1.8611, acc-0.4808\n",
      "Iter-50170, train loss-1.9192, acc-0.4600, valid loss-1.8602, acc-0.4856, test loss-1.8610, acc-0.4807\n",
      "Iter-50180, train loss-1.9250, acc-0.4000, valid loss-1.8601, acc-0.4858, test loss-1.8610, acc-0.4807\n",
      "Iter-50190, train loss-1.8651, acc-0.4200, valid loss-1.8601, acc-0.4858, test loss-1.8609, acc-0.4809\n",
      "Iter-50200, train loss-1.8844, acc-0.4800, valid loss-1.8600, acc-0.4858, test loss-1.8609, acc-0.4810\n",
      "Iter-50210, train loss-1.8589, acc-0.5000, valid loss-1.8600, acc-0.4858, test loss-1.8608, acc-0.4809\n",
      "Iter-50220, train loss-1.7921, acc-0.6000, valid loss-1.8599, acc-0.4856, test loss-1.8607, acc-0.4808\n",
      "Iter-50230, train loss-1.9497, acc-0.3800, valid loss-1.8599, acc-0.4858, test loss-1.8607, acc-0.4810\n",
      "Iter-50240, train loss-1.8515, acc-0.5000, valid loss-1.8598, acc-0.4858, test loss-1.8606, acc-0.4812\n",
      "Iter-50250, train loss-1.8567, acc-0.4400, valid loss-1.8598, acc-0.4860, test loss-1.8606, acc-0.4814\n",
      "Iter-50260, train loss-1.9331, acc-0.4200, valid loss-1.8597, acc-0.4860, test loss-1.8605, acc-0.4812\n",
      "Iter-50270, train loss-1.9281, acc-0.4000, valid loss-1.8596, acc-0.4860, test loss-1.8605, acc-0.4813\n",
      "Iter-50280, train loss-1.8310, acc-0.4400, valid loss-1.8596, acc-0.4860, test loss-1.8604, acc-0.4813\n",
      "Iter-50290, train loss-1.9511, acc-0.3600, valid loss-1.8595, acc-0.4858, test loss-1.8603, acc-0.4814\n",
      "Iter-50300, train loss-1.9081, acc-0.4200, valid loss-1.8595, acc-0.4860, test loss-1.8603, acc-0.4814\n",
      "Iter-50310, train loss-1.8742, acc-0.3800, valid loss-1.8594, acc-0.4860, test loss-1.8602, acc-0.4814\n",
      "Iter-50320, train loss-1.8568, acc-0.5600, valid loss-1.8594, acc-0.4860, test loss-1.8602, acc-0.4813\n",
      "Iter-50330, train loss-1.8914, acc-0.4800, valid loss-1.8593, acc-0.4860, test loss-1.8601, acc-0.4813\n",
      "Iter-50340, train loss-1.9254, acc-0.3600, valid loss-1.8592, acc-0.4860, test loss-1.8601, acc-0.4814\n",
      "Iter-50350, train loss-1.8594, acc-0.5200, valid loss-1.8592, acc-0.4858, test loss-1.8600, acc-0.4812\n",
      "Iter-50360, train loss-1.8208, acc-0.5800, valid loss-1.8591, acc-0.4858, test loss-1.8599, acc-0.4814\n",
      "Iter-50370, train loss-1.8069, acc-0.4400, valid loss-1.8591, acc-0.4858, test loss-1.8599, acc-0.4814\n",
      "Iter-50380, train loss-1.7888, acc-0.5600, valid loss-1.8590, acc-0.4860, test loss-1.8598, acc-0.4815\n",
      "Iter-50390, train loss-1.8827, acc-0.4200, valid loss-1.8589, acc-0.4860, test loss-1.8598, acc-0.4812\n",
      "Iter-50400, train loss-1.8666, acc-0.3600, valid loss-1.8589, acc-0.4862, test loss-1.8597, acc-0.4813\n",
      "Iter-50410, train loss-1.8788, acc-0.4200, valid loss-1.8588, acc-0.4860, test loss-1.8597, acc-0.4812\n",
      "Iter-50420, train loss-1.7879, acc-0.5800, valid loss-1.8588, acc-0.4860, test loss-1.8596, acc-0.4814\n",
      "Iter-50430, train loss-1.8848, acc-0.4600, valid loss-1.8587, acc-0.4858, test loss-1.8595, acc-0.4816\n",
      "Iter-50440, train loss-1.8853, acc-0.4600, valid loss-1.8587, acc-0.4858, test loss-1.8595, acc-0.4815\n",
      "Iter-50450, train loss-1.8578, acc-0.4400, valid loss-1.8586, acc-0.4858, test loss-1.8594, acc-0.4815\n",
      "Iter-50460, train loss-1.8683, acc-0.4400, valid loss-1.8585, acc-0.4858, test loss-1.8594, acc-0.4814\n",
      "Iter-50470, train loss-1.9783, acc-0.3400, valid loss-1.8585, acc-0.4860, test loss-1.8593, acc-0.4814\n",
      "Iter-50480, train loss-1.9444, acc-0.4400, valid loss-1.8584, acc-0.4860, test loss-1.8593, acc-0.4814\n",
      "Iter-50490, train loss-1.8965, acc-0.3600, valid loss-1.8584, acc-0.4862, test loss-1.8592, acc-0.4813\n",
      "Iter-50500, train loss-1.8172, acc-0.4600, valid loss-1.8583, acc-0.4860, test loss-1.8591, acc-0.4814\n",
      "Iter-50510, train loss-1.8980, acc-0.4800, valid loss-1.8582, acc-0.4860, test loss-1.8591, acc-0.4814\n",
      "Iter-50520, train loss-1.8382, acc-0.4400, valid loss-1.8582, acc-0.4860, test loss-1.8590, acc-0.4815\n",
      "Iter-50530, train loss-1.7886, acc-0.6400, valid loss-1.8581, acc-0.4860, test loss-1.8590, acc-0.4814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-50540, train loss-1.8988, acc-0.4400, valid loss-1.8581, acc-0.4860, test loss-1.8589, acc-0.4814\n",
      "Iter-50550, train loss-1.7449, acc-0.5800, valid loss-1.8580, acc-0.4860, test loss-1.8589, acc-0.4814\n",
      "Iter-50560, train loss-1.8558, acc-0.5000, valid loss-1.8580, acc-0.4860, test loss-1.8588, acc-0.4814\n",
      "Iter-50570, train loss-1.8340, acc-0.5800, valid loss-1.8579, acc-0.4860, test loss-1.8587, acc-0.4815\n",
      "Iter-50580, train loss-1.7955, acc-0.5000, valid loss-1.8578, acc-0.4862, test loss-1.8587, acc-0.4815\n",
      "Iter-50590, train loss-1.8242, acc-0.5200, valid loss-1.8578, acc-0.4860, test loss-1.8586, acc-0.4815\n",
      "Iter-50600, train loss-1.9442, acc-0.3400, valid loss-1.8577, acc-0.4860, test loss-1.8586, acc-0.4815\n",
      "Iter-50610, train loss-1.7958, acc-0.5200, valid loss-1.8577, acc-0.4860, test loss-1.8585, acc-0.4814\n",
      "Iter-50620, train loss-1.8857, acc-0.4400, valid loss-1.8576, acc-0.4858, test loss-1.8584, acc-0.4815\n",
      "Iter-50630, train loss-1.8870, acc-0.4800, valid loss-1.8575, acc-0.4860, test loss-1.8584, acc-0.4814\n",
      "Iter-50640, train loss-1.8353, acc-0.5600, valid loss-1.8575, acc-0.4858, test loss-1.8583, acc-0.4814\n",
      "Iter-50650, train loss-1.8398, acc-0.5000, valid loss-1.8574, acc-0.4856, test loss-1.8583, acc-0.4815\n",
      "Iter-50660, train loss-1.9203, acc-0.4200, valid loss-1.8574, acc-0.4858, test loss-1.8582, acc-0.4816\n",
      "Iter-50670, train loss-1.8456, acc-0.4200, valid loss-1.8573, acc-0.4856, test loss-1.8581, acc-0.4815\n",
      "Iter-50680, train loss-1.8761, acc-0.4400, valid loss-1.8572, acc-0.4856, test loss-1.8581, acc-0.4815\n",
      "Iter-50690, train loss-1.9316, acc-0.4000, valid loss-1.8572, acc-0.4856, test loss-1.8580, acc-0.4813\n",
      "Iter-50700, train loss-1.8530, acc-0.5200, valid loss-1.8571, acc-0.4854, test loss-1.8580, acc-0.4813\n",
      "Iter-50710, train loss-1.8839, acc-0.4600, valid loss-1.8571, acc-0.4858, test loss-1.8579, acc-0.4813\n",
      "Iter-50720, train loss-1.8564, acc-0.5000, valid loss-1.8570, acc-0.4856, test loss-1.8579, acc-0.4813\n",
      "Iter-50730, train loss-1.8694, acc-0.5000, valid loss-1.8570, acc-0.4858, test loss-1.8578, acc-0.4814\n",
      "Iter-50740, train loss-1.8670, acc-0.4400, valid loss-1.8569, acc-0.4856, test loss-1.8578, acc-0.4816\n",
      "Iter-50750, train loss-1.9506, acc-0.4000, valid loss-1.8569, acc-0.4858, test loss-1.8577, acc-0.4817\n",
      "Iter-50760, train loss-1.9603, acc-0.4200, valid loss-1.8568, acc-0.4858, test loss-1.8576, acc-0.4818\n",
      "Iter-50770, train loss-1.8423, acc-0.5000, valid loss-1.8567, acc-0.4856, test loss-1.8576, acc-0.4816\n",
      "Iter-50780, train loss-1.9362, acc-0.3800, valid loss-1.8567, acc-0.4860, test loss-1.8575, acc-0.4816\n",
      "Iter-50790, train loss-1.9313, acc-0.3600, valid loss-1.8566, acc-0.4864, test loss-1.8575, acc-0.4817\n",
      "Iter-50800, train loss-1.9381, acc-0.4000, valid loss-1.8566, acc-0.4862, test loss-1.8574, acc-0.4817\n",
      "Iter-50810, train loss-1.8015, acc-0.5600, valid loss-1.8565, acc-0.4864, test loss-1.8574, acc-0.4817\n",
      "Iter-50820, train loss-1.9289, acc-0.4000, valid loss-1.8565, acc-0.4864, test loss-1.8573, acc-0.4817\n",
      "Iter-50830, train loss-1.8956, acc-0.5200, valid loss-1.8564, acc-0.4860, test loss-1.8572, acc-0.4817\n",
      "Iter-50840, train loss-1.9096, acc-0.4600, valid loss-1.8564, acc-0.4862, test loss-1.8572, acc-0.4817\n",
      "Iter-50850, train loss-1.8469, acc-0.5000, valid loss-1.8563, acc-0.4860, test loss-1.8571, acc-0.4818\n",
      "Iter-50860, train loss-1.8693, acc-0.5000, valid loss-1.8562, acc-0.4862, test loss-1.8571, acc-0.4815\n",
      "Iter-50870, train loss-1.8584, acc-0.4800, valid loss-1.8562, acc-0.4860, test loss-1.8570, acc-0.4817\n",
      "Iter-50880, train loss-1.9086, acc-0.4000, valid loss-1.8561, acc-0.4862, test loss-1.8570, acc-0.4821\n",
      "Iter-50890, train loss-1.8276, acc-0.4800, valid loss-1.8561, acc-0.4862, test loss-1.8569, acc-0.4817\n",
      "Iter-50900, train loss-1.9000, acc-0.5200, valid loss-1.8560, acc-0.4860, test loss-1.8569, acc-0.4816\n",
      "Iter-50910, train loss-1.8513, acc-0.4400, valid loss-1.8560, acc-0.4860, test loss-1.8568, acc-0.4815\n",
      "Iter-50920, train loss-1.8172, acc-0.5200, valid loss-1.8559, acc-0.4860, test loss-1.8567, acc-0.4815\n",
      "Iter-50930, train loss-1.8662, acc-0.4800, valid loss-1.8558, acc-0.4860, test loss-1.8567, acc-0.4816\n",
      "Iter-50940, train loss-1.8821, acc-0.4000, valid loss-1.8558, acc-0.4858, test loss-1.8566, acc-0.4814\n",
      "Iter-50950, train loss-1.8815, acc-0.4600, valid loss-1.8557, acc-0.4856, test loss-1.8566, acc-0.4814\n",
      "Iter-50960, train loss-1.9205, acc-0.3800, valid loss-1.8557, acc-0.4854, test loss-1.8565, acc-0.4813\n",
      "Iter-50970, train loss-1.8084, acc-0.6200, valid loss-1.8556, acc-0.4854, test loss-1.8564, acc-0.4813\n",
      "Iter-50980, train loss-1.8735, acc-0.4000, valid loss-1.8555, acc-0.4854, test loss-1.8564, acc-0.4813\n",
      "Iter-50990, train loss-1.8409, acc-0.6000, valid loss-1.8555, acc-0.4858, test loss-1.8563, acc-0.4816\n",
      "Iter-51000, train loss-1.8647, acc-0.5000, valid loss-1.8554, acc-0.4860, test loss-1.8563, acc-0.4817\n",
      "Iter-51010, train loss-1.8690, acc-0.4600, valid loss-1.8554, acc-0.4860, test loss-1.8562, acc-0.4819\n",
      "Iter-51020, train loss-1.8712, acc-0.4000, valid loss-1.8553, acc-0.4856, test loss-1.8562, acc-0.4817\n",
      "Iter-51030, train loss-1.8557, acc-0.4600, valid loss-1.8553, acc-0.4860, test loss-1.8561, acc-0.4817\n",
      "Iter-51040, train loss-1.8546, acc-0.4200, valid loss-1.8552, acc-0.4864, test loss-1.8560, acc-0.4816\n",
      "Iter-51050, train loss-1.9305, acc-0.4000, valid loss-1.8551, acc-0.4866, test loss-1.8560, acc-0.4816\n",
      "Iter-51060, train loss-1.8682, acc-0.5800, valid loss-1.8551, acc-0.4866, test loss-1.8559, acc-0.4815\n",
      "Iter-51070, train loss-1.9703, acc-0.3000, valid loss-1.8550, acc-0.4868, test loss-1.8559, acc-0.4817\n",
      "Iter-51080, train loss-1.8240, acc-0.6400, valid loss-1.8550, acc-0.4866, test loss-1.8558, acc-0.4816\n",
      "Iter-51090, train loss-1.8512, acc-0.4000, valid loss-1.8549, acc-0.4868, test loss-1.8558, acc-0.4817\n",
      "Iter-51100, train loss-1.8008, acc-0.5000, valid loss-1.8549, acc-0.4864, test loss-1.8557, acc-0.4816\n",
      "Iter-51110, train loss-1.8783, acc-0.4400, valid loss-1.8548, acc-0.4860, test loss-1.8556, acc-0.4815\n",
      "Iter-51120, train loss-1.8532, acc-0.5200, valid loss-1.8547, acc-0.4864, test loss-1.8556, acc-0.4814\n",
      "Iter-51130, train loss-1.9605, acc-0.4400, valid loss-1.8547, acc-0.4862, test loss-1.8555, acc-0.4815\n",
      "Iter-51140, train loss-1.8982, acc-0.4600, valid loss-1.8546, acc-0.4862, test loss-1.8555, acc-0.4817\n",
      "Iter-51150, train loss-1.8111, acc-0.4400, valid loss-1.8546, acc-0.4866, test loss-1.8554, acc-0.4819\n",
      "Iter-51160, train loss-1.8741, acc-0.4200, valid loss-1.8545, acc-0.4866, test loss-1.8554, acc-0.4817\n",
      "Iter-51170, train loss-1.8560, acc-0.5600, valid loss-1.8545, acc-0.4868, test loss-1.8553, acc-0.4817\n",
      "Iter-51180, train loss-1.8881, acc-0.4400, valid loss-1.8544, acc-0.4868, test loss-1.8552, acc-0.4818\n",
      "Iter-51190, train loss-1.9097, acc-0.4800, valid loss-1.8544, acc-0.4872, test loss-1.8552, acc-0.4820\n",
      "Iter-51200, train loss-1.8604, acc-0.5200, valid loss-1.8543, acc-0.4870, test loss-1.8551, acc-0.4818\n",
      "Iter-51210, train loss-1.9399, acc-0.4400, valid loss-1.8543, acc-0.4870, test loss-1.8551, acc-0.4818\n",
      "Iter-51220, train loss-1.8995, acc-0.5000, valid loss-1.8542, acc-0.4870, test loss-1.8550, acc-0.4818\n",
      "Iter-51230, train loss-1.9020, acc-0.4800, valid loss-1.8541, acc-0.4870, test loss-1.8550, acc-0.4818\n",
      "Iter-51240, train loss-1.8055, acc-0.5200, valid loss-1.8541, acc-0.4872, test loss-1.8549, acc-0.4818\n",
      "Iter-51250, train loss-1.9421, acc-0.4000, valid loss-1.8540, acc-0.4872, test loss-1.8549, acc-0.4818\n",
      "Iter-51260, train loss-1.8584, acc-0.5400, valid loss-1.8540, acc-0.4872, test loss-1.8548, acc-0.4818\n",
      "Iter-51270, train loss-1.8527, acc-0.5000, valid loss-1.8539, acc-0.4872, test loss-1.8548, acc-0.4818\n",
      "Iter-51280, train loss-1.8125, acc-0.5600, valid loss-1.8539, acc-0.4870, test loss-1.8547, acc-0.4820\n",
      "Iter-51290, train loss-1.9339, acc-0.4600, valid loss-1.8538, acc-0.4870, test loss-1.8546, acc-0.4819\n",
      "Iter-51300, train loss-1.8106, acc-0.5200, valid loss-1.8538, acc-0.4874, test loss-1.8546, acc-0.4822\n",
      "Iter-51310, train loss-1.9077, acc-0.4800, valid loss-1.8537, acc-0.4872, test loss-1.8545, acc-0.4821\n",
      "Iter-51320, train loss-1.8366, acc-0.5200, valid loss-1.8537, acc-0.4872, test loss-1.8545, acc-0.4820\n",
      "Iter-51330, train loss-1.8834, acc-0.5000, valid loss-1.8536, acc-0.4874, test loss-1.8544, acc-0.4821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-51340, train loss-1.8535, acc-0.3800, valid loss-1.8535, acc-0.4874, test loss-1.8544, acc-0.4821\n",
      "Iter-51350, train loss-1.9064, acc-0.4400, valid loss-1.8535, acc-0.4874, test loss-1.8543, acc-0.4822\n",
      "Iter-51360, train loss-1.9010, acc-0.3800, valid loss-1.8534, acc-0.4874, test loss-1.8542, acc-0.4824\n",
      "Iter-51370, train loss-1.9143, acc-0.3800, valid loss-1.8534, acc-0.4874, test loss-1.8542, acc-0.4823\n",
      "Iter-51380, train loss-1.8537, acc-0.5400, valid loss-1.8533, acc-0.4874, test loss-1.8541, acc-0.4824\n",
      "Iter-51390, train loss-1.8693, acc-0.4800, valid loss-1.8533, acc-0.4874, test loss-1.8541, acc-0.4824\n",
      "Iter-51400, train loss-1.7489, acc-0.6400, valid loss-1.8532, acc-0.4874, test loss-1.8540, acc-0.4824\n",
      "Iter-51410, train loss-1.8841, acc-0.4400, valid loss-1.8532, acc-0.4876, test loss-1.8540, acc-0.4824\n",
      "Iter-51420, train loss-1.8361, acc-0.5000, valid loss-1.8531, acc-0.4878, test loss-1.8539, acc-0.4825\n",
      "Iter-51430, train loss-1.8008, acc-0.5600, valid loss-1.8530, acc-0.4878, test loss-1.8539, acc-0.4825\n",
      "Iter-51440, train loss-1.9064, acc-0.5400, valid loss-1.8530, acc-0.4876, test loss-1.8538, acc-0.4826\n",
      "Iter-51450, train loss-1.8621, acc-0.5400, valid loss-1.8529, acc-0.4876, test loss-1.8538, acc-0.4827\n",
      "Iter-51460, train loss-1.8520, acc-0.4800, valid loss-1.8529, acc-0.4876, test loss-1.8537, acc-0.4825\n",
      "Iter-51470, train loss-1.7764, acc-0.6000, valid loss-1.8528, acc-0.4874, test loss-1.8536, acc-0.4826\n",
      "Iter-51480, train loss-1.7908, acc-0.5800, valid loss-1.8528, acc-0.4876, test loss-1.8536, acc-0.4826\n",
      "Iter-51490, train loss-1.9980, acc-0.4200, valid loss-1.8527, acc-0.4876, test loss-1.8535, acc-0.4826\n",
      "Iter-51500, train loss-1.7873, acc-0.5200, valid loss-1.8526, acc-0.4876, test loss-1.8535, acc-0.4825\n",
      "Iter-51510, train loss-1.7843, acc-0.5000, valid loss-1.8526, acc-0.4876, test loss-1.8534, acc-0.4825\n",
      "Iter-51520, train loss-1.8677, acc-0.4400, valid loss-1.8525, acc-0.4876, test loss-1.8534, acc-0.4826\n",
      "Iter-51530, train loss-1.9387, acc-0.3800, valid loss-1.8525, acc-0.4876, test loss-1.8533, acc-0.4826\n",
      "Iter-51540, train loss-1.8192, acc-0.5000, valid loss-1.8524, acc-0.4878, test loss-1.8532, acc-0.4826\n",
      "Iter-51550, train loss-1.8544, acc-0.5000, valid loss-1.8524, acc-0.4876, test loss-1.8532, acc-0.4826\n",
      "Iter-51560, train loss-1.7127, acc-0.6600, valid loss-1.8523, acc-0.4876, test loss-1.8531, acc-0.4828\n",
      "Iter-51570, train loss-1.8743, acc-0.4800, valid loss-1.8523, acc-0.4876, test loss-1.8531, acc-0.4828\n",
      "Iter-51580, train loss-1.8228, acc-0.5000, valid loss-1.8522, acc-0.4876, test loss-1.8530, acc-0.4829\n",
      "Iter-51590, train loss-1.8216, acc-0.6000, valid loss-1.8521, acc-0.4878, test loss-1.8530, acc-0.4829\n",
      "Iter-51600, train loss-1.9503, acc-0.4200, valid loss-1.8521, acc-0.4878, test loss-1.8529, acc-0.4828\n",
      "Iter-51610, train loss-1.7591, acc-0.5200, valid loss-1.8520, acc-0.4878, test loss-1.8529, acc-0.4827\n",
      "Iter-51620, train loss-1.8312, acc-0.4800, valid loss-1.8520, acc-0.4878, test loss-1.8528, acc-0.4827\n",
      "Iter-51630, train loss-1.7774, acc-0.5600, valid loss-1.8519, acc-0.4876, test loss-1.8527, acc-0.4827\n",
      "Iter-51640, train loss-1.9071, acc-0.4400, valid loss-1.8519, acc-0.4878, test loss-1.8527, acc-0.4827\n",
      "Iter-51650, train loss-1.8981, acc-0.4800, valid loss-1.8518, acc-0.4878, test loss-1.8526, acc-0.4830\n",
      "Iter-51660, train loss-1.7925, acc-0.5000, valid loss-1.8518, acc-0.4878, test loss-1.8526, acc-0.4828\n",
      "Iter-51670, train loss-1.8121, acc-0.6000, valid loss-1.8517, acc-0.4880, test loss-1.8525, acc-0.4829\n",
      "Iter-51680, train loss-1.8652, acc-0.5000, valid loss-1.8516, acc-0.4882, test loss-1.8525, acc-0.4828\n",
      "Iter-51690, train loss-1.8037, acc-0.5400, valid loss-1.8516, acc-0.4878, test loss-1.8524, acc-0.4830\n",
      "Iter-51700, train loss-1.8306, acc-0.4000, valid loss-1.8515, acc-0.4882, test loss-1.8523, acc-0.4828\n",
      "Iter-51710, train loss-1.8577, acc-0.4800, valid loss-1.8515, acc-0.4878, test loss-1.8523, acc-0.4828\n",
      "Iter-51720, train loss-1.8899, acc-0.4400, valid loss-1.8514, acc-0.4880, test loss-1.8522, acc-0.4828\n",
      "Iter-51730, train loss-1.9464, acc-0.3600, valid loss-1.8514, acc-0.4882, test loss-1.8522, acc-0.4829\n",
      "Iter-51740, train loss-1.8442, acc-0.5000, valid loss-1.8513, acc-0.4882, test loss-1.8521, acc-0.4829\n",
      "Iter-51750, train loss-1.8092, acc-0.5400, valid loss-1.8513, acc-0.4882, test loss-1.8521, acc-0.4832\n",
      "Iter-51760, train loss-1.9053, acc-0.4200, valid loss-1.8512, acc-0.4882, test loss-1.8520, acc-0.4833\n",
      "Iter-51770, train loss-1.8751, acc-0.3800, valid loss-1.8511, acc-0.4884, test loss-1.8520, acc-0.4831\n",
      "Iter-51780, train loss-1.8653, acc-0.4400, valid loss-1.8511, acc-0.4884, test loss-1.8519, acc-0.4833\n",
      "Iter-51790, train loss-1.8021, acc-0.5600, valid loss-1.8510, acc-0.4884, test loss-1.8519, acc-0.4833\n",
      "Iter-51800, train loss-1.8896, acc-0.3800, valid loss-1.8510, acc-0.4884, test loss-1.8518, acc-0.4832\n",
      "Iter-51810, train loss-1.9091, acc-0.4600, valid loss-1.8509, acc-0.4884, test loss-1.8517, acc-0.4834\n",
      "Iter-51820, train loss-1.7913, acc-0.5800, valid loss-1.8509, acc-0.4886, test loss-1.8517, acc-0.4832\n",
      "Iter-51830, train loss-1.9107, acc-0.4200, valid loss-1.8508, acc-0.4886, test loss-1.8516, acc-0.4833\n",
      "Iter-51840, train loss-1.8282, acc-0.5000, valid loss-1.8507, acc-0.4884, test loss-1.8516, acc-0.4834\n",
      "Iter-51850, train loss-2.0028, acc-0.3400, valid loss-1.8507, acc-0.4884, test loss-1.8515, acc-0.4836\n",
      "Iter-51860, train loss-1.8402, acc-0.5000, valid loss-1.8506, acc-0.4886, test loss-1.8515, acc-0.4837\n",
      "Iter-51870, train loss-1.7604, acc-0.5200, valid loss-1.8506, acc-0.4886, test loss-1.8514, acc-0.4836\n",
      "Iter-51880, train loss-1.8255, acc-0.5400, valid loss-1.8505, acc-0.4884, test loss-1.8513, acc-0.4833\n",
      "Iter-51890, train loss-1.8693, acc-0.5000, valid loss-1.8505, acc-0.4886, test loss-1.8513, acc-0.4832\n",
      "Iter-51900, train loss-1.8178, acc-0.4600, valid loss-1.8504, acc-0.4886, test loss-1.8512, acc-0.4832\n",
      "Iter-51910, train loss-1.8844, acc-0.4400, valid loss-1.8503, acc-0.4884, test loss-1.8512, acc-0.4833\n",
      "Iter-51920, train loss-1.9418, acc-0.4400, valid loss-1.8503, acc-0.4884, test loss-1.8511, acc-0.4832\n",
      "Iter-51930, train loss-1.8002, acc-0.5600, valid loss-1.8502, acc-0.4884, test loss-1.8511, acc-0.4831\n",
      "Iter-51940, train loss-1.8523, acc-0.5400, valid loss-1.8502, acc-0.4886, test loss-1.8510, acc-0.4835\n",
      "Iter-51950, train loss-1.8837, acc-0.4200, valid loss-1.8501, acc-0.4886, test loss-1.8510, acc-0.4835\n",
      "Iter-51960, train loss-1.7517, acc-0.6200, valid loss-1.8501, acc-0.4884, test loss-1.8509, acc-0.4834\n",
      "Iter-51970, train loss-1.8619, acc-0.5200, valid loss-1.8500, acc-0.4884, test loss-1.8508, acc-0.4838\n",
      "Iter-51980, train loss-1.8575, acc-0.4200, valid loss-1.8500, acc-0.4886, test loss-1.8508, acc-0.4839\n",
      "Iter-51990, train loss-1.8257, acc-0.4600, valid loss-1.8499, acc-0.4886, test loss-1.8507, acc-0.4838\n",
      "Iter-52000, train loss-1.7257, acc-0.5800, valid loss-1.8498, acc-0.4886, test loss-1.8507, acc-0.4840\n",
      "Iter-52010, train loss-1.8474, acc-0.5200, valid loss-1.8498, acc-0.4886, test loss-1.8506, acc-0.4838\n",
      "Iter-52020, train loss-1.8726, acc-0.5400, valid loss-1.8497, acc-0.4884, test loss-1.8506, acc-0.4836\n",
      "Iter-52030, train loss-1.8804, acc-0.4400, valid loss-1.8497, acc-0.4882, test loss-1.8505, acc-0.4838\n",
      "Iter-52040, train loss-1.8884, acc-0.4600, valid loss-1.8496, acc-0.4882, test loss-1.8505, acc-0.4839\n",
      "Iter-52050, train loss-1.8636, acc-0.5600, valid loss-1.8496, acc-0.4880, test loss-1.8504, acc-0.4834\n",
      "Iter-52060, train loss-1.9230, acc-0.3400, valid loss-1.8495, acc-0.4882, test loss-1.8503, acc-0.4837\n",
      "Iter-52070, train loss-1.9481, acc-0.3000, valid loss-1.8494, acc-0.4882, test loss-1.8503, acc-0.4837\n",
      "Iter-52080, train loss-1.9036, acc-0.4200, valid loss-1.8494, acc-0.4882, test loss-1.8502, acc-0.4840\n",
      "Iter-52090, train loss-1.8692, acc-0.5200, valid loss-1.8493, acc-0.4880, test loss-1.8502, acc-0.4840\n",
      "Iter-52100, train loss-1.8562, acc-0.5600, valid loss-1.8493, acc-0.4878, test loss-1.8501, acc-0.4840\n",
      "Iter-52110, train loss-1.8816, acc-0.4200, valid loss-1.8492, acc-0.4882, test loss-1.8501, acc-0.4843\n",
      "Iter-52120, train loss-1.7775, acc-0.4600, valid loss-1.8492, acc-0.4880, test loss-1.8500, acc-0.4843\n",
      "Iter-52130, train loss-1.9420, acc-0.4400, valid loss-1.8491, acc-0.4878, test loss-1.8499, acc-0.4842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-52140, train loss-1.8326, acc-0.5200, valid loss-1.8490, acc-0.4880, test loss-1.8499, acc-0.4842\n",
      "Iter-52150, train loss-1.8750, acc-0.5600, valid loss-1.8490, acc-0.4878, test loss-1.8498, acc-0.4839\n",
      "Iter-52160, train loss-1.7854, acc-0.4600, valid loss-1.8489, acc-0.4882, test loss-1.8498, acc-0.4840\n",
      "Iter-52170, train loss-1.8581, acc-0.5000, valid loss-1.8489, acc-0.4880, test loss-1.8497, acc-0.4841\n",
      "Iter-52180, train loss-1.8278, acc-0.5600, valid loss-1.8488, acc-0.4882, test loss-1.8497, acc-0.4840\n",
      "Iter-52190, train loss-1.9363, acc-0.4200, valid loss-1.8488, acc-0.4884, test loss-1.8496, acc-0.4841\n",
      "Iter-52200, train loss-1.7823, acc-0.5200, valid loss-1.8487, acc-0.4882, test loss-1.8496, acc-0.4840\n",
      "Iter-52210, train loss-1.8429, acc-0.5600, valid loss-1.8487, acc-0.4882, test loss-1.8495, acc-0.4839\n",
      "Iter-52220, train loss-1.8615, acc-0.4600, valid loss-1.8486, acc-0.4884, test loss-1.8494, acc-0.4838\n",
      "Iter-52230, train loss-1.9515, acc-0.4200, valid loss-1.8485, acc-0.4886, test loss-1.8494, acc-0.4838\n",
      "Iter-52240, train loss-1.6957, acc-0.6600, valid loss-1.8485, acc-0.4888, test loss-1.8493, acc-0.4840\n",
      "Iter-52250, train loss-1.8707, acc-0.5600, valid loss-1.8484, acc-0.4888, test loss-1.8493, acc-0.4838\n",
      "Iter-52260, train loss-1.8084, acc-0.5600, valid loss-1.8484, acc-0.4886, test loss-1.8492, acc-0.4839\n",
      "Iter-52270, train loss-1.9199, acc-0.4000, valid loss-1.8483, acc-0.4888, test loss-1.8492, acc-0.4839\n",
      "Iter-52280, train loss-1.8901, acc-0.4800, valid loss-1.8483, acc-0.4884, test loss-1.8491, acc-0.4840\n",
      "Iter-52290, train loss-1.9071, acc-0.4200, valid loss-1.8482, acc-0.4888, test loss-1.8491, acc-0.4839\n",
      "Iter-52300, train loss-1.8411, acc-0.4400, valid loss-1.8482, acc-0.4888, test loss-1.8490, acc-0.4840\n",
      "Iter-52310, train loss-1.9619, acc-0.3800, valid loss-1.8481, acc-0.4888, test loss-1.8489, acc-0.4838\n",
      "Iter-52320, train loss-1.9286, acc-0.4000, valid loss-1.8481, acc-0.4886, test loss-1.8489, acc-0.4837\n",
      "Iter-52330, train loss-1.8518, acc-0.4800, valid loss-1.8480, acc-0.4886, test loss-1.8488, acc-0.4835\n",
      "Iter-52340, train loss-1.8390, acc-0.4600, valid loss-1.8479, acc-0.4886, test loss-1.8488, acc-0.4837\n",
      "Iter-52350, train loss-1.7818, acc-0.5400, valid loss-1.8479, acc-0.4888, test loss-1.8487, acc-0.4836\n",
      "Iter-52360, train loss-1.8187, acc-0.5400, valid loss-1.8478, acc-0.4888, test loss-1.8487, acc-0.4838\n",
      "Iter-52370, train loss-1.8834, acc-0.4800, valid loss-1.8478, acc-0.4888, test loss-1.8486, acc-0.4836\n",
      "Iter-52380, train loss-1.9158, acc-0.4200, valid loss-1.8477, acc-0.4886, test loss-1.8486, acc-0.4837\n",
      "Iter-52390, train loss-1.8359, acc-0.5200, valid loss-1.8477, acc-0.4888, test loss-1.8485, acc-0.4838\n",
      "Iter-52400, train loss-1.8685, acc-0.3600, valid loss-1.8476, acc-0.4892, test loss-1.8484, acc-0.4839\n",
      "Iter-52410, train loss-1.8277, acc-0.5000, valid loss-1.8476, acc-0.4890, test loss-1.8484, acc-0.4839\n",
      "Iter-52420, train loss-1.8497, acc-0.5000, valid loss-1.8475, acc-0.4892, test loss-1.8483, acc-0.4840\n",
      "Iter-52430, train loss-1.8144, acc-0.5400, valid loss-1.8474, acc-0.4892, test loss-1.8483, acc-0.4841\n",
      "Iter-52440, train loss-1.8339, acc-0.4800, valid loss-1.8474, acc-0.4892, test loss-1.8482, acc-0.4842\n",
      "Iter-52450, train loss-1.8593, acc-0.4400, valid loss-1.8473, acc-0.4892, test loss-1.8482, acc-0.4843\n",
      "Iter-52460, train loss-1.7888, acc-0.4600, valid loss-1.8473, acc-0.4894, test loss-1.8481, acc-0.4845\n",
      "Iter-52470, train loss-1.8182, acc-0.5600, valid loss-1.8472, acc-0.4896, test loss-1.8481, acc-0.4843\n",
      "Iter-52480, train loss-1.7874, acc-0.4800, valid loss-1.8472, acc-0.4896, test loss-1.8480, acc-0.4845\n",
      "Iter-52490, train loss-1.8078, acc-0.4800, valid loss-1.8471, acc-0.4896, test loss-1.8479, acc-0.4846\n",
      "Iter-52500, train loss-1.7924, acc-0.4800, valid loss-1.8471, acc-0.4896, test loss-1.8479, acc-0.4841\n",
      "Iter-52510, train loss-1.8212, acc-0.4800, valid loss-1.8470, acc-0.4896, test loss-1.8478, acc-0.4844\n",
      "Iter-52520, train loss-1.8754, acc-0.4800, valid loss-1.8470, acc-0.4896, test loss-1.8478, acc-0.4844\n",
      "Iter-52530, train loss-1.8912, acc-0.4200, valid loss-1.8469, acc-0.4898, test loss-1.8477, acc-0.4843\n",
      "Iter-52540, train loss-1.7511, acc-0.6000, valid loss-1.8468, acc-0.4898, test loss-1.8477, acc-0.4841\n",
      "Iter-52550, train loss-1.8167, acc-0.4800, valid loss-1.8468, acc-0.4898, test loss-1.8476, acc-0.4842\n",
      "Iter-52560, train loss-1.8660, acc-0.4600, valid loss-1.8467, acc-0.4894, test loss-1.8476, acc-0.4841\n",
      "Iter-52570, train loss-1.9139, acc-0.3400, valid loss-1.8467, acc-0.4896, test loss-1.8475, acc-0.4841\n",
      "Iter-52580, train loss-1.7901, acc-0.5400, valid loss-1.8466, acc-0.4898, test loss-1.8474, acc-0.4841\n",
      "Iter-52590, train loss-1.8669, acc-0.4400, valid loss-1.8466, acc-0.4898, test loss-1.8474, acc-0.4842\n",
      "Iter-52600, train loss-1.8647, acc-0.5200, valid loss-1.8465, acc-0.4898, test loss-1.8473, acc-0.4841\n",
      "Iter-52610, train loss-1.8307, acc-0.5600, valid loss-1.8465, acc-0.4896, test loss-1.8473, acc-0.4842\n",
      "Iter-52620, train loss-1.8332, acc-0.5000, valid loss-1.8464, acc-0.4896, test loss-1.8472, acc-0.4842\n",
      "Iter-52630, train loss-1.9939, acc-0.2800, valid loss-1.8464, acc-0.4898, test loss-1.8472, acc-0.4842\n",
      "Iter-52640, train loss-1.8646, acc-0.5600, valid loss-1.8463, acc-0.4898, test loss-1.8471, acc-0.4843\n",
      "Iter-52650, train loss-1.7744, acc-0.5600, valid loss-1.8462, acc-0.4900, test loss-1.8471, acc-0.4842\n",
      "Iter-52660, train loss-1.9269, acc-0.4200, valid loss-1.8462, acc-0.4900, test loss-1.8470, acc-0.4842\n",
      "Iter-52670, train loss-1.7671, acc-0.6600, valid loss-1.8461, acc-0.4898, test loss-1.8470, acc-0.4844\n",
      "Iter-52680, train loss-1.8374, acc-0.4800, valid loss-1.8461, acc-0.4898, test loss-1.8469, acc-0.4845\n",
      "Iter-52690, train loss-1.7240, acc-0.5600, valid loss-1.8460, acc-0.4898, test loss-1.8468, acc-0.4843\n",
      "Iter-52700, train loss-1.8829, acc-0.4000, valid loss-1.8460, acc-0.4896, test loss-1.8468, acc-0.4843\n",
      "Iter-52710, train loss-1.9342, acc-0.3800, valid loss-1.8459, acc-0.4896, test loss-1.8467, acc-0.4844\n",
      "Iter-52720, train loss-1.8896, acc-0.4000, valid loss-1.8459, acc-0.4896, test loss-1.8467, acc-0.4845\n",
      "Iter-52730, train loss-1.9089, acc-0.4000, valid loss-1.8458, acc-0.4898, test loss-1.8466, acc-0.4845\n",
      "Iter-52740, train loss-1.8385, acc-0.4600, valid loss-1.8457, acc-0.4900, test loss-1.8466, acc-0.4845\n",
      "Iter-52750, train loss-1.7917, acc-0.5600, valid loss-1.8457, acc-0.4902, test loss-1.8465, acc-0.4844\n",
      "Iter-52760, train loss-1.9422, acc-0.4400, valid loss-1.8456, acc-0.4902, test loss-1.8465, acc-0.4844\n",
      "Iter-52770, train loss-1.9135, acc-0.4600, valid loss-1.8456, acc-0.4902, test loss-1.8464, acc-0.4844\n",
      "Iter-52780, train loss-1.9069, acc-0.4800, valid loss-1.8455, acc-0.4902, test loss-1.8463, acc-0.4844\n",
      "Iter-52790, train loss-1.8643, acc-0.4200, valid loss-1.8455, acc-0.4902, test loss-1.8463, acc-0.4844\n",
      "Iter-52800, train loss-1.8384, acc-0.5000, valid loss-1.8454, acc-0.4900, test loss-1.8462, acc-0.4843\n",
      "Iter-52810, train loss-1.8598, acc-0.4000, valid loss-1.8453, acc-0.4902, test loss-1.8462, acc-0.4844\n",
      "Iter-52820, train loss-1.8329, acc-0.4800, valid loss-1.8453, acc-0.4904, test loss-1.8461, acc-0.4844\n",
      "Iter-52830, train loss-1.8773, acc-0.5200, valid loss-1.8452, acc-0.4902, test loss-1.8461, acc-0.4843\n",
      "Iter-52840, train loss-1.8283, acc-0.4600, valid loss-1.8452, acc-0.4902, test loss-1.8460, acc-0.4843\n",
      "Iter-52850, train loss-1.9684, acc-0.4000, valid loss-1.8451, acc-0.4902, test loss-1.8460, acc-0.4843\n",
      "Iter-52860, train loss-1.8261, acc-0.5000, valid loss-1.8451, acc-0.4902, test loss-1.8459, acc-0.4843\n",
      "Iter-52870, train loss-1.9340, acc-0.3400, valid loss-1.8450, acc-0.4900, test loss-1.8458, acc-0.4843\n",
      "Iter-52880, train loss-1.9400, acc-0.4000, valid loss-1.8450, acc-0.4900, test loss-1.8458, acc-0.4843\n",
      "Iter-52890, train loss-1.8939, acc-0.4000, valid loss-1.8449, acc-0.4902, test loss-1.8457, acc-0.4843\n",
      "Iter-52900, train loss-1.8680, acc-0.5200, valid loss-1.8448, acc-0.4900, test loss-1.8457, acc-0.4843\n",
      "Iter-52910, train loss-1.8996, acc-0.5000, valid loss-1.8448, acc-0.4898, test loss-1.8456, acc-0.4844\n",
      "Iter-52920, train loss-1.7275, acc-0.5200, valid loss-1.8447, acc-0.4900, test loss-1.8456, acc-0.4844\n",
      "Iter-52930, train loss-1.9672, acc-0.4000, valid loss-1.8447, acc-0.4900, test loss-1.8455, acc-0.4843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-52940, train loss-1.7966, acc-0.5800, valid loss-1.8446, acc-0.4900, test loss-1.8455, acc-0.4843\n",
      "Iter-52950, train loss-1.7637, acc-0.5200, valid loss-1.8446, acc-0.4904, test loss-1.8454, acc-0.4843\n",
      "Iter-52960, train loss-1.8050, acc-0.5000, valid loss-1.8445, acc-0.4902, test loss-1.8453, acc-0.4842\n",
      "Iter-52970, train loss-1.9334, acc-0.4000, valid loss-1.8445, acc-0.4904, test loss-1.8453, acc-0.4844\n",
      "Iter-52980, train loss-1.8814, acc-0.5000, valid loss-1.8444, acc-0.4906, test loss-1.8452, acc-0.4844\n",
      "Iter-52990, train loss-1.7428, acc-0.5600, valid loss-1.8443, acc-0.4904, test loss-1.8452, acc-0.4842\n",
      "Iter-53000, train loss-1.8993, acc-0.4400, valid loss-1.8443, acc-0.4904, test loss-1.8451, acc-0.4842\n",
      "Iter-53010, train loss-1.8577, acc-0.4200, valid loss-1.8442, acc-0.4904, test loss-1.8451, acc-0.4843\n",
      "Iter-53020, train loss-1.8718, acc-0.4000, valid loss-1.8442, acc-0.4904, test loss-1.8450, acc-0.4844\n",
      "Iter-53030, train loss-1.9306, acc-0.3200, valid loss-1.8441, acc-0.4904, test loss-1.8450, acc-0.4844\n",
      "Iter-53040, train loss-1.8650, acc-0.4600, valid loss-1.8441, acc-0.4902, test loss-1.8449, acc-0.4844\n",
      "Iter-53050, train loss-1.8987, acc-0.4400, valid loss-1.8440, acc-0.4904, test loss-1.8448, acc-0.4844\n",
      "Iter-53060, train loss-1.8624, acc-0.4600, valid loss-1.8439, acc-0.4906, test loss-1.8448, acc-0.4843\n",
      "Iter-53070, train loss-1.8240, acc-0.5200, valid loss-1.8439, acc-0.4906, test loss-1.8447, acc-0.4847\n",
      "Iter-53080, train loss-1.9056, acc-0.4400, valid loss-1.8438, acc-0.4908, test loss-1.8447, acc-0.4846\n",
      "Iter-53090, train loss-1.7985, acc-0.5600, valid loss-1.8438, acc-0.4904, test loss-1.8446, acc-0.4844\n",
      "Iter-53100, train loss-1.8752, acc-0.5200, valid loss-1.8437, acc-0.4908, test loss-1.8446, acc-0.4845\n",
      "Iter-53110, train loss-1.8624, acc-0.5200, valid loss-1.8437, acc-0.4908, test loss-1.8445, acc-0.4845\n",
      "Iter-53120, train loss-1.9338, acc-0.4200, valid loss-1.8436, acc-0.4906, test loss-1.8445, acc-0.4843\n",
      "Iter-53130, train loss-1.9349, acc-0.3400, valid loss-1.8436, acc-0.4910, test loss-1.8444, acc-0.4843\n",
      "Iter-53140, train loss-1.8055, acc-0.4200, valid loss-1.8435, acc-0.4904, test loss-1.8443, acc-0.4844\n",
      "Iter-53150, train loss-1.8311, acc-0.4200, valid loss-1.8434, acc-0.4904, test loss-1.8443, acc-0.4843\n",
      "Iter-53160, train loss-1.8216, acc-0.5000, valid loss-1.8434, acc-0.4906, test loss-1.8442, acc-0.4844\n",
      "Iter-53170, train loss-1.8676, acc-0.5200, valid loss-1.8433, acc-0.4906, test loss-1.8442, acc-0.4845\n",
      "Iter-53180, train loss-1.7494, acc-0.5600, valid loss-1.8433, acc-0.4898, test loss-1.8441, acc-0.4845\n",
      "Iter-53190, train loss-1.8448, acc-0.4200, valid loss-1.8432, acc-0.4896, test loss-1.8441, acc-0.4844\n",
      "Iter-53200, train loss-1.8269, acc-0.5000, valid loss-1.8432, acc-0.4900, test loss-1.8440, acc-0.4844\n",
      "Iter-53210, train loss-1.9972, acc-0.4200, valid loss-1.8431, acc-0.4898, test loss-1.8439, acc-0.4843\n",
      "Iter-53220, train loss-1.7998, acc-0.4800, valid loss-1.8430, acc-0.4900, test loss-1.8439, acc-0.4845\n",
      "Iter-53230, train loss-1.8721, acc-0.4200, valid loss-1.8430, acc-0.4894, test loss-1.8438, acc-0.4844\n",
      "Iter-53240, train loss-1.7702, acc-0.4800, valid loss-1.8429, acc-0.4894, test loss-1.8438, acc-0.4844\n",
      "Iter-53250, train loss-1.7641, acc-0.6000, valid loss-1.8429, acc-0.4896, test loss-1.8437, acc-0.4847\n",
      "Iter-53260, train loss-1.7868, acc-0.4800, valid loss-1.8428, acc-0.4894, test loss-1.8437, acc-0.4845\n",
      "Iter-53270, train loss-1.8257, acc-0.5200, valid loss-1.8428, acc-0.4894, test loss-1.8436, acc-0.4846\n",
      "Iter-53280, train loss-1.8623, acc-0.5200, valid loss-1.8427, acc-0.4898, test loss-1.8436, acc-0.4845\n",
      "Iter-53290, train loss-1.8846, acc-0.4800, valid loss-1.8427, acc-0.4898, test loss-1.8435, acc-0.4845\n",
      "Iter-53300, train loss-1.8572, acc-0.5000, valid loss-1.8426, acc-0.4900, test loss-1.8435, acc-0.4843\n",
      "Iter-53310, train loss-1.8974, acc-0.4400, valid loss-1.8426, acc-0.4900, test loss-1.8434, acc-0.4843\n",
      "Iter-53320, train loss-1.8065, acc-0.4600, valid loss-1.8425, acc-0.4904, test loss-1.8433, acc-0.4841\n",
      "Iter-53330, train loss-1.8868, acc-0.4000, valid loss-1.8424, acc-0.4902, test loss-1.8433, acc-0.4846\n",
      "Iter-53340, train loss-1.8934, acc-0.4800, valid loss-1.8424, acc-0.4904, test loss-1.8432, acc-0.4843\n",
      "Iter-53350, train loss-1.8214, acc-0.4600, valid loss-1.8423, acc-0.4902, test loss-1.8432, acc-0.4844\n",
      "Iter-53360, train loss-1.8939, acc-0.4800, valid loss-1.8423, acc-0.4902, test loss-1.8431, acc-0.4844\n",
      "Iter-53370, train loss-1.7535, acc-0.6000, valid loss-1.8422, acc-0.4900, test loss-1.8431, acc-0.4843\n",
      "Iter-53380, train loss-1.8351, acc-0.4800, valid loss-1.8422, acc-0.4902, test loss-1.8430, acc-0.4840\n",
      "Iter-53390, train loss-1.8913, acc-0.4400, valid loss-1.8421, acc-0.4900, test loss-1.8430, acc-0.4844\n",
      "Iter-53400, train loss-1.8811, acc-0.4000, valid loss-1.8421, acc-0.4902, test loss-1.8429, acc-0.4844\n",
      "Iter-53410, train loss-1.8608, acc-0.4400, valid loss-1.8420, acc-0.4902, test loss-1.8429, acc-0.4844\n",
      "Iter-53420, train loss-1.8381, acc-0.5800, valid loss-1.8420, acc-0.4904, test loss-1.8428, acc-0.4846\n",
      "Iter-53430, train loss-1.9114, acc-0.4200, valid loss-1.8419, acc-0.4900, test loss-1.8427, acc-0.4847\n",
      "Iter-53440, train loss-1.8286, acc-0.4600, valid loss-1.8419, acc-0.4900, test loss-1.8427, acc-0.4849\n",
      "Iter-53450, train loss-1.8954, acc-0.4400, valid loss-1.8418, acc-0.4902, test loss-1.8426, acc-0.4850\n",
      "Iter-53460, train loss-1.8993, acc-0.3800, valid loss-1.8417, acc-0.4904, test loss-1.8426, acc-0.4850\n",
      "Iter-53470, train loss-1.8014, acc-0.5200, valid loss-1.8417, acc-0.4906, test loss-1.8425, acc-0.4850\n",
      "Iter-53480, train loss-1.7900, acc-0.5800, valid loss-1.8416, acc-0.4902, test loss-1.8425, acc-0.4850\n",
      "Iter-53490, train loss-1.7962, acc-0.6000, valid loss-1.8416, acc-0.4906, test loss-1.8424, acc-0.4851\n",
      "Iter-53500, train loss-1.8473, acc-0.5200, valid loss-1.8415, acc-0.4902, test loss-1.8424, acc-0.4851\n",
      "Iter-53510, train loss-1.8137, acc-0.5000, valid loss-1.8415, acc-0.4900, test loss-1.8423, acc-0.4852\n",
      "Iter-53520, train loss-1.8363, acc-0.5000, valid loss-1.8414, acc-0.4904, test loss-1.8423, acc-0.4849\n",
      "Iter-53530, train loss-1.7723, acc-0.5800, valid loss-1.8414, acc-0.4904, test loss-1.8422, acc-0.4849\n",
      "Iter-53540, train loss-1.8279, acc-0.5200, valid loss-1.8413, acc-0.4906, test loss-1.8421, acc-0.4848\n",
      "Iter-53550, train loss-1.9044, acc-0.4200, valid loss-1.8413, acc-0.4904, test loss-1.8421, acc-0.4849\n",
      "Iter-53560, train loss-1.7947, acc-0.4600, valid loss-1.8412, acc-0.4904, test loss-1.8420, acc-0.4848\n",
      "Iter-53570, train loss-1.9068, acc-0.4600, valid loss-1.8411, acc-0.4904, test loss-1.8420, acc-0.4851\n",
      "Iter-53580, train loss-1.8222, acc-0.6000, valid loss-1.8411, acc-0.4904, test loss-1.8419, acc-0.4851\n",
      "Iter-53590, train loss-1.8785, acc-0.5400, valid loss-1.8410, acc-0.4904, test loss-1.8419, acc-0.4853\n",
      "Iter-53600, train loss-1.9198, acc-0.4400, valid loss-1.8410, acc-0.4902, test loss-1.8418, acc-0.4853\n",
      "Iter-53610, train loss-1.9703, acc-0.3200, valid loss-1.8409, acc-0.4904, test loss-1.8418, acc-0.4851\n",
      "Iter-53620, train loss-1.8453, acc-0.5000, valid loss-1.8409, acc-0.4902, test loss-1.8417, acc-0.4851\n",
      "Iter-53630, train loss-1.7936, acc-0.5200, valid loss-1.8408, acc-0.4904, test loss-1.8417, acc-0.4854\n",
      "Iter-53640, train loss-1.8672, acc-0.4400, valid loss-1.8408, acc-0.4904, test loss-1.8416, acc-0.4855\n",
      "Iter-53650, train loss-1.8358, acc-0.4400, valid loss-1.8407, acc-0.4906, test loss-1.8415, acc-0.4855\n",
      "Iter-53660, train loss-1.7730, acc-0.5400, valid loss-1.8407, acc-0.4904, test loss-1.8415, acc-0.4855\n",
      "Iter-53670, train loss-1.8861, acc-0.3800, valid loss-1.8406, acc-0.4902, test loss-1.8414, acc-0.4855\n",
      "Iter-53680, train loss-1.8571, acc-0.4400, valid loss-1.8406, acc-0.4902, test loss-1.8414, acc-0.4854\n",
      "Iter-53690, train loss-1.8666, acc-0.4000, valid loss-1.8405, acc-0.4902, test loss-1.8413, acc-0.4854\n",
      "Iter-53700, train loss-1.8851, acc-0.4600, valid loss-1.8404, acc-0.4902, test loss-1.8413, acc-0.4854\n",
      "Iter-53710, train loss-1.8833, acc-0.5000, valid loss-1.8404, acc-0.4902, test loss-1.8412, acc-0.4853\n",
      "Iter-53720, train loss-1.8365, acc-0.5400, valid loss-1.8403, acc-0.4904, test loss-1.8412, acc-0.4854\n",
      "Iter-53730, train loss-1.8098, acc-0.4800, valid loss-1.8403, acc-0.4904, test loss-1.8411, acc-0.4853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-53740, train loss-1.7921, acc-0.5200, valid loss-1.8402, acc-0.4906, test loss-1.8411, acc-0.4853\n",
      "Iter-53750, train loss-1.8527, acc-0.5000, valid loss-1.8402, acc-0.4906, test loss-1.8410, acc-0.4853\n",
      "Iter-53760, train loss-1.7794, acc-0.5000, valid loss-1.8401, acc-0.4904, test loss-1.8410, acc-0.4852\n",
      "Iter-53770, train loss-1.8856, acc-0.4000, valid loss-1.8401, acc-0.4904, test loss-1.8409, acc-0.4853\n",
      "Iter-53780, train loss-1.8079, acc-0.5000, valid loss-1.8400, acc-0.4906, test loss-1.8408, acc-0.4854\n",
      "Iter-53790, train loss-1.9151, acc-0.2400, valid loss-1.8400, acc-0.4904, test loss-1.8408, acc-0.4854\n",
      "Iter-53800, train loss-1.8573, acc-0.4400, valid loss-1.8399, acc-0.4906, test loss-1.8407, acc-0.4853\n",
      "Iter-53810, train loss-1.8752, acc-0.5000, valid loss-1.8398, acc-0.4906, test loss-1.8407, acc-0.4854\n",
      "Iter-53820, train loss-1.8571, acc-0.4800, valid loss-1.8398, acc-0.4906, test loss-1.8406, acc-0.4854\n",
      "Iter-53830, train loss-1.8936, acc-0.4600, valid loss-1.8397, acc-0.4908, test loss-1.8406, acc-0.4857\n",
      "Iter-53840, train loss-1.8685, acc-0.5000, valid loss-1.8397, acc-0.4906, test loss-1.8405, acc-0.4857\n",
      "Iter-53850, train loss-1.7956, acc-0.5000, valid loss-1.8396, acc-0.4906, test loss-1.8405, acc-0.4859\n",
      "Iter-53860, train loss-1.8338, acc-0.5200, valid loss-1.8396, acc-0.4908, test loss-1.8404, acc-0.4859\n",
      "Iter-53870, train loss-1.8126, acc-0.5200, valid loss-1.8395, acc-0.4910, test loss-1.8404, acc-0.4860\n",
      "Iter-53880, train loss-1.8292, acc-0.4800, valid loss-1.8395, acc-0.4908, test loss-1.8403, acc-0.4860\n",
      "Iter-53890, train loss-1.9832, acc-0.3600, valid loss-1.8394, acc-0.4906, test loss-1.8402, acc-0.4859\n",
      "Iter-53900, train loss-1.8574, acc-0.5000, valid loss-1.8394, acc-0.4906, test loss-1.8402, acc-0.4859\n",
      "Iter-53910, train loss-1.7465, acc-0.5600, valid loss-1.8393, acc-0.4906, test loss-1.8401, acc-0.4857\n",
      "Iter-53920, train loss-1.8120, acc-0.4600, valid loss-1.8393, acc-0.4906, test loss-1.8401, acc-0.4860\n",
      "Iter-53930, train loss-1.8510, acc-0.4200, valid loss-1.8392, acc-0.4908, test loss-1.8400, acc-0.4858\n",
      "Iter-53940, train loss-1.8440, acc-0.4600, valid loss-1.8392, acc-0.4908, test loss-1.8400, acc-0.4859\n",
      "Iter-53950, train loss-1.9005, acc-0.4600, valid loss-1.8391, acc-0.4910, test loss-1.8399, acc-0.4859\n",
      "Iter-53960, train loss-1.8416, acc-0.5800, valid loss-1.8390, acc-0.4908, test loss-1.8399, acc-0.4860\n",
      "Iter-53970, train loss-1.8778, acc-0.4000, valid loss-1.8390, acc-0.4910, test loss-1.8398, acc-0.4862\n",
      "Iter-53980, train loss-1.9022, acc-0.4000, valid loss-1.8389, acc-0.4910, test loss-1.8398, acc-0.4860\n",
      "Iter-53990, train loss-1.8865, acc-0.4400, valid loss-1.8389, acc-0.4912, test loss-1.8397, acc-0.4860\n",
      "Iter-54000, train loss-1.7976, acc-0.5600, valid loss-1.8388, acc-0.4912, test loss-1.8397, acc-0.4860\n",
      "Iter-54010, train loss-1.8884, acc-0.4800, valid loss-1.8388, acc-0.4912, test loss-1.8396, acc-0.4860\n",
      "Iter-54020, train loss-1.7644, acc-0.5800, valid loss-1.8387, acc-0.4912, test loss-1.8396, acc-0.4861\n",
      "Iter-54030, train loss-1.7800, acc-0.5200, valid loss-1.8387, acc-0.4912, test loss-1.8395, acc-0.4862\n",
      "Iter-54040, train loss-1.9939, acc-0.3400, valid loss-1.8386, acc-0.4912, test loss-1.8394, acc-0.4861\n",
      "Iter-54050, train loss-1.8591, acc-0.3800, valid loss-1.8386, acc-0.4912, test loss-1.8394, acc-0.4860\n",
      "Iter-54060, train loss-1.8437, acc-0.4400, valid loss-1.8385, acc-0.4912, test loss-1.8393, acc-0.4861\n",
      "Iter-54070, train loss-1.8260, acc-0.4600, valid loss-1.8384, acc-0.4912, test loss-1.8393, acc-0.4861\n",
      "Iter-54080, train loss-1.7741, acc-0.6000, valid loss-1.8384, acc-0.4912, test loss-1.8392, acc-0.4861\n",
      "Iter-54090, train loss-1.8542, acc-0.4000, valid loss-1.8383, acc-0.4912, test loss-1.8392, acc-0.4860\n",
      "Iter-54100, train loss-1.7591, acc-0.6000, valid loss-1.8383, acc-0.4914, test loss-1.8391, acc-0.4860\n",
      "Iter-54110, train loss-1.8764, acc-0.4400, valid loss-1.8382, acc-0.4912, test loss-1.8391, acc-0.4861\n",
      "Iter-54120, train loss-1.8191, acc-0.5200, valid loss-1.8382, acc-0.4914, test loss-1.8390, acc-0.4863\n",
      "Iter-54130, train loss-1.8310, acc-0.4800, valid loss-1.8381, acc-0.4910, test loss-1.8390, acc-0.4862\n",
      "Iter-54140, train loss-1.8933, acc-0.4200, valid loss-1.8381, acc-0.4912, test loss-1.8389, acc-0.4862\n",
      "Iter-54150, train loss-1.8472, acc-0.4800, valid loss-1.8380, acc-0.4912, test loss-1.8388, acc-0.4863\n",
      "Iter-54160, train loss-1.9211, acc-0.3800, valid loss-1.8379, acc-0.4914, test loss-1.8388, acc-0.4864\n",
      "Iter-54170, train loss-1.7989, acc-0.5800, valid loss-1.8379, acc-0.4912, test loss-1.8387, acc-0.4865\n",
      "Iter-54180, train loss-1.8099, acc-0.5400, valid loss-1.8378, acc-0.4916, test loss-1.8387, acc-0.4864\n",
      "Iter-54190, train loss-1.8940, acc-0.3800, valid loss-1.8378, acc-0.4912, test loss-1.8386, acc-0.4861\n",
      "Iter-54200, train loss-1.7261, acc-0.5800, valid loss-1.8377, acc-0.4912, test loss-1.8386, acc-0.4861\n",
      "Iter-54210, train loss-1.8269, acc-0.4400, valid loss-1.8377, acc-0.4914, test loss-1.8385, acc-0.4864\n",
      "Iter-54220, train loss-1.8021, acc-0.4600, valid loss-1.8376, acc-0.4916, test loss-1.8385, acc-0.4863\n",
      "Iter-54230, train loss-1.9428, acc-0.4200, valid loss-1.8376, acc-0.4916, test loss-1.8384, acc-0.4865\n",
      "Iter-54240, train loss-1.8698, acc-0.4400, valid loss-1.8375, acc-0.4914, test loss-1.8384, acc-0.4865\n",
      "Iter-54250, train loss-1.9333, acc-0.3400, valid loss-1.8375, acc-0.4916, test loss-1.8383, acc-0.4863\n",
      "Iter-54260, train loss-1.9442, acc-0.4000, valid loss-1.8374, acc-0.4916, test loss-1.8383, acc-0.4866\n",
      "Iter-54270, train loss-1.9034, acc-0.4400, valid loss-1.8374, acc-0.4918, test loss-1.8382, acc-0.4863\n",
      "Iter-54280, train loss-1.7976, acc-0.5400, valid loss-1.8373, acc-0.4916, test loss-1.8381, acc-0.4865\n",
      "Iter-54290, train loss-1.8628, acc-0.5000, valid loss-1.8373, acc-0.4914, test loss-1.8381, acc-0.4864\n",
      "Iter-54300, train loss-1.8255, acc-0.5000, valid loss-1.8372, acc-0.4916, test loss-1.8380, acc-0.4865\n",
      "Iter-54310, train loss-1.8922, acc-0.4200, valid loss-1.8372, acc-0.4916, test loss-1.8380, acc-0.4865\n",
      "Iter-54320, train loss-1.8431, acc-0.4800, valid loss-1.8371, acc-0.4918, test loss-1.8379, acc-0.4867\n",
      "Iter-54330, train loss-1.9181, acc-0.4400, valid loss-1.8370, acc-0.4918, test loss-1.8379, acc-0.4866\n",
      "Iter-54340, train loss-1.9333, acc-0.4000, valid loss-1.8370, acc-0.4918, test loss-1.8378, acc-0.4867\n",
      "Iter-54350, train loss-1.8789, acc-0.4200, valid loss-1.8369, acc-0.4916, test loss-1.8378, acc-0.4867\n",
      "Iter-54360, train loss-1.8870, acc-0.4800, valid loss-1.8369, acc-0.4916, test loss-1.8377, acc-0.4868\n",
      "Iter-54370, train loss-1.7807, acc-0.4800, valid loss-1.8368, acc-0.4914, test loss-1.8377, acc-0.4869\n",
      "Iter-54380, train loss-1.8464, acc-0.4800, valid loss-1.8368, acc-0.4914, test loss-1.8376, acc-0.4868\n",
      "Iter-54390, train loss-1.9325, acc-0.4400, valid loss-1.8367, acc-0.4916, test loss-1.8376, acc-0.4868\n",
      "Iter-54400, train loss-1.7870, acc-0.5000, valid loss-1.8367, acc-0.4916, test loss-1.8375, acc-0.4867\n",
      "Iter-54410, train loss-1.8221, acc-0.4400, valid loss-1.8366, acc-0.4918, test loss-1.8374, acc-0.4868\n",
      "Iter-54420, train loss-1.8418, acc-0.4200, valid loss-1.8365, acc-0.4918, test loss-1.8374, acc-0.4869\n",
      "Iter-54430, train loss-1.9261, acc-0.4200, valid loss-1.8365, acc-0.4916, test loss-1.8373, acc-0.4869\n",
      "Iter-54440, train loss-1.8188, acc-0.5000, valid loss-1.8364, acc-0.4918, test loss-1.8373, acc-0.4869\n",
      "Iter-54450, train loss-1.8277, acc-0.4800, valid loss-1.8364, acc-0.4918, test loss-1.8372, acc-0.4869\n",
      "Iter-54460, train loss-1.8777, acc-0.4600, valid loss-1.8363, acc-0.4920, test loss-1.8372, acc-0.4868\n",
      "Iter-54470, train loss-1.8845, acc-0.4400, valid loss-1.8363, acc-0.4920, test loss-1.8371, acc-0.4870\n",
      "Iter-54480, train loss-1.8083, acc-0.4800, valid loss-1.8362, acc-0.4920, test loss-1.8371, acc-0.4871\n",
      "Iter-54490, train loss-1.8411, acc-0.5400, valid loss-1.8362, acc-0.4920, test loss-1.8370, acc-0.4871\n",
      "Iter-54500, train loss-1.8176, acc-0.4400, valid loss-1.8361, acc-0.4920, test loss-1.8370, acc-0.4871\n",
      "Iter-54510, train loss-1.8487, acc-0.4000, valid loss-1.8361, acc-0.4922, test loss-1.8369, acc-0.4870\n",
      "Iter-54520, train loss-1.8397, acc-0.5800, valid loss-1.8360, acc-0.4922, test loss-1.8369, acc-0.4869\n",
      "Iter-54530, train loss-1.7495, acc-0.5600, valid loss-1.8359, acc-0.4920, test loss-1.8368, acc-0.4868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-54540, train loss-1.8305, acc-0.5200, valid loss-1.8359, acc-0.4920, test loss-1.8367, acc-0.4868\n",
      "Iter-54550, train loss-1.8745, acc-0.4200, valid loss-1.8358, acc-0.4920, test loss-1.8367, acc-0.4868\n",
      "Iter-54560, train loss-1.8076, acc-0.4600, valid loss-1.8358, acc-0.4920, test loss-1.8366, acc-0.4868\n",
      "Iter-54570, train loss-1.8107, acc-0.5600, valid loss-1.8357, acc-0.4920, test loss-1.8366, acc-0.4868\n",
      "Iter-54580, train loss-1.7414, acc-0.5600, valid loss-1.8357, acc-0.4922, test loss-1.8365, acc-0.4870\n",
      "Iter-54590, train loss-1.8129, acc-0.5000, valid loss-1.8356, acc-0.4922, test loss-1.8365, acc-0.4869\n",
      "Iter-54600, train loss-1.8626, acc-0.5200, valid loss-1.8356, acc-0.4922, test loss-1.8364, acc-0.4870\n",
      "Iter-54610, train loss-1.7197, acc-0.5800, valid loss-1.8355, acc-0.4924, test loss-1.8364, acc-0.4870\n",
      "Iter-54620, train loss-1.7628, acc-0.6400, valid loss-1.8355, acc-0.4922, test loss-1.8363, acc-0.4869\n",
      "Iter-54630, train loss-1.7949, acc-0.5400, valid loss-1.8354, acc-0.4924, test loss-1.8363, acc-0.4870\n",
      "Iter-54640, train loss-1.8340, acc-0.4200, valid loss-1.8354, acc-0.4922, test loss-1.8362, acc-0.4870\n",
      "Iter-54650, train loss-1.7274, acc-0.6400, valid loss-1.8353, acc-0.4924, test loss-1.8361, acc-0.4870\n",
      "Iter-54660, train loss-1.8529, acc-0.5000, valid loss-1.8352, acc-0.4922, test loss-1.8361, acc-0.4872\n",
      "Iter-54670, train loss-1.8623, acc-0.4800, valid loss-1.8352, acc-0.4922, test loss-1.8360, acc-0.4872\n",
      "Iter-54680, train loss-1.8762, acc-0.4400, valid loss-1.8351, acc-0.4924, test loss-1.8360, acc-0.4872\n",
      "Iter-54690, train loss-1.8431, acc-0.6000, valid loss-1.8351, acc-0.4924, test loss-1.8359, acc-0.4872\n",
      "Iter-54700, train loss-1.8218, acc-0.4800, valid loss-1.8350, acc-0.4924, test loss-1.8359, acc-0.4871\n",
      "Iter-54710, train loss-1.8061, acc-0.5000, valid loss-1.8350, acc-0.4922, test loss-1.8358, acc-0.4872\n",
      "Iter-54720, train loss-1.7556, acc-0.6000, valid loss-1.8349, acc-0.4924, test loss-1.8358, acc-0.4872\n",
      "Iter-54730, train loss-1.9528, acc-0.3600, valid loss-1.8349, acc-0.4922, test loss-1.8357, acc-0.4872\n",
      "Iter-54740, train loss-1.7750, acc-0.5400, valid loss-1.8348, acc-0.4926, test loss-1.8357, acc-0.4873\n",
      "Iter-54750, train loss-1.8064, acc-0.4600, valid loss-1.8348, acc-0.4924, test loss-1.8356, acc-0.4874\n",
      "Iter-54760, train loss-1.8559, acc-0.5000, valid loss-1.8347, acc-0.4924, test loss-1.8356, acc-0.4874\n",
      "Iter-54770, train loss-1.7286, acc-0.6000, valid loss-1.8346, acc-0.4926, test loss-1.8355, acc-0.4875\n",
      "Iter-54780, train loss-1.8896, acc-0.4800, valid loss-1.8346, acc-0.4926, test loss-1.8355, acc-0.4874\n",
      "Iter-54790, train loss-1.8103, acc-0.5400, valid loss-1.8345, acc-0.4924, test loss-1.8354, acc-0.4874\n",
      "Iter-54800, train loss-1.8098, acc-0.5000, valid loss-1.8345, acc-0.4924, test loss-1.8353, acc-0.4874\n",
      "Iter-54810, train loss-1.8731, acc-0.4600, valid loss-1.8344, acc-0.4924, test loss-1.8353, acc-0.4874\n",
      "Iter-54820, train loss-1.8773, acc-0.5000, valid loss-1.8344, acc-0.4920, test loss-1.8352, acc-0.4873\n",
      "Iter-54830, train loss-1.8173, acc-0.5800, valid loss-1.8343, acc-0.4920, test loss-1.8352, acc-0.4872\n",
      "Iter-54840, train loss-1.8669, acc-0.4800, valid loss-1.8343, acc-0.4920, test loss-1.8351, acc-0.4872\n",
      "Iter-54850, train loss-1.8014, acc-0.5000, valid loss-1.8342, acc-0.4920, test loss-1.8351, acc-0.4872\n",
      "Iter-54860, train loss-1.9176, acc-0.4200, valid loss-1.8342, acc-0.4922, test loss-1.8350, acc-0.4873\n",
      "Iter-54870, train loss-1.8847, acc-0.4200, valid loss-1.8341, acc-0.4922, test loss-1.8350, acc-0.4872\n",
      "Iter-54880, train loss-1.8266, acc-0.5200, valid loss-1.8341, acc-0.4922, test loss-1.8349, acc-0.4873\n",
      "Iter-54890, train loss-1.8463, acc-0.5000, valid loss-1.8340, acc-0.4922, test loss-1.8349, acc-0.4874\n",
      "Iter-54900, train loss-1.9159, acc-0.4600, valid loss-1.8339, acc-0.4922, test loss-1.8348, acc-0.4875\n",
      "Iter-54910, train loss-1.8399, acc-0.4800, valid loss-1.8339, acc-0.4922, test loss-1.8347, acc-0.4876\n",
      "Iter-54920, train loss-1.8190, acc-0.4800, valid loss-1.8338, acc-0.4922, test loss-1.8347, acc-0.4877\n",
      "Iter-54930, train loss-1.8644, acc-0.4600, valid loss-1.8338, acc-0.4922, test loss-1.8346, acc-0.4877\n",
      "Iter-54940, train loss-1.9059, acc-0.3800, valid loss-1.8337, acc-0.4922, test loss-1.8346, acc-0.4878\n",
      "Iter-54950, train loss-1.7855, acc-0.5600, valid loss-1.8337, acc-0.4924, test loss-1.8345, acc-0.4878\n",
      "Iter-54960, train loss-1.7738, acc-0.5200, valid loss-1.8336, acc-0.4924, test loss-1.8345, acc-0.4877\n",
      "Iter-54970, train loss-1.9327, acc-0.4400, valid loss-1.8336, acc-0.4922, test loss-1.8344, acc-0.4878\n",
      "Iter-54980, train loss-1.8495, acc-0.4200, valid loss-1.8335, acc-0.4924, test loss-1.8344, acc-0.4877\n",
      "Iter-54990, train loss-1.8450, acc-0.4200, valid loss-1.8335, acc-0.4924, test loss-1.8343, acc-0.4878\n",
      "Iter-55000, train loss-1.8437, acc-0.5000, valid loss-1.8334, acc-0.4924, test loss-1.8343, acc-0.4878\n",
      "Iter-55010, train loss-1.8447, acc-0.5800, valid loss-1.8334, acc-0.4926, test loss-1.8342, acc-0.4878\n",
      "Iter-55020, train loss-1.8410, acc-0.5000, valid loss-1.8333, acc-0.4926, test loss-1.8342, acc-0.4879\n",
      "Iter-55030, train loss-1.8048, acc-0.5200, valid loss-1.8333, acc-0.4922, test loss-1.8341, acc-0.4877\n",
      "Iter-55040, train loss-1.8829, acc-0.3400, valid loss-1.8332, acc-0.4922, test loss-1.8340, acc-0.4879\n",
      "Iter-55050, train loss-1.8069, acc-0.4800, valid loss-1.8331, acc-0.4924, test loss-1.8340, acc-0.4879\n",
      "Iter-55060, train loss-1.7853, acc-0.5000, valid loss-1.8331, acc-0.4922, test loss-1.8339, acc-0.4878\n",
      "Iter-55070, train loss-1.7721, acc-0.5800, valid loss-1.8330, acc-0.4924, test loss-1.8339, acc-0.4878\n",
      "Iter-55080, train loss-1.8644, acc-0.4400, valid loss-1.8330, acc-0.4924, test loss-1.8338, acc-0.4878\n",
      "Iter-55090, train loss-1.8223, acc-0.5600, valid loss-1.8329, acc-0.4924, test loss-1.8338, acc-0.4878\n",
      "Iter-55100, train loss-1.7759, acc-0.5200, valid loss-1.8329, acc-0.4926, test loss-1.8337, acc-0.4879\n",
      "Iter-55110, train loss-1.8339, acc-0.5800, valid loss-1.8328, acc-0.4930, test loss-1.8337, acc-0.4879\n",
      "Iter-55120, train loss-1.7689, acc-0.5000, valid loss-1.8328, acc-0.4928, test loss-1.8336, acc-0.4879\n",
      "Iter-55130, train loss-1.8262, acc-0.5200, valid loss-1.8327, acc-0.4930, test loss-1.8336, acc-0.4881\n",
      "Iter-55140, train loss-1.7968, acc-0.4800, valid loss-1.8327, acc-0.4928, test loss-1.8335, acc-0.4879\n",
      "Iter-55150, train loss-1.9305, acc-0.4800, valid loss-1.8326, acc-0.4928, test loss-1.8335, acc-0.4880\n",
      "Iter-55160, train loss-1.8356, acc-0.4200, valid loss-1.8326, acc-0.4928, test loss-1.8334, acc-0.4880\n",
      "Iter-55170, train loss-1.8068, acc-0.5800, valid loss-1.8325, acc-0.4928, test loss-1.8333, acc-0.4881\n",
      "Iter-55180, train loss-1.8815, acc-0.4000, valid loss-1.8325, acc-0.4928, test loss-1.8333, acc-0.4879\n",
      "Iter-55190, train loss-1.8514, acc-0.4800, valid loss-1.8324, acc-0.4928, test loss-1.8332, acc-0.4879\n",
      "Iter-55200, train loss-1.8548, acc-0.4200, valid loss-1.8323, acc-0.4928, test loss-1.8332, acc-0.4881\n",
      "Iter-55210, train loss-1.8449, acc-0.4600, valid loss-1.8323, acc-0.4928, test loss-1.8331, acc-0.4880\n",
      "Iter-55220, train loss-1.8447, acc-0.4800, valid loss-1.8322, acc-0.4926, test loss-1.8331, acc-0.4881\n",
      "Iter-55230, train loss-1.8256, acc-0.5400, valid loss-1.8322, acc-0.4926, test loss-1.8330, acc-0.4881\n",
      "Iter-55240, train loss-1.9014, acc-0.3200, valid loss-1.8321, acc-0.4926, test loss-1.8330, acc-0.4880\n",
      "Iter-55250, train loss-1.8917, acc-0.5200, valid loss-1.8321, acc-0.4926, test loss-1.8329, acc-0.4882\n",
      "Iter-55260, train loss-1.8482, acc-0.5400, valid loss-1.8320, acc-0.4926, test loss-1.8329, acc-0.4882\n",
      "Iter-55270, train loss-1.8275, acc-0.5000, valid loss-1.8320, acc-0.4928, test loss-1.8328, acc-0.4881\n",
      "Iter-55280, train loss-1.7773, acc-0.5000, valid loss-1.8319, acc-0.4928, test loss-1.8328, acc-0.4881\n",
      "Iter-55290, train loss-1.7561, acc-0.6200, valid loss-1.8319, acc-0.4932, test loss-1.8327, acc-0.4882\n",
      "Iter-55300, train loss-1.8292, acc-0.5000, valid loss-1.8318, acc-0.4930, test loss-1.8327, acc-0.4882\n",
      "Iter-55310, train loss-1.9174, acc-0.3800, valid loss-1.8318, acc-0.4932, test loss-1.8326, acc-0.4883\n",
      "Iter-55320, train loss-1.7532, acc-0.5600, valid loss-1.8317, acc-0.4928, test loss-1.8325, acc-0.4884\n",
      "Iter-55330, train loss-1.7847, acc-0.5200, valid loss-1.8316, acc-0.4930, test loss-1.8325, acc-0.4884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-55340, train loss-1.7892, acc-0.5200, valid loss-1.8316, acc-0.4928, test loss-1.8324, acc-0.4885\n",
      "Iter-55350, train loss-1.9074, acc-0.4400, valid loss-1.8315, acc-0.4930, test loss-1.8324, acc-0.4884\n",
      "Iter-55360, train loss-1.8495, acc-0.4400, valid loss-1.8315, acc-0.4930, test loss-1.8323, acc-0.4881\n",
      "Iter-55370, train loss-1.9684, acc-0.4000, valid loss-1.8314, acc-0.4930, test loss-1.8323, acc-0.4882\n",
      "Iter-55380, train loss-1.9530, acc-0.4400, valid loss-1.8314, acc-0.4928, test loss-1.8322, acc-0.4881\n",
      "Iter-55390, train loss-1.7966, acc-0.3800, valid loss-1.8313, acc-0.4930, test loss-1.8322, acc-0.4883\n",
      "Iter-55400, train loss-1.8226, acc-0.4200, valid loss-1.8313, acc-0.4930, test loss-1.8321, acc-0.4882\n",
      "Iter-55410, train loss-1.7994, acc-0.5600, valid loss-1.8312, acc-0.4928, test loss-1.8321, acc-0.4881\n",
      "Iter-55420, train loss-1.8751, acc-0.4000, valid loss-1.8312, acc-0.4928, test loss-1.8320, acc-0.4881\n",
      "Iter-55430, train loss-1.7749, acc-0.5200, valid loss-1.8311, acc-0.4930, test loss-1.8320, acc-0.4882\n",
      "Iter-55440, train loss-1.8425, acc-0.4200, valid loss-1.8311, acc-0.4930, test loss-1.8319, acc-0.4881\n",
      "Iter-55450, train loss-1.8203, acc-0.4400, valid loss-1.8310, acc-0.4930, test loss-1.8319, acc-0.4884\n",
      "Iter-55460, train loss-1.8923, acc-0.4400, valid loss-1.8310, acc-0.4932, test loss-1.8318, acc-0.4884\n",
      "Iter-55470, train loss-1.8437, acc-0.5000, valid loss-1.8309, acc-0.4932, test loss-1.8318, acc-0.4883\n",
      "Iter-55480, train loss-1.8218, acc-0.5000, valid loss-1.8309, acc-0.4934, test loss-1.8317, acc-0.4883\n",
      "Iter-55490, train loss-1.8169, acc-0.5600, valid loss-1.8308, acc-0.4934, test loss-1.8317, acc-0.4885\n",
      "Iter-55500, train loss-1.8047, acc-0.5600, valid loss-1.8308, acc-0.4934, test loss-1.8316, acc-0.4881\n",
      "Iter-55510, train loss-1.8223, acc-0.5000, valid loss-1.8307, acc-0.4934, test loss-1.8316, acc-0.4883\n",
      "Iter-55520, train loss-1.8081, acc-0.5400, valid loss-1.8306, acc-0.4934, test loss-1.8315, acc-0.4884\n",
      "Iter-55530, train loss-1.8966, acc-0.4400, valid loss-1.8306, acc-0.4934, test loss-1.8315, acc-0.4883\n",
      "Iter-55540, train loss-1.8616, acc-0.4400, valid loss-1.8305, acc-0.4932, test loss-1.8314, acc-0.4883\n",
      "Iter-55550, train loss-1.8150, acc-0.5000, valid loss-1.8305, acc-0.4936, test loss-1.8313, acc-0.4883\n",
      "Iter-55560, train loss-1.8347, acc-0.5800, valid loss-1.8304, acc-0.4936, test loss-1.8313, acc-0.4883\n",
      "Iter-55570, train loss-1.8787, acc-0.4800, valid loss-1.8304, acc-0.4936, test loss-1.8312, acc-0.4884\n",
      "Iter-55580, train loss-1.8020, acc-0.5000, valid loss-1.8303, acc-0.4936, test loss-1.8312, acc-0.4884\n",
      "Iter-55590, train loss-1.8327, acc-0.3800, valid loss-1.8303, acc-0.4934, test loss-1.8311, acc-0.4883\n",
      "Iter-55600, train loss-1.8631, acc-0.4400, valid loss-1.8302, acc-0.4936, test loss-1.8311, acc-0.4885\n",
      "Iter-55610, train loss-1.9153, acc-0.3400, valid loss-1.8302, acc-0.4938, test loss-1.8310, acc-0.4884\n",
      "Iter-55620, train loss-1.7528, acc-0.6000, valid loss-1.8301, acc-0.4936, test loss-1.8310, acc-0.4884\n",
      "Iter-55630, train loss-1.9046, acc-0.4600, valid loss-1.8301, acc-0.4936, test loss-1.8309, acc-0.4884\n",
      "Iter-55640, train loss-1.8699, acc-0.4600, valid loss-1.8300, acc-0.4936, test loss-1.8309, acc-0.4884\n",
      "Iter-55650, train loss-1.8350, acc-0.5000, valid loss-1.8300, acc-0.4932, test loss-1.8308, acc-0.4884\n",
      "Iter-55660, train loss-1.8198, acc-0.4600, valid loss-1.8299, acc-0.4932, test loss-1.8308, acc-0.4884\n",
      "Iter-55670, train loss-1.7540, acc-0.4800, valid loss-1.8299, acc-0.4930, test loss-1.8307, acc-0.4885\n",
      "Iter-55680, train loss-1.8575, acc-0.4200, valid loss-1.8298, acc-0.4932, test loss-1.8307, acc-0.4883\n",
      "Iter-55690, train loss-1.8612, acc-0.4400, valid loss-1.8298, acc-0.4942, test loss-1.8306, acc-0.4883\n",
      "Iter-55700, train loss-1.7766, acc-0.4600, valid loss-1.8297, acc-0.4942, test loss-1.8306, acc-0.4885\n",
      "Iter-55710, train loss-1.8196, acc-0.4600, valid loss-1.8297, acc-0.4942, test loss-1.8305, acc-0.4886\n",
      "Iter-55720, train loss-1.8586, acc-0.4800, valid loss-1.8296, acc-0.4942, test loss-1.8305, acc-0.4886\n",
      "Iter-55730, train loss-1.7683, acc-0.6200, valid loss-1.8296, acc-0.4940, test loss-1.8304, acc-0.4884\n",
      "Iter-55740, train loss-1.8098, acc-0.4400, valid loss-1.8295, acc-0.4942, test loss-1.8304, acc-0.4887\n",
      "Iter-55750, train loss-1.8514, acc-0.4800, valid loss-1.8294, acc-0.4940, test loss-1.8303, acc-0.4887\n",
      "Iter-55760, train loss-1.8482, acc-0.4400, valid loss-1.8294, acc-0.4940, test loss-1.8303, acc-0.4887\n",
      "Iter-55770, train loss-1.8329, acc-0.5200, valid loss-1.8293, acc-0.4940, test loss-1.8302, acc-0.4886\n",
      "Iter-55780, train loss-1.8240, acc-0.3600, valid loss-1.8293, acc-0.4940, test loss-1.8302, acc-0.4888\n",
      "Iter-55790, train loss-1.9130, acc-0.4000, valid loss-1.8292, acc-0.4942, test loss-1.8301, acc-0.4887\n",
      "Iter-55800, train loss-1.7709, acc-0.6200, valid loss-1.8292, acc-0.4942, test loss-1.8301, acc-0.4886\n",
      "Iter-55810, train loss-1.7836, acc-0.4800, valid loss-1.8291, acc-0.4940, test loss-1.8300, acc-0.4886\n",
      "Iter-55820, train loss-1.7415, acc-0.6000, valid loss-1.8291, acc-0.4942, test loss-1.8300, acc-0.4887\n",
      "Iter-55830, train loss-1.8276, acc-0.4800, valid loss-1.8290, acc-0.4942, test loss-1.8299, acc-0.4888\n",
      "Iter-55840, train loss-1.8133, acc-0.5400, valid loss-1.8290, acc-0.4942, test loss-1.8298, acc-0.4889\n",
      "Iter-55850, train loss-1.8932, acc-0.4200, valid loss-1.8289, acc-0.4940, test loss-1.8298, acc-0.4888\n",
      "Iter-55860, train loss-1.9381, acc-0.3800, valid loss-1.8289, acc-0.4940, test loss-1.8297, acc-0.4889\n",
      "Iter-55870, train loss-1.8884, acc-0.4800, valid loss-1.8288, acc-0.4940, test loss-1.8297, acc-0.4889\n",
      "Iter-55880, train loss-1.8452, acc-0.4600, valid loss-1.8288, acc-0.4942, test loss-1.8296, acc-0.4889\n",
      "Iter-55890, train loss-1.8845, acc-0.4400, valid loss-1.8287, acc-0.4942, test loss-1.8296, acc-0.4889\n",
      "Iter-55900, train loss-1.8017, acc-0.4600, valid loss-1.8287, acc-0.4942, test loss-1.8295, acc-0.4889\n",
      "Iter-55910, train loss-1.8668, acc-0.4400, valid loss-1.8286, acc-0.4942, test loss-1.8295, acc-0.4889\n",
      "Iter-55920, train loss-1.7524, acc-0.6400, valid loss-1.8286, acc-0.4942, test loss-1.8294, acc-0.4889\n",
      "Iter-55930, train loss-1.9545, acc-0.3400, valid loss-1.8285, acc-0.4942, test loss-1.8294, acc-0.4889\n",
      "Iter-55940, train loss-1.7676, acc-0.6400, valid loss-1.8284, acc-0.4942, test loss-1.8293, acc-0.4889\n",
      "Iter-55950, train loss-1.8435, acc-0.4800, valid loss-1.8284, acc-0.4944, test loss-1.8293, acc-0.4889\n",
      "Iter-55960, train loss-1.8831, acc-0.4200, valid loss-1.8283, acc-0.4944, test loss-1.8292, acc-0.4889\n",
      "Iter-55970, train loss-1.8416, acc-0.4400, valid loss-1.8283, acc-0.4944, test loss-1.8292, acc-0.4889\n",
      "Iter-55980, train loss-1.8862, acc-0.5800, valid loss-1.8282, acc-0.4944, test loss-1.8291, acc-0.4886\n",
      "Iter-55990, train loss-1.8525, acc-0.4600, valid loss-1.8282, acc-0.4944, test loss-1.8291, acc-0.4887\n",
      "Iter-56000, train loss-1.8448, acc-0.4800, valid loss-1.8281, acc-0.4944, test loss-1.8290, acc-0.4887\n",
      "Iter-56010, train loss-1.8498, acc-0.5000, valid loss-1.8281, acc-0.4944, test loss-1.8290, acc-0.4888\n",
      "Iter-56020, train loss-1.9140, acc-0.3800, valid loss-1.8280, acc-0.4944, test loss-1.8289, acc-0.4887\n",
      "Iter-56030, train loss-1.8577, acc-0.3800, valid loss-1.8280, acc-0.4944, test loss-1.8288, acc-0.4888\n",
      "Iter-56040, train loss-1.7854, acc-0.5800, valid loss-1.8279, acc-0.4946, test loss-1.8288, acc-0.4887\n",
      "Iter-56050, train loss-1.9608, acc-0.2800, valid loss-1.8279, acc-0.4946, test loss-1.8287, acc-0.4889\n",
      "Iter-56060, train loss-1.7885, acc-0.4600, valid loss-1.8278, acc-0.4946, test loss-1.8287, acc-0.4887\n",
      "Iter-56070, train loss-1.7683, acc-0.5600, valid loss-1.8278, acc-0.4946, test loss-1.8286, acc-0.4887\n",
      "Iter-56080, train loss-1.8609, acc-0.4600, valid loss-1.8277, acc-0.4946, test loss-1.8286, acc-0.4887\n",
      "Iter-56090, train loss-1.8170, acc-0.4800, valid loss-1.8277, acc-0.4946, test loss-1.8285, acc-0.4888\n",
      "Iter-56100, train loss-1.8474, acc-0.5000, valid loss-1.8276, acc-0.4944, test loss-1.8285, acc-0.4890\n",
      "Iter-56110, train loss-1.8767, acc-0.4000, valid loss-1.8276, acc-0.4944, test loss-1.8284, acc-0.4891\n",
      "Iter-56120, train loss-1.7955, acc-0.5400, valid loss-1.8275, acc-0.4946, test loss-1.8284, acc-0.4890\n",
      "Iter-56130, train loss-1.7960, acc-0.5200, valid loss-1.8275, acc-0.4946, test loss-1.8283, acc-0.4893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56140, train loss-1.8474, acc-0.4400, valid loss-1.8274, acc-0.4946, test loss-1.8283, acc-0.4892\n",
      "Iter-56150, train loss-1.7934, acc-0.5000, valid loss-1.8274, acc-0.4948, test loss-1.8282, acc-0.4892\n",
      "Iter-56160, train loss-1.7942, acc-0.4800, valid loss-1.8273, acc-0.4948, test loss-1.8282, acc-0.4892\n",
      "Iter-56170, train loss-1.8251, acc-0.4000, valid loss-1.8272, acc-0.4948, test loss-1.8281, acc-0.4891\n",
      "Iter-56180, train loss-1.8094, acc-0.5000, valid loss-1.8272, acc-0.4948, test loss-1.8281, acc-0.4892\n",
      "Iter-56190, train loss-1.8778, acc-0.4800, valid loss-1.8271, acc-0.4948, test loss-1.8280, acc-0.4891\n",
      "Iter-56200, train loss-1.8233, acc-0.4000, valid loss-1.8271, acc-0.4948, test loss-1.8280, acc-0.4890\n",
      "Iter-56210, train loss-1.8756, acc-0.3800, valid loss-1.8270, acc-0.4948, test loss-1.8279, acc-0.4893\n",
      "Iter-56220, train loss-1.7717, acc-0.5600, valid loss-1.8270, acc-0.4948, test loss-1.8279, acc-0.4891\n",
      "Iter-56230, train loss-1.9308, acc-0.3200, valid loss-1.8269, acc-0.4948, test loss-1.8278, acc-0.4893\n",
      "Iter-56240, train loss-1.7458, acc-0.5000, valid loss-1.8269, acc-0.4948, test loss-1.8278, acc-0.4896\n",
      "Iter-56250, train loss-1.8220, acc-0.4800, valid loss-1.8268, acc-0.4946, test loss-1.8277, acc-0.4895\n",
      "Iter-56260, train loss-1.8510, acc-0.4000, valid loss-1.8268, acc-0.4946, test loss-1.8276, acc-0.4894\n",
      "Iter-56270, train loss-1.8025, acc-0.4400, valid loss-1.8267, acc-0.4948, test loss-1.8276, acc-0.4895\n",
      "Iter-56280, train loss-1.8283, acc-0.4200, valid loss-1.8267, acc-0.4946, test loss-1.8275, acc-0.4895\n",
      "Iter-56290, train loss-1.8019, acc-0.5800, valid loss-1.8266, acc-0.4948, test loss-1.8275, acc-0.4894\n",
      "Iter-56300, train loss-1.9032, acc-0.4000, valid loss-1.8266, acc-0.4948, test loss-1.8274, acc-0.4896\n",
      "Iter-56310, train loss-1.7732, acc-0.4600, valid loss-1.8265, acc-0.4948, test loss-1.8274, acc-0.4896\n",
      "Iter-56320, train loss-1.9267, acc-0.3600, valid loss-1.8265, acc-0.4948, test loss-1.8273, acc-0.4895\n",
      "Iter-56330, train loss-1.8769, acc-0.4600, valid loss-1.8264, acc-0.4948, test loss-1.8273, acc-0.4895\n",
      "Iter-56340, train loss-1.8529, acc-0.4600, valid loss-1.8264, acc-0.4948, test loss-1.8272, acc-0.4894\n",
      "Iter-56350, train loss-1.7739, acc-0.6000, valid loss-1.8263, acc-0.4948, test loss-1.8272, acc-0.4894\n",
      "Iter-56360, train loss-1.8633, acc-0.4400, valid loss-1.8263, acc-0.4948, test loss-1.8271, acc-0.4894\n",
      "Iter-56370, train loss-1.8579, acc-0.4400, valid loss-1.8262, acc-0.4948, test loss-1.8271, acc-0.4896\n",
      "Iter-56380, train loss-1.7250, acc-0.5200, valid loss-1.8261, acc-0.4948, test loss-1.8270, acc-0.4896\n",
      "Iter-56390, train loss-1.8585, acc-0.4600, valid loss-1.8261, acc-0.4948, test loss-1.8270, acc-0.4896\n",
      "Iter-56400, train loss-1.8801, acc-0.4400, valid loss-1.8260, acc-0.4948, test loss-1.8269, acc-0.4895\n",
      "Iter-56410, train loss-1.8018, acc-0.5200, valid loss-1.8260, acc-0.4948, test loss-1.8269, acc-0.4897\n",
      "Iter-56420, train loss-1.8410, acc-0.4400, valid loss-1.8259, acc-0.4948, test loss-1.8268, acc-0.4898\n",
      "Iter-56430, train loss-1.8541, acc-0.5000, valid loss-1.8259, acc-0.4948, test loss-1.8268, acc-0.4898\n",
      "Iter-56440, train loss-1.7827, acc-0.4200, valid loss-1.8258, acc-0.4948, test loss-1.8267, acc-0.4897\n",
      "Iter-56450, train loss-1.8108, acc-0.4600, valid loss-1.8258, acc-0.4950, test loss-1.8266, acc-0.4898\n",
      "Iter-56460, train loss-1.8840, acc-0.5400, valid loss-1.8257, acc-0.4950, test loss-1.8266, acc-0.4898\n",
      "Iter-56470, train loss-1.8850, acc-0.4800, valid loss-1.8257, acc-0.4950, test loss-1.8265, acc-0.4898\n",
      "Iter-56480, train loss-1.7892, acc-0.5000, valid loss-1.8256, acc-0.4950, test loss-1.8265, acc-0.4897\n",
      "Iter-56490, train loss-1.7438, acc-0.5800, valid loss-1.8256, acc-0.4952, test loss-1.8264, acc-0.4898\n",
      "Iter-56500, train loss-1.8707, acc-0.4800, valid loss-1.8255, acc-0.4952, test loss-1.8264, acc-0.4898\n",
      "Iter-56510, train loss-1.8078, acc-0.4400, valid loss-1.8255, acc-0.4952, test loss-1.8263, acc-0.4900\n",
      "Iter-56520, train loss-1.9321, acc-0.4000, valid loss-1.8254, acc-0.4948, test loss-1.8263, acc-0.4900\n",
      "Iter-56530, train loss-1.8435, acc-0.4400, valid loss-1.8254, acc-0.4948, test loss-1.8262, acc-0.4901\n",
      "Iter-56540, train loss-1.7216, acc-0.6200, valid loss-1.8253, acc-0.4948, test loss-1.8262, acc-0.4901\n",
      "Iter-56550, train loss-1.8631, acc-0.4200, valid loss-1.8253, acc-0.4950, test loss-1.8261, acc-0.4899\n",
      "Iter-56560, train loss-1.7905, acc-0.5000, valid loss-1.8252, acc-0.4950, test loss-1.8261, acc-0.4899\n",
      "Iter-56570, train loss-1.8437, acc-0.4800, valid loss-1.8251, acc-0.4950, test loss-1.8260, acc-0.4899\n",
      "Iter-56580, train loss-1.9000, acc-0.4000, valid loss-1.8251, acc-0.4948, test loss-1.8260, acc-0.4897\n",
      "Iter-56590, train loss-1.8776, acc-0.4400, valid loss-1.8250, acc-0.4948, test loss-1.8259, acc-0.4897\n",
      "Iter-56600, train loss-1.9161, acc-0.4000, valid loss-1.8250, acc-0.4948, test loss-1.8259, acc-0.4895\n",
      "Iter-56610, train loss-1.7816, acc-0.5200, valid loss-1.8249, acc-0.4948, test loss-1.8258, acc-0.4896\n",
      "Iter-56620, train loss-1.8696, acc-0.4200, valid loss-1.8249, acc-0.4950, test loss-1.8258, acc-0.4898\n",
      "Iter-56630, train loss-1.8661, acc-0.4600, valid loss-1.8248, acc-0.4948, test loss-1.8257, acc-0.4899\n",
      "Iter-56640, train loss-1.8617, acc-0.4800, valid loss-1.8248, acc-0.4948, test loss-1.8257, acc-0.4900\n",
      "Iter-56650, train loss-1.8675, acc-0.4600, valid loss-1.8247, acc-0.4950, test loss-1.8256, acc-0.4899\n",
      "Iter-56660, train loss-1.8855, acc-0.5000, valid loss-1.8247, acc-0.4950, test loss-1.8256, acc-0.4899\n",
      "Iter-56670, train loss-1.8919, acc-0.4800, valid loss-1.8246, acc-0.4948, test loss-1.8255, acc-0.4899\n",
      "Iter-56680, train loss-1.7861, acc-0.5400, valid loss-1.8246, acc-0.4948, test loss-1.8254, acc-0.4900\n",
      "Iter-56690, train loss-1.9573, acc-0.4000, valid loss-1.8245, acc-0.4948, test loss-1.8254, acc-0.4900\n",
      "Iter-56700, train loss-1.8411, acc-0.5200, valid loss-1.8245, acc-0.4950, test loss-1.8253, acc-0.4900\n",
      "Iter-56710, train loss-1.8805, acc-0.4400, valid loss-1.8244, acc-0.4950, test loss-1.8253, acc-0.4900\n",
      "Iter-56720, train loss-1.8400, acc-0.4800, valid loss-1.8244, acc-0.4952, test loss-1.8252, acc-0.4900\n",
      "Iter-56730, train loss-1.7740, acc-0.5400, valid loss-1.8243, acc-0.4950, test loss-1.8252, acc-0.4901\n",
      "Iter-56740, train loss-1.8214, acc-0.5200, valid loss-1.8243, acc-0.4950, test loss-1.8251, acc-0.4901\n",
      "Iter-56750, train loss-1.9162, acc-0.4400, valid loss-1.8242, acc-0.4948, test loss-1.8251, acc-0.4900\n",
      "Iter-56760, train loss-1.7819, acc-0.5400, valid loss-1.8242, acc-0.4950, test loss-1.8250, acc-0.4901\n",
      "Iter-56770, train loss-1.8227, acc-0.4800, valid loss-1.8241, acc-0.4952, test loss-1.8250, acc-0.4900\n",
      "Iter-56780, train loss-1.7848, acc-0.5000, valid loss-1.8241, acc-0.4952, test loss-1.8249, acc-0.4902\n",
      "Iter-56790, train loss-1.8384, acc-0.5600, valid loss-1.8240, acc-0.4950, test loss-1.8249, acc-0.4902\n",
      "Iter-56800, train loss-1.9302, acc-0.3600, valid loss-1.8240, acc-0.4952, test loss-1.8248, acc-0.4901\n",
      "Iter-56810, train loss-1.8289, acc-0.4600, valid loss-1.8239, acc-0.4952, test loss-1.8248, acc-0.4901\n",
      "Iter-56820, train loss-1.8355, acc-0.4600, valid loss-1.8239, acc-0.4952, test loss-1.8247, acc-0.4902\n",
      "Iter-56830, train loss-1.9793, acc-0.4800, valid loss-1.8238, acc-0.4952, test loss-1.8247, acc-0.4902\n",
      "Iter-56840, train loss-1.8088, acc-0.5600, valid loss-1.8238, acc-0.4952, test loss-1.8246, acc-0.4901\n",
      "Iter-56850, train loss-1.9120, acc-0.4000, valid loss-1.8237, acc-0.4954, test loss-1.8246, acc-0.4902\n",
      "Iter-56860, train loss-1.8308, acc-0.4800, valid loss-1.8237, acc-0.4952, test loss-1.8245, acc-0.4900\n",
      "Iter-56870, train loss-1.8200, acc-0.5000, valid loss-1.8236, acc-0.4948, test loss-1.8245, acc-0.4902\n",
      "Iter-56880, train loss-1.8608, acc-0.5000, valid loss-1.8235, acc-0.4952, test loss-1.8244, acc-0.4902\n",
      "Iter-56890, train loss-1.8141, acc-0.5200, valid loss-1.8235, acc-0.4950, test loss-1.8244, acc-0.4900\n",
      "Iter-56900, train loss-1.8456, acc-0.4000, valid loss-1.8234, acc-0.4950, test loss-1.8243, acc-0.4901\n",
      "Iter-56910, train loss-1.7813, acc-0.5000, valid loss-1.8234, acc-0.4952, test loss-1.8242, acc-0.4901\n",
      "Iter-56920, train loss-1.8827, acc-0.4000, valid loss-1.8233, acc-0.4954, test loss-1.8242, acc-0.4900\n",
      "Iter-56930, train loss-1.8114, acc-0.4400, valid loss-1.8233, acc-0.4954, test loss-1.8241, acc-0.4900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56940, train loss-1.8542, acc-0.4200, valid loss-1.8232, acc-0.4954, test loss-1.8241, acc-0.4900\n",
      "Iter-56950, train loss-1.7942, acc-0.5200, valid loss-1.8232, acc-0.4954, test loss-1.8240, acc-0.4900\n",
      "Iter-56960, train loss-1.8212, acc-0.5400, valid loss-1.8231, acc-0.4956, test loss-1.8240, acc-0.4901\n",
      "Iter-56970, train loss-1.8378, acc-0.4400, valid loss-1.8231, acc-0.4952, test loss-1.8239, acc-0.4901\n",
      "Iter-56980, train loss-1.7396, acc-0.5400, valid loss-1.8230, acc-0.4954, test loss-1.8239, acc-0.4901\n",
      "Iter-56990, train loss-1.9113, acc-0.4200, valid loss-1.8230, acc-0.4952, test loss-1.8238, acc-0.4903\n",
      "Iter-57000, train loss-1.8605, acc-0.4800, valid loss-1.8229, acc-0.4954, test loss-1.8238, acc-0.4903\n",
      "Iter-57010, train loss-1.9910, acc-0.3800, valid loss-1.8229, acc-0.4956, test loss-1.8237, acc-0.4904\n",
      "Iter-57020, train loss-1.8936, acc-0.3800, valid loss-1.8228, acc-0.4954, test loss-1.8237, acc-0.4904\n",
      "Iter-57030, train loss-1.9280, acc-0.4400, valid loss-1.8228, acc-0.4954, test loss-1.8236, acc-0.4904\n",
      "Iter-57040, train loss-1.6975, acc-0.7000, valid loss-1.8227, acc-0.4954, test loss-1.8236, acc-0.4904\n",
      "Iter-57050, train loss-1.8171, acc-0.5400, valid loss-1.8227, acc-0.4954, test loss-1.8235, acc-0.4904\n",
      "Iter-57060, train loss-1.8457, acc-0.4600, valid loss-1.8226, acc-0.4954, test loss-1.8235, acc-0.4904\n",
      "Iter-57070, train loss-1.8864, acc-0.4800, valid loss-1.8226, acc-0.4958, test loss-1.8234, acc-0.4904\n",
      "Iter-57080, train loss-1.8814, acc-0.3800, valid loss-1.8225, acc-0.4956, test loss-1.8234, acc-0.4904\n",
      "Iter-57090, train loss-1.8491, acc-0.4400, valid loss-1.8225, acc-0.4956, test loss-1.8233, acc-0.4901\n",
      "Iter-57100, train loss-1.8200, acc-0.4800, valid loss-1.8224, acc-0.4956, test loss-1.8233, acc-0.4901\n",
      "Iter-57110, train loss-1.8322, acc-0.5000, valid loss-1.8224, acc-0.4956, test loss-1.8232, acc-0.4901\n",
      "Iter-57120, train loss-1.8561, acc-0.5200, valid loss-1.8223, acc-0.4956, test loss-1.8232, acc-0.4900\n",
      "Iter-57130, train loss-1.8360, acc-0.4400, valid loss-1.8222, acc-0.4956, test loss-1.8231, acc-0.4900\n",
      "Iter-57140, train loss-1.8967, acc-0.4800, valid loss-1.8222, acc-0.4954, test loss-1.8231, acc-0.4900\n",
      "Iter-57150, train loss-1.8280, acc-0.5200, valid loss-1.8221, acc-0.4954, test loss-1.8230, acc-0.4901\n",
      "Iter-57160, train loss-1.8163, acc-0.5400, valid loss-1.8221, acc-0.4954, test loss-1.8230, acc-0.4900\n",
      "Iter-57170, train loss-1.9122, acc-0.4800, valid loss-1.8220, acc-0.4954, test loss-1.8229, acc-0.4899\n",
      "Iter-57180, train loss-1.8522, acc-0.5400, valid loss-1.8220, acc-0.4954, test loss-1.8229, acc-0.4899\n",
      "Iter-57190, train loss-1.9110, acc-0.3400, valid loss-1.8219, acc-0.4954, test loss-1.8228, acc-0.4901\n",
      "Iter-57200, train loss-1.8038, acc-0.4800, valid loss-1.8219, acc-0.4954, test loss-1.8227, acc-0.4900\n",
      "Iter-57210, train loss-1.7702, acc-0.5600, valid loss-1.8218, acc-0.4954, test loss-1.8227, acc-0.4900\n",
      "Iter-57220, train loss-1.8361, acc-0.3800, valid loss-1.8218, acc-0.4952, test loss-1.8226, acc-0.4900\n",
      "Iter-57230, train loss-1.8067, acc-0.5800, valid loss-1.8217, acc-0.4952, test loss-1.8226, acc-0.4902\n",
      "Iter-57240, train loss-1.8147, acc-0.6000, valid loss-1.8217, acc-0.4954, test loss-1.8225, acc-0.4903\n",
      "Iter-57250, train loss-1.9148, acc-0.4000, valid loss-1.8216, acc-0.4952, test loss-1.8225, acc-0.4903\n",
      "Iter-57260, train loss-1.8284, acc-0.4400, valid loss-1.8216, acc-0.4954, test loss-1.8224, acc-0.4903\n",
      "Iter-57270, train loss-1.8614, acc-0.4800, valid loss-1.8215, acc-0.4954, test loss-1.8224, acc-0.4903\n",
      "Iter-57280, train loss-1.9017, acc-0.3800, valid loss-1.8215, acc-0.4954, test loss-1.8223, acc-0.4903\n",
      "Iter-57290, train loss-1.8831, acc-0.4800, valid loss-1.8214, acc-0.4954, test loss-1.8223, acc-0.4902\n",
      "Iter-57300, train loss-1.7746, acc-0.5000, valid loss-1.8213, acc-0.4952, test loss-1.8222, acc-0.4902\n",
      "Iter-57310, train loss-1.7142, acc-0.6400, valid loss-1.8213, acc-0.4954, test loss-1.8222, acc-0.4903\n",
      "Iter-57320, train loss-1.8384, acc-0.4600, valid loss-1.8212, acc-0.4950, test loss-1.8221, acc-0.4902\n",
      "Iter-57330, train loss-1.8510, acc-0.5200, valid loss-1.8212, acc-0.4958, test loss-1.8221, acc-0.4902\n",
      "Iter-57340, train loss-1.8914, acc-0.4400, valid loss-1.8211, acc-0.4950, test loss-1.8220, acc-0.4902\n",
      "Iter-57350, train loss-1.8795, acc-0.4800, valid loss-1.8211, acc-0.4954, test loss-1.8220, acc-0.4903\n",
      "Iter-57360, train loss-1.8592, acc-0.4800, valid loss-1.8210, acc-0.4956, test loss-1.8219, acc-0.4903\n",
      "Iter-57370, train loss-1.8178, acc-0.5200, valid loss-1.8210, acc-0.4954, test loss-1.8219, acc-0.4903\n",
      "Iter-57380, train loss-1.8730, acc-0.5000, valid loss-1.8209, acc-0.4956, test loss-1.8218, acc-0.4903\n",
      "Iter-57390, train loss-1.8480, acc-0.4000, valid loss-1.8209, acc-0.4958, test loss-1.8218, acc-0.4903\n",
      "Iter-57400, train loss-1.8917, acc-0.3600, valid loss-1.8208, acc-0.4958, test loss-1.8217, acc-0.4903\n",
      "Iter-57410, train loss-1.9217, acc-0.5000, valid loss-1.8208, acc-0.4954, test loss-1.8217, acc-0.4903\n",
      "Iter-57420, train loss-1.8494, acc-0.5200, valid loss-1.8207, acc-0.4954, test loss-1.8216, acc-0.4903\n",
      "Iter-57430, train loss-1.9175, acc-0.4600, valid loss-1.8207, acc-0.4956, test loss-1.8216, acc-0.4904\n",
      "Iter-57440, train loss-1.7961, acc-0.4400, valid loss-1.8206, acc-0.4956, test loss-1.8215, acc-0.4904\n",
      "Iter-57450, train loss-1.8139, acc-0.5600, valid loss-1.8206, acc-0.4956, test loss-1.8215, acc-0.4904\n",
      "Iter-57460, train loss-1.8164, acc-0.4600, valid loss-1.8205, acc-0.4956, test loss-1.8214, acc-0.4903\n",
      "Iter-57470, train loss-1.7902, acc-0.5000, valid loss-1.8205, acc-0.4958, test loss-1.8214, acc-0.4903\n",
      "Iter-57480, train loss-1.7565, acc-0.5400, valid loss-1.8204, acc-0.4958, test loss-1.8213, acc-0.4903\n",
      "Iter-57490, train loss-1.8294, acc-0.5600, valid loss-1.8203, acc-0.4958, test loss-1.8213, acc-0.4903\n",
      "Iter-57500, train loss-1.7992, acc-0.5000, valid loss-1.8203, acc-0.4958, test loss-1.8212, acc-0.4904\n",
      "Iter-57510, train loss-1.7703, acc-0.5000, valid loss-1.8202, acc-0.4958, test loss-1.8212, acc-0.4904\n",
      "Iter-57520, train loss-1.8087, acc-0.4800, valid loss-1.8202, acc-0.4956, test loss-1.8211, acc-0.4903\n",
      "Iter-57530, train loss-1.8413, acc-0.4800, valid loss-1.8201, acc-0.4960, test loss-1.8210, acc-0.4904\n",
      "Iter-57540, train loss-1.7926, acc-0.5200, valid loss-1.8201, acc-0.4958, test loss-1.8210, acc-0.4904\n",
      "Iter-57550, train loss-1.8325, acc-0.5200, valid loss-1.8200, acc-0.4958, test loss-1.8209, acc-0.4905\n",
      "Iter-57560, train loss-1.8166, acc-0.4000, valid loss-1.8200, acc-0.4958, test loss-1.8209, acc-0.4905\n",
      "Iter-57570, train loss-1.9736, acc-0.3600, valid loss-1.8199, acc-0.4958, test loss-1.8208, acc-0.4906\n",
      "Iter-57580, train loss-1.8403, acc-0.4200, valid loss-1.8199, acc-0.4960, test loss-1.8208, acc-0.4907\n",
      "Iter-57590, train loss-1.8242, acc-0.4600, valid loss-1.8198, acc-0.4960, test loss-1.8207, acc-0.4904\n",
      "Iter-57600, train loss-1.8853, acc-0.3800, valid loss-1.8198, acc-0.4958, test loss-1.8207, acc-0.4905\n",
      "Iter-57610, train loss-1.8446, acc-0.4200, valid loss-1.8197, acc-0.4960, test loss-1.8206, acc-0.4904\n",
      "Iter-57620, train loss-1.7985, acc-0.5800, valid loss-1.8197, acc-0.4960, test loss-1.8206, acc-0.4904\n",
      "Iter-57630, train loss-1.9781, acc-0.3800, valid loss-1.8196, acc-0.4960, test loss-1.8205, acc-0.4904\n",
      "Iter-57640, train loss-2.0100, acc-0.3400, valid loss-1.8196, acc-0.4960, test loss-1.8205, acc-0.4906\n",
      "Iter-57650, train loss-1.7131, acc-0.5600, valid loss-1.8195, acc-0.4958, test loss-1.8204, acc-0.4907\n",
      "Iter-57660, train loss-1.8089, acc-0.4400, valid loss-1.8195, acc-0.4956, test loss-1.8204, acc-0.4905\n",
      "Iter-57670, train loss-1.7981, acc-0.5000, valid loss-1.8194, acc-0.4956, test loss-1.8203, acc-0.4907\n",
      "Iter-57680, train loss-1.8892, acc-0.4400, valid loss-1.8194, acc-0.4956, test loss-1.8203, acc-0.4903\n",
      "Iter-57690, train loss-1.7814, acc-0.5000, valid loss-1.8193, acc-0.4956, test loss-1.8202, acc-0.4905\n",
      "Iter-57700, train loss-1.9436, acc-0.4200, valid loss-1.8193, acc-0.4954, test loss-1.8202, acc-0.4905\n",
      "Iter-57710, train loss-1.8398, acc-0.4600, valid loss-1.8192, acc-0.4954, test loss-1.8201, acc-0.4905\n",
      "Iter-57720, train loss-1.9042, acc-0.4200, valid loss-1.8192, acc-0.4954, test loss-1.8201, acc-0.4905\n",
      "Iter-57730, train loss-1.8251, acc-0.5000, valid loss-1.8191, acc-0.4960, test loss-1.8200, acc-0.4907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-57740, train loss-1.7546, acc-0.6000, valid loss-1.8191, acc-0.4954, test loss-1.8200, acc-0.4907\n",
      "Iter-57750, train loss-1.8657, acc-0.3400, valid loss-1.8190, acc-0.4956, test loss-1.8199, acc-0.4907\n",
      "Iter-57760, train loss-1.8238, acc-0.5600, valid loss-1.8190, acc-0.4956, test loss-1.8199, acc-0.4907\n",
      "Iter-57770, train loss-1.7442, acc-0.5000, valid loss-1.8189, acc-0.4958, test loss-1.8198, acc-0.4907\n",
      "Iter-57780, train loss-1.8949, acc-0.4600, valid loss-1.8189, acc-0.4958, test loss-1.8198, acc-0.4908\n",
      "Iter-57790, train loss-1.8370, acc-0.5000, valid loss-1.8188, acc-0.4960, test loss-1.8197, acc-0.4906\n",
      "Iter-57800, train loss-1.9071, acc-0.4600, valid loss-1.8188, acc-0.4964, test loss-1.8197, acc-0.4908\n",
      "Iter-57810, train loss-1.8447, acc-0.4800, valid loss-1.8187, acc-0.4964, test loss-1.8196, acc-0.4908\n",
      "Iter-57820, train loss-1.7387, acc-0.5800, valid loss-1.8187, acc-0.4968, test loss-1.8196, acc-0.4907\n",
      "Iter-57830, train loss-1.7473, acc-0.5200, valid loss-1.8186, acc-0.4964, test loss-1.8195, acc-0.4905\n",
      "Iter-57840, train loss-1.8333, acc-0.5200, valid loss-1.8186, acc-0.4962, test loss-1.8195, acc-0.4905\n",
      "Iter-57850, train loss-1.8991, acc-0.4200, valid loss-1.8185, acc-0.4962, test loss-1.8194, acc-0.4904\n",
      "Iter-57860, train loss-1.8798, acc-0.4600, valid loss-1.8184, acc-0.4964, test loss-1.8194, acc-0.4906\n",
      "Iter-57870, train loss-1.8689, acc-0.3800, valid loss-1.8184, acc-0.4964, test loss-1.8193, acc-0.4906\n",
      "Iter-57880, train loss-1.9147, acc-0.3200, valid loss-1.8183, acc-0.4962, test loss-1.8193, acc-0.4906\n",
      "Iter-57890, train loss-1.8524, acc-0.4400, valid loss-1.8183, acc-0.4964, test loss-1.8192, acc-0.4907\n",
      "Iter-57900, train loss-1.9396, acc-0.3800, valid loss-1.8182, acc-0.4964, test loss-1.8191, acc-0.4907\n",
      "Iter-57910, train loss-1.8550, acc-0.4400, valid loss-1.8182, acc-0.4962, test loss-1.8191, acc-0.4907\n",
      "Iter-57920, train loss-1.7943, acc-0.4200, valid loss-1.8181, acc-0.4960, test loss-1.8190, acc-0.4907\n",
      "Iter-57930, train loss-1.7732, acc-0.5000, valid loss-1.8181, acc-0.4962, test loss-1.8190, acc-0.4907\n",
      "Iter-57940, train loss-1.7587, acc-0.6200, valid loss-1.8180, acc-0.4964, test loss-1.8189, acc-0.4908\n",
      "Iter-57950, train loss-1.8397, acc-0.4800, valid loss-1.8180, acc-0.4964, test loss-1.8189, acc-0.4906\n",
      "Iter-57960, train loss-1.8292, acc-0.4800, valid loss-1.8179, acc-0.4964, test loss-1.8188, acc-0.4906\n",
      "Iter-57970, train loss-1.7942, acc-0.4600, valid loss-1.8179, acc-0.4962, test loss-1.8188, acc-0.4907\n",
      "Iter-57980, train loss-1.7705, acc-0.4800, valid loss-1.8178, acc-0.4964, test loss-1.8187, acc-0.4906\n",
      "Iter-57990, train loss-1.8249, acc-0.4400, valid loss-1.8178, acc-0.4962, test loss-1.8187, acc-0.4908\n",
      "Iter-58000, train loss-1.7943, acc-0.6000, valid loss-1.8177, acc-0.4964, test loss-1.8186, acc-0.4906\n",
      "Iter-58010, train loss-1.8102, acc-0.3600, valid loss-1.8177, acc-0.4964, test loss-1.8186, acc-0.4907\n",
      "Iter-58020, train loss-1.8565, acc-0.4800, valid loss-1.8176, acc-0.4964, test loss-1.8185, acc-0.4906\n",
      "Iter-58030, train loss-1.7831, acc-0.5200, valid loss-1.8176, acc-0.4964, test loss-1.8185, acc-0.4906\n",
      "Iter-58040, train loss-1.8704, acc-0.4600, valid loss-1.8175, acc-0.4964, test loss-1.8184, acc-0.4906\n",
      "Iter-58050, train loss-1.7200, acc-0.5200, valid loss-1.8175, acc-0.4964, test loss-1.8184, acc-0.4907\n",
      "Iter-58060, train loss-1.8294, acc-0.5400, valid loss-1.8174, acc-0.4964, test loss-1.8183, acc-0.4907\n",
      "Iter-58070, train loss-1.8047, acc-0.5200, valid loss-1.8174, acc-0.4964, test loss-1.8183, acc-0.4907\n",
      "Iter-58080, train loss-1.8255, acc-0.5800, valid loss-1.8173, acc-0.4964, test loss-1.8182, acc-0.4907\n",
      "Iter-58090, train loss-1.8766, acc-0.5000, valid loss-1.8173, acc-0.4964, test loss-1.8182, acc-0.4906\n",
      "Iter-58100, train loss-1.8267, acc-0.5000, valid loss-1.8172, acc-0.4964, test loss-1.8181, acc-0.4905\n",
      "Iter-58110, train loss-1.8531, acc-0.3800, valid loss-1.8172, acc-0.4964, test loss-1.8181, acc-0.4906\n",
      "Iter-58120, train loss-1.8075, acc-0.4200, valid loss-1.8171, acc-0.4964, test loss-1.8180, acc-0.4906\n",
      "Iter-58130, train loss-1.6974, acc-0.4600, valid loss-1.8171, acc-0.4964, test loss-1.8180, acc-0.4906\n",
      "Iter-58140, train loss-1.8338, acc-0.4600, valid loss-1.8170, acc-0.4964, test loss-1.8179, acc-0.4907\n",
      "Iter-58150, train loss-1.9263, acc-0.4200, valid loss-1.8170, acc-0.4964, test loss-1.8179, acc-0.4907\n",
      "Iter-58160, train loss-1.8480, acc-0.5000, valid loss-1.8169, acc-0.4964, test loss-1.8178, acc-0.4908\n",
      "Iter-58170, train loss-1.8556, acc-0.4200, valid loss-1.8169, acc-0.4964, test loss-1.8178, acc-0.4908\n",
      "Iter-58180, train loss-1.8054, acc-0.4800, valid loss-1.8168, acc-0.4964, test loss-1.8177, acc-0.4908\n",
      "Iter-58190, train loss-1.9029, acc-0.4000, valid loss-1.8168, acc-0.4966, test loss-1.8177, acc-0.4908\n",
      "Iter-58200, train loss-1.8269, acc-0.5400, valid loss-1.8167, acc-0.4966, test loss-1.8176, acc-0.4910\n",
      "Iter-58210, train loss-1.9818, acc-0.3800, valid loss-1.8167, acc-0.4968, test loss-1.8176, acc-0.4910\n",
      "Iter-58220, train loss-1.7417, acc-0.5400, valid loss-1.8166, acc-0.4968, test loss-1.8175, acc-0.4910\n",
      "Iter-58230, train loss-1.8526, acc-0.5200, valid loss-1.8165, acc-0.4968, test loss-1.8175, acc-0.4910\n",
      "Iter-58240, train loss-1.9125, acc-0.4400, valid loss-1.8165, acc-0.4968, test loss-1.8174, acc-0.4910\n",
      "Iter-58250, train loss-1.8386, acc-0.5200, valid loss-1.8165, acc-0.4968, test loss-1.8174, acc-0.4910\n",
      "Iter-58260, train loss-1.8710, acc-0.4200, valid loss-1.8164, acc-0.4970, test loss-1.8173, acc-0.4911\n",
      "Iter-58270, train loss-1.9034, acc-0.4000, valid loss-1.8163, acc-0.4970, test loss-1.8173, acc-0.4911\n",
      "Iter-58280, train loss-1.8099, acc-0.5000, valid loss-1.8163, acc-0.4968, test loss-1.8172, acc-0.4911\n",
      "Iter-58290, train loss-1.8540, acc-0.4000, valid loss-1.8163, acc-0.4970, test loss-1.8172, acc-0.4910\n",
      "Iter-58300, train loss-1.8712, acc-0.4600, valid loss-1.8162, acc-0.4970, test loss-1.8171, acc-0.4909\n",
      "Iter-58310, train loss-1.7919, acc-0.4800, valid loss-1.8162, acc-0.4968, test loss-1.8171, acc-0.4911\n",
      "Iter-58320, train loss-1.7811, acc-0.5800, valid loss-1.8161, acc-0.4968, test loss-1.8170, acc-0.4909\n",
      "Iter-58330, train loss-1.7523, acc-0.5000, valid loss-1.8161, acc-0.4966, test loss-1.8170, acc-0.4911\n",
      "Iter-58340, train loss-1.8622, acc-0.2800, valid loss-1.8160, acc-0.4964, test loss-1.8169, acc-0.4910\n",
      "Iter-58350, train loss-1.8788, acc-0.4800, valid loss-1.8160, acc-0.4966, test loss-1.8169, acc-0.4913\n",
      "Iter-58360, train loss-1.8276, acc-0.4800, valid loss-1.8159, acc-0.4966, test loss-1.8168, acc-0.4912\n",
      "Iter-58370, train loss-1.9489, acc-0.3400, valid loss-1.8159, acc-0.4966, test loss-1.8168, acc-0.4914\n",
      "Iter-58380, train loss-1.9055, acc-0.5000, valid loss-1.8158, acc-0.4966, test loss-1.8167, acc-0.4913\n",
      "Iter-58390, train loss-1.9525, acc-0.3600, valid loss-1.8158, acc-0.4964, test loss-1.8167, acc-0.4913\n",
      "Iter-58400, train loss-1.7428, acc-0.5600, valid loss-1.8157, acc-0.4964, test loss-1.8166, acc-0.4913\n",
      "Iter-58410, train loss-1.9015, acc-0.4200, valid loss-1.8157, acc-0.4966, test loss-1.8166, acc-0.4913\n",
      "Iter-58420, train loss-1.8750, acc-0.3800, valid loss-1.8156, acc-0.4966, test loss-1.8165, acc-0.4914\n",
      "Iter-58430, train loss-1.8407, acc-0.4400, valid loss-1.8156, acc-0.4964, test loss-1.8165, acc-0.4913\n",
      "Iter-58440, train loss-1.7887, acc-0.5800, valid loss-1.8155, acc-0.4964, test loss-1.8164, acc-0.4914\n",
      "Iter-58450, train loss-1.8731, acc-0.4400, valid loss-1.8155, acc-0.4964, test loss-1.8164, acc-0.4914\n",
      "Iter-58460, train loss-1.7845, acc-0.5400, valid loss-1.8154, acc-0.4968, test loss-1.8163, acc-0.4914\n",
      "Iter-58470, train loss-1.8424, acc-0.3800, valid loss-1.8154, acc-0.4968, test loss-1.8163, acc-0.4913\n",
      "Iter-58480, train loss-1.8443, acc-0.3600, valid loss-1.8153, acc-0.4968, test loss-1.8162, acc-0.4912\n",
      "Iter-58490, train loss-1.8688, acc-0.4600, valid loss-1.8152, acc-0.4968, test loss-1.8162, acc-0.4912\n",
      "Iter-58500, train loss-1.7877, acc-0.4800, valid loss-1.8152, acc-0.4970, test loss-1.8161, acc-0.4912\n",
      "Iter-58510, train loss-1.7984, acc-0.4800, valid loss-1.8151, acc-0.4970, test loss-1.8161, acc-0.4913\n",
      "Iter-58520, train loss-1.8535, acc-0.4800, valid loss-1.8151, acc-0.4970, test loss-1.8160, acc-0.4915\n",
      "Iter-58530, train loss-1.9022, acc-0.3600, valid loss-1.8150, acc-0.4972, test loss-1.8160, acc-0.4916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-58540, train loss-1.7658, acc-0.5400, valid loss-1.8150, acc-0.4972, test loss-1.8159, acc-0.4915\n",
      "Iter-58550, train loss-1.8862, acc-0.4000, valid loss-1.8149, acc-0.4974, test loss-1.8159, acc-0.4914\n",
      "Iter-58560, train loss-1.8126, acc-0.4400, valid loss-1.8149, acc-0.4972, test loss-1.8158, acc-0.4914\n",
      "Iter-58570, train loss-1.7531, acc-0.5000, valid loss-1.8148, acc-0.4972, test loss-1.8158, acc-0.4914\n",
      "Iter-58580, train loss-1.8659, acc-0.5200, valid loss-1.8148, acc-0.4972, test loss-1.8157, acc-0.4915\n",
      "Iter-58590, train loss-1.8516, acc-0.5200, valid loss-1.8147, acc-0.4974, test loss-1.8157, acc-0.4915\n",
      "Iter-58600, train loss-1.7925, acc-0.5600, valid loss-1.8147, acc-0.4972, test loss-1.8156, acc-0.4916\n",
      "Iter-58610, train loss-1.7680, acc-0.5800, valid loss-1.8146, acc-0.4974, test loss-1.8156, acc-0.4916\n",
      "Iter-58620, train loss-1.7842, acc-0.5000, valid loss-1.8146, acc-0.4972, test loss-1.8155, acc-0.4915\n",
      "Iter-58630, train loss-1.8324, acc-0.4600, valid loss-1.8145, acc-0.4972, test loss-1.8155, acc-0.4914\n",
      "Iter-58640, train loss-1.8171, acc-0.5800, valid loss-1.8145, acc-0.4972, test loss-1.8154, acc-0.4914\n",
      "Iter-58650, train loss-1.9018, acc-0.5000, valid loss-1.8144, acc-0.4972, test loss-1.8154, acc-0.4914\n",
      "Iter-58660, train loss-1.9078, acc-0.3800, valid loss-1.8144, acc-0.4974, test loss-1.8153, acc-0.4915\n",
      "Iter-58670, train loss-1.7317, acc-0.5000, valid loss-1.8143, acc-0.4972, test loss-1.8153, acc-0.4917\n",
      "Iter-58680, train loss-1.8000, acc-0.5400, valid loss-1.8143, acc-0.4972, test loss-1.8152, acc-0.4918\n",
      "Iter-58690, train loss-1.8185, acc-0.5200, valid loss-1.8142, acc-0.4972, test loss-1.8152, acc-0.4918\n",
      "Iter-58700, train loss-1.8309, acc-0.3600, valid loss-1.8142, acc-0.4972, test loss-1.8151, acc-0.4918\n",
      "Iter-58710, train loss-1.8406, acc-0.4800, valid loss-1.8141, acc-0.4972, test loss-1.8151, acc-0.4916\n",
      "Iter-58720, train loss-1.7415, acc-0.6600, valid loss-1.8141, acc-0.4972, test loss-1.8150, acc-0.4916\n",
      "Iter-58730, train loss-1.8268, acc-0.5000, valid loss-1.8140, acc-0.4974, test loss-1.8150, acc-0.4916\n",
      "Iter-58740, train loss-1.7220, acc-0.5200, valid loss-1.8140, acc-0.4974, test loss-1.8149, acc-0.4916\n",
      "Iter-58750, train loss-1.8988, acc-0.4000, valid loss-1.8139, acc-0.4972, test loss-1.8149, acc-0.4916\n",
      "Iter-58760, train loss-1.8334, acc-0.5200, valid loss-1.8139, acc-0.4972, test loss-1.8148, acc-0.4916\n",
      "Iter-58770, train loss-1.7724, acc-0.5400, valid loss-1.8138, acc-0.4972, test loss-1.8148, acc-0.4916\n",
      "Iter-58780, train loss-1.8476, acc-0.4600, valid loss-1.8138, acc-0.4970, test loss-1.8147, acc-0.4915\n",
      "Iter-58790, train loss-1.7745, acc-0.5000, valid loss-1.8137, acc-0.4972, test loss-1.8147, acc-0.4914\n",
      "Iter-58800, train loss-1.8824, acc-0.4400, valid loss-1.8137, acc-0.4972, test loss-1.8146, acc-0.4916\n",
      "Iter-58810, train loss-1.8233, acc-0.4800, valid loss-1.8136, acc-0.4972, test loss-1.8146, acc-0.4916\n",
      "Iter-58820, train loss-1.7836, acc-0.5600, valid loss-1.8136, acc-0.4972, test loss-1.8145, acc-0.4917\n",
      "Iter-58830, train loss-1.8357, acc-0.5000, valid loss-1.8135, acc-0.4972, test loss-1.8145, acc-0.4917\n",
      "Iter-58840, train loss-1.7712, acc-0.5400, valid loss-1.8135, acc-0.4972, test loss-1.8144, acc-0.4919\n",
      "Iter-58850, train loss-1.7771, acc-0.5200, valid loss-1.8134, acc-0.4970, test loss-1.8144, acc-0.4916\n",
      "Iter-58860, train loss-1.8465, acc-0.4000, valid loss-1.8134, acc-0.4970, test loss-1.8143, acc-0.4918\n",
      "Iter-58870, train loss-1.8112, acc-0.5400, valid loss-1.8133, acc-0.4970, test loss-1.8142, acc-0.4919\n",
      "Iter-58880, train loss-1.7677, acc-0.5000, valid loss-1.8133, acc-0.4970, test loss-1.8142, acc-0.4918\n",
      "Iter-58890, train loss-1.8056, acc-0.5200, valid loss-1.8132, acc-0.4972, test loss-1.8141, acc-0.4918\n",
      "Iter-58900, train loss-1.8495, acc-0.4200, valid loss-1.8132, acc-0.4972, test loss-1.8141, acc-0.4920\n",
      "Iter-58910, train loss-1.8483, acc-0.4200, valid loss-1.8131, acc-0.4972, test loss-1.8140, acc-0.4919\n",
      "Iter-58920, train loss-1.8775, acc-0.5000, valid loss-1.8131, acc-0.4970, test loss-1.8140, acc-0.4921\n",
      "Iter-58930, train loss-1.8326, acc-0.5600, valid loss-1.8130, acc-0.4970, test loss-1.8139, acc-0.4922\n",
      "Iter-58940, train loss-1.8909, acc-0.4200, valid loss-1.8130, acc-0.4970, test loss-1.8139, acc-0.4921\n",
      "Iter-58950, train loss-1.7611, acc-0.5800, valid loss-1.8129, acc-0.4970, test loss-1.8138, acc-0.4922\n",
      "Iter-58960, train loss-1.9202, acc-0.4200, valid loss-1.8129, acc-0.4970, test loss-1.8138, acc-0.4921\n",
      "Iter-58970, train loss-1.8053, acc-0.5800, valid loss-1.8128, acc-0.4972, test loss-1.8137, acc-0.4921\n",
      "Iter-58980, train loss-1.8643, acc-0.3600, valid loss-1.8128, acc-0.4972, test loss-1.8137, acc-0.4921\n",
      "Iter-58990, train loss-1.8929, acc-0.4600, valid loss-1.8127, acc-0.4972, test loss-1.8136, acc-0.4922\n",
      "Iter-59000, train loss-1.7922, acc-0.5600, valid loss-1.8127, acc-0.4970, test loss-1.8136, acc-0.4923\n",
      "Iter-59010, train loss-1.8001, acc-0.5200, valid loss-1.8126, acc-0.4970, test loss-1.8135, acc-0.4924\n",
      "Iter-59020, train loss-1.8071, acc-0.5200, valid loss-1.8126, acc-0.4970, test loss-1.8135, acc-0.4924\n",
      "Iter-59030, train loss-1.8551, acc-0.3600, valid loss-1.8125, acc-0.4972, test loss-1.8134, acc-0.4924\n",
      "Iter-59040, train loss-1.9956, acc-0.3400, valid loss-1.8125, acc-0.4974, test loss-1.8134, acc-0.4925\n",
      "Iter-59050, train loss-1.8731, acc-0.4200, valid loss-1.8124, acc-0.4972, test loss-1.8133, acc-0.4927\n",
      "Iter-59060, train loss-1.8241, acc-0.5600, valid loss-1.8124, acc-0.4974, test loss-1.8133, acc-0.4926\n",
      "Iter-59070, train loss-1.7736, acc-0.4800, valid loss-1.8123, acc-0.4974, test loss-1.8132, acc-0.4927\n",
      "Iter-59080, train loss-1.8543, acc-0.4200, valid loss-1.8123, acc-0.4976, test loss-1.8132, acc-0.4927\n",
      "Iter-59090, train loss-1.8248, acc-0.4400, valid loss-1.8122, acc-0.4976, test loss-1.8131, acc-0.4926\n",
      "Iter-59100, train loss-1.7533, acc-0.6400, valid loss-1.8122, acc-0.4980, test loss-1.8131, acc-0.4927\n",
      "Iter-59110, train loss-1.8600, acc-0.5000, valid loss-1.8121, acc-0.4980, test loss-1.8130, acc-0.4927\n",
      "Iter-59120, train loss-1.8276, acc-0.5400, valid loss-1.8121, acc-0.4980, test loss-1.8130, acc-0.4929\n",
      "Iter-59130, train loss-1.8725, acc-0.4400, valid loss-1.8120, acc-0.4980, test loss-1.8129, acc-0.4929\n",
      "Iter-59140, train loss-1.8694, acc-0.3800, valid loss-1.8120, acc-0.4982, test loss-1.8129, acc-0.4932\n",
      "Iter-59150, train loss-1.8805, acc-0.4600, valid loss-1.8119, acc-0.4980, test loss-1.8128, acc-0.4933\n",
      "Iter-59160, train loss-1.9006, acc-0.4800, valid loss-1.8119, acc-0.4980, test loss-1.8128, acc-0.4930\n",
      "Iter-59170, train loss-1.8398, acc-0.5200, valid loss-1.8118, acc-0.4980, test loss-1.8127, acc-0.4930\n",
      "Iter-59180, train loss-1.9517, acc-0.4800, valid loss-1.8118, acc-0.4980, test loss-1.8127, acc-0.4931\n",
      "Iter-59190, train loss-1.7177, acc-0.6400, valid loss-1.8117, acc-0.4980, test loss-1.8126, acc-0.4928\n",
      "Iter-59200, train loss-1.7634, acc-0.5800, valid loss-1.8117, acc-0.4980, test loss-1.8126, acc-0.4928\n",
      "Iter-59210, train loss-1.6942, acc-0.6000, valid loss-1.8116, acc-0.4980, test loss-1.8125, acc-0.4929\n",
      "Iter-59220, train loss-1.7539, acc-0.6000, valid loss-1.8116, acc-0.4982, test loss-1.8125, acc-0.4928\n",
      "Iter-59230, train loss-1.7688, acc-0.5200, valid loss-1.8115, acc-0.4982, test loss-1.8124, acc-0.4927\n",
      "Iter-59240, train loss-1.8455, acc-0.4800, valid loss-1.8115, acc-0.4982, test loss-1.8124, acc-0.4930\n",
      "Iter-59250, train loss-1.8135, acc-0.4400, valid loss-1.8114, acc-0.4982, test loss-1.8124, acc-0.4930\n",
      "Iter-59260, train loss-1.8743, acc-0.4200, valid loss-1.8114, acc-0.4980, test loss-1.8123, acc-0.4929\n",
      "Iter-59270, train loss-1.8683, acc-0.4200, valid loss-1.8113, acc-0.4982, test loss-1.8123, acc-0.4928\n",
      "Iter-59280, train loss-1.9077, acc-0.4800, valid loss-1.8113, acc-0.4984, test loss-1.8122, acc-0.4930\n",
      "Iter-59290, train loss-1.7979, acc-0.5200, valid loss-1.8112, acc-0.4986, test loss-1.8121, acc-0.4929\n",
      "Iter-59300, train loss-1.8074, acc-0.5000, valid loss-1.8112, acc-0.4982, test loss-1.8121, acc-0.4929\n",
      "Iter-59310, train loss-1.8011, acc-0.4400, valid loss-1.8111, acc-0.4982, test loss-1.8120, acc-0.4927\n",
      "Iter-59320, train loss-1.7643, acc-0.5200, valid loss-1.8111, acc-0.4984, test loss-1.8120, acc-0.4927\n",
      "Iter-59330, train loss-1.8541, acc-0.5200, valid loss-1.8110, acc-0.4986, test loss-1.8119, acc-0.4927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-59340, train loss-1.7987, acc-0.4200, valid loss-1.8110, acc-0.4986, test loss-1.8119, acc-0.4928\n",
      "Iter-59350, train loss-1.7445, acc-0.6400, valid loss-1.8109, acc-0.4982, test loss-1.8118, acc-0.4929\n",
      "Iter-59360, train loss-1.9643, acc-0.4400, valid loss-1.8109, acc-0.4982, test loss-1.8118, acc-0.4929\n",
      "Iter-59370, train loss-1.8991, acc-0.4200, valid loss-1.8108, acc-0.4982, test loss-1.8118, acc-0.4928\n",
      "Iter-59380, train loss-1.7397, acc-0.5800, valid loss-1.8108, acc-0.4982, test loss-1.8117, acc-0.4928\n",
      "Iter-59390, train loss-1.8799, acc-0.4200, valid loss-1.8107, acc-0.4982, test loss-1.8117, acc-0.4928\n",
      "Iter-59400, train loss-1.8287, acc-0.5200, valid loss-1.8107, acc-0.4982, test loss-1.8116, acc-0.4927\n",
      "Iter-59410, train loss-1.8844, acc-0.5000, valid loss-1.8106, acc-0.4984, test loss-1.8116, acc-0.4928\n",
      "Iter-59420, train loss-1.8362, acc-0.4600, valid loss-1.8106, acc-0.4984, test loss-1.8115, acc-0.4927\n",
      "Iter-59430, train loss-1.8473, acc-0.5200, valid loss-1.8105, acc-0.4982, test loss-1.8115, acc-0.4927\n",
      "Iter-59440, train loss-1.7865, acc-0.4800, valid loss-1.8105, acc-0.4982, test loss-1.8114, acc-0.4929\n",
      "Iter-59450, train loss-1.8809, acc-0.4200, valid loss-1.8104, acc-0.4982, test loss-1.8114, acc-0.4928\n",
      "Iter-59460, train loss-1.8653, acc-0.4000, valid loss-1.8104, acc-0.4982, test loss-1.8113, acc-0.4928\n",
      "Iter-59470, train loss-1.8548, acc-0.5600, valid loss-1.8103, acc-0.4986, test loss-1.8113, acc-0.4929\n",
      "Iter-59480, train loss-1.8422, acc-0.5000, valid loss-1.8103, acc-0.4986, test loss-1.8112, acc-0.4929\n",
      "Iter-59490, train loss-1.8690, acc-0.4400, valid loss-1.8102, acc-0.4984, test loss-1.8112, acc-0.4928\n",
      "Iter-59500, train loss-1.7909, acc-0.4800, valid loss-1.8102, acc-0.4986, test loss-1.8111, acc-0.4929\n",
      "Iter-59510, train loss-1.7621, acc-0.5600, valid loss-1.8101, acc-0.4986, test loss-1.8111, acc-0.4929\n",
      "Iter-59520, train loss-1.8861, acc-0.4200, valid loss-1.8101, acc-0.4986, test loss-1.8110, acc-0.4931\n",
      "Iter-59530, train loss-1.8555, acc-0.4200, valid loss-1.8100, acc-0.4984, test loss-1.8110, acc-0.4932\n",
      "Iter-59540, train loss-1.9133, acc-0.3800, valid loss-1.8100, acc-0.4984, test loss-1.8109, acc-0.4932\n",
      "Iter-59550, train loss-1.8313, acc-0.5000, valid loss-1.8099, acc-0.4984, test loss-1.8109, acc-0.4931\n",
      "Iter-59560, train loss-1.8856, acc-0.3800, valid loss-1.8099, acc-0.4988, test loss-1.8108, acc-0.4928\n",
      "Iter-59570, train loss-1.8479, acc-0.4400, valid loss-1.8098, acc-0.4986, test loss-1.8108, acc-0.4930\n",
      "Iter-59580, train loss-1.8047, acc-0.4600, valid loss-1.8098, acc-0.4984, test loss-1.8107, acc-0.4928\n",
      "Iter-59590, train loss-1.8723, acc-0.4600, valid loss-1.8097, acc-0.4984, test loss-1.8107, acc-0.4930\n",
      "Iter-59600, train loss-1.8064, acc-0.5600, valid loss-1.8097, acc-0.4984, test loss-1.8106, acc-0.4930\n",
      "Iter-59610, train loss-1.7722, acc-0.5200, valid loss-1.8096, acc-0.4986, test loss-1.8106, acc-0.4928\n",
      "Iter-59620, train loss-1.7303, acc-0.6000, valid loss-1.8096, acc-0.4986, test loss-1.8105, acc-0.4930\n",
      "Iter-59630, train loss-1.8174, acc-0.4400, valid loss-1.8095, acc-0.4986, test loss-1.8105, acc-0.4930\n",
      "Iter-59640, train loss-1.8508, acc-0.4400, valid loss-1.8095, acc-0.4986, test loss-1.8104, acc-0.4929\n",
      "Iter-59650, train loss-1.8522, acc-0.4600, valid loss-1.8094, acc-0.4988, test loss-1.8104, acc-0.4929\n",
      "Iter-59660, train loss-1.8596, acc-0.4600, valid loss-1.8094, acc-0.4990, test loss-1.8103, acc-0.4929\n",
      "Iter-59670, train loss-1.7998, acc-0.5000, valid loss-1.8093, acc-0.4986, test loss-1.8103, acc-0.4929\n",
      "Iter-59680, train loss-1.8328, acc-0.4800, valid loss-1.8093, acc-0.4990, test loss-1.8102, acc-0.4930\n",
      "Iter-59690, train loss-1.9846, acc-0.3400, valid loss-1.8092, acc-0.4986, test loss-1.8102, acc-0.4931\n",
      "Iter-59700, train loss-1.8017, acc-0.6000, valid loss-1.8092, acc-0.4986, test loss-1.8101, acc-0.4931\n",
      "Iter-59710, train loss-1.7832, acc-0.5400, valid loss-1.8091, acc-0.4986, test loss-1.8101, acc-0.4930\n",
      "Iter-59720, train loss-1.8616, acc-0.4200, valid loss-1.8091, acc-0.4990, test loss-1.8100, acc-0.4931\n",
      "Iter-59730, train loss-1.7926, acc-0.5800, valid loss-1.8090, acc-0.4986, test loss-1.8100, acc-0.4932\n",
      "Iter-59740, train loss-1.7868, acc-0.5400, valid loss-1.8090, acc-0.4988, test loss-1.8099, acc-0.4930\n",
      "Iter-59750, train loss-1.8266, acc-0.5400, valid loss-1.8089, acc-0.4988, test loss-1.8099, acc-0.4930\n",
      "Iter-59760, train loss-1.8342, acc-0.4000, valid loss-1.8089, acc-0.4986, test loss-1.8098, acc-0.4931\n",
      "Iter-59770, train loss-1.7820, acc-0.5600, valid loss-1.8088, acc-0.4986, test loss-1.8098, acc-0.4931\n",
      "Iter-59780, train loss-1.7827, acc-0.6000, valid loss-1.8088, acc-0.4990, test loss-1.8097, acc-0.4932\n",
      "Iter-59790, train loss-1.7811, acc-0.4200, valid loss-1.8088, acc-0.4990, test loss-1.8097, acc-0.4931\n",
      "Iter-59800, train loss-1.8688, acc-0.5000, valid loss-1.8087, acc-0.4990, test loss-1.8096, acc-0.4931\n",
      "Iter-59810, train loss-1.8840, acc-0.4200, valid loss-1.8087, acc-0.4990, test loss-1.8096, acc-0.4930\n",
      "Iter-59820, train loss-1.8167, acc-0.4400, valid loss-1.8086, acc-0.4990, test loss-1.8095, acc-0.4930\n",
      "Iter-59830, train loss-1.7817, acc-0.4400, valid loss-1.8086, acc-0.4988, test loss-1.8095, acc-0.4930\n",
      "Iter-59840, train loss-1.7537, acc-0.5800, valid loss-1.8085, acc-0.4988, test loss-1.8094, acc-0.4930\n",
      "Iter-59850, train loss-1.8923, acc-0.4200, valid loss-1.8085, acc-0.4990, test loss-1.8094, acc-0.4930\n",
      "Iter-59860, train loss-1.8408, acc-0.4800, valid loss-1.8084, acc-0.4990, test loss-1.8093, acc-0.4929\n",
      "Iter-59870, train loss-1.7731, acc-0.6000, valid loss-1.8084, acc-0.4990, test loss-1.8093, acc-0.4928\n",
      "Iter-59880, train loss-1.9210, acc-0.4200, valid loss-1.8083, acc-0.4990, test loss-1.8092, acc-0.4928\n",
      "Iter-59890, train loss-1.8786, acc-0.4800, valid loss-1.8083, acc-0.4990, test loss-1.8092, acc-0.4929\n",
      "Iter-59900, train loss-1.9008, acc-0.3600, valid loss-1.8082, acc-0.4990, test loss-1.8091, acc-0.4929\n",
      "Iter-59910, train loss-1.7631, acc-0.6400, valid loss-1.8082, acc-0.4988, test loss-1.8091, acc-0.4929\n",
      "Iter-59920, train loss-1.7749, acc-0.6200, valid loss-1.8081, acc-0.4990, test loss-1.8090, acc-0.4927\n",
      "Iter-59930, train loss-1.7544, acc-0.5600, valid loss-1.8081, acc-0.4988, test loss-1.8090, acc-0.4929\n",
      "Iter-59940, train loss-1.8080, acc-0.5000, valid loss-1.8080, acc-0.4986, test loss-1.8089, acc-0.4929\n",
      "Iter-59950, train loss-1.6949, acc-0.5400, valid loss-1.8080, acc-0.4984, test loss-1.8089, acc-0.4929\n",
      "Iter-59960, train loss-1.9224, acc-0.5200, valid loss-1.8079, acc-0.4986, test loss-1.8088, acc-0.4930\n",
      "Iter-59970, train loss-1.9220, acc-0.3600, valid loss-1.8079, acc-0.4988, test loss-1.8088, acc-0.4927\n",
      "Iter-59980, train loss-1.7210, acc-0.5400, valid loss-1.8078, acc-0.4992, test loss-1.8087, acc-0.4927\n",
      "Iter-59990, train loss-1.7906, acc-0.5200, valid loss-1.8078, acc-0.4990, test loss-1.8087, acc-0.4928\n",
      "Iter-60000, train loss-1.9421, acc-0.3800, valid loss-1.8077, acc-0.4990, test loss-1.8086, acc-0.4927\n",
      "Iter-60010, train loss-1.8129, acc-0.5200, valid loss-1.8077, acc-0.4990, test loss-1.8086, acc-0.4927\n",
      "Iter-60020, train loss-1.7465, acc-0.5800, valid loss-1.8076, acc-0.4988, test loss-1.8085, acc-0.4928\n",
      "Iter-60030, train loss-1.8166, acc-0.5000, valid loss-1.8076, acc-0.4988, test loss-1.8085, acc-0.4928\n",
      "Iter-60040, train loss-1.7760, acc-0.5800, valid loss-1.8075, acc-0.4988, test loss-1.8084, acc-0.4928\n",
      "Iter-60050, train loss-1.9247, acc-0.3800, valid loss-1.8075, acc-0.4988, test loss-1.8084, acc-0.4929\n",
      "Iter-60060, train loss-1.8724, acc-0.4800, valid loss-1.8074, acc-0.4990, test loss-1.8083, acc-0.4929\n",
      "Iter-60070, train loss-1.7607, acc-0.6000, valid loss-1.8074, acc-0.4988, test loss-1.8083, acc-0.4929\n",
      "Iter-60080, train loss-1.8235, acc-0.5000, valid loss-1.8073, acc-0.4988, test loss-1.8082, acc-0.4929\n",
      "Iter-60090, train loss-1.8475, acc-0.4200, valid loss-1.8073, acc-0.4988, test loss-1.8082, acc-0.4931\n",
      "Iter-60100, train loss-1.7678, acc-0.5200, valid loss-1.8072, acc-0.4988, test loss-1.8081, acc-0.4930\n",
      "Iter-60110, train loss-1.7341, acc-0.5600, valid loss-1.8072, acc-0.4988, test loss-1.8081, acc-0.4930\n",
      "Iter-60120, train loss-1.7375, acc-0.5600, valid loss-1.8071, acc-0.4990, test loss-1.8080, acc-0.4930\n",
      "Iter-60130, train loss-1.8554, acc-0.5000, valid loss-1.8071, acc-0.4990, test loss-1.8080, acc-0.4930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-60140, train loss-1.8266, acc-0.5000, valid loss-1.8070, acc-0.4990, test loss-1.8079, acc-0.4930\n",
      "Iter-60150, train loss-1.8505, acc-0.4800, valid loss-1.8070, acc-0.4992, test loss-1.8079, acc-0.4929\n",
      "Iter-60160, train loss-1.7538, acc-0.6000, valid loss-1.8069, acc-0.4996, test loss-1.8078, acc-0.4931\n",
      "Iter-60170, train loss-1.9024, acc-0.4800, valid loss-1.8069, acc-0.4992, test loss-1.8078, acc-0.4931\n",
      "Iter-60180, train loss-1.7978, acc-0.5400, valid loss-1.8068, acc-0.4994, test loss-1.8077, acc-0.4930\n",
      "Iter-60190, train loss-1.8476, acc-0.4600, valid loss-1.8068, acc-0.4994, test loss-1.8077, acc-0.4929\n",
      "Iter-60200, train loss-1.7611, acc-0.5200, valid loss-1.8067, acc-0.4994, test loss-1.8076, acc-0.4929\n",
      "Iter-60210, train loss-1.7770, acc-0.6000, valid loss-1.8067, acc-0.4994, test loss-1.8076, acc-0.4930\n",
      "Iter-60220, train loss-1.8962, acc-0.4200, valid loss-1.8066, acc-0.4994, test loss-1.8076, acc-0.4928\n",
      "Iter-60230, train loss-1.8860, acc-0.4200, valid loss-1.8066, acc-0.4994, test loss-1.8075, acc-0.4928\n",
      "Iter-60240, train loss-1.8157, acc-0.4600, valid loss-1.8065, acc-0.4992, test loss-1.8074, acc-0.4929\n",
      "Iter-60250, train loss-1.7933, acc-0.5600, valid loss-1.8065, acc-0.4996, test loss-1.8074, acc-0.4931\n",
      "Iter-60260, train loss-1.8698, acc-0.4200, valid loss-1.8064, acc-0.4996, test loss-1.8074, acc-0.4932\n",
      "Iter-60270, train loss-1.7861, acc-0.4400, valid loss-1.8064, acc-0.4996, test loss-1.8073, acc-0.4931\n",
      "Iter-60280, train loss-1.7949, acc-0.4800, valid loss-1.8064, acc-0.4998, test loss-1.8073, acc-0.4932\n",
      "Iter-60290, train loss-1.7714, acc-0.5200, valid loss-1.8063, acc-0.4996, test loss-1.8072, acc-0.4932\n",
      "Iter-60300, train loss-1.8749, acc-0.4000, valid loss-1.8063, acc-0.4996, test loss-1.8072, acc-0.4933\n",
      "Iter-60310, train loss-1.8566, acc-0.5000, valid loss-1.8062, acc-0.4994, test loss-1.8071, acc-0.4933\n",
      "Iter-60320, train loss-1.8188, acc-0.5800, valid loss-1.8062, acc-0.4996, test loss-1.8071, acc-0.4934\n",
      "Iter-60330, train loss-1.8268, acc-0.5200, valid loss-1.8061, acc-0.4998, test loss-1.8070, acc-0.4935\n",
      "Iter-60340, train loss-1.7791, acc-0.5600, valid loss-1.8061, acc-0.4998, test loss-1.8070, acc-0.4933\n",
      "Iter-60350, train loss-1.7706, acc-0.6200, valid loss-1.8060, acc-0.4996, test loss-1.8069, acc-0.4932\n",
      "Iter-60360, train loss-1.8062, acc-0.5200, valid loss-1.8060, acc-0.4998, test loss-1.8069, acc-0.4933\n",
      "Iter-60370, train loss-1.8202, acc-0.4600, valid loss-1.8059, acc-0.4998, test loss-1.8068, acc-0.4931\n",
      "Iter-60380, train loss-1.8401, acc-0.4800, valid loss-1.8059, acc-0.4996, test loss-1.8068, acc-0.4930\n",
      "Iter-60390, train loss-1.8404, acc-0.5000, valid loss-1.8058, acc-0.4998, test loss-1.8067, acc-0.4934\n",
      "Iter-60400, train loss-1.8102, acc-0.5000, valid loss-1.8058, acc-0.5000, test loss-1.8067, acc-0.4932\n",
      "Iter-60410, train loss-1.8788, acc-0.4200, valid loss-1.8057, acc-0.4998, test loss-1.8066, acc-0.4933\n",
      "Iter-60420, train loss-1.8647, acc-0.3600, valid loss-1.8057, acc-0.5000, test loss-1.8066, acc-0.4933\n",
      "Iter-60430, train loss-1.8248, acc-0.5600, valid loss-1.8056, acc-0.4998, test loss-1.8065, acc-0.4933\n",
      "Iter-60440, train loss-1.7960, acc-0.4800, valid loss-1.8056, acc-0.4998, test loss-1.8065, acc-0.4933\n",
      "Iter-60450, train loss-1.8127, acc-0.5400, valid loss-1.8055, acc-0.4998, test loss-1.8064, acc-0.4934\n",
      "Iter-60460, train loss-1.9557, acc-0.4200, valid loss-1.8055, acc-0.4998, test loss-1.8064, acc-0.4934\n",
      "Iter-60470, train loss-1.7162, acc-0.5400, valid loss-1.8054, acc-0.4998, test loss-1.8063, acc-0.4934\n",
      "Iter-60480, train loss-1.8552, acc-0.4600, valid loss-1.8054, acc-0.4998, test loss-1.8063, acc-0.4934\n",
      "Iter-60490, train loss-1.9282, acc-0.3800, valid loss-1.8053, acc-0.4998, test loss-1.8062, acc-0.4932\n",
      "Iter-60500, train loss-1.9069, acc-0.4000, valid loss-1.8053, acc-0.4998, test loss-1.8062, acc-0.4931\n",
      "Iter-60510, train loss-1.7826, acc-0.5400, valid loss-1.8052, acc-0.4996, test loss-1.8061, acc-0.4932\n",
      "Iter-60520, train loss-1.7825, acc-0.4800, valid loss-1.8052, acc-0.4998, test loss-1.8061, acc-0.4930\n",
      "Iter-60530, train loss-1.8649, acc-0.3800, valid loss-1.8051, acc-0.4996, test loss-1.8060, acc-0.4932\n",
      "Iter-60540, train loss-1.7049, acc-0.5400, valid loss-1.8051, acc-0.4998, test loss-1.8060, acc-0.4936\n",
      "Iter-60550, train loss-1.8379, acc-0.4800, valid loss-1.8050, acc-0.4998, test loss-1.8059, acc-0.4938\n",
      "Iter-60560, train loss-1.7892, acc-0.5400, valid loss-1.8050, acc-0.5000, test loss-1.8059, acc-0.4935\n",
      "Iter-60570, train loss-1.8219, acc-0.5400, valid loss-1.8049, acc-0.4998, test loss-1.8058, acc-0.4936\n",
      "Iter-60580, train loss-1.8283, acc-0.5600, valid loss-1.8049, acc-0.4998, test loss-1.8058, acc-0.4937\n",
      "Iter-60590, train loss-1.7926, acc-0.4600, valid loss-1.8048, acc-0.4998, test loss-1.8057, acc-0.4937\n",
      "Iter-60600, train loss-1.8882, acc-0.3600, valid loss-1.8048, acc-0.4998, test loss-1.8057, acc-0.4939\n",
      "Iter-60610, train loss-1.7457, acc-0.5400, valid loss-1.8047, acc-0.4996, test loss-1.8056, acc-0.4939\n",
      "Iter-60620, train loss-1.8519, acc-0.4200, valid loss-1.8047, acc-0.4998, test loss-1.8056, acc-0.4937\n",
      "Iter-60630, train loss-1.8533, acc-0.4600, valid loss-1.8046, acc-0.4996, test loss-1.8055, acc-0.4937\n",
      "Iter-60640, train loss-1.8054, acc-0.5200, valid loss-1.8046, acc-0.4998, test loss-1.8055, acc-0.4938\n",
      "Iter-60650, train loss-1.8528, acc-0.4000, valid loss-1.8045, acc-0.5000, test loss-1.8054, acc-0.4938\n",
      "Iter-60660, train loss-1.8164, acc-0.4600, valid loss-1.8045, acc-0.5000, test loss-1.8054, acc-0.4936\n",
      "Iter-60670, train loss-1.7351, acc-0.5800, valid loss-1.8044, acc-0.5000, test loss-1.8053, acc-0.4938\n",
      "Iter-60680, train loss-1.8075, acc-0.5000, valid loss-1.8044, acc-0.5000, test loss-1.8053, acc-0.4939\n",
      "Iter-60690, train loss-1.8005, acc-0.4600, valid loss-1.8043, acc-0.5000, test loss-1.8052, acc-0.4939\n",
      "Iter-60700, train loss-1.7905, acc-0.5600, valid loss-1.8043, acc-0.5000, test loss-1.8052, acc-0.4938\n",
      "Iter-60710, train loss-1.8883, acc-0.4400, valid loss-1.8042, acc-0.5000, test loss-1.8051, acc-0.4939\n",
      "Iter-60720, train loss-1.8320, acc-0.4800, valid loss-1.8042, acc-0.5000, test loss-1.8051, acc-0.4940\n",
      "Iter-60730, train loss-1.8491, acc-0.5200, valid loss-1.8041, acc-0.5000, test loss-1.8050, acc-0.4939\n",
      "Iter-60740, train loss-1.7808, acc-0.4800, valid loss-1.8041, acc-0.4998, test loss-1.8050, acc-0.4941\n",
      "Iter-60750, train loss-1.8681, acc-0.3400, valid loss-1.8040, acc-0.4998, test loss-1.8049, acc-0.4941\n",
      "Iter-60760, train loss-1.7895, acc-0.5800, valid loss-1.8040, acc-0.4998, test loss-1.8049, acc-0.4941\n",
      "Iter-60770, train loss-1.8394, acc-0.5000, valid loss-1.8039, acc-0.5000, test loss-1.8049, acc-0.4939\n",
      "Iter-60780, train loss-1.8994, acc-0.4600, valid loss-1.8039, acc-0.5000, test loss-1.8048, acc-0.4941\n",
      "Iter-60790, train loss-1.6738, acc-0.6000, valid loss-1.8038, acc-0.5000, test loss-1.8048, acc-0.4940\n",
      "Iter-60800, train loss-1.8087, acc-0.5400, valid loss-1.8038, acc-0.5002, test loss-1.8047, acc-0.4941\n",
      "Iter-60810, train loss-1.8181, acc-0.5000, valid loss-1.8038, acc-0.5002, test loss-1.8047, acc-0.4943\n",
      "Iter-60820, train loss-1.7969, acc-0.5000, valid loss-1.8037, acc-0.5000, test loss-1.8046, acc-0.4943\n",
      "Iter-60830, train loss-1.7386, acc-0.5600, valid loss-1.8037, acc-0.5000, test loss-1.8046, acc-0.4944\n",
      "Iter-60840, train loss-1.8146, acc-0.4400, valid loss-1.8036, acc-0.5000, test loss-1.8045, acc-0.4944\n",
      "Iter-60850, train loss-1.8473, acc-0.4200, valid loss-1.8036, acc-0.5000, test loss-1.8045, acc-0.4945\n",
      "Iter-60860, train loss-1.8135, acc-0.5400, valid loss-1.8035, acc-0.5000, test loss-1.8044, acc-0.4946\n",
      "Iter-60870, train loss-1.7464, acc-0.5800, valid loss-1.8035, acc-0.5000, test loss-1.8044, acc-0.4945\n",
      "Iter-60880, train loss-1.9134, acc-0.4000, valid loss-1.8034, acc-0.5000, test loss-1.8043, acc-0.4946\n",
      "Iter-60890, train loss-1.8005, acc-0.5800, valid loss-1.8034, acc-0.5000, test loss-1.8043, acc-0.4946\n",
      "Iter-60900, train loss-1.7269, acc-0.4800, valid loss-1.8033, acc-0.5000, test loss-1.8042, acc-0.4944\n",
      "Iter-60910, train loss-1.8192, acc-0.5000, valid loss-1.8033, acc-0.5000, test loss-1.8042, acc-0.4946\n",
      "Iter-60920, train loss-1.8911, acc-0.4800, valid loss-1.8032, acc-0.5000, test loss-1.8041, acc-0.4946\n",
      "Iter-60930, train loss-1.7872, acc-0.4000, valid loss-1.8032, acc-0.5000, test loss-1.8041, acc-0.4946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-60940, train loss-1.8190, acc-0.5600, valid loss-1.8031, acc-0.5000, test loss-1.8040, acc-0.4947\n",
      "Iter-60950, train loss-1.8646, acc-0.4200, valid loss-1.8031, acc-0.5000, test loss-1.8040, acc-0.4947\n",
      "Iter-60960, train loss-1.8338, acc-0.4400, valid loss-1.8030, acc-0.5000, test loss-1.8039, acc-0.4947\n",
      "Iter-60970, train loss-1.8673, acc-0.3600, valid loss-1.8030, acc-0.4996, test loss-1.8039, acc-0.4948\n",
      "Iter-60980, train loss-1.8441, acc-0.4400, valid loss-1.8030, acc-0.5000, test loss-1.8038, acc-0.4947\n",
      "Iter-60990, train loss-1.8682, acc-0.4800, valid loss-1.8029, acc-0.4998, test loss-1.8038, acc-0.4948\n",
      "Iter-61000, train loss-1.7980, acc-0.5200, valid loss-1.8029, acc-0.5000, test loss-1.8037, acc-0.4947\n",
      "Iter-61010, train loss-1.7365, acc-0.6400, valid loss-1.8028, acc-0.4998, test loss-1.8037, acc-0.4947\n",
      "Iter-61020, train loss-1.8626, acc-0.5200, valid loss-1.8028, acc-0.4996, test loss-1.8036, acc-0.4948\n",
      "Iter-61030, train loss-1.9121, acc-0.4200, valid loss-1.8027, acc-0.4996, test loss-1.8036, acc-0.4948\n",
      "Iter-61040, train loss-1.8902, acc-0.4600, valid loss-1.8026, acc-0.4996, test loss-1.8035, acc-0.4948\n",
      "Iter-61050, train loss-1.8188, acc-0.4800, valid loss-1.8026, acc-0.4998, test loss-1.8035, acc-0.4948\n",
      "Iter-61060, train loss-1.8374, acc-0.5200, valid loss-1.8026, acc-0.5000, test loss-1.8035, acc-0.4948\n",
      "Iter-61070, train loss-1.7451, acc-0.6200, valid loss-1.8025, acc-0.5000, test loss-1.8034, acc-0.4949\n",
      "Iter-61080, train loss-1.6720, acc-0.5600, valid loss-1.8025, acc-0.5000, test loss-1.8034, acc-0.4951\n",
      "Iter-61090, train loss-1.8563, acc-0.4400, valid loss-1.8024, acc-0.4998, test loss-1.8033, acc-0.4950\n",
      "Iter-61100, train loss-1.7298, acc-0.5000, valid loss-1.8024, acc-0.4998, test loss-1.8033, acc-0.4950\n",
      "Iter-61110, train loss-1.7330, acc-0.6400, valid loss-1.8023, acc-0.4996, test loss-1.8032, acc-0.4949\n",
      "Iter-61120, train loss-1.6692, acc-0.6800, valid loss-1.8023, acc-0.4998, test loss-1.8031, acc-0.4950\n",
      "Iter-61130, train loss-1.8013, acc-0.5200, valid loss-1.8022, acc-0.4998, test loss-1.8031, acc-0.4949\n",
      "Iter-61140, train loss-1.7364, acc-0.4600, valid loss-1.8022, acc-0.4998, test loss-1.8031, acc-0.4949\n",
      "Iter-61150, train loss-1.8852, acc-0.4800, valid loss-1.8021, acc-0.4998, test loss-1.8030, acc-0.4951\n",
      "Iter-61160, train loss-1.9751, acc-0.3200, valid loss-1.8021, acc-0.4996, test loss-1.8030, acc-0.4951\n",
      "Iter-61170, train loss-1.8303, acc-0.5200, valid loss-1.8020, acc-0.4998, test loss-1.8029, acc-0.4951\n",
      "Iter-61180, train loss-1.7484, acc-0.4400, valid loss-1.8020, acc-0.4996, test loss-1.8029, acc-0.4951\n",
      "Iter-61190, train loss-1.7462, acc-0.6000, valid loss-1.8019, acc-0.4998, test loss-1.8028, acc-0.4952\n",
      "Iter-61200, train loss-1.8508, acc-0.4600, valid loss-1.8019, acc-0.4998, test loss-1.8028, acc-0.4952\n",
      "Iter-61210, train loss-1.8488, acc-0.5200, valid loss-1.8018, acc-0.4996, test loss-1.8027, acc-0.4952\n",
      "Iter-61220, train loss-1.8008, acc-0.5400, valid loss-1.8018, acc-0.4994, test loss-1.8027, acc-0.4951\n",
      "Iter-61230, train loss-1.8285, acc-0.5200, valid loss-1.8017, acc-0.4996, test loss-1.8026, acc-0.4951\n",
      "Iter-61240, train loss-1.8036, acc-0.5400, valid loss-1.8017, acc-0.4994, test loss-1.8026, acc-0.4951\n",
      "Iter-61250, train loss-1.8292, acc-0.4800, valid loss-1.8016, acc-0.4994, test loss-1.8025, acc-0.4949\n",
      "Iter-61260, train loss-1.8941, acc-0.3200, valid loss-1.8016, acc-0.4992, test loss-1.8025, acc-0.4947\n",
      "Iter-61270, train loss-1.7528, acc-0.5000, valid loss-1.8015, acc-0.4990, test loss-1.8024, acc-0.4948\n",
      "Iter-61280, train loss-1.8714, acc-0.3600, valid loss-1.8015, acc-0.4992, test loss-1.8024, acc-0.4949\n",
      "Iter-61290, train loss-1.7937, acc-0.4800, valid loss-1.8014, acc-0.4990, test loss-1.8023, acc-0.4948\n",
      "Iter-61300, train loss-1.7343, acc-0.5200, valid loss-1.8014, acc-0.4992, test loss-1.8023, acc-0.4948\n",
      "Iter-61310, train loss-1.8579, acc-0.4800, valid loss-1.8013, acc-0.4994, test loss-1.8022, acc-0.4951\n",
      "Iter-61320, train loss-1.8302, acc-0.4000, valid loss-1.8013, acc-0.4994, test loss-1.8022, acc-0.4950\n",
      "Iter-61330, train loss-1.8279, acc-0.4400, valid loss-1.8012, acc-0.4994, test loss-1.8021, acc-0.4949\n",
      "Iter-61340, train loss-1.8851, acc-0.4400, valid loss-1.8012, acc-0.4992, test loss-1.8021, acc-0.4948\n",
      "Iter-61350, train loss-1.7613, acc-0.6000, valid loss-1.8011, acc-0.4992, test loss-1.8020, acc-0.4950\n",
      "Iter-61360, train loss-1.8813, acc-0.3800, valid loss-1.8011, acc-0.4992, test loss-1.8020, acc-0.4950\n",
      "Iter-61370, train loss-1.7233, acc-0.6600, valid loss-1.8010, acc-0.4992, test loss-1.8019, acc-0.4953\n",
      "Iter-61380, train loss-1.6633, acc-0.6800, valid loss-1.8010, acc-0.4992, test loss-1.8019, acc-0.4952\n",
      "Iter-61390, train loss-1.7450, acc-0.5400, valid loss-1.8009, acc-0.4992, test loss-1.8018, acc-0.4949\n",
      "Iter-61400, train loss-1.7534, acc-0.5800, valid loss-1.8009, acc-0.4992, test loss-1.8018, acc-0.4949\n",
      "Iter-61410, train loss-1.8448, acc-0.5200, valid loss-1.8008, acc-0.4992, test loss-1.8017, acc-0.4950\n",
      "Iter-61420, train loss-1.7778, acc-0.5200, valid loss-1.8008, acc-0.4992, test loss-1.8017, acc-0.4949\n",
      "Iter-61430, train loss-1.7136, acc-0.5000, valid loss-1.8007, acc-0.4992, test loss-1.8016, acc-0.4949\n",
      "Iter-61440, train loss-1.8543, acc-0.4000, valid loss-1.8007, acc-0.4990, test loss-1.8016, acc-0.4949\n",
      "Iter-61450, train loss-1.7317, acc-0.5200, valid loss-1.8006, acc-0.4990, test loss-1.8015, acc-0.4949\n",
      "Iter-61460, train loss-1.8002, acc-0.5000, valid loss-1.8006, acc-0.4994, test loss-1.8015, acc-0.4949\n",
      "Iter-61470, train loss-1.7858, acc-0.4800, valid loss-1.8005, acc-0.5000, test loss-1.8014, acc-0.4946\n",
      "Iter-61480, train loss-1.9002, acc-0.4000, valid loss-1.8005, acc-0.4998, test loss-1.8014, acc-0.4946\n",
      "Iter-61490, train loss-1.8628, acc-0.5800, valid loss-1.8004, acc-0.4996, test loss-1.8013, acc-0.4947\n",
      "Iter-61500, train loss-1.8693, acc-0.5600, valid loss-1.8004, acc-0.5000, test loss-1.8013, acc-0.4948\n",
      "Iter-61510, train loss-1.8045, acc-0.4800, valid loss-1.8003, acc-0.5002, test loss-1.8012, acc-0.4947\n",
      "Iter-61520, train loss-1.8426, acc-0.5000, valid loss-1.8003, acc-0.5002, test loss-1.8012, acc-0.4947\n",
      "Iter-61530, train loss-1.6947, acc-0.6000, valid loss-1.8002, acc-0.5000, test loss-1.8011, acc-0.4947\n",
      "Iter-61540, train loss-1.8804, acc-0.5000, valid loss-1.8002, acc-0.4998, test loss-1.8011, acc-0.4947\n",
      "Iter-61550, train loss-1.8025, acc-0.4800, valid loss-1.8001, acc-0.4998, test loss-1.8010, acc-0.4947\n",
      "Iter-61560, train loss-1.8290, acc-0.5200, valid loss-1.8001, acc-0.4998, test loss-1.8010, acc-0.4947\n",
      "Iter-61570, train loss-1.7974, acc-0.4400, valid loss-1.8000, acc-0.5000, test loss-1.8009, acc-0.4946\n",
      "Iter-61580, train loss-1.7912, acc-0.5200, valid loss-1.8000, acc-0.5004, test loss-1.8009, acc-0.4946\n",
      "Iter-61590, train loss-1.7806, acc-0.5200, valid loss-1.7999, acc-0.5004, test loss-1.8008, acc-0.4946\n",
      "Iter-61600, train loss-1.8636, acc-0.4200, valid loss-1.7999, acc-0.5004, test loss-1.8008, acc-0.4947\n",
      "Iter-61610, train loss-1.8516, acc-0.4400, valid loss-1.7998, acc-0.5002, test loss-1.8007, acc-0.4949\n",
      "Iter-61620, train loss-1.7695, acc-0.4800, valid loss-1.7998, acc-0.5002, test loss-1.8007, acc-0.4950\n",
      "Iter-61630, train loss-1.8505, acc-0.4400, valid loss-1.7997, acc-0.5004, test loss-1.8007, acc-0.4951\n",
      "Iter-61640, train loss-1.7892, acc-0.4200, valid loss-1.7997, acc-0.5004, test loss-1.8006, acc-0.4952\n",
      "Iter-61650, train loss-1.7358, acc-0.5200, valid loss-1.7996, acc-0.5004, test loss-1.8006, acc-0.4953\n",
      "Iter-61660, train loss-1.7034, acc-0.6600, valid loss-1.7996, acc-0.5006, test loss-1.8005, acc-0.4953\n",
      "Iter-61670, train loss-1.8974, acc-0.4200, valid loss-1.7995, acc-0.5004, test loss-1.8005, acc-0.4954\n",
      "Iter-61680, train loss-1.8057, acc-0.5600, valid loss-1.7995, acc-0.5006, test loss-1.8004, acc-0.4955\n",
      "Iter-61690, train loss-1.7866, acc-0.5000, valid loss-1.7994, acc-0.5006, test loss-1.8004, acc-0.4955\n",
      "Iter-61700, train loss-1.8503, acc-0.4200, valid loss-1.7994, acc-0.5006, test loss-1.8003, acc-0.4953\n",
      "Iter-61710, train loss-1.9013, acc-0.4000, valid loss-1.7994, acc-0.5006, test loss-1.8003, acc-0.4954\n",
      "Iter-61720, train loss-1.7941, acc-0.5200, valid loss-1.7993, acc-0.5008, test loss-1.8002, acc-0.4953\n",
      "Iter-61730, train loss-1.9169, acc-0.3400, valid loss-1.7993, acc-0.5008, test loss-1.8002, acc-0.4953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-61740, train loss-1.7298, acc-0.5200, valid loss-1.7992, acc-0.5008, test loss-1.8001, acc-0.4952\n",
      "Iter-61750, train loss-1.7655, acc-0.5400, valid loss-1.7992, acc-0.5010, test loss-1.8001, acc-0.4952\n",
      "Iter-61760, train loss-1.7717, acc-0.5200, valid loss-1.7991, acc-0.5010, test loss-1.8000, acc-0.4952\n",
      "Iter-61770, train loss-1.8310, acc-0.4400, valid loss-1.7991, acc-0.5010, test loss-1.8000, acc-0.4953\n",
      "Iter-61780, train loss-1.8022, acc-0.5600, valid loss-1.7990, acc-0.5010, test loss-1.7999, acc-0.4953\n",
      "Iter-61790, train loss-1.8723, acc-0.5000, valid loss-1.7990, acc-0.5010, test loss-1.7999, acc-0.4956\n",
      "Iter-61800, train loss-1.7844, acc-0.4800, valid loss-1.7989, acc-0.5010, test loss-1.7998, acc-0.4955\n",
      "Iter-61810, train loss-1.7525, acc-0.4400, valid loss-1.7989, acc-0.5010, test loss-1.7998, acc-0.4954\n",
      "Iter-61820, train loss-1.7788, acc-0.4400, valid loss-1.7988, acc-0.5010, test loss-1.7997, acc-0.4956\n",
      "Iter-61830, train loss-1.8369, acc-0.4600, valid loss-1.7988, acc-0.5010, test loss-1.7997, acc-0.4955\n",
      "Iter-61840, train loss-1.8909, acc-0.4400, valid loss-1.7987, acc-0.5010, test loss-1.7996, acc-0.4955\n",
      "Iter-61850, train loss-1.8470, acc-0.4600, valid loss-1.7987, acc-0.5010, test loss-1.7996, acc-0.4955\n",
      "Iter-61860, train loss-1.7604, acc-0.5200, valid loss-1.7986, acc-0.5010, test loss-1.7996, acc-0.4954\n",
      "Iter-61870, train loss-1.6950, acc-0.5200, valid loss-1.7986, acc-0.5010, test loss-1.7995, acc-0.4955\n",
      "Iter-61880, train loss-1.6863, acc-0.5800, valid loss-1.7985, acc-0.5008, test loss-1.7995, acc-0.4955\n",
      "Iter-61890, train loss-1.8623, acc-0.4800, valid loss-1.7985, acc-0.5008, test loss-1.7994, acc-0.4955\n",
      "Iter-61900, train loss-1.8353, acc-0.5600, valid loss-1.7985, acc-0.5008, test loss-1.7994, acc-0.4956\n",
      "Iter-61910, train loss-1.8665, acc-0.4400, valid loss-1.7984, acc-0.5012, test loss-1.7993, acc-0.4957\n",
      "Iter-61920, train loss-1.8248, acc-0.4400, valid loss-1.7984, acc-0.5012, test loss-1.7993, acc-0.4959\n",
      "Iter-61930, train loss-1.7978, acc-0.5800, valid loss-1.7983, acc-0.5012, test loss-1.7992, acc-0.4960\n",
      "Iter-61940, train loss-1.7328, acc-0.5200, valid loss-1.7983, acc-0.5012, test loss-1.7992, acc-0.4959\n",
      "Iter-61950, train loss-1.8964, acc-0.4200, valid loss-1.7982, acc-0.5014, test loss-1.7991, acc-0.4960\n",
      "Iter-61960, train loss-1.7948, acc-0.5600, valid loss-1.7982, acc-0.5012, test loss-1.7991, acc-0.4957\n",
      "Iter-61970, train loss-1.7676, acc-0.6400, valid loss-1.7981, acc-0.5012, test loss-1.7990, acc-0.4958\n",
      "Iter-61980, train loss-1.7386, acc-0.4200, valid loss-1.7981, acc-0.5012, test loss-1.7990, acc-0.4958\n",
      "Iter-61990, train loss-1.8846, acc-0.5000, valid loss-1.7980, acc-0.5012, test loss-1.7989, acc-0.4958\n",
      "Iter-62000, train loss-1.8057, acc-0.5000, valid loss-1.7980, acc-0.5012, test loss-1.7989, acc-0.4959\n",
      "Iter-62010, train loss-1.7821, acc-0.5200, valid loss-1.7979, acc-0.5012, test loss-1.7988, acc-0.4960\n",
      "Iter-62020, train loss-1.8302, acc-0.4800, valid loss-1.7979, acc-0.5012, test loss-1.7988, acc-0.4963\n",
      "Iter-62030, train loss-1.8761, acc-0.5200, valid loss-1.7978, acc-0.5016, test loss-1.7987, acc-0.4960\n",
      "Iter-62040, train loss-1.8204, acc-0.5200, valid loss-1.7978, acc-0.5016, test loss-1.7987, acc-0.4960\n",
      "Iter-62050, train loss-1.8788, acc-0.3600, valid loss-1.7977, acc-0.5016, test loss-1.7986, acc-0.4962\n",
      "Iter-62060, train loss-1.7750, acc-0.5000, valid loss-1.7977, acc-0.5016, test loss-1.7986, acc-0.4963\n",
      "Iter-62070, train loss-1.7741, acc-0.5000, valid loss-1.7976, acc-0.5016, test loss-1.7986, acc-0.4963\n",
      "Iter-62080, train loss-1.8448, acc-0.4600, valid loss-1.7976, acc-0.5014, test loss-1.7985, acc-0.4963\n",
      "Iter-62090, train loss-1.8373, acc-0.5200, valid loss-1.7976, acc-0.5014, test loss-1.7985, acc-0.4960\n",
      "Iter-62100, train loss-1.8041, acc-0.4800, valid loss-1.7975, acc-0.5014, test loss-1.7984, acc-0.4962\n",
      "Iter-62110, train loss-1.8959, acc-0.3800, valid loss-1.7975, acc-0.5012, test loss-1.7984, acc-0.4960\n",
      "Iter-62120, train loss-1.7512, acc-0.5000, valid loss-1.7974, acc-0.5014, test loss-1.7983, acc-0.4959\n",
      "Iter-62130, train loss-1.8409, acc-0.4600, valid loss-1.7974, acc-0.5016, test loss-1.7983, acc-0.4961\n",
      "Iter-62140, train loss-1.8517, acc-0.3800, valid loss-1.7973, acc-0.5016, test loss-1.7982, acc-0.4960\n",
      "Iter-62150, train loss-1.8252, acc-0.4800, valid loss-1.7973, acc-0.5022, test loss-1.7982, acc-0.4960\n",
      "Iter-62160, train loss-1.7983, acc-0.5400, valid loss-1.7972, acc-0.5018, test loss-1.7981, acc-0.4959\n",
      "Iter-62170, train loss-1.7486, acc-0.5200, valid loss-1.7972, acc-0.5018, test loss-1.7981, acc-0.4960\n",
      "Iter-62180, train loss-1.9020, acc-0.4800, valid loss-1.7971, acc-0.5016, test loss-1.7980, acc-0.4960\n",
      "Iter-62190, train loss-1.7118, acc-0.4400, valid loss-1.7971, acc-0.5018, test loss-1.7980, acc-0.4963\n",
      "Iter-62200, train loss-1.8081, acc-0.5000, valid loss-1.7970, acc-0.5016, test loss-1.7979, acc-0.4963\n",
      "Iter-62210, train loss-1.8415, acc-0.4400, valid loss-1.7970, acc-0.5014, test loss-1.7979, acc-0.4962\n",
      "Iter-62220, train loss-1.7320, acc-0.5200, valid loss-1.7969, acc-0.5018, test loss-1.7978, acc-0.4963\n",
      "Iter-62230, train loss-1.8494, acc-0.4200, valid loss-1.7969, acc-0.5020, test loss-1.7978, acc-0.4963\n",
      "Iter-62240, train loss-1.8603, acc-0.4800, valid loss-1.7968, acc-0.5016, test loss-1.7977, acc-0.4964\n",
      "Iter-62250, train loss-1.7841, acc-0.5200, valid loss-1.7968, acc-0.5016, test loss-1.7977, acc-0.4964\n",
      "Iter-62260, train loss-1.7642, acc-0.3800, valid loss-1.7967, acc-0.5016, test loss-1.7976, acc-0.4963\n",
      "Iter-62270, train loss-1.8518, acc-0.4800, valid loss-1.7967, acc-0.5016, test loss-1.7976, acc-0.4964\n",
      "Iter-62280, train loss-1.9136, acc-0.3200, valid loss-1.7966, acc-0.5018, test loss-1.7975, acc-0.4965\n",
      "Iter-62290, train loss-1.8144, acc-0.5600, valid loss-1.7966, acc-0.5018, test loss-1.7975, acc-0.4964\n",
      "Iter-62300, train loss-1.8480, acc-0.4200, valid loss-1.7965, acc-0.5020, test loss-1.7974, acc-0.4963\n",
      "Iter-62310, train loss-1.8692, acc-0.4000, valid loss-1.7965, acc-0.5020, test loss-1.7974, acc-0.4962\n",
      "Iter-62320, train loss-1.8777, acc-0.4400, valid loss-1.7964, acc-0.5016, test loss-1.7973, acc-0.4963\n",
      "Iter-62330, train loss-1.8231, acc-0.4400, valid loss-1.7964, acc-0.5016, test loss-1.7973, acc-0.4963\n",
      "Iter-62340, train loss-1.8093, acc-0.5400, valid loss-1.7964, acc-0.5016, test loss-1.7973, acc-0.4966\n",
      "Iter-62350, train loss-1.9246, acc-0.3800, valid loss-1.7963, acc-0.5018, test loss-1.7972, acc-0.4965\n",
      "Iter-62360, train loss-1.8544, acc-0.4200, valid loss-1.7963, acc-0.5018, test loss-1.7972, acc-0.4964\n",
      "Iter-62370, train loss-1.8545, acc-0.4600, valid loss-1.7962, acc-0.5020, test loss-1.7971, acc-0.4964\n",
      "Iter-62380, train loss-1.8252, acc-0.5000, valid loss-1.7962, acc-0.5022, test loss-1.7971, acc-0.4965\n",
      "Iter-62390, train loss-1.8380, acc-0.5000, valid loss-1.7961, acc-0.5020, test loss-1.7970, acc-0.4964\n",
      "Iter-62400, train loss-1.8287, acc-0.4800, valid loss-1.7961, acc-0.5018, test loss-1.7970, acc-0.4964\n",
      "Iter-62410, train loss-1.9543, acc-0.3600, valid loss-1.7960, acc-0.5020, test loss-1.7969, acc-0.4965\n",
      "Iter-62420, train loss-1.8270, acc-0.4400, valid loss-1.7960, acc-0.5022, test loss-1.7969, acc-0.4965\n",
      "Iter-62430, train loss-1.6487, acc-0.6800, valid loss-1.7959, acc-0.5022, test loss-1.7968, acc-0.4964\n",
      "Iter-62440, train loss-1.9055, acc-0.3400, valid loss-1.7959, acc-0.5022, test loss-1.7968, acc-0.4964\n",
      "Iter-62450, train loss-1.7783, acc-0.5400, valid loss-1.7958, acc-0.5022, test loss-1.7967, acc-0.4965\n",
      "Iter-62460, train loss-1.7878, acc-0.5400, valid loss-1.7958, acc-0.5022, test loss-1.7967, acc-0.4964\n",
      "Iter-62470, train loss-1.8221, acc-0.5600, valid loss-1.7957, acc-0.5022, test loss-1.7966, acc-0.4965\n",
      "Iter-62480, train loss-1.8589, acc-0.4000, valid loss-1.7957, acc-0.5020, test loss-1.7966, acc-0.4963\n",
      "Iter-62490, train loss-1.8097, acc-0.4600, valid loss-1.7956, acc-0.5020, test loss-1.7965, acc-0.4963\n",
      "Iter-62500, train loss-1.6886, acc-0.5400, valid loss-1.7956, acc-0.5020, test loss-1.7965, acc-0.4963\n",
      "Iter-62510, train loss-1.8388, acc-0.4200, valid loss-1.7955, acc-0.5020, test loss-1.7964, acc-0.4963\n",
      "Iter-62520, train loss-1.8450, acc-0.5400, valid loss-1.7955, acc-0.5022, test loss-1.7964, acc-0.4964\n",
      "Iter-62530, train loss-1.8230, acc-0.3800, valid loss-1.7954, acc-0.5020, test loss-1.7963, acc-0.4963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-62540, train loss-1.7904, acc-0.5200, valid loss-1.7954, acc-0.5022, test loss-1.7963, acc-0.4962\n",
      "Iter-62550, train loss-1.8005, acc-0.5400, valid loss-1.7953, acc-0.5018, test loss-1.7963, acc-0.4961\n",
      "Iter-62560, train loss-1.8124, acc-0.4800, valid loss-1.7953, acc-0.5020, test loss-1.7962, acc-0.4961\n",
      "Iter-62570, train loss-1.7872, acc-0.4600, valid loss-1.7952, acc-0.5022, test loss-1.7962, acc-0.4965\n",
      "Iter-62580, train loss-1.8004, acc-0.4400, valid loss-1.7952, acc-0.5024, test loss-1.7961, acc-0.4965\n",
      "Iter-62590, train loss-1.8005, acc-0.4400, valid loss-1.7951, acc-0.5022, test loss-1.7961, acc-0.4965\n",
      "Iter-62600, train loss-1.7696, acc-0.5800, valid loss-1.7951, acc-0.5024, test loss-1.7960, acc-0.4965\n",
      "Iter-62610, train loss-1.8626, acc-0.4600, valid loss-1.7950, acc-0.5030, test loss-1.7960, acc-0.4964\n",
      "Iter-62620, train loss-1.7198, acc-0.5600, valid loss-1.7950, acc-0.5026, test loss-1.7959, acc-0.4965\n",
      "Iter-62630, train loss-1.8081, acc-0.5200, valid loss-1.7949, acc-0.5024, test loss-1.7959, acc-0.4964\n",
      "Iter-62640, train loss-1.8828, acc-0.4000, valid loss-1.7949, acc-0.5024, test loss-1.7958, acc-0.4966\n",
      "Iter-62650, train loss-1.7837, acc-0.4600, valid loss-1.7948, acc-0.5022, test loss-1.7958, acc-0.4964\n",
      "Iter-62660, train loss-1.7721, acc-0.4800, valid loss-1.7948, acc-0.5024, test loss-1.7957, acc-0.4965\n",
      "Iter-62670, train loss-1.9293, acc-0.4800, valid loss-1.7948, acc-0.5026, test loss-1.7957, acc-0.4965\n",
      "Iter-62680, train loss-1.8501, acc-0.4600, valid loss-1.7947, acc-0.5026, test loss-1.7956, acc-0.4967\n",
      "Iter-62690, train loss-1.8329, acc-0.4800, valid loss-1.7947, acc-0.5028, test loss-1.7956, acc-0.4965\n",
      "Iter-62700, train loss-1.7768, acc-0.5400, valid loss-1.7946, acc-0.5026, test loss-1.7955, acc-0.4967\n",
      "Iter-62710, train loss-1.8872, acc-0.4200, valid loss-1.7946, acc-0.5028, test loss-1.7955, acc-0.4968\n",
      "Iter-62720, train loss-1.9108, acc-0.4000, valid loss-1.7945, acc-0.5030, test loss-1.7954, acc-0.4968\n",
      "Iter-62730, train loss-1.7973, acc-0.5200, valid loss-1.7945, acc-0.5032, test loss-1.7954, acc-0.4968\n",
      "Iter-62740, train loss-1.7988, acc-0.5400, valid loss-1.7944, acc-0.5032, test loss-1.7953, acc-0.4968\n",
      "Iter-62750, train loss-1.7663, acc-0.5000, valid loss-1.7944, acc-0.5030, test loss-1.7953, acc-0.4968\n",
      "Iter-62760, train loss-1.8175, acc-0.5600, valid loss-1.7943, acc-0.5030, test loss-1.7952, acc-0.4968\n",
      "Iter-62770, train loss-1.6774, acc-0.5800, valid loss-1.7943, acc-0.5028, test loss-1.7952, acc-0.4968\n",
      "Iter-62780, train loss-1.7355, acc-0.5600, valid loss-1.7942, acc-0.5030, test loss-1.7952, acc-0.4969\n",
      "Iter-62790, train loss-1.8210, acc-0.5200, valid loss-1.7942, acc-0.5028, test loss-1.7951, acc-0.4968\n",
      "Iter-62800, train loss-1.8367, acc-0.4800, valid loss-1.7941, acc-0.5030, test loss-1.7951, acc-0.4967\n",
      "Iter-62810, train loss-1.8912, acc-0.3800, valid loss-1.7941, acc-0.5030, test loss-1.7950, acc-0.4968\n",
      "Iter-62820, train loss-1.7537, acc-0.5800, valid loss-1.7940, acc-0.5030, test loss-1.7950, acc-0.4967\n",
      "Iter-62830, train loss-1.7581, acc-0.5000, valid loss-1.7940, acc-0.5028, test loss-1.7949, acc-0.4967\n",
      "Iter-62840, train loss-1.9693, acc-0.3600, valid loss-1.7940, acc-0.5030, test loss-1.7949, acc-0.4968\n",
      "Iter-62850, train loss-1.7715, acc-0.5400, valid loss-1.7939, acc-0.5032, test loss-1.7948, acc-0.4969\n",
      "Iter-62860, train loss-1.7814, acc-0.5000, valid loss-1.7939, acc-0.5034, test loss-1.7948, acc-0.4969\n",
      "Iter-62870, train loss-1.6635, acc-0.6000, valid loss-1.7938, acc-0.5032, test loss-1.7947, acc-0.4969\n",
      "Iter-62880, train loss-1.8293, acc-0.5800, valid loss-1.7938, acc-0.5034, test loss-1.7947, acc-0.4968\n",
      "Iter-62890, train loss-1.8352, acc-0.4200, valid loss-1.7937, acc-0.5034, test loss-1.7946, acc-0.4967\n",
      "Iter-62900, train loss-1.7910, acc-0.4800, valid loss-1.7937, acc-0.5034, test loss-1.7946, acc-0.4969\n",
      "Iter-62910, train loss-1.8448, acc-0.4600, valid loss-1.7936, acc-0.5034, test loss-1.7945, acc-0.4968\n",
      "Iter-62920, train loss-1.7712, acc-0.4200, valid loss-1.7936, acc-0.5032, test loss-1.7945, acc-0.4968\n",
      "Iter-62930, train loss-1.8556, acc-0.4600, valid loss-1.7935, acc-0.5032, test loss-1.7944, acc-0.4969\n",
      "Iter-62940, train loss-1.8439, acc-0.4600, valid loss-1.7935, acc-0.5032, test loss-1.7944, acc-0.4970\n",
      "Iter-62950, train loss-1.7904, acc-0.5400, valid loss-1.7934, acc-0.5030, test loss-1.7943, acc-0.4970\n",
      "Iter-62960, train loss-1.7610, acc-0.4400, valid loss-1.7934, acc-0.5030, test loss-1.7943, acc-0.4971\n",
      "Iter-62970, train loss-1.7964, acc-0.4800, valid loss-1.7933, acc-0.5032, test loss-1.7942, acc-0.4970\n",
      "Iter-62980, train loss-1.7541, acc-0.5800, valid loss-1.7933, acc-0.5030, test loss-1.7942, acc-0.4972\n",
      "Iter-62990, train loss-1.7742, acc-0.5000, valid loss-1.7932, acc-0.5030, test loss-1.7941, acc-0.4969\n",
      "Iter-63000, train loss-1.7735, acc-0.5600, valid loss-1.7932, acc-0.5032, test loss-1.7941, acc-0.4967\n",
      "Iter-63010, train loss-1.7554, acc-0.5400, valid loss-1.7931, acc-0.5032, test loss-1.7940, acc-0.4969\n",
      "Iter-63020, train loss-1.7125, acc-0.5200, valid loss-1.7931, acc-0.5032, test loss-1.7940, acc-0.4970\n",
      "Iter-63030, train loss-1.6883, acc-0.5600, valid loss-1.7931, acc-0.5032, test loss-1.7939, acc-0.4972\n",
      "Iter-63040, train loss-1.6571, acc-0.6600, valid loss-1.7930, acc-0.5030, test loss-1.7939, acc-0.4972\n",
      "Iter-63050, train loss-1.8039, acc-0.4800, valid loss-1.7930, acc-0.5030, test loss-1.7939, acc-0.4973\n",
      "Iter-63060, train loss-1.8175, acc-0.5600, valid loss-1.7929, acc-0.5030, test loss-1.7938, acc-0.4974\n",
      "Iter-63070, train loss-1.8599, acc-0.3800, valid loss-1.7929, acc-0.5032, test loss-1.7938, acc-0.4975\n",
      "Iter-63080, train loss-1.9520, acc-0.4000, valid loss-1.7928, acc-0.5034, test loss-1.7937, acc-0.4976\n",
      "Iter-63090, train loss-1.7598, acc-0.5400, valid loss-1.7928, acc-0.5034, test loss-1.7937, acc-0.4977\n",
      "Iter-63100, train loss-1.8393, acc-0.4800, valid loss-1.7927, acc-0.5038, test loss-1.7936, acc-0.4976\n",
      "Iter-63110, train loss-1.8207, acc-0.5200, valid loss-1.7927, acc-0.5040, test loss-1.7936, acc-0.4976\n",
      "Iter-63120, train loss-1.8809, acc-0.4200, valid loss-1.7926, acc-0.5038, test loss-1.7935, acc-0.4978\n",
      "Iter-63130, train loss-1.9126, acc-0.3400, valid loss-1.7926, acc-0.5038, test loss-1.7935, acc-0.4976\n",
      "Iter-63140, train loss-1.8049, acc-0.4600, valid loss-1.7925, acc-0.5038, test loss-1.7934, acc-0.4976\n",
      "Iter-63150, train loss-1.7048, acc-0.5200, valid loss-1.7925, acc-0.5038, test loss-1.7934, acc-0.4975\n",
      "Iter-63160, train loss-1.7224, acc-0.6600, valid loss-1.7924, acc-0.5038, test loss-1.7933, acc-0.4976\n",
      "Iter-63170, train loss-1.8037, acc-0.5000, valid loss-1.7924, acc-0.5040, test loss-1.7933, acc-0.4976\n",
      "Iter-63180, train loss-1.9315, acc-0.4000, valid loss-1.7923, acc-0.5038, test loss-1.7932, acc-0.4976\n",
      "Iter-63190, train loss-1.7690, acc-0.4800, valid loss-1.7923, acc-0.5040, test loss-1.7932, acc-0.4977\n",
      "Iter-63200, train loss-1.8409, acc-0.4800, valid loss-1.7922, acc-0.5040, test loss-1.7931, acc-0.4977\n",
      "Iter-63210, train loss-1.8969, acc-0.4800, valid loss-1.7922, acc-0.5042, test loss-1.7931, acc-0.4976\n",
      "Iter-63220, train loss-1.7450, acc-0.5600, valid loss-1.7922, acc-0.5042, test loss-1.7931, acc-0.4975\n",
      "Iter-63230, train loss-1.8009, acc-0.5400, valid loss-1.7921, acc-0.5042, test loss-1.7930, acc-0.4975\n",
      "Iter-63240, train loss-1.7858, acc-0.4800, valid loss-1.7921, acc-0.5042, test loss-1.7930, acc-0.4976\n",
      "Iter-63250, train loss-1.8617, acc-0.4400, valid loss-1.7920, acc-0.5044, test loss-1.7929, acc-0.4976\n",
      "Iter-63260, train loss-1.8012, acc-0.4800, valid loss-1.7920, acc-0.5044, test loss-1.7929, acc-0.4975\n",
      "Iter-63270, train loss-1.6852, acc-0.6000, valid loss-1.7919, acc-0.5042, test loss-1.7928, acc-0.4975\n",
      "Iter-63280, train loss-1.9196, acc-0.3200, valid loss-1.7919, acc-0.5042, test loss-1.7928, acc-0.4977\n",
      "Iter-63290, train loss-1.7948, acc-0.5400, valid loss-1.7918, acc-0.5042, test loss-1.7927, acc-0.4974\n",
      "Iter-63300, train loss-1.8330, acc-0.4600, valid loss-1.7918, acc-0.5042, test loss-1.7927, acc-0.4978\n",
      "Iter-63310, train loss-1.7730, acc-0.4600, valid loss-1.7917, acc-0.5042, test loss-1.7926, acc-0.4980\n",
      "Iter-63320, train loss-1.7421, acc-0.5600, valid loss-1.7917, acc-0.5042, test loss-1.7926, acc-0.4979\n",
      "Iter-63330, train loss-1.7634, acc-0.5800, valid loss-1.7916, acc-0.5042, test loss-1.7925, acc-0.4981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-63340, train loss-1.8305, acc-0.4600, valid loss-1.7916, acc-0.5042, test loss-1.7925, acc-0.4980\n",
      "Iter-63350, train loss-1.7498, acc-0.5400, valid loss-1.7915, acc-0.5040, test loss-1.7924, acc-0.4983\n",
      "Iter-63360, train loss-1.7737, acc-0.4400, valid loss-1.7915, acc-0.5040, test loss-1.7924, acc-0.4984\n",
      "Iter-63370, train loss-1.8283, acc-0.3800, valid loss-1.7914, acc-0.5040, test loss-1.7923, acc-0.4983\n",
      "Iter-63380, train loss-1.8491, acc-0.5000, valid loss-1.7914, acc-0.5040, test loss-1.7923, acc-0.4984\n",
      "Iter-63390, train loss-1.8022, acc-0.5000, valid loss-1.7913, acc-0.5040, test loss-1.7923, acc-0.4983\n",
      "Iter-63400, train loss-1.8335, acc-0.4400, valid loss-1.7913, acc-0.5040, test loss-1.7922, acc-0.4983\n",
      "Iter-63410, train loss-1.7938, acc-0.5200, valid loss-1.7912, acc-0.5040, test loss-1.7922, acc-0.4982\n",
      "Iter-63420, train loss-1.8061, acc-0.5400, valid loss-1.7912, acc-0.5042, test loss-1.7921, acc-0.4982\n",
      "Iter-63430, train loss-1.7742, acc-0.6000, valid loss-1.7912, acc-0.5042, test loss-1.7921, acc-0.4981\n",
      "Iter-63440, train loss-1.7115, acc-0.4600, valid loss-1.7911, acc-0.5042, test loss-1.7920, acc-0.4980\n",
      "Iter-63450, train loss-1.8037, acc-0.4800, valid loss-1.7911, acc-0.5042, test loss-1.7920, acc-0.4980\n",
      "Iter-63460, train loss-1.7672, acc-0.5600, valid loss-1.7910, acc-0.5042, test loss-1.7919, acc-0.4980\n",
      "Iter-63470, train loss-1.8218, acc-0.4400, valid loss-1.7910, acc-0.5040, test loss-1.7919, acc-0.4980\n",
      "Iter-63480, train loss-1.8427, acc-0.4200, valid loss-1.7909, acc-0.5042, test loss-1.7918, acc-0.4980\n",
      "Iter-63490, train loss-1.7682, acc-0.5200, valid loss-1.7909, acc-0.5042, test loss-1.7918, acc-0.4979\n",
      "Iter-63500, train loss-1.8567, acc-0.4400, valid loss-1.7908, acc-0.5044, test loss-1.7917, acc-0.4980\n",
      "Iter-63510, train loss-1.8389, acc-0.4000, valid loss-1.7908, acc-0.5042, test loss-1.7917, acc-0.4981\n",
      "Iter-63520, train loss-1.8685, acc-0.3800, valid loss-1.7907, acc-0.5042, test loss-1.7917, acc-0.4980\n",
      "Iter-63530, train loss-1.7762, acc-0.5000, valid loss-1.7907, acc-0.5044, test loss-1.7916, acc-0.4981\n",
      "Iter-63540, train loss-1.8586, acc-0.4600, valid loss-1.7906, acc-0.5046, test loss-1.7916, acc-0.4979\n",
      "Iter-63550, train loss-1.8922, acc-0.5000, valid loss-1.7906, acc-0.5044, test loss-1.7915, acc-0.4981\n",
      "Iter-63560, train loss-1.8257, acc-0.5000, valid loss-1.7905, acc-0.5042, test loss-1.7915, acc-0.4981\n",
      "Iter-63570, train loss-1.8539, acc-0.5200, valid loss-1.7905, acc-0.5044, test loss-1.7914, acc-0.4983\n",
      "Iter-63580, train loss-1.7174, acc-0.6400, valid loss-1.7905, acc-0.5042, test loss-1.7914, acc-0.4983\n",
      "Iter-63590, train loss-1.7451, acc-0.6200, valid loss-1.7904, acc-0.5046, test loss-1.7913, acc-0.4984\n",
      "Iter-63600, train loss-1.7711, acc-0.5800, valid loss-1.7904, acc-0.5046, test loss-1.7913, acc-0.4984\n",
      "Iter-63610, train loss-1.7963, acc-0.4800, valid loss-1.7903, acc-0.5046, test loss-1.7912, acc-0.4984\n",
      "Iter-63620, train loss-1.7491, acc-0.6000, valid loss-1.7903, acc-0.5042, test loss-1.7912, acc-0.4983\n",
      "Iter-63630, train loss-1.8378, acc-0.4000, valid loss-1.7902, acc-0.5042, test loss-1.7911, acc-0.4984\n",
      "Iter-63640, train loss-1.7989, acc-0.5000, valid loss-1.7902, acc-0.5040, test loss-1.7911, acc-0.4985\n",
      "Iter-63650, train loss-1.7983, acc-0.4400, valid loss-1.7901, acc-0.5040, test loss-1.7910, acc-0.4982\n",
      "Iter-63660, train loss-1.8694, acc-0.4600, valid loss-1.7901, acc-0.5040, test loss-1.7910, acc-0.4981\n",
      "Iter-63670, train loss-1.8508, acc-0.4800, valid loss-1.7900, acc-0.5040, test loss-1.7910, acc-0.4982\n",
      "Iter-63680, train loss-1.7602, acc-0.5800, valid loss-1.7900, acc-0.5040, test loss-1.7909, acc-0.4981\n",
      "Iter-63690, train loss-1.8085, acc-0.4400, valid loss-1.7900, acc-0.5040, test loss-1.7909, acc-0.4981\n",
      "Iter-63700, train loss-1.7917, acc-0.5000, valid loss-1.7899, acc-0.5044, test loss-1.7908, acc-0.4982\n",
      "Iter-63710, train loss-1.7896, acc-0.5000, valid loss-1.7899, acc-0.5042, test loss-1.7908, acc-0.4983\n",
      "Iter-63720, train loss-1.7289, acc-0.4600, valid loss-1.7898, acc-0.5044, test loss-1.7907, acc-0.4983\n",
      "Iter-63730, train loss-1.7317, acc-0.5200, valid loss-1.7898, acc-0.5040, test loss-1.7907, acc-0.4983\n",
      "Iter-63740, train loss-1.8325, acc-0.4600, valid loss-1.7897, acc-0.5044, test loss-1.7906, acc-0.4985\n",
      "Iter-63750, train loss-1.8345, acc-0.4200, valid loss-1.7897, acc-0.5042, test loss-1.7906, acc-0.4984\n",
      "Iter-63760, train loss-1.7801, acc-0.4600, valid loss-1.7896, acc-0.5042, test loss-1.7905, acc-0.4984\n",
      "Iter-63770, train loss-1.8069, acc-0.5000, valid loss-1.7896, acc-0.5044, test loss-1.7905, acc-0.4984\n",
      "Iter-63780, train loss-1.6966, acc-0.5400, valid loss-1.7895, acc-0.5044, test loss-1.7904, acc-0.4985\n",
      "Iter-63790, train loss-1.7374, acc-0.5200, valid loss-1.7895, acc-0.5046, test loss-1.7904, acc-0.4986\n",
      "Iter-63800, train loss-1.8407, acc-0.4800, valid loss-1.7894, acc-0.5046, test loss-1.7903, acc-0.4985\n",
      "Iter-63810, train loss-1.8080, acc-0.5000, valid loss-1.7894, acc-0.5046, test loss-1.7903, acc-0.4988\n",
      "Iter-63820, train loss-1.8023, acc-0.4400, valid loss-1.7893, acc-0.5046, test loss-1.7902, acc-0.4987\n",
      "Iter-63830, train loss-1.8655, acc-0.4600, valid loss-1.7893, acc-0.5046, test loss-1.7902, acc-0.4987\n",
      "Iter-63840, train loss-1.7483, acc-0.5800, valid loss-1.7893, acc-0.5048, test loss-1.7901, acc-0.4985\n",
      "Iter-63850, train loss-1.8931, acc-0.3800, valid loss-1.7892, acc-0.5048, test loss-1.7901, acc-0.4987\n",
      "Iter-63860, train loss-1.7588, acc-0.5400, valid loss-1.7892, acc-0.5048, test loss-1.7901, acc-0.4986\n",
      "Iter-63870, train loss-1.8250, acc-0.4800, valid loss-1.7891, acc-0.5048, test loss-1.7900, acc-0.4983\n",
      "Iter-63880, train loss-1.7660, acc-0.5000, valid loss-1.7891, acc-0.5046, test loss-1.7900, acc-0.4981\n",
      "Iter-63890, train loss-1.8367, acc-0.4600, valid loss-1.7890, acc-0.5046, test loss-1.7899, acc-0.4985\n",
      "Iter-63900, train loss-1.7419, acc-0.5600, valid loss-1.7890, acc-0.5044, test loss-1.7899, acc-0.4984\n",
      "Iter-63910, train loss-1.7507, acc-0.5600, valid loss-1.7889, acc-0.5044, test loss-1.7898, acc-0.4986\n",
      "Iter-63920, train loss-1.8257, acc-0.4400, valid loss-1.7889, acc-0.5044, test loss-1.7898, acc-0.4985\n",
      "Iter-63930, train loss-1.7342, acc-0.5200, valid loss-1.7889, acc-0.5044, test loss-1.7897, acc-0.4985\n",
      "Iter-63940, train loss-1.9309, acc-0.4000, valid loss-1.7888, acc-0.5046, test loss-1.7897, acc-0.4984\n",
      "Iter-63950, train loss-1.9297, acc-0.3800, valid loss-1.7888, acc-0.5046, test loss-1.7896, acc-0.4985\n",
      "Iter-63960, train loss-1.7574, acc-0.4800, valid loss-1.7887, acc-0.5046, test loss-1.7896, acc-0.4986\n",
      "Iter-63970, train loss-1.8631, acc-0.4000, valid loss-1.7887, acc-0.5046, test loss-1.7895, acc-0.4985\n",
      "Iter-63980, train loss-1.8580, acc-0.4000, valid loss-1.7886, acc-0.5046, test loss-1.7895, acc-0.4984\n",
      "Iter-63990, train loss-1.8851, acc-0.3600, valid loss-1.7886, acc-0.5046, test loss-1.7895, acc-0.4985\n",
      "Iter-64000, train loss-1.8029, acc-0.6000, valid loss-1.7885, acc-0.5046, test loss-1.7894, acc-0.4984\n",
      "Iter-64010, train loss-1.9035, acc-0.4600, valid loss-1.7885, acc-0.5046, test loss-1.7894, acc-0.4984\n",
      "Iter-64020, train loss-1.8071, acc-0.5200, valid loss-1.7884, acc-0.5044, test loss-1.7893, acc-0.4985\n",
      "Iter-64030, train loss-1.7146, acc-0.5800, valid loss-1.7884, acc-0.5046, test loss-1.7893, acc-0.4985\n",
      "Iter-64040, train loss-1.8435, acc-0.5400, valid loss-1.7884, acc-0.5046, test loss-1.7892, acc-0.4986\n",
      "Iter-64050, train loss-1.7218, acc-0.5000, valid loss-1.7883, acc-0.5046, test loss-1.7892, acc-0.4987\n",
      "Iter-64060, train loss-1.6887, acc-0.5600, valid loss-1.7883, acc-0.5048, test loss-1.7891, acc-0.4987\n",
      "Iter-64070, train loss-1.7779, acc-0.4600, valid loss-1.7882, acc-0.5048, test loss-1.7891, acc-0.4987\n",
      "Iter-64080, train loss-1.8802, acc-0.4400, valid loss-1.7882, acc-0.5048, test loss-1.7890, acc-0.4986\n",
      "Iter-64090, train loss-1.7492, acc-0.5200, valid loss-1.7881, acc-0.5048, test loss-1.7890, acc-0.4986\n",
      "Iter-64100, train loss-1.9264, acc-0.3400, valid loss-1.7881, acc-0.5046, test loss-1.7889, acc-0.4987\n",
      "Iter-64110, train loss-1.7288, acc-0.5400, valid loss-1.7880, acc-0.5046, test loss-1.7889, acc-0.4985\n",
      "Iter-64120, train loss-1.8235, acc-0.5200, valid loss-1.7880, acc-0.5046, test loss-1.7888, acc-0.4988\n",
      "Iter-64130, train loss-1.8386, acc-0.4800, valid loss-1.7879, acc-0.5046, test loss-1.7888, acc-0.4989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-64140, train loss-1.8146, acc-0.5600, valid loss-1.7879, acc-0.5044, test loss-1.7887, acc-0.4989\n",
      "Iter-64150, train loss-1.8691, acc-0.4200, valid loss-1.7878, acc-0.5044, test loss-1.7887, acc-0.4989\n",
      "Iter-64160, train loss-1.7857, acc-0.5400, valid loss-1.7878, acc-0.5048, test loss-1.7887, acc-0.4991\n",
      "Iter-64170, train loss-1.6930, acc-0.5600, valid loss-1.7877, acc-0.5046, test loss-1.7886, acc-0.4989\n",
      "Iter-64180, train loss-1.8683, acc-0.3400, valid loss-1.7877, acc-0.5048, test loss-1.7886, acc-0.4989\n",
      "Iter-64190, train loss-1.7537, acc-0.4800, valid loss-1.7876, acc-0.5048, test loss-1.7885, acc-0.4989\n",
      "Iter-64200, train loss-1.8284, acc-0.5200, valid loss-1.7876, acc-0.5048, test loss-1.7885, acc-0.4989\n",
      "Iter-64210, train loss-1.8188, acc-0.5400, valid loss-1.7876, acc-0.5048, test loss-1.7884, acc-0.4989\n",
      "Iter-64220, train loss-1.7780, acc-0.5000, valid loss-1.7875, acc-0.5044, test loss-1.7884, acc-0.4987\n",
      "Iter-64230, train loss-1.8297, acc-0.4400, valid loss-1.7875, acc-0.5046, test loss-1.7883, acc-0.4988\n",
      "Iter-64240, train loss-1.8217, acc-0.4400, valid loss-1.7874, acc-0.5046, test loss-1.7883, acc-0.4987\n",
      "Iter-64250, train loss-1.7664, acc-0.5600, valid loss-1.7874, acc-0.5046, test loss-1.7882, acc-0.4987\n",
      "Iter-64260, train loss-1.7686, acc-0.5400, valid loss-1.7873, acc-0.5046, test loss-1.7882, acc-0.4990\n",
      "Iter-64270, train loss-1.8186, acc-0.5600, valid loss-1.7873, acc-0.5046, test loss-1.7881, acc-0.4988\n",
      "Iter-64280, train loss-1.6710, acc-0.6400, valid loss-1.7872, acc-0.5046, test loss-1.7881, acc-0.4988\n",
      "Iter-64290, train loss-1.8629, acc-0.4600, valid loss-1.7872, acc-0.5046, test loss-1.7881, acc-0.4988\n",
      "Iter-64300, train loss-1.7423, acc-0.5000, valid loss-1.7871, acc-0.5046, test loss-1.7880, acc-0.4989\n",
      "Iter-64310, train loss-1.8817, acc-0.4200, valid loss-1.7871, acc-0.5046, test loss-1.7880, acc-0.4991\n",
      "Iter-64320, train loss-1.7624, acc-0.4800, valid loss-1.7870, acc-0.5046, test loss-1.7879, acc-0.4988\n",
      "Iter-64330, train loss-1.8554, acc-0.4400, valid loss-1.7870, acc-0.5048, test loss-1.7879, acc-0.4990\n",
      "Iter-64340, train loss-1.7797, acc-0.4800, valid loss-1.7870, acc-0.5050, test loss-1.7878, acc-0.4992\n",
      "Iter-64350, train loss-1.7959, acc-0.5000, valid loss-1.7869, acc-0.5052, test loss-1.7878, acc-0.4992\n",
      "Iter-64360, train loss-1.9275, acc-0.3800, valid loss-1.7869, acc-0.5052, test loss-1.7877, acc-0.4990\n",
      "Iter-64370, train loss-1.7101, acc-0.5600, valid loss-1.7868, acc-0.5052, test loss-1.7877, acc-0.4990\n",
      "Iter-64380, train loss-1.6939, acc-0.6400, valid loss-1.7868, acc-0.5052, test loss-1.7876, acc-0.4991\n",
      "Iter-64390, train loss-1.8422, acc-0.4600, valid loss-1.7867, acc-0.5050, test loss-1.7876, acc-0.4990\n",
      "Iter-64400, train loss-1.7264, acc-0.4800, valid loss-1.7867, acc-0.5054, test loss-1.7875, acc-0.4991\n",
      "Iter-64410, train loss-1.7963, acc-0.4400, valid loss-1.7866, acc-0.5052, test loss-1.7875, acc-0.4989\n",
      "Iter-64420, train loss-1.8030, acc-0.4800, valid loss-1.7866, acc-0.5054, test loss-1.7875, acc-0.4992\n",
      "Iter-64430, train loss-1.7913, acc-0.4600, valid loss-1.7865, acc-0.5056, test loss-1.7874, acc-0.4991\n",
      "Iter-64440, train loss-1.8720, acc-0.4200, valid loss-1.7865, acc-0.5056, test loss-1.7874, acc-0.4991\n",
      "Iter-64450, train loss-1.8362, acc-0.4400, valid loss-1.7865, acc-0.5056, test loss-1.7873, acc-0.4991\n",
      "Iter-64460, train loss-1.7722, acc-0.5400, valid loss-1.7864, acc-0.5056, test loss-1.7873, acc-0.4990\n",
      "Iter-64470, train loss-1.8147, acc-0.5400, valid loss-1.7864, acc-0.5054, test loss-1.7872, acc-0.4991\n",
      "Iter-64480, train loss-1.8209, acc-0.5000, valid loss-1.7863, acc-0.5052, test loss-1.7872, acc-0.4990\n",
      "Iter-64490, train loss-1.7855, acc-0.5000, valid loss-1.7863, acc-0.5052, test loss-1.7871, acc-0.4990\n",
      "Iter-64500, train loss-1.7506, acc-0.6200, valid loss-1.7862, acc-0.5050, test loss-1.7871, acc-0.4991\n",
      "Iter-64510, train loss-1.7645, acc-0.6000, valid loss-1.7862, acc-0.5050, test loss-1.7870, acc-0.4990\n",
      "Iter-64520, train loss-1.8068, acc-0.4400, valid loss-1.7861, acc-0.5052, test loss-1.7870, acc-0.4989\n",
      "Iter-64530, train loss-1.8008, acc-0.4800, valid loss-1.7861, acc-0.5050, test loss-1.7869, acc-0.4990\n",
      "Iter-64540, train loss-1.7325, acc-0.6200, valid loss-1.7860, acc-0.5050, test loss-1.7869, acc-0.4991\n",
      "Iter-64550, train loss-1.8365, acc-0.4200, valid loss-1.7860, acc-0.5052, test loss-1.7869, acc-0.4990\n",
      "Iter-64560, train loss-1.7703, acc-0.4800, valid loss-1.7859, acc-0.5050, test loss-1.7868, acc-0.4990\n",
      "Iter-64570, train loss-1.8288, acc-0.5000, valid loss-1.7859, acc-0.5050, test loss-1.7868, acc-0.4991\n",
      "Iter-64580, train loss-1.7377, acc-0.5400, valid loss-1.7858, acc-0.5050, test loss-1.7867, acc-0.4992\n",
      "Iter-64590, train loss-1.7004, acc-0.6000, valid loss-1.7858, acc-0.5050, test loss-1.7867, acc-0.4990\n",
      "Iter-64600, train loss-1.7884, acc-0.4200, valid loss-1.7858, acc-0.5050, test loss-1.7866, acc-0.4991\n",
      "Iter-64610, train loss-1.8060, acc-0.5000, valid loss-1.7857, acc-0.5050, test loss-1.7866, acc-0.4991\n",
      "Iter-64620, train loss-1.7002, acc-0.4800, valid loss-1.7857, acc-0.5050, test loss-1.7865, acc-0.4992\n",
      "Iter-64630, train loss-1.8582, acc-0.4800, valid loss-1.7856, acc-0.5050, test loss-1.7865, acc-0.4991\n",
      "Iter-64640, train loss-1.6928, acc-0.6600, valid loss-1.7856, acc-0.5048, test loss-1.7864, acc-0.4991\n",
      "Iter-64650, train loss-1.7823, acc-0.5200, valid loss-1.7855, acc-0.5046, test loss-1.7864, acc-0.4992\n",
      "Iter-64660, train loss-1.8074, acc-0.5200, valid loss-1.7855, acc-0.5048, test loss-1.7863, acc-0.4992\n",
      "Iter-64670, train loss-1.7315, acc-0.6400, valid loss-1.7854, acc-0.5048, test loss-1.7863, acc-0.4992\n",
      "Iter-64680, train loss-1.8446, acc-0.4800, valid loss-1.7854, acc-0.5048, test loss-1.7862, acc-0.4992\n",
      "Iter-64690, train loss-1.7660, acc-0.6000, valid loss-1.7853, acc-0.5048, test loss-1.7862, acc-0.4992\n",
      "Iter-64700, train loss-1.8238, acc-0.4800, valid loss-1.7853, acc-0.5050, test loss-1.7862, acc-0.4992\n",
      "Iter-64710, train loss-1.8354, acc-0.4200, valid loss-1.7853, acc-0.5050, test loss-1.7861, acc-0.4992\n",
      "Iter-64720, train loss-1.8080, acc-0.5000, valid loss-1.7852, acc-0.5050, test loss-1.7861, acc-0.4990\n",
      "Iter-64730, train loss-1.7207, acc-0.5800, valid loss-1.7852, acc-0.5050, test loss-1.7860, acc-0.4989\n",
      "Iter-64740, train loss-1.9128, acc-0.4200, valid loss-1.7851, acc-0.5056, test loss-1.7860, acc-0.4989\n",
      "Iter-64750, train loss-1.7947, acc-0.4800, valid loss-1.7851, acc-0.5054, test loss-1.7859, acc-0.4989\n",
      "Iter-64760, train loss-1.7906, acc-0.5000, valid loss-1.7850, acc-0.5060, test loss-1.7859, acc-0.4991\n",
      "Iter-64770, train loss-1.8344, acc-0.4400, valid loss-1.7850, acc-0.5058, test loss-1.7858, acc-0.4992\n",
      "Iter-64780, train loss-1.7024, acc-0.4200, valid loss-1.7849, acc-0.5058, test loss-1.7858, acc-0.4992\n",
      "Iter-64790, train loss-1.7038, acc-0.5200, valid loss-1.7849, acc-0.5058, test loss-1.7857, acc-0.4993\n",
      "Iter-64800, train loss-1.7745, acc-0.5000, valid loss-1.7848, acc-0.5058, test loss-1.7857, acc-0.4993\n",
      "Iter-64810, train loss-1.7053, acc-0.7200, valid loss-1.7848, acc-0.5060, test loss-1.7856, acc-0.4994\n",
      "Iter-64820, train loss-1.8136, acc-0.4800, valid loss-1.7848, acc-0.5062, test loss-1.7856, acc-0.4993\n",
      "Iter-64830, train loss-1.7757, acc-0.5400, valid loss-1.7847, acc-0.5060, test loss-1.7856, acc-0.4992\n",
      "Iter-64840, train loss-1.8926, acc-0.3800, valid loss-1.7847, acc-0.5058, test loss-1.7855, acc-0.4992\n",
      "Iter-64850, train loss-1.7252, acc-0.4400, valid loss-1.7846, acc-0.5060, test loss-1.7855, acc-0.4993\n",
      "Iter-64860, train loss-1.8167, acc-0.4400, valid loss-1.7846, acc-0.5062, test loss-1.7854, acc-0.4994\n",
      "Iter-64870, train loss-1.8129, acc-0.4200, valid loss-1.7845, acc-0.5062, test loss-1.7854, acc-0.4992\n",
      "Iter-64880, train loss-1.7878, acc-0.5000, valid loss-1.7845, acc-0.5062, test loss-1.7853, acc-0.4991\n",
      "Iter-64890, train loss-1.8738, acc-0.4600, valid loss-1.7844, acc-0.5064, test loss-1.7853, acc-0.4993\n",
      "Iter-64900, train loss-1.8629, acc-0.3600, valid loss-1.7844, acc-0.5064, test loss-1.7852, acc-0.4992\n",
      "Iter-64910, train loss-1.8094, acc-0.5200, valid loss-1.7843, acc-0.5060, test loss-1.7852, acc-0.4992\n",
      "Iter-64920, train loss-1.8601, acc-0.4800, valid loss-1.7843, acc-0.5062, test loss-1.7851, acc-0.4992\n",
      "Iter-64930, train loss-1.9109, acc-0.3200, valid loss-1.7843, acc-0.5064, test loss-1.7851, acc-0.4992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-64940, train loss-1.8677, acc-0.5200, valid loss-1.7842, acc-0.5068, test loss-1.7850, acc-0.4993\n",
      "Iter-64950, train loss-1.8604, acc-0.4800, valid loss-1.7842, acc-0.5068, test loss-1.7850, acc-0.4992\n",
      "Iter-64960, train loss-1.7571, acc-0.7000, valid loss-1.7841, acc-0.5062, test loss-1.7849, acc-0.4994\n",
      "Iter-64970, train loss-1.8029, acc-0.4600, valid loss-1.7841, acc-0.5064, test loss-1.7849, acc-0.4991\n",
      "Iter-64980, train loss-1.7643, acc-0.4600, valid loss-1.7840, acc-0.5064, test loss-1.7849, acc-0.4992\n",
      "Iter-64990, train loss-1.7920, acc-0.4000, valid loss-1.7840, acc-0.5066, test loss-1.7848, acc-0.4992\n",
      "Iter-65000, train loss-1.7467, acc-0.5600, valid loss-1.7839, acc-0.5064, test loss-1.7848, acc-0.4991\n",
      "Iter-65010, train loss-1.8465, acc-0.5000, valid loss-1.7839, acc-0.5064, test loss-1.7847, acc-0.4993\n",
      "Iter-65020, train loss-1.7882, acc-0.4200, valid loss-1.7838, acc-0.5066, test loss-1.7847, acc-0.4992\n",
      "Iter-65030, train loss-1.7192, acc-0.5400, valid loss-1.7838, acc-0.5066, test loss-1.7846, acc-0.4992\n",
      "Iter-65040, train loss-1.7669, acc-0.5200, valid loss-1.7838, acc-0.5068, test loss-1.7846, acc-0.4993\n",
      "Iter-65050, train loss-1.8696, acc-0.4400, valid loss-1.7837, acc-0.5068, test loss-1.7845, acc-0.4992\n",
      "Iter-65060, train loss-1.8145, acc-0.4000, valid loss-1.7837, acc-0.5068, test loss-1.7845, acc-0.4991\n",
      "Iter-65070, train loss-1.7867, acc-0.5600, valid loss-1.7836, acc-0.5066, test loss-1.7844, acc-0.4994\n",
      "Iter-65080, train loss-1.7957, acc-0.5000, valid loss-1.7836, acc-0.5070, test loss-1.7844, acc-0.4993\n",
      "Iter-65090, train loss-1.7930, acc-0.4800, valid loss-1.7835, acc-0.5072, test loss-1.7843, acc-0.4994\n",
      "Iter-65100, train loss-1.8851, acc-0.3800, valid loss-1.7835, acc-0.5072, test loss-1.7843, acc-0.4993\n",
      "Iter-65110, train loss-1.7712, acc-0.5600, valid loss-1.7834, acc-0.5072, test loss-1.7843, acc-0.4992\n",
      "Iter-65120, train loss-1.6888, acc-0.6000, valid loss-1.7834, acc-0.5068, test loss-1.7842, acc-0.4991\n",
      "Iter-65130, train loss-1.7757, acc-0.4800, valid loss-1.7833, acc-0.5068, test loss-1.7842, acc-0.4991\n",
      "Iter-65140, train loss-1.8536, acc-0.4600, valid loss-1.7833, acc-0.5072, test loss-1.7841, acc-0.4993\n",
      "Iter-65150, train loss-1.8120, acc-0.4800, valid loss-1.7832, acc-0.5070, test loss-1.7841, acc-0.4992\n",
      "Iter-65160, train loss-1.7575, acc-0.5600, valid loss-1.7832, acc-0.5072, test loss-1.7840, acc-0.4993\n",
      "Iter-65170, train loss-1.8449, acc-0.5000, valid loss-1.7832, acc-0.5072, test loss-1.7840, acc-0.4993\n",
      "Iter-65180, train loss-1.7553, acc-0.5800, valid loss-1.7831, acc-0.5072, test loss-1.7839, acc-0.4994\n",
      "Iter-65190, train loss-1.7894, acc-0.4800, valid loss-1.7831, acc-0.5072, test loss-1.7839, acc-0.4993\n",
      "Iter-65200, train loss-1.7553, acc-0.5800, valid loss-1.7830, acc-0.5072, test loss-1.7838, acc-0.4993\n",
      "Iter-65210, train loss-1.6890, acc-0.5200, valid loss-1.7830, acc-0.5072, test loss-1.7838, acc-0.4993\n",
      "Iter-65220, train loss-1.6939, acc-0.5600, valid loss-1.7829, acc-0.5072, test loss-1.7838, acc-0.4994\n",
      "Iter-65230, train loss-1.8340, acc-0.4600, valid loss-1.7829, acc-0.5072, test loss-1.7837, acc-0.4993\n",
      "Iter-65240, train loss-1.8227, acc-0.5200, valid loss-1.7828, acc-0.5072, test loss-1.7837, acc-0.4993\n",
      "Iter-65250, train loss-1.6959, acc-0.5600, valid loss-1.7828, acc-0.5072, test loss-1.7836, acc-0.4993\n",
      "Iter-65260, train loss-1.7476, acc-0.5200, valid loss-1.7827, acc-0.5072, test loss-1.7836, acc-0.4992\n",
      "Iter-65270, train loss-1.8517, acc-0.4400, valid loss-1.7827, acc-0.5072, test loss-1.7835, acc-0.4992\n",
      "Iter-65280, train loss-1.7874, acc-0.5600, valid loss-1.7826, acc-0.5072, test loss-1.7835, acc-0.4993\n",
      "Iter-65290, train loss-1.7809, acc-0.4200, valid loss-1.7826, acc-0.5072, test loss-1.7834, acc-0.4993\n",
      "Iter-65300, train loss-1.7775, acc-0.4600, valid loss-1.7826, acc-0.5072, test loss-1.7834, acc-0.4994\n",
      "Iter-65310, train loss-1.8178, acc-0.5600, valid loss-1.7825, acc-0.5072, test loss-1.7833, acc-0.4994\n",
      "Iter-65320, train loss-1.8152, acc-0.4200, valid loss-1.7825, acc-0.5072, test loss-1.7833, acc-0.4993\n",
      "Iter-65330, train loss-1.7975, acc-0.5000, valid loss-1.7824, acc-0.5070, test loss-1.7833, acc-0.4994\n",
      "Iter-65340, train loss-1.8282, acc-0.5200, valid loss-1.7824, acc-0.5070, test loss-1.7832, acc-0.4995\n",
      "Iter-65350, train loss-1.7510, acc-0.5000, valid loss-1.7823, acc-0.5072, test loss-1.7832, acc-0.4995\n",
      "Iter-65360, train loss-1.7759, acc-0.6000, valid loss-1.7823, acc-0.5072, test loss-1.7831, acc-0.4995\n",
      "Iter-65370, train loss-1.7880, acc-0.4200, valid loss-1.7822, acc-0.5072, test loss-1.7831, acc-0.4993\n",
      "Iter-65380, train loss-1.8527, acc-0.4800, valid loss-1.7822, acc-0.5072, test loss-1.7830, acc-0.4993\n",
      "Iter-65390, train loss-1.8219, acc-0.6200, valid loss-1.7821, acc-0.5070, test loss-1.7830, acc-0.4994\n",
      "Iter-65400, train loss-1.8197, acc-0.4200, valid loss-1.7821, acc-0.5072, test loss-1.7829, acc-0.4994\n",
      "Iter-65410, train loss-1.7201, acc-0.5400, valid loss-1.7821, acc-0.5068, test loss-1.7829, acc-0.4994\n",
      "Iter-65420, train loss-1.7616, acc-0.5400, valid loss-1.7820, acc-0.5068, test loss-1.7828, acc-0.4995\n",
      "Iter-65430, train loss-1.6881, acc-0.5600, valid loss-1.7820, acc-0.5072, test loss-1.7828, acc-0.4995\n",
      "Iter-65440, train loss-1.6826, acc-0.6000, valid loss-1.7819, acc-0.5068, test loss-1.7827, acc-0.4996\n",
      "Iter-65450, train loss-1.7470, acc-0.5600, valid loss-1.7819, acc-0.5068, test loss-1.7827, acc-0.4993\n",
      "Iter-65460, train loss-1.7455, acc-0.5600, valid loss-1.7818, acc-0.5074, test loss-1.7827, acc-0.4994\n",
      "Iter-65470, train loss-1.8298, acc-0.4600, valid loss-1.7818, acc-0.5072, test loss-1.7826, acc-0.4992\n",
      "Iter-65480, train loss-1.8957, acc-0.4000, valid loss-1.7817, acc-0.5072, test loss-1.7826, acc-0.4992\n",
      "Iter-65490, train loss-1.7982, acc-0.5200, valid loss-1.7817, acc-0.5072, test loss-1.7825, acc-0.4995\n",
      "Iter-65500, train loss-1.7583, acc-0.5600, valid loss-1.7816, acc-0.5072, test loss-1.7825, acc-0.4995\n",
      "Iter-65510, train loss-1.8248, acc-0.4400, valid loss-1.7816, acc-0.5072, test loss-1.7824, acc-0.4996\n",
      "Iter-65520, train loss-1.8444, acc-0.4200, valid loss-1.7816, acc-0.5072, test loss-1.7824, acc-0.4996\n",
      "Iter-65530, train loss-1.7154, acc-0.5200, valid loss-1.7815, acc-0.5074, test loss-1.7823, acc-0.4996\n",
      "Iter-65540, train loss-1.8110, acc-0.4600, valid loss-1.7815, acc-0.5074, test loss-1.7823, acc-0.4997\n",
      "Iter-65550, train loss-1.7231, acc-0.5000, valid loss-1.7814, acc-0.5072, test loss-1.7822, acc-0.4994\n",
      "Iter-65560, train loss-1.7820, acc-0.5200, valid loss-1.7814, acc-0.5072, test loss-1.7822, acc-0.4993\n",
      "Iter-65570, train loss-1.8761, acc-0.3400, valid loss-1.7813, acc-0.5072, test loss-1.7821, acc-0.4994\n",
      "Iter-65580, train loss-1.8457, acc-0.5600, valid loss-1.7813, acc-0.5074, test loss-1.7821, acc-0.4994\n",
      "Iter-65590, train loss-1.7402, acc-0.5800, valid loss-1.7812, acc-0.5074, test loss-1.7821, acc-0.4994\n",
      "Iter-65600, train loss-1.7004, acc-0.5600, valid loss-1.7812, acc-0.5074, test loss-1.7820, acc-0.4994\n",
      "Iter-65610, train loss-1.8114, acc-0.6000, valid loss-1.7812, acc-0.5076, test loss-1.7820, acc-0.4996\n",
      "Iter-65620, train loss-1.9572, acc-0.3000, valid loss-1.7811, acc-0.5074, test loss-1.7819, acc-0.4995\n",
      "Iter-65630, train loss-1.7661, acc-0.4600, valid loss-1.7811, acc-0.5074, test loss-1.7819, acc-0.4995\n",
      "Iter-65640, train loss-1.8221, acc-0.4800, valid loss-1.7810, acc-0.5074, test loss-1.7818, acc-0.4994\n",
      "Iter-65650, train loss-1.8865, acc-0.4000, valid loss-1.7810, acc-0.5074, test loss-1.7818, acc-0.4994\n",
      "Iter-65660, train loss-1.9474, acc-0.3600, valid loss-1.7809, acc-0.5074, test loss-1.7817, acc-0.4994\n",
      "Iter-65670, train loss-1.6968, acc-0.6400, valid loss-1.7809, acc-0.5074, test loss-1.7817, acc-0.4993\n",
      "Iter-65680, train loss-1.8817, acc-0.4200, valid loss-1.7808, acc-0.5076, test loss-1.7816, acc-0.4993\n",
      "Iter-65690, train loss-1.8739, acc-0.5400, valid loss-1.7808, acc-0.5074, test loss-1.7816, acc-0.4993\n",
      "Iter-65700, train loss-1.7619, acc-0.5800, valid loss-1.7807, acc-0.5074, test loss-1.7816, acc-0.4994\n",
      "Iter-65710, train loss-1.8674, acc-0.4800, valid loss-1.7807, acc-0.5074, test loss-1.7815, acc-0.4995\n",
      "Iter-65720, train loss-1.8449, acc-0.5000, valid loss-1.7806, acc-0.5074, test loss-1.7815, acc-0.4994\n",
      "Iter-65730, train loss-1.8127, acc-0.6000, valid loss-1.7806, acc-0.5074, test loss-1.7814, acc-0.4994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-65740, train loss-1.7570, acc-0.5800, valid loss-1.7805, acc-0.5074, test loss-1.7814, acc-0.4994\n",
      "Iter-65750, train loss-1.8024, acc-0.4800, valid loss-1.7805, acc-0.5074, test loss-1.7813, acc-0.4996\n",
      "Iter-65760, train loss-1.7528, acc-0.5200, valid loss-1.7805, acc-0.5074, test loss-1.7813, acc-0.4994\n",
      "Iter-65770, train loss-1.8655, acc-0.3600, valid loss-1.7804, acc-0.5074, test loss-1.7812, acc-0.4995\n",
      "Iter-65780, train loss-1.8003, acc-0.5200, valid loss-1.7804, acc-0.5074, test loss-1.7812, acc-0.4994\n",
      "Iter-65790, train loss-1.7965, acc-0.5200, valid loss-1.7803, acc-0.5074, test loss-1.7811, acc-0.4995\n",
      "Iter-65800, train loss-1.8231, acc-0.4600, valid loss-1.7803, acc-0.5074, test loss-1.7811, acc-0.4994\n",
      "Iter-65810, train loss-1.7894, acc-0.4800, valid loss-1.7802, acc-0.5074, test loss-1.7811, acc-0.4994\n",
      "Iter-65820, train loss-1.8029, acc-0.5000, valid loss-1.7802, acc-0.5074, test loss-1.7810, acc-0.4995\n",
      "Iter-65830, train loss-1.8678, acc-0.3800, valid loss-1.7801, acc-0.5076, test loss-1.7810, acc-0.4996\n",
      "Iter-65840, train loss-1.8204, acc-0.5000, valid loss-1.7801, acc-0.5076, test loss-1.7809, acc-0.4996\n",
      "Iter-65850, train loss-1.7922, acc-0.5200, valid loss-1.7800, acc-0.5076, test loss-1.7809, acc-0.4997\n",
      "Iter-65860, train loss-1.7645, acc-0.5400, valid loss-1.7800, acc-0.5076, test loss-1.7808, acc-0.4997\n",
      "Iter-65870, train loss-1.7588, acc-0.4800, valid loss-1.7800, acc-0.5076, test loss-1.7808, acc-0.4997\n",
      "Iter-65880, train loss-1.8024, acc-0.5000, valid loss-1.7799, acc-0.5076, test loss-1.7807, acc-0.4998\n",
      "Iter-65890, train loss-1.8340, acc-0.4800, valid loss-1.7799, acc-0.5076, test loss-1.7807, acc-0.4997\n",
      "Iter-65900, train loss-1.7060, acc-0.5400, valid loss-1.7798, acc-0.5076, test loss-1.7806, acc-0.4997\n",
      "Iter-65910, train loss-1.8996, acc-0.4400, valid loss-1.7798, acc-0.5076, test loss-1.7806, acc-0.4996\n",
      "Iter-65920, train loss-1.7632, acc-0.5000, valid loss-1.7797, acc-0.5076, test loss-1.7806, acc-0.4999\n",
      "Iter-65930, train loss-1.7431, acc-0.4200, valid loss-1.7797, acc-0.5076, test loss-1.7805, acc-0.5004\n",
      "Iter-65940, train loss-1.7864, acc-0.4800, valid loss-1.7796, acc-0.5076, test loss-1.7805, acc-0.5000\n",
      "Iter-65950, train loss-1.7880, acc-0.6200, valid loss-1.7796, acc-0.5082, test loss-1.7804, acc-0.5001\n",
      "Iter-65960, train loss-1.7275, acc-0.5600, valid loss-1.7795, acc-0.5082, test loss-1.7804, acc-0.5000\n",
      "Iter-65970, train loss-1.8220, acc-0.5200, valid loss-1.7795, acc-0.5084, test loss-1.7803, acc-0.5003\n",
      "Iter-65980, train loss-1.7690, acc-0.6200, valid loss-1.7795, acc-0.5080, test loss-1.7803, acc-0.5001\n",
      "Iter-65990, train loss-1.8755, acc-0.4600, valid loss-1.7794, acc-0.5080, test loss-1.7802, acc-0.5004\n",
      "Iter-66000, train loss-1.8217, acc-0.4600, valid loss-1.7794, acc-0.5080, test loss-1.7802, acc-0.5003\n",
      "Iter-66010, train loss-1.8711, acc-0.4600, valid loss-1.7793, acc-0.5080, test loss-1.7802, acc-0.5003\n",
      "Iter-66020, train loss-1.8058, acc-0.5200, valid loss-1.7793, acc-0.5080, test loss-1.7801, acc-0.5001\n",
      "Iter-66030, train loss-1.8280, acc-0.4400, valid loss-1.7792, acc-0.5078, test loss-1.7801, acc-0.5001\n",
      "Iter-66040, train loss-1.7609, acc-0.5400, valid loss-1.7792, acc-0.5080, test loss-1.7800, acc-0.5003\n",
      "Iter-66050, train loss-1.8877, acc-0.4400, valid loss-1.7792, acc-0.5076, test loss-1.7800, acc-0.5003\n",
      "Iter-66060, train loss-1.8433, acc-0.4800, valid loss-1.7791, acc-0.5076, test loss-1.7799, acc-0.5003\n",
      "Iter-66070, train loss-1.7573, acc-0.5000, valid loss-1.7791, acc-0.5076, test loss-1.7799, acc-0.5002\n",
      "Iter-66080, train loss-1.7875, acc-0.5000, valid loss-1.7790, acc-0.5076, test loss-1.7798, acc-0.5004\n",
      "Iter-66090, train loss-1.8797, acc-0.4600, valid loss-1.7790, acc-0.5078, test loss-1.7798, acc-0.5004\n",
      "Iter-66100, train loss-1.9041, acc-0.4000, valid loss-1.7789, acc-0.5078, test loss-1.7797, acc-0.5001\n",
      "Iter-66110, train loss-1.7861, acc-0.4600, valid loss-1.7789, acc-0.5080, test loss-1.7797, acc-0.5004\n",
      "Iter-66120, train loss-1.6639, acc-0.5800, valid loss-1.7788, acc-0.5080, test loss-1.7797, acc-0.5003\n",
      "Iter-66130, train loss-1.7040, acc-0.4400, valid loss-1.7788, acc-0.5078, test loss-1.7796, acc-0.5005\n",
      "Iter-66140, train loss-1.7372, acc-0.5400, valid loss-1.7788, acc-0.5074, test loss-1.7796, acc-0.5005\n",
      "Iter-66150, train loss-1.7761, acc-0.4400, valid loss-1.7787, acc-0.5074, test loss-1.7795, acc-0.5004\n",
      "Iter-66160, train loss-1.8027, acc-0.4600, valid loss-1.7787, acc-0.5076, test loss-1.7795, acc-0.5008\n",
      "Iter-66170, train loss-1.7765, acc-0.4800, valid loss-1.7786, acc-0.5076, test loss-1.7794, acc-0.5004\n",
      "Iter-66180, train loss-1.8311, acc-0.4800, valid loss-1.7786, acc-0.5076, test loss-1.7794, acc-0.5001\n",
      "Iter-66190, train loss-1.7672, acc-0.4600, valid loss-1.7785, acc-0.5076, test loss-1.7793, acc-0.5001\n",
      "Iter-66200, train loss-1.8170, acc-0.4600, valid loss-1.7785, acc-0.5076, test loss-1.7793, acc-0.5003\n",
      "Iter-66210, train loss-1.8138, acc-0.3600, valid loss-1.7784, acc-0.5076, test loss-1.7792, acc-0.5004\n",
      "Iter-66220, train loss-1.7808, acc-0.4000, valid loss-1.7784, acc-0.5076, test loss-1.7792, acc-0.5003\n",
      "Iter-66230, train loss-1.7611, acc-0.5600, valid loss-1.7784, acc-0.5076, test loss-1.7792, acc-0.5004\n",
      "Iter-66240, train loss-1.7559, acc-0.5000, valid loss-1.7783, acc-0.5076, test loss-1.7791, acc-0.5004\n",
      "Iter-66250, train loss-1.8722, acc-0.3600, valid loss-1.7783, acc-0.5074, test loss-1.7791, acc-0.5006\n",
      "Iter-66260, train loss-1.5365, acc-0.7200, valid loss-1.7782, acc-0.5074, test loss-1.7790, acc-0.5006\n",
      "Iter-66270, train loss-1.7543, acc-0.5600, valid loss-1.7782, acc-0.5076, test loss-1.7790, acc-0.5006\n",
      "Iter-66280, train loss-1.8253, acc-0.3800, valid loss-1.7781, acc-0.5074, test loss-1.7789, acc-0.5005\n",
      "Iter-66290, train loss-1.8047, acc-0.6200, valid loss-1.7781, acc-0.5074, test loss-1.7789, acc-0.5006\n",
      "Iter-66300, train loss-1.7196, acc-0.6400, valid loss-1.7780, acc-0.5074, test loss-1.7788, acc-0.5005\n",
      "Iter-66310, train loss-1.7488, acc-0.5200, valid loss-1.7780, acc-0.5078, test loss-1.7788, acc-0.5003\n",
      "Iter-66320, train loss-1.7908, acc-0.4400, valid loss-1.7779, acc-0.5076, test loss-1.7787, acc-0.5004\n",
      "Iter-66330, train loss-1.7862, acc-0.5000, valid loss-1.7779, acc-0.5076, test loss-1.7787, acc-0.5004\n",
      "Iter-66340, train loss-1.7580, acc-0.5600, valid loss-1.7779, acc-0.5076, test loss-1.7787, acc-0.5003\n",
      "Iter-66350, train loss-1.7554, acc-0.4800, valid loss-1.7778, acc-0.5076, test loss-1.7786, acc-0.5003\n",
      "Iter-66360, train loss-1.8038, acc-0.4400, valid loss-1.7778, acc-0.5076, test loss-1.7786, acc-0.5003\n",
      "Iter-66370, train loss-1.7318, acc-0.5600, valid loss-1.7777, acc-0.5078, test loss-1.7785, acc-0.5003\n",
      "Iter-66380, train loss-1.9733, acc-0.4200, valid loss-1.7777, acc-0.5078, test loss-1.7785, acc-0.5002\n",
      "Iter-66390, train loss-1.8304, acc-0.5400, valid loss-1.7776, acc-0.5076, test loss-1.7784, acc-0.5005\n",
      "Iter-66400, train loss-1.8419, acc-0.3800, valid loss-1.7776, acc-0.5078, test loss-1.7784, acc-0.5005\n",
      "Iter-66410, train loss-1.7928, acc-0.4400, valid loss-1.7775, acc-0.5076, test loss-1.7783, acc-0.5005\n",
      "Iter-66420, train loss-1.7512, acc-0.5000, valid loss-1.7775, acc-0.5074, test loss-1.7783, acc-0.5005\n",
      "Iter-66430, train loss-1.7468, acc-0.5200, valid loss-1.7775, acc-0.5078, test loss-1.7782, acc-0.5002\n",
      "Iter-66440, train loss-1.8322, acc-0.5200, valid loss-1.7774, acc-0.5078, test loss-1.7782, acc-0.5002\n",
      "Iter-66450, train loss-1.7341, acc-0.5600, valid loss-1.7774, acc-0.5078, test loss-1.7782, acc-0.5002\n",
      "Iter-66460, train loss-1.7486, acc-0.5000, valid loss-1.7773, acc-0.5078, test loss-1.7781, acc-0.5002\n",
      "Iter-66470, train loss-1.7781, acc-0.5000, valid loss-1.7773, acc-0.5078, test loss-1.7781, acc-0.5006\n",
      "Iter-66480, train loss-1.7057, acc-0.5400, valid loss-1.7772, acc-0.5078, test loss-1.7780, acc-0.5005\n",
      "Iter-66490, train loss-1.8042, acc-0.4600, valid loss-1.7772, acc-0.5076, test loss-1.7780, acc-0.5007\n",
      "Iter-66500, train loss-1.7727, acc-0.4600, valid loss-1.7771, acc-0.5080, test loss-1.7779, acc-0.5007\n",
      "Iter-66510, train loss-1.8049, acc-0.4600, valid loss-1.7771, acc-0.5080, test loss-1.7779, acc-0.5005\n",
      "Iter-66520, train loss-1.8553, acc-0.5000, valid loss-1.7770, acc-0.5078, test loss-1.7778, acc-0.5007\n",
      "Iter-66530, train loss-1.7436, acc-0.5600, valid loss-1.7770, acc-0.5078, test loss-1.7778, acc-0.5007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-66540, train loss-1.7816, acc-0.5200, valid loss-1.7770, acc-0.5076, test loss-1.7778, acc-0.5007\n",
      "Iter-66550, train loss-1.7307, acc-0.5400, valid loss-1.7769, acc-0.5074, test loss-1.7777, acc-0.5006\n",
      "Iter-66560, train loss-1.8409, acc-0.3800, valid loss-1.7769, acc-0.5076, test loss-1.7777, acc-0.5004\n",
      "Iter-66570, train loss-1.7942, acc-0.4200, valid loss-1.7768, acc-0.5076, test loss-1.7776, acc-0.5005\n",
      "Iter-66580, train loss-1.7736, acc-0.4600, valid loss-1.7768, acc-0.5076, test loss-1.7776, acc-0.5004\n",
      "Iter-66590, train loss-1.6598, acc-0.5800, valid loss-1.7767, acc-0.5078, test loss-1.7775, acc-0.5006\n",
      "Iter-66600, train loss-1.8183, acc-0.4800, valid loss-1.7767, acc-0.5076, test loss-1.7775, acc-0.5005\n",
      "Iter-66610, train loss-1.7965, acc-0.4000, valid loss-1.7766, acc-0.5076, test loss-1.7774, acc-0.5005\n",
      "Iter-66620, train loss-1.7414, acc-0.5600, valid loss-1.7766, acc-0.5076, test loss-1.7774, acc-0.5006\n",
      "Iter-66630, train loss-1.7950, acc-0.4600, valid loss-1.7766, acc-0.5076, test loss-1.7774, acc-0.5005\n",
      "Iter-66640, train loss-1.7824, acc-0.4600, valid loss-1.7765, acc-0.5078, test loss-1.7773, acc-0.5007\n",
      "Iter-66650, train loss-1.7489, acc-0.5600, valid loss-1.7765, acc-0.5078, test loss-1.7773, acc-0.5007\n",
      "Iter-66660, train loss-1.8144, acc-0.4200, valid loss-1.7764, acc-0.5076, test loss-1.7772, acc-0.5008\n",
      "Iter-66670, train loss-1.8922, acc-0.4800, valid loss-1.7764, acc-0.5080, test loss-1.7772, acc-0.5007\n",
      "Iter-66680, train loss-1.7119, acc-0.5800, valid loss-1.7763, acc-0.5078, test loss-1.7771, acc-0.5008\n",
      "Iter-66690, train loss-1.8971, acc-0.3800, valid loss-1.7763, acc-0.5080, test loss-1.7771, acc-0.5011\n",
      "Iter-66700, train loss-1.8613, acc-0.4200, valid loss-1.7762, acc-0.5080, test loss-1.7770, acc-0.5012\n",
      "Iter-66710, train loss-1.7503, acc-0.5600, valid loss-1.7762, acc-0.5080, test loss-1.7770, acc-0.5010\n",
      "Iter-66720, train loss-1.8625, acc-0.4400, valid loss-1.7762, acc-0.5080, test loss-1.7770, acc-0.5010\n",
      "Iter-66730, train loss-1.6984, acc-0.6000, valid loss-1.7761, acc-0.5080, test loss-1.7769, acc-0.5011\n",
      "Iter-66740, train loss-1.8689, acc-0.3600, valid loss-1.7761, acc-0.5080, test loss-1.7769, acc-0.5010\n",
      "Iter-66750, train loss-1.7910, acc-0.4400, valid loss-1.7760, acc-0.5080, test loss-1.7768, acc-0.5011\n",
      "Iter-66760, train loss-1.7550, acc-0.5200, valid loss-1.7760, acc-0.5080, test loss-1.7768, acc-0.5010\n",
      "Iter-66770, train loss-1.8938, acc-0.4400, valid loss-1.7759, acc-0.5080, test loss-1.7767, acc-0.5010\n",
      "Iter-66780, train loss-1.8499, acc-0.4400, valid loss-1.7759, acc-0.5078, test loss-1.7767, acc-0.5010\n",
      "Iter-66790, train loss-1.7389, acc-0.6600, valid loss-1.7758, acc-0.5080, test loss-1.7766, acc-0.5009\n",
      "Iter-66800, train loss-1.6867, acc-0.5600, valid loss-1.7758, acc-0.5080, test loss-1.7766, acc-0.5009\n",
      "Iter-66810, train loss-1.8012, acc-0.4800, valid loss-1.7757, acc-0.5080, test loss-1.7765, acc-0.5009\n",
      "Iter-66820, train loss-1.7759, acc-0.5600, valid loss-1.7757, acc-0.5080, test loss-1.7765, acc-0.5010\n",
      "Iter-66830, train loss-1.7978, acc-0.4200, valid loss-1.7757, acc-0.5080, test loss-1.7765, acc-0.5011\n",
      "Iter-66840, train loss-1.6913, acc-0.5400, valid loss-1.7756, acc-0.5080, test loss-1.7764, acc-0.5010\n",
      "Iter-66850, train loss-1.8525, acc-0.4800, valid loss-1.7756, acc-0.5078, test loss-1.7764, acc-0.5010\n",
      "Iter-66860, train loss-1.7534, acc-0.5000, valid loss-1.7755, acc-0.5078, test loss-1.7763, acc-0.5010\n",
      "Iter-66870, train loss-1.7791, acc-0.5600, valid loss-1.7755, acc-0.5078, test loss-1.7763, acc-0.5009\n",
      "Iter-66880, train loss-1.8235, acc-0.4400, valid loss-1.7754, acc-0.5078, test loss-1.7762, acc-0.5009\n",
      "Iter-66890, train loss-1.8356, acc-0.5400, valid loss-1.7754, acc-0.5078, test loss-1.7762, acc-0.5010\n",
      "Iter-66900, train loss-1.7669, acc-0.5000, valid loss-1.7754, acc-0.5076, test loss-1.7761, acc-0.5010\n",
      "Iter-66910, train loss-1.7509, acc-0.5800, valid loss-1.7753, acc-0.5078, test loss-1.7761, acc-0.5010\n",
      "Iter-66920, train loss-1.7654, acc-0.5400, valid loss-1.7753, acc-0.5076, test loss-1.7761, acc-0.5010\n",
      "Iter-66930, train loss-1.8734, acc-0.4000, valid loss-1.7752, acc-0.5076, test loss-1.7760, acc-0.5011\n",
      "Iter-66940, train loss-1.8160, acc-0.5000, valid loss-1.7752, acc-0.5076, test loss-1.7760, acc-0.5009\n",
      "Iter-66950, train loss-1.7339, acc-0.5000, valid loss-1.7751, acc-0.5076, test loss-1.7759, acc-0.5010\n",
      "Iter-66960, train loss-1.7810, acc-0.4200, valid loss-1.7751, acc-0.5076, test loss-1.7759, acc-0.5010\n",
      "Iter-66970, train loss-1.7413, acc-0.5800, valid loss-1.7750, acc-0.5076, test loss-1.7758, acc-0.5010\n",
      "Iter-66980, train loss-1.7388, acc-0.4800, valid loss-1.7750, acc-0.5078, test loss-1.7758, acc-0.5010\n",
      "Iter-66990, train loss-1.8142, acc-0.4600, valid loss-1.7749, acc-0.5076, test loss-1.7757, acc-0.5008\n",
      "Iter-67000, train loss-1.9059, acc-0.3600, valid loss-1.7749, acc-0.5076, test loss-1.7757, acc-0.5008\n",
      "Iter-67010, train loss-1.8066, acc-0.5000, valid loss-1.7748, acc-0.5076, test loss-1.7756, acc-0.5010\n",
      "Iter-67020, train loss-1.8801, acc-0.4600, valid loss-1.7748, acc-0.5076, test loss-1.7756, acc-0.5008\n",
      "Iter-67030, train loss-1.8746, acc-0.3800, valid loss-1.7748, acc-0.5076, test loss-1.7756, acc-0.5008\n",
      "Iter-67040, train loss-1.8854, acc-0.4600, valid loss-1.7747, acc-0.5078, test loss-1.7755, acc-0.5009\n",
      "Iter-67050, train loss-1.6846, acc-0.6000, valid loss-1.7747, acc-0.5078, test loss-1.7755, acc-0.5009\n",
      "Iter-67060, train loss-1.7670, acc-0.5000, valid loss-1.7746, acc-0.5076, test loss-1.7754, acc-0.5009\n",
      "Iter-67070, train loss-1.6789, acc-0.5200, valid loss-1.7746, acc-0.5078, test loss-1.7754, acc-0.5008\n",
      "Iter-67080, train loss-1.8486, acc-0.3800, valid loss-1.7745, acc-0.5080, test loss-1.7753, acc-0.5008\n",
      "Iter-67090, train loss-1.8309, acc-0.5200, valid loss-1.7745, acc-0.5080, test loss-1.7753, acc-0.5008\n",
      "Iter-67100, train loss-1.7896, acc-0.5400, valid loss-1.7744, acc-0.5082, test loss-1.7752, acc-0.5010\n",
      "Iter-67110, train loss-1.8952, acc-0.4000, valid loss-1.7744, acc-0.5080, test loss-1.7752, acc-0.5011\n",
      "Iter-67120, train loss-1.8198, acc-0.4000, valid loss-1.7744, acc-0.5082, test loss-1.7752, acc-0.5010\n",
      "Iter-67130, train loss-1.7604, acc-0.5600, valid loss-1.7743, acc-0.5080, test loss-1.7751, acc-0.5011\n",
      "Iter-67140, train loss-1.7417, acc-0.5800, valid loss-1.7743, acc-0.5082, test loss-1.7751, acc-0.5011\n",
      "Iter-67150, train loss-1.8594, acc-0.3800, valid loss-1.7742, acc-0.5082, test loss-1.7750, acc-0.5013\n",
      "Iter-67160, train loss-1.8145, acc-0.5200, valid loss-1.7742, acc-0.5082, test loss-1.7750, acc-0.5012\n",
      "Iter-67170, train loss-1.6688, acc-0.6200, valid loss-1.7741, acc-0.5082, test loss-1.7749, acc-0.5011\n",
      "Iter-67180, train loss-1.8507, acc-0.3600, valid loss-1.7741, acc-0.5082, test loss-1.7749, acc-0.5011\n",
      "Iter-67190, train loss-1.7402, acc-0.5200, valid loss-1.7740, acc-0.5082, test loss-1.7748, acc-0.5011\n",
      "Iter-67200, train loss-1.7867, acc-0.5000, valid loss-1.7740, acc-0.5080, test loss-1.7748, acc-0.5012\n",
      "Iter-67210, train loss-1.8411, acc-0.4400, valid loss-1.7739, acc-0.5082, test loss-1.7748, acc-0.5011\n",
      "Iter-67220, train loss-1.7130, acc-0.6000, valid loss-1.7739, acc-0.5082, test loss-1.7747, acc-0.5012\n",
      "Iter-67230, train loss-1.8096, acc-0.5000, valid loss-1.7739, acc-0.5080, test loss-1.7747, acc-0.5011\n",
      "Iter-67240, train loss-1.8315, acc-0.4000, valid loss-1.7738, acc-0.5084, test loss-1.7746, acc-0.5012\n",
      "Iter-67250, train loss-1.7636, acc-0.5200, valid loss-1.7738, acc-0.5084, test loss-1.7746, acc-0.5012\n",
      "Iter-67260, train loss-1.8108, acc-0.5400, valid loss-1.7737, acc-0.5084, test loss-1.7745, acc-0.5012\n",
      "Iter-67270, train loss-1.8390, acc-0.4800, valid loss-1.7737, acc-0.5084, test loss-1.7745, acc-0.5014\n",
      "Iter-67280, train loss-1.7935, acc-0.5000, valid loss-1.7736, acc-0.5084, test loss-1.7744, acc-0.5013\n",
      "Iter-67290, train loss-1.8535, acc-0.3400, valid loss-1.7736, acc-0.5084, test loss-1.7744, acc-0.5010\n",
      "Iter-67300, train loss-1.8611, acc-0.4600, valid loss-1.7736, acc-0.5084, test loss-1.7744, acc-0.5013\n",
      "Iter-67310, train loss-1.8514, acc-0.3800, valid loss-1.7735, acc-0.5084, test loss-1.7743, acc-0.5012\n",
      "Iter-67320, train loss-1.8622, acc-0.3600, valid loss-1.7735, acc-0.5084, test loss-1.7743, acc-0.5009\n",
      "Iter-67330, train loss-1.8016, acc-0.4600, valid loss-1.7734, acc-0.5084, test loss-1.7742, acc-0.5010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-67340, train loss-1.8011, acc-0.5600, valid loss-1.7734, acc-0.5084, test loss-1.7742, acc-0.5011\n",
      "Iter-67350, train loss-1.8166, acc-0.4800, valid loss-1.7733, acc-0.5084, test loss-1.7741, acc-0.5011\n",
      "Iter-67360, train loss-1.7391, acc-0.5600, valid loss-1.7733, acc-0.5084, test loss-1.7741, acc-0.5010\n",
      "Iter-67370, train loss-1.8328, acc-0.4600, valid loss-1.7733, acc-0.5082, test loss-1.7741, acc-0.5013\n",
      "Iter-67380, train loss-1.7439, acc-0.5400, valid loss-1.7732, acc-0.5084, test loss-1.7740, acc-0.5012\n",
      "Iter-67390, train loss-1.7508, acc-0.5000, valid loss-1.7732, acc-0.5084, test loss-1.7740, acc-0.5012\n",
      "Iter-67400, train loss-1.7348, acc-0.5600, valid loss-1.7731, acc-0.5084, test loss-1.7739, acc-0.5014\n",
      "Iter-67410, train loss-1.7851, acc-0.4600, valid loss-1.7731, acc-0.5084, test loss-1.7739, acc-0.5014\n",
      "Iter-67420, train loss-1.8473, acc-0.4600, valid loss-1.7730, acc-0.5086, test loss-1.7738, acc-0.5012\n",
      "Iter-67430, train loss-1.7489, acc-0.5200, valid loss-1.7730, acc-0.5086, test loss-1.7738, acc-0.5012\n",
      "Iter-67440, train loss-1.8489, acc-0.3800, valid loss-1.7730, acc-0.5088, test loss-1.7737, acc-0.5014\n",
      "Iter-67450, train loss-1.8292, acc-0.4200, valid loss-1.7729, acc-0.5090, test loss-1.7737, acc-0.5012\n",
      "Iter-67460, train loss-1.7820, acc-0.5200, valid loss-1.7729, acc-0.5088, test loss-1.7737, acc-0.5012\n",
      "Iter-67470, train loss-1.7699, acc-0.5600, valid loss-1.7728, acc-0.5088, test loss-1.7736, acc-0.5016\n",
      "Iter-67480, train loss-1.7110, acc-0.5400, valid loss-1.7728, acc-0.5088, test loss-1.7736, acc-0.5015\n",
      "Iter-67490, train loss-1.7761, acc-0.5400, valid loss-1.7727, acc-0.5088, test loss-1.7735, acc-0.5016\n",
      "Iter-67500, train loss-1.7579, acc-0.4800, valid loss-1.7727, acc-0.5090, test loss-1.7735, acc-0.5017\n",
      "Iter-67510, train loss-1.7638, acc-0.5400, valid loss-1.7727, acc-0.5092, test loss-1.7734, acc-0.5017\n",
      "Iter-67520, train loss-1.7583, acc-0.5600, valid loss-1.7726, acc-0.5090, test loss-1.7734, acc-0.5016\n",
      "Iter-67530, train loss-1.8359, acc-0.4200, valid loss-1.7726, acc-0.5090, test loss-1.7734, acc-0.5017\n",
      "Iter-67540, train loss-1.7117, acc-0.5400, valid loss-1.7725, acc-0.5090, test loss-1.7733, acc-0.5019\n",
      "Iter-67550, train loss-1.7763, acc-0.5200, valid loss-1.7725, acc-0.5090, test loss-1.7733, acc-0.5017\n",
      "Iter-67560, train loss-1.7420, acc-0.5600, valid loss-1.7724, acc-0.5090, test loss-1.7732, acc-0.5019\n",
      "Iter-67570, train loss-1.7635, acc-0.6000, valid loss-1.7724, acc-0.5090, test loss-1.7732, acc-0.5018\n",
      "Iter-67580, train loss-1.8479, acc-0.4000, valid loss-1.7724, acc-0.5090, test loss-1.7731, acc-0.5018\n",
      "Iter-67590, train loss-1.7970, acc-0.4000, valid loss-1.7723, acc-0.5092, test loss-1.7731, acc-0.5018\n",
      "Iter-67600, train loss-1.8208, acc-0.5000, valid loss-1.7723, acc-0.5090, test loss-1.7730, acc-0.5019\n",
      "Iter-67610, train loss-1.7451, acc-0.5600, valid loss-1.7722, acc-0.5090, test loss-1.7730, acc-0.5019\n",
      "Iter-67620, train loss-1.7666, acc-0.5600, valid loss-1.7722, acc-0.5090, test loss-1.7730, acc-0.5020\n",
      "Iter-67630, train loss-1.8673, acc-0.4200, valid loss-1.7721, acc-0.5090, test loss-1.7729, acc-0.5018\n",
      "Iter-67640, train loss-1.8655, acc-0.4400, valid loss-1.7721, acc-0.5092, test loss-1.7729, acc-0.5019\n",
      "Iter-67650, train loss-1.7523, acc-0.5600, valid loss-1.7720, acc-0.5092, test loss-1.7728, acc-0.5020\n",
      "Iter-67660, train loss-1.8560, acc-0.5200, valid loss-1.7720, acc-0.5094, test loss-1.7728, acc-0.5022\n",
      "Iter-67670, train loss-1.7615, acc-0.5600, valid loss-1.7720, acc-0.5092, test loss-1.7727, acc-0.5022\n",
      "Iter-67680, train loss-1.8350, acc-0.5200, valid loss-1.7719, acc-0.5090, test loss-1.7727, acc-0.5022\n",
      "Iter-67690, train loss-1.7702, acc-0.5000, valid loss-1.7719, acc-0.5090, test loss-1.7726, acc-0.5023\n",
      "Iter-67700, train loss-1.7643, acc-0.5000, valid loss-1.7718, acc-0.5090, test loss-1.7726, acc-0.5022\n",
      "Iter-67710, train loss-1.7535, acc-0.4400, valid loss-1.7718, acc-0.5092, test loss-1.7726, acc-0.5022\n",
      "Iter-67720, train loss-1.8040, acc-0.4600, valid loss-1.7717, acc-0.5088, test loss-1.7725, acc-0.5021\n",
      "Iter-67730, train loss-1.8104, acc-0.4800, valid loss-1.7717, acc-0.5092, test loss-1.7725, acc-0.5022\n",
      "Iter-67740, train loss-1.7761, acc-0.5800, valid loss-1.7716, acc-0.5092, test loss-1.7724, acc-0.5021\n",
      "Iter-67750, train loss-1.8073, acc-0.5400, valid loss-1.7716, acc-0.5092, test loss-1.7724, acc-0.5021\n",
      "Iter-67760, train loss-1.9010, acc-0.4000, valid loss-1.7715, acc-0.5092, test loss-1.7723, acc-0.5021\n",
      "Iter-67770, train loss-1.7838, acc-0.4000, valid loss-1.7715, acc-0.5092, test loss-1.7723, acc-0.5021\n",
      "Iter-67780, train loss-1.9032, acc-0.3600, valid loss-1.7715, acc-0.5092, test loss-1.7723, acc-0.5021\n",
      "Iter-67790, train loss-1.8418, acc-0.5200, valid loss-1.7714, acc-0.5094, test loss-1.7722, acc-0.5023\n",
      "Iter-67800, train loss-1.7488, acc-0.5000, valid loss-1.7714, acc-0.5094, test loss-1.7722, acc-0.5022\n",
      "Iter-67810, train loss-1.7481, acc-0.6200, valid loss-1.7713, acc-0.5096, test loss-1.7721, acc-0.5022\n",
      "Iter-67820, train loss-1.7532, acc-0.5400, valid loss-1.7713, acc-0.5096, test loss-1.7721, acc-0.5021\n",
      "Iter-67830, train loss-1.7605, acc-0.4800, valid loss-1.7712, acc-0.5096, test loss-1.7720, acc-0.5022\n",
      "Iter-67840, train loss-1.8339, acc-0.4200, valid loss-1.7712, acc-0.5094, test loss-1.7720, acc-0.5021\n",
      "Iter-67850, train loss-1.7043, acc-0.5600, valid loss-1.7711, acc-0.5092, test loss-1.7719, acc-0.5021\n",
      "Iter-67860, train loss-1.7903, acc-0.4600, valid loss-1.7711, acc-0.5090, test loss-1.7719, acc-0.5020\n",
      "Iter-67870, train loss-1.7676, acc-0.5200, valid loss-1.7710, acc-0.5090, test loss-1.7718, acc-0.5020\n",
      "Iter-67880, train loss-1.8451, acc-0.4800, valid loss-1.7710, acc-0.5092, test loss-1.7718, acc-0.5020\n",
      "Iter-67890, train loss-1.7633, acc-0.4800, valid loss-1.7710, acc-0.5092, test loss-1.7718, acc-0.5019\n",
      "Iter-67900, train loss-1.8312, acc-0.5400, valid loss-1.7709, acc-0.5094, test loss-1.7717, acc-0.5020\n",
      "Iter-67910, train loss-1.7337, acc-0.5800, valid loss-1.7709, acc-0.5092, test loss-1.7717, acc-0.5019\n",
      "Iter-67920, train loss-1.7468, acc-0.5200, valid loss-1.7708, acc-0.5094, test loss-1.7716, acc-0.5020\n",
      "Iter-67930, train loss-1.7754, acc-0.4000, valid loss-1.7708, acc-0.5092, test loss-1.7716, acc-0.5020\n",
      "Iter-67940, train loss-1.8466, acc-0.4600, valid loss-1.7707, acc-0.5092, test loss-1.7715, acc-0.5019\n",
      "Iter-67950, train loss-1.7466, acc-0.5200, valid loss-1.7707, acc-0.5092, test loss-1.7715, acc-0.5018\n",
      "Iter-67960, train loss-1.7973, acc-0.4600, valid loss-1.7706, acc-0.5094, test loss-1.7714, acc-0.5020\n",
      "Iter-67970, train loss-1.8443, acc-0.5000, valid loss-1.7706, acc-0.5094, test loss-1.7714, acc-0.5021\n",
      "Iter-67980, train loss-1.7367, acc-0.5400, valid loss-1.7706, acc-0.5094, test loss-1.7713, acc-0.5022\n",
      "Iter-67990, train loss-1.7629, acc-0.5000, valid loss-1.7705, acc-0.5094, test loss-1.7713, acc-0.5021\n",
      "Iter-68000, train loss-1.8311, acc-0.4600, valid loss-1.7705, acc-0.5096, test loss-1.7713, acc-0.5018\n",
      "Iter-68010, train loss-1.7787, acc-0.4800, valid loss-1.7704, acc-0.5096, test loss-1.7712, acc-0.5020\n",
      "Iter-68020, train loss-1.8262, acc-0.4400, valid loss-1.7704, acc-0.5094, test loss-1.7712, acc-0.5020\n",
      "Iter-68030, train loss-1.8773, acc-0.4000, valid loss-1.7703, acc-0.5096, test loss-1.7711, acc-0.5020\n",
      "Iter-68040, train loss-1.7802, acc-0.5600, valid loss-1.7703, acc-0.5094, test loss-1.7711, acc-0.5020\n",
      "Iter-68050, train loss-1.7385, acc-0.5600, valid loss-1.7702, acc-0.5096, test loss-1.7710, acc-0.5020\n",
      "Iter-68060, train loss-1.8823, acc-0.4000, valid loss-1.7702, acc-0.5094, test loss-1.7710, acc-0.5022\n",
      "Iter-68070, train loss-1.9230, acc-0.5000, valid loss-1.7702, acc-0.5092, test loss-1.7710, acc-0.5020\n",
      "Iter-68080, train loss-1.7784, acc-0.5400, valid loss-1.7701, acc-0.5092, test loss-1.7709, acc-0.5020\n",
      "Iter-68090, train loss-1.8544, acc-0.4600, valid loss-1.7701, acc-0.5092, test loss-1.7709, acc-0.5020\n",
      "Iter-68100, train loss-1.9006, acc-0.4400, valid loss-1.7700, acc-0.5096, test loss-1.7708, acc-0.5021\n",
      "Iter-68110, train loss-1.9091, acc-0.3200, valid loss-1.7700, acc-0.5094, test loss-1.7708, acc-0.5019\n",
      "Iter-68120, train loss-1.7894, acc-0.5600, valid loss-1.7699, acc-0.5092, test loss-1.7707, acc-0.5021\n",
      "Iter-68130, train loss-1.7638, acc-0.5200, valid loss-1.7699, acc-0.5094, test loss-1.7707, acc-0.5022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68140, train loss-1.7243, acc-0.5200, valid loss-1.7699, acc-0.5094, test loss-1.7706, acc-0.5021\n",
      "Iter-68150, train loss-1.7285, acc-0.4800, valid loss-1.7698, acc-0.5094, test loss-1.7706, acc-0.5021\n",
      "Iter-68160, train loss-1.8069, acc-0.4800, valid loss-1.7698, acc-0.5094, test loss-1.7706, acc-0.5022\n",
      "Iter-68170, train loss-1.7565, acc-0.5600, valid loss-1.7697, acc-0.5094, test loss-1.7705, acc-0.5022\n",
      "Iter-68180, train loss-1.7495, acc-0.5200, valid loss-1.7697, acc-0.5092, test loss-1.7705, acc-0.5021\n",
      "Iter-68190, train loss-1.8479, acc-0.5200, valid loss-1.7696, acc-0.5090, test loss-1.7704, acc-0.5020\n",
      "Iter-68200, train loss-1.7474, acc-0.5200, valid loss-1.7696, acc-0.5092, test loss-1.7704, acc-0.5021\n",
      "Iter-68210, train loss-1.9250, acc-0.3600, valid loss-1.7696, acc-0.5090, test loss-1.7703, acc-0.5021\n",
      "Iter-68220, train loss-1.6880, acc-0.6200, valid loss-1.7695, acc-0.5090, test loss-1.7703, acc-0.5021\n",
      "Iter-68230, train loss-1.7642, acc-0.5000, valid loss-1.7695, acc-0.5092, test loss-1.7702, acc-0.5023\n",
      "Iter-68240, train loss-1.6813, acc-0.6200, valid loss-1.7694, acc-0.5088, test loss-1.7702, acc-0.5020\n",
      "Iter-68250, train loss-1.7700, acc-0.5000, valid loss-1.7694, acc-0.5092, test loss-1.7701, acc-0.5023\n",
      "Iter-68260, train loss-1.8034, acc-0.4800, valid loss-1.7693, acc-0.5090, test loss-1.7701, acc-0.5024\n",
      "Iter-68270, train loss-1.7522, acc-0.6400, valid loss-1.7693, acc-0.5090, test loss-1.7701, acc-0.5025\n",
      "Iter-68280, train loss-1.7954, acc-0.5600, valid loss-1.7692, acc-0.5090, test loss-1.7700, acc-0.5025\n",
      "Iter-68290, train loss-1.6848, acc-0.5800, valid loss-1.7692, acc-0.5090, test loss-1.7700, acc-0.5026\n",
      "Iter-68300, train loss-1.8358, acc-0.4200, valid loss-1.7692, acc-0.5090, test loss-1.7699, acc-0.5025\n",
      "Iter-68310, train loss-1.8730, acc-0.3600, valid loss-1.7691, acc-0.5090, test loss-1.7699, acc-0.5025\n",
      "Iter-68320, train loss-1.7651, acc-0.5000, valid loss-1.7691, acc-0.5090, test loss-1.7698, acc-0.5025\n",
      "Iter-68330, train loss-1.7065, acc-0.5600, valid loss-1.7690, acc-0.5090, test loss-1.7698, acc-0.5025\n",
      "Iter-68340, train loss-1.8067, acc-0.6000, valid loss-1.7690, acc-0.5090, test loss-1.7698, acc-0.5021\n",
      "Iter-68350, train loss-1.8153, acc-0.4400, valid loss-1.7689, acc-0.5090, test loss-1.7697, acc-0.5023\n",
      "Iter-68360, train loss-1.8230, acc-0.5600, valid loss-1.7689, acc-0.5090, test loss-1.7697, acc-0.5025\n",
      "Iter-68370, train loss-1.8754, acc-0.4800, valid loss-1.7688, acc-0.5090, test loss-1.7696, acc-0.5025\n",
      "Iter-68380, train loss-1.7663, acc-0.4400, valid loss-1.7688, acc-0.5092, test loss-1.7696, acc-0.5025\n",
      "Iter-68390, train loss-1.6881, acc-0.5000, valid loss-1.7688, acc-0.5092, test loss-1.7695, acc-0.5025\n",
      "Iter-68400, train loss-1.8757, acc-0.3600, valid loss-1.7687, acc-0.5090, test loss-1.7695, acc-0.5023\n",
      "Iter-68410, train loss-1.7879, acc-0.4600, valid loss-1.7687, acc-0.5092, test loss-1.7694, acc-0.5024\n",
      "Iter-68420, train loss-1.8221, acc-0.3600, valid loss-1.7686, acc-0.5088, test loss-1.7694, acc-0.5025\n",
      "Iter-68430, train loss-1.8113, acc-0.5000, valid loss-1.7686, acc-0.5092, test loss-1.7694, acc-0.5027\n",
      "Iter-68440, train loss-1.8596, acc-0.4400, valid loss-1.7685, acc-0.5092, test loss-1.7693, acc-0.5026\n",
      "Iter-68450, train loss-1.9167, acc-0.4600, valid loss-1.7685, acc-0.5090, test loss-1.7693, acc-0.5027\n",
      "Iter-68460, train loss-1.7551, acc-0.5400, valid loss-1.7685, acc-0.5090, test loss-1.7692, acc-0.5028\n",
      "Iter-68470, train loss-1.7389, acc-0.5200, valid loss-1.7684, acc-0.5092, test loss-1.7692, acc-0.5027\n",
      "Iter-68480, train loss-1.8368, acc-0.4400, valid loss-1.7684, acc-0.5092, test loss-1.7691, acc-0.5027\n",
      "Iter-68490, train loss-1.8701, acc-0.5000, valid loss-1.7683, acc-0.5094, test loss-1.7691, acc-0.5028\n",
      "Iter-68500, train loss-1.7562, acc-0.4800, valid loss-1.7683, acc-0.5092, test loss-1.7690, acc-0.5027\n",
      "Iter-68510, train loss-1.8632, acc-0.4600, valid loss-1.7682, acc-0.5094, test loss-1.7690, acc-0.5028\n",
      "Iter-68520, train loss-1.8250, acc-0.4600, valid loss-1.7682, acc-0.5094, test loss-1.7690, acc-0.5028\n",
      "Iter-68530, train loss-1.6665, acc-0.6400, valid loss-1.7681, acc-0.5094, test loss-1.7689, acc-0.5027\n",
      "Iter-68540, train loss-1.7788, acc-0.5400, valid loss-1.7681, acc-0.5094, test loss-1.7689, acc-0.5027\n",
      "Iter-68550, train loss-1.7245, acc-0.4800, valid loss-1.7680, acc-0.5092, test loss-1.7688, acc-0.5029\n",
      "Iter-68560, train loss-1.7724, acc-0.5600, valid loss-1.7680, acc-0.5090, test loss-1.7688, acc-0.5029\n",
      "Iter-68570, train loss-1.8138, acc-0.4800, valid loss-1.7680, acc-0.5090, test loss-1.7687, acc-0.5027\n",
      "Iter-68580, train loss-1.7284, acc-0.5600, valid loss-1.7679, acc-0.5096, test loss-1.7687, acc-0.5028\n",
      "Iter-68590, train loss-1.7477, acc-0.4800, valid loss-1.7679, acc-0.5094, test loss-1.7686, acc-0.5027\n",
      "Iter-68600, train loss-1.7778, acc-0.5200, valid loss-1.7678, acc-0.5094, test loss-1.7686, acc-0.5027\n",
      "Iter-68610, train loss-1.8396, acc-0.4200, valid loss-1.7678, acc-0.5094, test loss-1.7685, acc-0.5027\n",
      "Iter-68620, train loss-1.7957, acc-0.5600, valid loss-1.7677, acc-0.5092, test loss-1.7685, acc-0.5027\n",
      "Iter-68630, train loss-1.7640, acc-0.5400, valid loss-1.7677, acc-0.5092, test loss-1.7685, acc-0.5027\n",
      "Iter-68640, train loss-1.7602, acc-0.5400, valid loss-1.7676, acc-0.5092, test loss-1.7684, acc-0.5026\n",
      "Iter-68650, train loss-1.7618, acc-0.5400, valid loss-1.7676, acc-0.5092, test loss-1.7684, acc-0.5026\n",
      "Iter-68660, train loss-1.7875, acc-0.5000, valid loss-1.7675, acc-0.5092, test loss-1.7683, acc-0.5026\n",
      "Iter-68670, train loss-1.8607, acc-0.4800, valid loss-1.7675, acc-0.5094, test loss-1.7683, acc-0.5025\n",
      "Iter-68680, train loss-1.6911, acc-0.5800, valid loss-1.7675, acc-0.5094, test loss-1.7682, acc-0.5025\n",
      "Iter-68690, train loss-1.7311, acc-0.5200, valid loss-1.7674, acc-0.5094, test loss-1.7682, acc-0.5025\n",
      "Iter-68700, train loss-1.7465, acc-0.4600, valid loss-1.7674, acc-0.5094, test loss-1.7682, acc-0.5026\n",
      "Iter-68710, train loss-1.7105, acc-0.4600, valid loss-1.7673, acc-0.5094, test loss-1.7681, acc-0.5026\n",
      "Iter-68720, train loss-1.7964, acc-0.5200, valid loss-1.7673, acc-0.5096, test loss-1.7681, acc-0.5027\n",
      "Iter-68730, train loss-1.8418, acc-0.4200, valid loss-1.7672, acc-0.5096, test loss-1.7680, acc-0.5028\n",
      "Iter-68740, train loss-1.7801, acc-0.5600, valid loss-1.7672, acc-0.5096, test loss-1.7680, acc-0.5028\n",
      "Iter-68750, train loss-1.8377, acc-0.4000, valid loss-1.7672, acc-0.5094, test loss-1.7679, acc-0.5031\n",
      "Iter-68760, train loss-1.8564, acc-0.4400, valid loss-1.7671, acc-0.5094, test loss-1.7679, acc-0.5029\n",
      "Iter-68770, train loss-1.7273, acc-0.5400, valid loss-1.7671, acc-0.5094, test loss-1.7678, acc-0.5028\n",
      "Iter-68780, train loss-1.9206, acc-0.3800, valid loss-1.7670, acc-0.5094, test loss-1.7678, acc-0.5027\n",
      "Iter-68790, train loss-1.8084, acc-0.4200, valid loss-1.7670, acc-0.5094, test loss-1.7678, acc-0.5028\n",
      "Iter-68800, train loss-1.8685, acc-0.4200, valid loss-1.7669, acc-0.5094, test loss-1.7677, acc-0.5027\n",
      "Iter-68810, train loss-1.8125, acc-0.4400, valid loss-1.7669, acc-0.5094, test loss-1.7677, acc-0.5026\n",
      "Iter-68820, train loss-1.7389, acc-0.5400, valid loss-1.7668, acc-0.5096, test loss-1.7676, acc-0.5028\n",
      "Iter-68830, train loss-1.7689, acc-0.5000, valid loss-1.7668, acc-0.5098, test loss-1.7676, acc-0.5027\n",
      "Iter-68840, train loss-1.6974, acc-0.5200, valid loss-1.7668, acc-0.5096, test loss-1.7675, acc-0.5028\n",
      "Iter-68850, train loss-1.8797, acc-0.3800, valid loss-1.7667, acc-0.5100, test loss-1.7675, acc-0.5027\n",
      "Iter-68860, train loss-1.8099, acc-0.5200, valid loss-1.7667, acc-0.5100, test loss-1.7675, acc-0.5026\n",
      "Iter-68870, train loss-1.8243, acc-0.4800, valid loss-1.7666, acc-0.5100, test loss-1.7674, acc-0.5026\n",
      "Iter-68880, train loss-1.7407, acc-0.5800, valid loss-1.7666, acc-0.5098, test loss-1.7674, acc-0.5026\n",
      "Iter-68890, train loss-1.7754, acc-0.4800, valid loss-1.7665, acc-0.5098, test loss-1.7673, acc-0.5027\n",
      "Iter-68900, train loss-1.8340, acc-0.3800, valid loss-1.7665, acc-0.5098, test loss-1.7673, acc-0.5027\n",
      "Iter-68910, train loss-1.7740, acc-0.5200, valid loss-1.7665, acc-0.5098, test loss-1.7672, acc-0.5027\n",
      "Iter-68920, train loss-1.7855, acc-0.5600, valid loss-1.7664, acc-0.5100, test loss-1.7672, acc-0.5027\n",
      "Iter-68930, train loss-1.9088, acc-0.3600, valid loss-1.7664, acc-0.5104, test loss-1.7671, acc-0.5029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68940, train loss-1.7824, acc-0.5800, valid loss-1.7663, acc-0.5100, test loss-1.7671, acc-0.5029\n",
      "Iter-68950, train loss-1.6930, acc-0.5000, valid loss-1.7663, acc-0.5106, test loss-1.7671, acc-0.5029\n",
      "Iter-68960, train loss-1.8283, acc-0.5200, valid loss-1.7662, acc-0.5106, test loss-1.7670, acc-0.5029\n",
      "Iter-68970, train loss-1.7428, acc-0.5000, valid loss-1.7662, acc-0.5106, test loss-1.7670, acc-0.5029\n",
      "Iter-68980, train loss-1.8305, acc-0.4400, valid loss-1.7662, acc-0.5106, test loss-1.7669, acc-0.5028\n",
      "Iter-68990, train loss-1.7235, acc-0.5600, valid loss-1.7661, acc-0.5102, test loss-1.7669, acc-0.5029\n",
      "Iter-69000, train loss-1.7496, acc-0.6000, valid loss-1.7661, acc-0.5104, test loss-1.7668, acc-0.5030\n",
      "Iter-69010, train loss-1.7119, acc-0.6400, valid loss-1.7660, acc-0.5098, test loss-1.7668, acc-0.5029\n",
      "Iter-69020, train loss-1.8201, acc-0.4600, valid loss-1.7660, acc-0.5100, test loss-1.7667, acc-0.5029\n",
      "Iter-69030, train loss-1.8737, acc-0.4000, valid loss-1.7659, acc-0.5100, test loss-1.7667, acc-0.5032\n",
      "Iter-69040, train loss-1.7533, acc-0.4800, valid loss-1.7659, acc-0.5100, test loss-1.7667, acc-0.5030\n",
      "Iter-69050, train loss-1.7908, acc-0.5600, valid loss-1.7658, acc-0.5100, test loss-1.7666, acc-0.5033\n",
      "Iter-69060, train loss-1.7245, acc-0.5600, valid loss-1.7658, acc-0.5104, test loss-1.7666, acc-0.5030\n",
      "Iter-69070, train loss-1.7217, acc-0.6200, valid loss-1.7658, acc-0.5104, test loss-1.7665, acc-0.5031\n",
      "Iter-69080, train loss-1.7627, acc-0.5600, valid loss-1.7657, acc-0.5104, test loss-1.7665, acc-0.5031\n",
      "Iter-69090, train loss-1.7869, acc-0.5000, valid loss-1.7657, acc-0.5106, test loss-1.7664, acc-0.5031\n",
      "Iter-69100, train loss-1.8180, acc-0.5600, valid loss-1.7656, acc-0.5106, test loss-1.7664, acc-0.5031\n",
      "Iter-69110, train loss-1.7564, acc-0.4400, valid loss-1.7656, acc-0.5106, test loss-1.7664, acc-0.5031\n",
      "Iter-69120, train loss-1.8598, acc-0.3600, valid loss-1.7655, acc-0.5106, test loss-1.7663, acc-0.5031\n",
      "Iter-69130, train loss-1.6999, acc-0.5600, valid loss-1.7655, acc-0.5104, test loss-1.7663, acc-0.5031\n",
      "Iter-69140, train loss-1.8546, acc-0.4600, valid loss-1.7654, acc-0.5104, test loss-1.7662, acc-0.5031\n",
      "Iter-69150, train loss-1.7454, acc-0.5800, valid loss-1.7654, acc-0.5104, test loss-1.7662, acc-0.5031\n",
      "Iter-69160, train loss-1.7739, acc-0.5400, valid loss-1.7654, acc-0.5106, test loss-1.7661, acc-0.5031\n",
      "Iter-69170, train loss-1.8526, acc-0.4800, valid loss-1.7653, acc-0.5106, test loss-1.7661, acc-0.5031\n",
      "Iter-69180, train loss-1.7215, acc-0.5000, valid loss-1.7653, acc-0.5108, test loss-1.7660, acc-0.5031\n",
      "Iter-69190, train loss-1.7648, acc-0.5800, valid loss-1.7652, acc-0.5108, test loss-1.7660, acc-0.5031\n",
      "Iter-69200, train loss-1.8483, acc-0.4000, valid loss-1.7652, acc-0.5108, test loss-1.7660, acc-0.5031\n",
      "Iter-69210, train loss-1.8181, acc-0.4400, valid loss-1.7651, acc-0.5108, test loss-1.7659, acc-0.5032\n",
      "Iter-69220, train loss-1.6872, acc-0.6200, valid loss-1.7651, acc-0.5108, test loss-1.7659, acc-0.5032\n",
      "Iter-69230, train loss-1.8356, acc-0.4600, valid loss-1.7650, acc-0.5108, test loss-1.7658, acc-0.5032\n",
      "Iter-69240, train loss-1.7649, acc-0.5000, valid loss-1.7650, acc-0.5108, test loss-1.7658, acc-0.5033\n",
      "Iter-69250, train loss-1.7747, acc-0.5400, valid loss-1.7650, acc-0.5106, test loss-1.7657, acc-0.5033\n",
      "Iter-69260, train loss-1.7175, acc-0.4200, valid loss-1.7649, acc-0.5106, test loss-1.7657, acc-0.5033\n",
      "Iter-69270, train loss-1.7557, acc-0.5600, valid loss-1.7649, acc-0.5104, test loss-1.7657, acc-0.5033\n",
      "Iter-69280, train loss-1.7296, acc-0.5600, valid loss-1.7648, acc-0.5108, test loss-1.7656, acc-0.5033\n",
      "Iter-69290, train loss-1.7371, acc-0.4600, valid loss-1.7648, acc-0.5106, test loss-1.7656, acc-0.5035\n",
      "Iter-69300, train loss-1.8426, acc-0.4800, valid loss-1.7647, acc-0.5108, test loss-1.7655, acc-0.5033\n",
      "Iter-69310, train loss-1.8412, acc-0.4800, valid loss-1.7647, acc-0.5108, test loss-1.7655, acc-0.5034\n",
      "Iter-69320, train loss-1.8040, acc-0.5000, valid loss-1.7646, acc-0.5110, test loss-1.7654, acc-0.5033\n",
      "Iter-69330, train loss-1.7592, acc-0.4800, valid loss-1.7646, acc-0.5110, test loss-1.7654, acc-0.5033\n",
      "Iter-69340, train loss-1.7387, acc-0.5200, valid loss-1.7646, acc-0.5110, test loss-1.7653, acc-0.5033\n",
      "Iter-69350, train loss-1.8344, acc-0.4400, valid loss-1.7645, acc-0.5112, test loss-1.7653, acc-0.5032\n",
      "Iter-69360, train loss-1.7983, acc-0.4600, valid loss-1.7645, acc-0.5112, test loss-1.7653, acc-0.5032\n",
      "Iter-69370, train loss-1.7533, acc-0.4600, valid loss-1.7644, acc-0.5112, test loss-1.7652, acc-0.5032\n",
      "Iter-69380, train loss-1.7958, acc-0.5400, valid loss-1.7644, acc-0.5110, test loss-1.7652, acc-0.5032\n",
      "Iter-69390, train loss-1.7203, acc-0.5600, valid loss-1.7643, acc-0.5110, test loss-1.7651, acc-0.5031\n",
      "Iter-69400, train loss-1.7900, acc-0.5800, valid loss-1.7643, acc-0.5112, test loss-1.7651, acc-0.5033\n",
      "Iter-69410, train loss-1.8349, acc-0.4600, valid loss-1.7643, acc-0.5112, test loss-1.7650, acc-0.5033\n",
      "Iter-69420, train loss-1.7682, acc-0.4600, valid loss-1.7642, acc-0.5112, test loss-1.7650, acc-0.5033\n",
      "Iter-69430, train loss-1.7287, acc-0.4800, valid loss-1.7642, acc-0.5112, test loss-1.7650, acc-0.5034\n",
      "Iter-69440, train loss-1.7041, acc-0.6400, valid loss-1.7641, acc-0.5112, test loss-1.7649, acc-0.5033\n",
      "Iter-69450, train loss-1.8304, acc-0.4800, valid loss-1.7641, acc-0.5114, test loss-1.7649, acc-0.5033\n",
      "Iter-69460, train loss-1.7265, acc-0.5800, valid loss-1.7640, acc-0.5110, test loss-1.7648, acc-0.5033\n",
      "Iter-69470, train loss-1.7701, acc-0.5200, valid loss-1.7640, acc-0.5112, test loss-1.7648, acc-0.5032\n",
      "Iter-69480, train loss-1.6818, acc-0.6600, valid loss-1.7640, acc-0.5114, test loss-1.7647, acc-0.5032\n",
      "Iter-69490, train loss-1.6708, acc-0.5800, valid loss-1.7639, acc-0.5114, test loss-1.7647, acc-0.5032\n",
      "Iter-69500, train loss-1.7583, acc-0.5000, valid loss-1.7639, acc-0.5116, test loss-1.7647, acc-0.5033\n",
      "Iter-69510, train loss-1.8591, acc-0.3800, valid loss-1.7638, acc-0.5114, test loss-1.7646, acc-0.5031\n",
      "Iter-69520, train loss-1.7024, acc-0.4800, valid loss-1.7638, acc-0.5114, test loss-1.7646, acc-0.5032\n",
      "Iter-69530, train loss-1.8655, acc-0.4200, valid loss-1.7637, acc-0.5114, test loss-1.7645, acc-0.5030\n",
      "Iter-69540, train loss-1.7445, acc-0.4600, valid loss-1.7637, acc-0.5114, test loss-1.7645, acc-0.5030\n",
      "Iter-69550, train loss-1.7658, acc-0.5200, valid loss-1.7636, acc-0.5112, test loss-1.7644, acc-0.5032\n",
      "Iter-69560, train loss-1.7493, acc-0.5000, valid loss-1.7636, acc-0.5114, test loss-1.7644, acc-0.5031\n",
      "Iter-69570, train loss-1.8379, acc-0.5000, valid loss-1.7636, acc-0.5114, test loss-1.7643, acc-0.5031\n",
      "Iter-69580, train loss-1.8073, acc-0.5600, valid loss-1.7635, acc-0.5116, test loss-1.7643, acc-0.5031\n",
      "Iter-69590, train loss-1.7789, acc-0.5400, valid loss-1.7635, acc-0.5116, test loss-1.7643, acc-0.5031\n",
      "Iter-69600, train loss-1.7219, acc-0.5200, valid loss-1.7634, acc-0.5116, test loss-1.7642, acc-0.5030\n",
      "Iter-69610, train loss-1.5905, acc-0.5400, valid loss-1.7634, acc-0.5118, test loss-1.7642, acc-0.5030\n",
      "Iter-69620, train loss-1.7963, acc-0.5000, valid loss-1.7633, acc-0.5118, test loss-1.7641, acc-0.5031\n",
      "Iter-69630, train loss-1.7903, acc-0.4800, valid loss-1.7633, acc-0.5120, test loss-1.7641, acc-0.5032\n",
      "Iter-69640, train loss-1.7697, acc-0.4600, valid loss-1.7633, acc-0.5120, test loss-1.7640, acc-0.5031\n",
      "Iter-69650, train loss-1.8156, acc-0.4400, valid loss-1.7632, acc-0.5116, test loss-1.7640, acc-0.5031\n",
      "Iter-69660, train loss-1.6891, acc-0.5400, valid loss-1.7632, acc-0.5122, test loss-1.7640, acc-0.5033\n",
      "Iter-69670, train loss-1.7200, acc-0.5200, valid loss-1.7631, acc-0.5122, test loss-1.7639, acc-0.5032\n",
      "Iter-69680, train loss-1.8266, acc-0.5200, valid loss-1.7631, acc-0.5122, test loss-1.7639, acc-0.5033\n",
      "Iter-69690, train loss-1.6909, acc-0.6000, valid loss-1.7630, acc-0.5122, test loss-1.7638, acc-0.5033\n",
      "Iter-69700, train loss-1.7544, acc-0.5000, valid loss-1.7630, acc-0.5122, test loss-1.7638, acc-0.5034\n",
      "Iter-69710, train loss-1.7542, acc-0.4800, valid loss-1.7630, acc-0.5122, test loss-1.7637, acc-0.5033\n",
      "Iter-69720, train loss-1.6889, acc-0.5400, valid loss-1.7629, acc-0.5118, test loss-1.7637, acc-0.5033\n",
      "Iter-69730, train loss-1.6356, acc-0.5600, valid loss-1.7629, acc-0.5120, test loss-1.7636, acc-0.5033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-69740, train loss-1.6328, acc-0.6400, valid loss-1.7628, acc-0.5120, test loss-1.7636, acc-0.5034\n",
      "Iter-69750, train loss-1.9194, acc-0.4800, valid loss-1.7628, acc-0.5120, test loss-1.7636, acc-0.5034\n",
      "Iter-69760, train loss-1.7912, acc-0.4800, valid loss-1.7627, acc-0.5118, test loss-1.7635, acc-0.5034\n",
      "Iter-69770, train loss-1.8478, acc-0.4600, valid loss-1.7627, acc-0.5120, test loss-1.7635, acc-0.5033\n",
      "Iter-69780, train loss-1.8039, acc-0.5800, valid loss-1.7627, acc-0.5118, test loss-1.7634, acc-0.5035\n",
      "Iter-69790, train loss-1.6151, acc-0.5000, valid loss-1.7626, acc-0.5116, test loss-1.7634, acc-0.5036\n",
      "Iter-69800, train loss-1.7578, acc-0.4000, valid loss-1.7626, acc-0.5118, test loss-1.7633, acc-0.5035\n",
      "Iter-69810, train loss-1.7546, acc-0.4400, valid loss-1.7625, acc-0.5118, test loss-1.7633, acc-0.5035\n",
      "Iter-69820, train loss-1.6926, acc-0.5600, valid loss-1.7625, acc-0.5118, test loss-1.7633, acc-0.5035\n",
      "Iter-69830, train loss-1.7239, acc-0.5400, valid loss-1.7624, acc-0.5116, test loss-1.7632, acc-0.5035\n",
      "Iter-69840, train loss-1.7165, acc-0.5400, valid loss-1.7624, acc-0.5118, test loss-1.7632, acc-0.5035\n",
      "Iter-69850, train loss-1.7631, acc-0.6000, valid loss-1.7623, acc-0.5120, test loss-1.7631, acc-0.5034\n",
      "Iter-69860, train loss-1.8204, acc-0.5200, valid loss-1.7623, acc-0.5122, test loss-1.7631, acc-0.5036\n",
      "Iter-69870, train loss-1.7133, acc-0.5800, valid loss-1.7623, acc-0.5124, test loss-1.7630, acc-0.5036\n",
      "Iter-69880, train loss-1.8033, acc-0.5000, valid loss-1.7622, acc-0.5126, test loss-1.7630, acc-0.5036\n",
      "Iter-69890, train loss-1.8255, acc-0.4600, valid loss-1.7622, acc-0.5126, test loss-1.7630, acc-0.5036\n",
      "Iter-69900, train loss-1.7312, acc-0.5200, valid loss-1.7621, acc-0.5124, test loss-1.7629, acc-0.5036\n",
      "Iter-69910, train loss-1.8617, acc-0.4400, valid loss-1.7621, acc-0.5126, test loss-1.7629, acc-0.5036\n",
      "Iter-69920, train loss-1.7294, acc-0.4200, valid loss-1.7621, acc-0.5128, test loss-1.7628, acc-0.5036\n",
      "Iter-69930, train loss-1.7126, acc-0.6000, valid loss-1.7620, acc-0.5128, test loss-1.7628, acc-0.5035\n",
      "Iter-69940, train loss-1.8653, acc-0.5400, valid loss-1.7620, acc-0.5126, test loss-1.7628, acc-0.5035\n",
      "Iter-69950, train loss-1.7591, acc-0.5400, valid loss-1.7619, acc-0.5130, test loss-1.7627, acc-0.5035\n",
      "Iter-69960, train loss-1.7020, acc-0.5400, valid loss-1.7619, acc-0.5130, test loss-1.7627, acc-0.5035\n",
      "Iter-69970, train loss-1.7553, acc-0.4600, valid loss-1.7618, acc-0.5126, test loss-1.7626, acc-0.5035\n",
      "Iter-69980, train loss-1.6914, acc-0.5600, valid loss-1.7618, acc-0.5128, test loss-1.7626, acc-0.5036\n",
      "Iter-69990, train loss-1.7669, acc-0.4200, valid loss-1.7618, acc-0.5128, test loss-1.7625, acc-0.5035\n",
      "Iter-70000, train loss-1.6910, acc-0.6200, valid loss-1.7617, acc-0.5128, test loss-1.7625, acc-0.5036\n",
      "Iter-70010, train loss-1.7279, acc-0.4200, valid loss-1.7617, acc-0.5128, test loss-1.7624, acc-0.5036\n",
      "Iter-70020, train loss-1.7661, acc-0.4800, valid loss-1.7616, acc-0.5128, test loss-1.7624, acc-0.5036\n",
      "Iter-70030, train loss-1.7300, acc-0.5200, valid loss-1.7616, acc-0.5130, test loss-1.7624, acc-0.5036\n",
      "Iter-70040, train loss-1.7551, acc-0.5000, valid loss-1.7615, acc-0.5130, test loss-1.7623, acc-0.5036\n",
      "Iter-70050, train loss-1.7412, acc-0.6000, valid loss-1.7615, acc-0.5130, test loss-1.7623, acc-0.5035\n",
      "Iter-70060, train loss-1.6961, acc-0.5600, valid loss-1.7614, acc-0.5130, test loss-1.7622, acc-0.5035\n",
      "Iter-70070, train loss-1.8288, acc-0.5400, valid loss-1.7614, acc-0.5130, test loss-1.7622, acc-0.5036\n",
      "Iter-70080, train loss-1.7144, acc-0.6600, valid loss-1.7614, acc-0.5132, test loss-1.7621, acc-0.5036\n",
      "Iter-70090, train loss-1.8520, acc-0.4400, valid loss-1.7613, acc-0.5132, test loss-1.7621, acc-0.5035\n",
      "Iter-70100, train loss-1.7968, acc-0.5000, valid loss-1.7613, acc-0.5130, test loss-1.7621, acc-0.5036\n",
      "Iter-70110, train loss-1.8799, acc-0.3200, valid loss-1.7612, acc-0.5130, test loss-1.7620, acc-0.5035\n",
      "Iter-70120, train loss-1.7870, acc-0.4800, valid loss-1.7612, acc-0.5130, test loss-1.7620, acc-0.5035\n",
      "Iter-70130, train loss-1.6359, acc-0.5800, valid loss-1.7611, acc-0.5130, test loss-1.7619, acc-0.5034\n",
      "Iter-70140, train loss-1.8457, acc-0.4400, valid loss-1.7611, acc-0.5126, test loss-1.7619, acc-0.5036\n",
      "Iter-70150, train loss-1.7497, acc-0.5600, valid loss-1.7610, acc-0.5128, test loss-1.7618, acc-0.5036\n",
      "Iter-70160, train loss-1.7532, acc-0.5800, valid loss-1.7610, acc-0.5130, test loss-1.7618, acc-0.5035\n",
      "Iter-70170, train loss-1.6570, acc-0.5600, valid loss-1.7610, acc-0.5128, test loss-1.7617, acc-0.5035\n",
      "Iter-70180, train loss-1.8453, acc-0.4600, valid loss-1.7609, acc-0.5128, test loss-1.7617, acc-0.5035\n",
      "Iter-70190, train loss-1.7869, acc-0.4400, valid loss-1.7609, acc-0.5124, test loss-1.7617, acc-0.5036\n",
      "Iter-70200, train loss-1.6979, acc-0.5400, valid loss-1.7608, acc-0.5124, test loss-1.7616, acc-0.5037\n",
      "Iter-70210, train loss-1.8188, acc-0.5000, valid loss-1.7608, acc-0.5124, test loss-1.7616, acc-0.5036\n",
      "Iter-70220, train loss-2.0315, acc-0.3400, valid loss-1.7607, acc-0.5126, test loss-1.7615, acc-0.5037\n",
      "Iter-70230, train loss-1.8549, acc-0.3400, valid loss-1.7607, acc-0.5128, test loss-1.7615, acc-0.5038\n",
      "Iter-70240, train loss-1.8194, acc-0.4400, valid loss-1.7607, acc-0.5128, test loss-1.7614, acc-0.5038\n",
      "Iter-70250, train loss-1.8221, acc-0.5000, valid loss-1.7606, acc-0.5130, test loss-1.7614, acc-0.5037\n",
      "Iter-70260, train loss-1.7377, acc-0.5000, valid loss-1.7606, acc-0.5132, test loss-1.7614, acc-0.5037\n",
      "Iter-70270, train loss-1.8114, acc-0.4600, valid loss-1.7605, acc-0.5126, test loss-1.7613, acc-0.5036\n",
      "Iter-70280, train loss-1.7417, acc-0.5400, valid loss-1.7605, acc-0.5126, test loss-1.7613, acc-0.5036\n",
      "Iter-70290, train loss-1.8722, acc-0.4400, valid loss-1.7605, acc-0.5126, test loss-1.7612, acc-0.5037\n",
      "Iter-70300, train loss-1.8143, acc-0.4400, valid loss-1.7604, acc-0.5128, test loss-1.7612, acc-0.5037\n",
      "Iter-70310, train loss-1.8269, acc-0.4200, valid loss-1.7604, acc-0.5132, test loss-1.7612, acc-0.5037\n",
      "Iter-70320, train loss-1.6889, acc-0.5600, valid loss-1.7603, acc-0.5132, test loss-1.7611, acc-0.5038\n",
      "Iter-70330, train loss-1.7915, acc-0.4400, valid loss-1.7603, acc-0.5132, test loss-1.7611, acc-0.5038\n",
      "Iter-70340, train loss-1.8749, acc-0.3800, valid loss-1.7602, acc-0.5132, test loss-1.7610, acc-0.5038\n",
      "Iter-70350, train loss-1.7149, acc-0.5400, valid loss-1.7602, acc-0.5132, test loss-1.7610, acc-0.5039\n",
      "Iter-70360, train loss-1.8393, acc-0.4600, valid loss-1.7601, acc-0.5132, test loss-1.7609, acc-0.5038\n",
      "Iter-70370, train loss-1.8410, acc-0.4800, valid loss-1.7601, acc-0.5134, test loss-1.7609, acc-0.5040\n",
      "Iter-70380, train loss-1.7470, acc-0.5200, valid loss-1.7601, acc-0.5134, test loss-1.7609, acc-0.5038\n",
      "Iter-70390, train loss-1.7172, acc-0.6200, valid loss-1.7600, acc-0.5134, test loss-1.7608, acc-0.5041\n",
      "Iter-70400, train loss-1.7805, acc-0.4600, valid loss-1.7600, acc-0.5134, test loss-1.7608, acc-0.5040\n",
      "Iter-70410, train loss-1.7749, acc-0.4400, valid loss-1.7599, acc-0.5134, test loss-1.7607, acc-0.5039\n",
      "Iter-70420, train loss-1.7658, acc-0.5800, valid loss-1.7599, acc-0.5134, test loss-1.7607, acc-0.5041\n",
      "Iter-70430, train loss-1.8065, acc-0.5200, valid loss-1.7598, acc-0.5134, test loss-1.7606, acc-0.5043\n",
      "Iter-70440, train loss-1.7591, acc-0.5400, valid loss-1.7598, acc-0.5136, test loss-1.7606, acc-0.5041\n",
      "Iter-70450, train loss-1.8213, acc-0.4600, valid loss-1.7598, acc-0.5134, test loss-1.7606, acc-0.5039\n",
      "Iter-70460, train loss-1.7774, acc-0.4000, valid loss-1.7597, acc-0.5136, test loss-1.7605, acc-0.5040\n",
      "Iter-70470, train loss-1.7246, acc-0.4400, valid loss-1.7597, acc-0.5136, test loss-1.7605, acc-0.5039\n",
      "Iter-70480, train loss-1.6821, acc-0.5000, valid loss-1.7596, acc-0.5134, test loss-1.7604, acc-0.5040\n",
      "Iter-70490, train loss-1.7077, acc-0.5600, valid loss-1.7596, acc-0.5134, test loss-1.7604, acc-0.5038\n",
      "Iter-70500, train loss-1.7270, acc-0.5400, valid loss-1.7595, acc-0.5134, test loss-1.7603, acc-0.5039\n",
      "Iter-70510, train loss-1.7913, acc-0.6000, valid loss-1.7595, acc-0.5134, test loss-1.7603, acc-0.5040\n",
      "Iter-70520, train loss-1.7506, acc-0.5800, valid loss-1.7594, acc-0.5134, test loss-1.7602, acc-0.5041\n",
      "Iter-70530, train loss-1.6798, acc-0.5200, valid loss-1.7594, acc-0.5134, test loss-1.7602, acc-0.5040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-70540, train loss-1.7049, acc-0.6000, valid loss-1.7594, acc-0.5134, test loss-1.7602, acc-0.5040\n",
      "Iter-70550, train loss-1.8180, acc-0.4800, valid loss-1.7593, acc-0.5136, test loss-1.7601, acc-0.5039\n",
      "Iter-70560, train loss-1.8387, acc-0.3800, valid loss-1.7593, acc-0.5134, test loss-1.7601, acc-0.5039\n",
      "Iter-70570, train loss-1.7023, acc-0.5400, valid loss-1.7592, acc-0.5134, test loss-1.7600, acc-0.5039\n",
      "Iter-70580, train loss-1.7232, acc-0.5800, valid loss-1.7592, acc-0.5134, test loss-1.7600, acc-0.5040\n",
      "Iter-70590, train loss-1.7835, acc-0.5000, valid loss-1.7591, acc-0.5132, test loss-1.7599, acc-0.5042\n",
      "Iter-70600, train loss-1.7370, acc-0.5200, valid loss-1.7591, acc-0.5134, test loss-1.7599, acc-0.5040\n",
      "Iter-70610, train loss-1.7518, acc-0.4600, valid loss-1.7591, acc-0.5132, test loss-1.7598, acc-0.5041\n",
      "Iter-70620, train loss-1.8387, acc-0.4200, valid loss-1.7590, acc-0.5132, test loss-1.7598, acc-0.5045\n",
      "Iter-70630, train loss-1.7946, acc-0.5200, valid loss-1.7590, acc-0.5134, test loss-1.7598, acc-0.5042\n",
      "Iter-70640, train loss-1.8345, acc-0.5000, valid loss-1.7589, acc-0.5132, test loss-1.7597, acc-0.5041\n",
      "Iter-70650, train loss-1.8925, acc-0.4800, valid loss-1.7589, acc-0.5136, test loss-1.7597, acc-0.5042\n",
      "Iter-70660, train loss-1.7555, acc-0.4800, valid loss-1.7588, acc-0.5134, test loss-1.7596, acc-0.5041\n",
      "Iter-70670, train loss-1.7833, acc-0.5600, valid loss-1.7588, acc-0.5132, test loss-1.7596, acc-0.5041\n",
      "Iter-70680, train loss-1.7540, acc-0.5600, valid loss-1.7588, acc-0.5134, test loss-1.7596, acc-0.5040\n",
      "Iter-70690, train loss-1.6917, acc-0.6000, valid loss-1.7587, acc-0.5134, test loss-1.7595, acc-0.5041\n",
      "Iter-70700, train loss-1.6990, acc-0.5800, valid loss-1.7587, acc-0.5134, test loss-1.7595, acc-0.5040\n",
      "Iter-70710, train loss-1.7819, acc-0.5000, valid loss-1.7586, acc-0.5136, test loss-1.7594, acc-0.5044\n",
      "Iter-70720, train loss-1.7580, acc-0.4600, valid loss-1.7586, acc-0.5134, test loss-1.7594, acc-0.5043\n",
      "Iter-70730, train loss-1.8327, acc-0.4600, valid loss-1.7585, acc-0.5134, test loss-1.7593, acc-0.5043\n",
      "Iter-70740, train loss-1.8228, acc-0.4200, valid loss-1.7585, acc-0.5134, test loss-1.7593, acc-0.5043\n",
      "Iter-70750, train loss-1.6480, acc-0.6000, valid loss-1.7585, acc-0.5134, test loss-1.7593, acc-0.5042\n",
      "Iter-70760, train loss-1.8567, acc-0.4200, valid loss-1.7584, acc-0.5134, test loss-1.7592, acc-0.5042\n",
      "Iter-70770, train loss-1.7301, acc-0.6200, valid loss-1.7584, acc-0.5134, test loss-1.7592, acc-0.5040\n",
      "Iter-70780, train loss-1.6923, acc-0.5000, valid loss-1.7583, acc-0.5134, test loss-1.7591, acc-0.5040\n",
      "Iter-70790, train loss-1.8565, acc-0.4200, valid loss-1.7583, acc-0.5134, test loss-1.7591, acc-0.5040\n",
      "Iter-70800, train loss-1.6074, acc-0.6600, valid loss-1.7582, acc-0.5134, test loss-1.7590, acc-0.5040\n",
      "Iter-70810, train loss-1.7590, acc-0.4600, valid loss-1.7582, acc-0.5134, test loss-1.7590, acc-0.5041\n",
      "Iter-70820, train loss-1.7868, acc-0.5400, valid loss-1.7582, acc-0.5134, test loss-1.7590, acc-0.5041\n",
      "Iter-70830, train loss-1.8280, acc-0.4600, valid loss-1.7581, acc-0.5134, test loss-1.7589, acc-0.5042\n",
      "Iter-70840, train loss-1.7427, acc-0.5200, valid loss-1.7581, acc-0.5136, test loss-1.7589, acc-0.5045\n",
      "Iter-70850, train loss-1.7818, acc-0.5800, valid loss-1.7580, acc-0.5136, test loss-1.7588, acc-0.5044\n",
      "Iter-70860, train loss-1.8350, acc-0.4000, valid loss-1.7580, acc-0.5134, test loss-1.7588, acc-0.5042\n",
      "Iter-70870, train loss-1.7700, acc-0.4600, valid loss-1.7580, acc-0.5136, test loss-1.7587, acc-0.5044\n",
      "Iter-70880, train loss-1.8252, acc-0.4800, valid loss-1.7579, acc-0.5136, test loss-1.7587, acc-0.5043\n",
      "Iter-70890, train loss-1.7971, acc-0.5000, valid loss-1.7579, acc-0.5138, test loss-1.7587, acc-0.5043\n",
      "Iter-70900, train loss-1.8027, acc-0.5400, valid loss-1.7578, acc-0.5136, test loss-1.7586, acc-0.5042\n",
      "Iter-70910, train loss-1.7533, acc-0.5000, valid loss-1.7578, acc-0.5138, test loss-1.7586, acc-0.5042\n",
      "Iter-70920, train loss-1.8482, acc-0.5000, valid loss-1.7578, acc-0.5138, test loss-1.7585, acc-0.5045\n",
      "Iter-70930, train loss-1.7753, acc-0.6200, valid loss-1.7577, acc-0.5138, test loss-1.7585, acc-0.5046\n",
      "Iter-70940, train loss-1.8375, acc-0.4200, valid loss-1.7577, acc-0.5138, test loss-1.7585, acc-0.5045\n",
      "Iter-70950, train loss-1.7901, acc-0.5200, valid loss-1.7576, acc-0.5140, test loss-1.7584, acc-0.5045\n",
      "Iter-70960, train loss-1.7748, acc-0.5600, valid loss-1.7576, acc-0.5138, test loss-1.7584, acc-0.5046\n",
      "Iter-70970, train loss-1.7722, acc-0.4800, valid loss-1.7575, acc-0.5138, test loss-1.7583, acc-0.5045\n",
      "Iter-70980, train loss-1.6643, acc-0.4600, valid loss-1.7575, acc-0.5138, test loss-1.7583, acc-0.5046\n",
      "Iter-70990, train loss-1.7367, acc-0.5600, valid loss-1.7575, acc-0.5136, test loss-1.7582, acc-0.5048\n",
      "Iter-71000, train loss-1.8111, acc-0.4400, valid loss-1.7574, acc-0.5138, test loss-1.7582, acc-0.5047\n",
      "Iter-71010, train loss-1.7799, acc-0.4200, valid loss-1.7574, acc-0.5134, test loss-1.7582, acc-0.5046\n",
      "Iter-71020, train loss-1.6971, acc-0.5400, valid loss-1.7573, acc-0.5134, test loss-1.7581, acc-0.5047\n",
      "Iter-71030, train loss-1.6615, acc-0.6000, valid loss-1.7573, acc-0.5136, test loss-1.7581, acc-0.5046\n",
      "Iter-71040, train loss-1.6679, acc-0.6000, valid loss-1.7572, acc-0.5138, test loss-1.7580, acc-0.5045\n",
      "Iter-71050, train loss-1.7462, acc-0.4800, valid loss-1.7572, acc-0.5138, test loss-1.7580, acc-0.5047\n",
      "Iter-71060, train loss-1.8056, acc-0.4000, valid loss-1.7572, acc-0.5136, test loss-1.7579, acc-0.5045\n",
      "Iter-71070, train loss-1.7419, acc-0.5400, valid loss-1.7571, acc-0.5136, test loss-1.7579, acc-0.5045\n",
      "Iter-71080, train loss-1.8273, acc-0.3800, valid loss-1.7571, acc-0.5138, test loss-1.7579, acc-0.5046\n",
      "Iter-71090, train loss-1.9560, acc-0.3800, valid loss-1.7570, acc-0.5138, test loss-1.7578, acc-0.5047\n",
      "Iter-71100, train loss-1.8080, acc-0.4800, valid loss-1.7570, acc-0.5140, test loss-1.7578, acc-0.5046\n",
      "Iter-71110, train loss-1.7341, acc-0.5200, valid loss-1.7569, acc-0.5140, test loss-1.7577, acc-0.5048\n",
      "Iter-71120, train loss-1.7244, acc-0.5800, valid loss-1.7569, acc-0.5140, test loss-1.7577, acc-0.5048\n",
      "Iter-71130, train loss-1.7627, acc-0.4800, valid loss-1.7569, acc-0.5140, test loss-1.7576, acc-0.5048\n",
      "Iter-71140, train loss-1.7820, acc-0.5000, valid loss-1.7568, acc-0.5140, test loss-1.7576, acc-0.5049\n",
      "Iter-71150, train loss-1.7122, acc-0.5200, valid loss-1.7568, acc-0.5138, test loss-1.7576, acc-0.5047\n",
      "Iter-71160, train loss-1.6952, acc-0.5400, valid loss-1.7567, acc-0.5134, test loss-1.7575, acc-0.5047\n",
      "Iter-71170, train loss-1.7553, acc-0.4800, valid loss-1.7567, acc-0.5138, test loss-1.7575, acc-0.5047\n",
      "Iter-71180, train loss-1.6600, acc-0.5600, valid loss-1.7566, acc-0.5138, test loss-1.7574, acc-0.5047\n",
      "Iter-71190, train loss-1.6916, acc-0.4800, valid loss-1.7566, acc-0.5136, test loss-1.7574, acc-0.5050\n",
      "Iter-71200, train loss-1.7743, acc-0.4600, valid loss-1.7566, acc-0.5138, test loss-1.7573, acc-0.5045\n",
      "Iter-71210, train loss-1.7577, acc-0.5200, valid loss-1.7565, acc-0.5138, test loss-1.7573, acc-0.5046\n",
      "Iter-71220, train loss-1.8107, acc-0.5600, valid loss-1.7565, acc-0.5136, test loss-1.7573, acc-0.5047\n",
      "Iter-71230, train loss-1.8714, acc-0.5000, valid loss-1.7564, acc-0.5136, test loss-1.7572, acc-0.5047\n",
      "Iter-71240, train loss-1.8144, acc-0.5400, valid loss-1.7564, acc-0.5140, test loss-1.7572, acc-0.5047\n",
      "Iter-71250, train loss-1.7536, acc-0.4000, valid loss-1.7564, acc-0.5142, test loss-1.7571, acc-0.5045\n",
      "Iter-71260, train loss-1.6955, acc-0.6400, valid loss-1.7563, acc-0.5142, test loss-1.7571, acc-0.5044\n",
      "Iter-71270, train loss-1.7749, acc-0.5000, valid loss-1.7563, acc-0.5142, test loss-1.7571, acc-0.5047\n",
      "Iter-71280, train loss-1.7353, acc-0.5000, valid loss-1.7562, acc-0.5142, test loss-1.7570, acc-0.5047\n",
      "Iter-71290, train loss-1.7484, acc-0.5200, valid loss-1.7562, acc-0.5142, test loss-1.7570, acc-0.5048\n",
      "Iter-71300, train loss-1.8413, acc-0.4000, valid loss-1.7561, acc-0.5144, test loss-1.7569, acc-0.5052\n",
      "Iter-71310, train loss-1.7974, acc-0.4600, valid loss-1.7561, acc-0.5140, test loss-1.7569, acc-0.5051\n",
      "Iter-71320, train loss-1.7920, acc-0.5000, valid loss-1.7561, acc-0.5140, test loss-1.7568, acc-0.5050\n",
      "Iter-71330, train loss-1.8817, acc-0.4200, valid loss-1.7560, acc-0.5140, test loss-1.7568, acc-0.5047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-71340, train loss-1.8271, acc-0.4400, valid loss-1.7560, acc-0.5138, test loss-1.7568, acc-0.5051\n",
      "Iter-71350, train loss-1.7244, acc-0.6000, valid loss-1.7559, acc-0.5140, test loss-1.7567, acc-0.5050\n",
      "Iter-71360, train loss-1.7995, acc-0.5400, valid loss-1.7559, acc-0.5140, test loss-1.7567, acc-0.5052\n",
      "Iter-71370, train loss-1.6425, acc-0.5600, valid loss-1.7559, acc-0.5140, test loss-1.7566, acc-0.5052\n",
      "Iter-71380, train loss-1.8270, acc-0.4800, valid loss-1.7558, acc-0.5138, test loss-1.7566, acc-0.5052\n",
      "Iter-71390, train loss-1.7437, acc-0.4800, valid loss-1.7558, acc-0.5138, test loss-1.7566, acc-0.5051\n",
      "Iter-71400, train loss-1.7522, acc-0.5600, valid loss-1.7557, acc-0.5140, test loss-1.7565, acc-0.5050\n",
      "Iter-71410, train loss-1.7646, acc-0.5400, valid loss-1.7557, acc-0.5140, test loss-1.7565, acc-0.5049\n",
      "Iter-71420, train loss-1.7653, acc-0.5600, valid loss-1.7556, acc-0.5140, test loss-1.7564, acc-0.5049\n",
      "Iter-71430, train loss-1.8429, acc-0.3400, valid loss-1.7556, acc-0.5140, test loss-1.7564, acc-0.5051\n",
      "Iter-71440, train loss-1.8908, acc-0.4800, valid loss-1.7556, acc-0.5142, test loss-1.7563, acc-0.5050\n",
      "Iter-71450, train loss-1.7674, acc-0.5800, valid loss-1.7555, acc-0.5142, test loss-1.7563, acc-0.5048\n",
      "Iter-71460, train loss-1.8194, acc-0.4400, valid loss-1.7555, acc-0.5142, test loss-1.7563, acc-0.5048\n",
      "Iter-71470, train loss-1.6657, acc-0.5600, valid loss-1.7554, acc-0.5142, test loss-1.7562, acc-0.5049\n",
      "Iter-71480, train loss-1.7544, acc-0.4800, valid loss-1.7554, acc-0.5142, test loss-1.7562, acc-0.5049\n",
      "Iter-71490, train loss-1.7780, acc-0.5400, valid loss-1.7553, acc-0.5142, test loss-1.7561, acc-0.5050\n",
      "Iter-71500, train loss-1.7857, acc-0.5000, valid loss-1.7553, acc-0.5142, test loss-1.7561, acc-0.5049\n",
      "Iter-71510, train loss-1.6816, acc-0.6000, valid loss-1.7552, acc-0.5142, test loss-1.7560, acc-0.5048\n",
      "Iter-71520, train loss-1.7289, acc-0.5800, valid loss-1.7552, acc-0.5142, test loss-1.7560, acc-0.5046\n",
      "Iter-71530, train loss-1.6862, acc-0.5200, valid loss-1.7552, acc-0.5144, test loss-1.7560, acc-0.5048\n",
      "Iter-71540, train loss-1.7916, acc-0.4400, valid loss-1.7551, acc-0.5142, test loss-1.7559, acc-0.5049\n",
      "Iter-71550, train loss-1.8867, acc-0.3000, valid loss-1.7551, acc-0.5144, test loss-1.7559, acc-0.5049\n",
      "Iter-71560, train loss-1.6515, acc-0.6000, valid loss-1.7550, acc-0.5146, test loss-1.7558, acc-0.5051\n",
      "Iter-71570, train loss-1.7960, acc-0.5000, valid loss-1.7550, acc-0.5146, test loss-1.7558, acc-0.5049\n",
      "Iter-71580, train loss-1.7800, acc-0.4600, valid loss-1.7550, acc-0.5144, test loss-1.7557, acc-0.5048\n",
      "Iter-71590, train loss-1.6890, acc-0.6200, valid loss-1.7549, acc-0.5146, test loss-1.7557, acc-0.5048\n",
      "Iter-71600, train loss-1.7835, acc-0.4400, valid loss-1.7549, acc-0.5144, test loss-1.7557, acc-0.5048\n",
      "Iter-71610, train loss-1.7370, acc-0.5800, valid loss-1.7548, acc-0.5144, test loss-1.7556, acc-0.5049\n",
      "Iter-71620, train loss-1.8639, acc-0.4200, valid loss-1.7548, acc-0.5144, test loss-1.7556, acc-0.5047\n",
      "Iter-71630, train loss-1.7508, acc-0.4600, valid loss-1.7547, acc-0.5144, test loss-1.7555, acc-0.5047\n",
      "Iter-71640, train loss-1.7806, acc-0.4800, valid loss-1.7547, acc-0.5146, test loss-1.7555, acc-0.5046\n",
      "Iter-71650, train loss-1.8066, acc-0.4800, valid loss-1.7547, acc-0.5146, test loss-1.7555, acc-0.5047\n",
      "Iter-71660, train loss-1.6843, acc-0.5400, valid loss-1.7546, acc-0.5148, test loss-1.7554, acc-0.5043\n",
      "Iter-71670, train loss-1.7649, acc-0.5200, valid loss-1.7546, acc-0.5148, test loss-1.7554, acc-0.5044\n",
      "Iter-71680, train loss-1.7437, acc-0.5600, valid loss-1.7545, acc-0.5148, test loss-1.7553, acc-0.5044\n",
      "Iter-71690, train loss-1.8393, acc-0.5000, valid loss-1.7545, acc-0.5146, test loss-1.7553, acc-0.5045\n",
      "Iter-71700, train loss-1.6971, acc-0.5000, valid loss-1.7544, acc-0.5146, test loss-1.7552, acc-0.5044\n",
      "Iter-71710, train loss-1.7832, acc-0.4800, valid loss-1.7544, acc-0.5144, test loss-1.7552, acc-0.5043\n",
      "Iter-71720, train loss-1.6846, acc-0.6400, valid loss-1.7544, acc-0.5142, test loss-1.7552, acc-0.5044\n",
      "Iter-71730, train loss-1.8113, acc-0.4000, valid loss-1.7543, acc-0.5148, test loss-1.7551, acc-0.5043\n",
      "Iter-71740, train loss-1.7881, acc-0.5200, valid loss-1.7543, acc-0.5148, test loss-1.7551, acc-0.5042\n",
      "Iter-71750, train loss-1.7631, acc-0.5800, valid loss-1.7542, acc-0.5150, test loss-1.7550, acc-0.5043\n",
      "Iter-71760, train loss-1.7192, acc-0.5600, valid loss-1.7542, acc-0.5148, test loss-1.7550, acc-0.5043\n",
      "Iter-71770, train loss-1.7513, acc-0.5200, valid loss-1.7542, acc-0.5146, test loss-1.7550, acc-0.5042\n",
      "Iter-71780, train loss-1.6252, acc-0.6000, valid loss-1.7541, acc-0.5144, test loss-1.7549, acc-0.5044\n",
      "Iter-71790, train loss-1.7418, acc-0.5200, valid loss-1.7541, acc-0.5146, test loss-1.7549, acc-0.5044\n",
      "Iter-71800, train loss-1.7225, acc-0.5800, valid loss-1.7540, acc-0.5148, test loss-1.7548, acc-0.5043\n",
      "Iter-71810, train loss-1.7191, acc-0.5800, valid loss-1.7540, acc-0.5148, test loss-1.7548, acc-0.5044\n",
      "Iter-71820, train loss-1.8131, acc-0.5600, valid loss-1.7539, acc-0.5148, test loss-1.7547, acc-0.5045\n",
      "Iter-71830, train loss-1.7924, acc-0.5000, valid loss-1.7539, acc-0.5146, test loss-1.7547, acc-0.5044\n",
      "Iter-71840, train loss-1.8156, acc-0.4600, valid loss-1.7538, acc-0.5146, test loss-1.7547, acc-0.5044\n",
      "Iter-71850, train loss-1.8531, acc-0.4200, valid loss-1.7538, acc-0.5146, test loss-1.7546, acc-0.5044\n",
      "Iter-71860, train loss-1.6659, acc-0.5400, valid loss-1.7538, acc-0.5148, test loss-1.7546, acc-0.5044\n",
      "Iter-71870, train loss-1.6382, acc-0.6200, valid loss-1.7537, acc-0.5146, test loss-1.7545, acc-0.5043\n",
      "Iter-71880, train loss-1.7192, acc-0.6000, valid loss-1.7537, acc-0.5146, test loss-1.7545, acc-0.5044\n",
      "Iter-71890, train loss-1.6484, acc-0.6000, valid loss-1.7536, acc-0.5146, test loss-1.7544, acc-0.5044\n",
      "Iter-71900, train loss-1.7468, acc-0.4600, valid loss-1.7536, acc-0.5148, test loss-1.7544, acc-0.5044\n",
      "Iter-71910, train loss-1.6688, acc-0.7200, valid loss-1.7536, acc-0.5146, test loss-1.7544, acc-0.5045\n",
      "Iter-71920, train loss-1.7550, acc-0.5800, valid loss-1.7535, acc-0.5148, test loss-1.7543, acc-0.5044\n",
      "Iter-71930, train loss-1.8240, acc-0.5200, valid loss-1.7535, acc-0.5150, test loss-1.7543, acc-0.5044\n",
      "Iter-71940, train loss-1.7488, acc-0.6200, valid loss-1.7534, acc-0.5146, test loss-1.7542, acc-0.5044\n",
      "Iter-71950, train loss-1.7415, acc-0.5200, valid loss-1.7534, acc-0.5148, test loss-1.7542, acc-0.5043\n",
      "Iter-71960, train loss-1.7368, acc-0.5400, valid loss-1.7534, acc-0.5146, test loss-1.7542, acc-0.5043\n",
      "Iter-71970, train loss-1.7476, acc-0.5000, valid loss-1.7533, acc-0.5144, test loss-1.7541, acc-0.5043\n",
      "Iter-71980, train loss-1.7353, acc-0.4800, valid loss-1.7533, acc-0.5148, test loss-1.7541, acc-0.5044\n",
      "Iter-71990, train loss-1.6992, acc-0.5800, valid loss-1.7532, acc-0.5146, test loss-1.7540, acc-0.5045\n",
      "Iter-72000, train loss-1.7028, acc-0.5800, valid loss-1.7532, acc-0.5148, test loss-1.7540, acc-0.5044\n",
      "Iter-72010, train loss-1.8379, acc-0.3600, valid loss-1.7531, acc-0.5150, test loss-1.7539, acc-0.5045\n",
      "Iter-72020, train loss-1.7913, acc-0.5200, valid loss-1.7531, acc-0.5150, test loss-1.7539, acc-0.5045\n",
      "Iter-72030, train loss-1.7748, acc-0.4400, valid loss-1.7531, acc-0.5150, test loss-1.7539, acc-0.5046\n",
      "Iter-72040, train loss-1.8354, acc-0.5200, valid loss-1.7530, acc-0.5148, test loss-1.7538, acc-0.5044\n",
      "Iter-72050, train loss-1.7596, acc-0.5600, valid loss-1.7530, acc-0.5150, test loss-1.7538, acc-0.5044\n",
      "Iter-72060, train loss-1.7256, acc-0.5400, valid loss-1.7529, acc-0.5150, test loss-1.7537, acc-0.5047\n",
      "Iter-72070, train loss-1.7829, acc-0.4400, valid loss-1.7529, acc-0.5150, test loss-1.7537, acc-0.5048\n",
      "Iter-72080, train loss-1.7795, acc-0.5200, valid loss-1.7529, acc-0.5148, test loss-1.7536, acc-0.5048\n",
      "Iter-72090, train loss-1.8146, acc-0.4200, valid loss-1.7528, acc-0.5148, test loss-1.7536, acc-0.5048\n",
      "Iter-72100, train loss-1.7773, acc-0.5200, valid loss-1.7528, acc-0.5148, test loss-1.7536, acc-0.5047\n",
      "Iter-72110, train loss-1.7919, acc-0.5000, valid loss-1.7527, acc-0.5150, test loss-1.7535, acc-0.5047\n",
      "Iter-72120, train loss-1.7403, acc-0.5600, valid loss-1.7527, acc-0.5148, test loss-1.7535, acc-0.5048\n",
      "Iter-72130, train loss-1.8106, acc-0.4000, valid loss-1.7527, acc-0.5148, test loss-1.7534, acc-0.5046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-72140, train loss-1.8294, acc-0.4800, valid loss-1.7526, acc-0.5148, test loss-1.7534, acc-0.5046\n",
      "Iter-72150, train loss-1.6764, acc-0.5000, valid loss-1.7526, acc-0.5148, test loss-1.7534, acc-0.5047\n",
      "Iter-72160, train loss-1.7610, acc-0.5600, valid loss-1.7525, acc-0.5150, test loss-1.7533, acc-0.5044\n",
      "Iter-72170, train loss-1.6840, acc-0.6000, valid loss-1.7525, acc-0.5152, test loss-1.7533, acc-0.5047\n",
      "Iter-72180, train loss-1.7457, acc-0.5000, valid loss-1.7524, acc-0.5152, test loss-1.7532, acc-0.5047\n",
      "Iter-72190, train loss-1.7183, acc-0.5200, valid loss-1.7524, acc-0.5150, test loss-1.7532, acc-0.5048\n",
      "Iter-72200, train loss-1.7820, acc-0.4600, valid loss-1.7524, acc-0.5150, test loss-1.7532, acc-0.5047\n",
      "Iter-72210, train loss-1.6421, acc-0.5600, valid loss-1.7523, acc-0.5148, test loss-1.7531, acc-0.5046\n",
      "Iter-72220, train loss-1.8420, acc-0.3400, valid loss-1.7523, acc-0.5150, test loss-1.7531, acc-0.5046\n",
      "Iter-72230, train loss-1.8075, acc-0.4800, valid loss-1.7522, acc-0.5150, test loss-1.7530, acc-0.5047\n",
      "Iter-72240, train loss-1.7703, acc-0.4800, valid loss-1.7522, acc-0.5150, test loss-1.7530, acc-0.5048\n",
      "Iter-72250, train loss-1.7153, acc-0.5000, valid loss-1.7521, acc-0.5150, test loss-1.7529, acc-0.5047\n",
      "Iter-72260, train loss-1.7618, acc-0.4000, valid loss-1.7521, acc-0.5150, test loss-1.7529, acc-0.5048\n",
      "Iter-72270, train loss-1.7197, acc-0.5400, valid loss-1.7521, acc-0.5150, test loss-1.7528, acc-0.5047\n",
      "Iter-72280, train loss-1.7644, acc-0.5200, valid loss-1.7520, acc-0.5146, test loss-1.7528, acc-0.5047\n",
      "Iter-72290, train loss-1.7638, acc-0.4200, valid loss-1.7520, acc-0.5148, test loss-1.7528, acc-0.5048\n",
      "Iter-72300, train loss-1.6790, acc-0.5400, valid loss-1.7519, acc-0.5150, test loss-1.7527, acc-0.5048\n",
      "Iter-72310, train loss-1.8114, acc-0.5400, valid loss-1.7519, acc-0.5150, test loss-1.7527, acc-0.5047\n",
      "Iter-72320, train loss-1.6888, acc-0.6400, valid loss-1.7518, acc-0.5150, test loss-1.7526, acc-0.5045\n",
      "Iter-72330, train loss-1.7864, acc-0.5600, valid loss-1.7518, acc-0.5152, test loss-1.7526, acc-0.5046\n",
      "Iter-72340, train loss-1.8542, acc-0.5000, valid loss-1.7518, acc-0.5154, test loss-1.7526, acc-0.5044\n",
      "Iter-72350, train loss-1.8399, acc-0.5000, valid loss-1.7517, acc-0.5154, test loss-1.7525, acc-0.5044\n",
      "Iter-72360, train loss-1.7469, acc-0.5200, valid loss-1.7517, acc-0.5152, test loss-1.7525, acc-0.5044\n",
      "Iter-72370, train loss-1.7659, acc-0.4800, valid loss-1.7516, acc-0.5150, test loss-1.7524, acc-0.5045\n",
      "Iter-72380, train loss-1.6117, acc-0.5800, valid loss-1.7516, acc-0.5148, test loss-1.7524, acc-0.5047\n",
      "Iter-72390, train loss-1.7662, acc-0.4400, valid loss-1.7515, acc-0.5148, test loss-1.7524, acc-0.5047\n",
      "Iter-72400, train loss-1.6929, acc-0.5200, valid loss-1.7515, acc-0.5150, test loss-1.7523, acc-0.5047\n",
      "Iter-72410, train loss-1.8414, acc-0.5400, valid loss-1.7515, acc-0.5150, test loss-1.7523, acc-0.5047\n",
      "Iter-72420, train loss-1.7994, acc-0.4400, valid loss-1.7514, acc-0.5150, test loss-1.7522, acc-0.5047\n",
      "Iter-72430, train loss-1.7052, acc-0.5600, valid loss-1.7514, acc-0.5152, test loss-1.7522, acc-0.5046\n",
      "Iter-72440, train loss-1.7727, acc-0.5200, valid loss-1.7513, acc-0.5152, test loss-1.7521, acc-0.5046\n",
      "Iter-72450, train loss-1.7643, acc-0.5800, valid loss-1.7513, acc-0.5154, test loss-1.7521, acc-0.5044\n",
      "Iter-72460, train loss-1.7676, acc-0.5200, valid loss-1.7513, acc-0.5156, test loss-1.7521, acc-0.5045\n",
      "Iter-72470, train loss-1.7937, acc-0.5800, valid loss-1.7512, acc-0.5156, test loss-1.7520, acc-0.5046\n",
      "Iter-72480, train loss-1.7391, acc-0.5400, valid loss-1.7512, acc-0.5156, test loss-1.7520, acc-0.5045\n",
      "Iter-72490, train loss-1.7530, acc-0.5200, valid loss-1.7511, acc-0.5156, test loss-1.7519, acc-0.5048\n",
      "Iter-72500, train loss-1.7139, acc-0.5800, valid loss-1.7511, acc-0.5158, test loss-1.7519, acc-0.5048\n",
      "Iter-72510, train loss-1.7884, acc-0.4600, valid loss-1.7510, acc-0.5160, test loss-1.7519, acc-0.5047\n",
      "Iter-72520, train loss-1.9083, acc-0.4000, valid loss-1.7510, acc-0.5162, test loss-1.7518, acc-0.5047\n",
      "Iter-72530, train loss-1.7520, acc-0.5000, valid loss-1.7510, acc-0.5158, test loss-1.7518, acc-0.5048\n",
      "Iter-72540, train loss-1.7081, acc-0.5600, valid loss-1.7509, acc-0.5158, test loss-1.7517, acc-0.5047\n",
      "Iter-72550, train loss-1.7091, acc-0.6400, valid loss-1.7509, acc-0.5158, test loss-1.7517, acc-0.5047\n",
      "Iter-72560, train loss-1.6148, acc-0.5800, valid loss-1.7508, acc-0.5158, test loss-1.7516, acc-0.5049\n",
      "Iter-72570, train loss-1.7937, acc-0.5000, valid loss-1.7508, acc-0.5160, test loss-1.7516, acc-0.5046\n",
      "Iter-72580, train loss-1.7514, acc-0.5200, valid loss-1.7508, acc-0.5158, test loss-1.7516, acc-0.5049\n",
      "Iter-72590, train loss-1.7397, acc-0.5000, valid loss-1.7507, acc-0.5158, test loss-1.7515, acc-0.5048\n",
      "Iter-72600, train loss-1.7404, acc-0.4800, valid loss-1.7507, acc-0.5158, test loss-1.7515, acc-0.5049\n",
      "Iter-72610, train loss-1.7339, acc-0.6000, valid loss-1.7506, acc-0.5154, test loss-1.7515, acc-0.5048\n",
      "Iter-72620, train loss-1.8062, acc-0.4200, valid loss-1.7506, acc-0.5156, test loss-1.7514, acc-0.5048\n",
      "Iter-72630, train loss-1.8430, acc-0.4200, valid loss-1.7506, acc-0.5158, test loss-1.7514, acc-0.5048\n",
      "Iter-72640, train loss-1.7613, acc-0.5000, valid loss-1.7505, acc-0.5158, test loss-1.7513, acc-0.5049\n",
      "Iter-72650, train loss-1.7854, acc-0.4800, valid loss-1.7505, acc-0.5158, test loss-1.7513, acc-0.5049\n",
      "Iter-72660, train loss-1.7859, acc-0.5000, valid loss-1.7504, acc-0.5158, test loss-1.7512, acc-0.5048\n",
      "Iter-72670, train loss-1.7326, acc-0.4600, valid loss-1.7504, acc-0.5158, test loss-1.7512, acc-0.5048\n",
      "Iter-72680, train loss-1.7679, acc-0.4800, valid loss-1.7503, acc-0.5160, test loss-1.7512, acc-0.5046\n",
      "Iter-72690, train loss-1.7154, acc-0.5800, valid loss-1.7503, acc-0.5160, test loss-1.7511, acc-0.5045\n",
      "Iter-72700, train loss-1.7998, acc-0.3000, valid loss-1.7503, acc-0.5160, test loss-1.7511, acc-0.5046\n",
      "Iter-72710, train loss-1.8627, acc-0.4800, valid loss-1.7502, acc-0.5160, test loss-1.7510, acc-0.5047\n",
      "Iter-72720, train loss-1.6957, acc-0.5800, valid loss-1.7502, acc-0.5160, test loss-1.7510, acc-0.5047\n",
      "Iter-72730, train loss-1.7251, acc-0.5200, valid loss-1.7501, acc-0.5160, test loss-1.7510, acc-0.5045\n",
      "Iter-72740, train loss-1.7773, acc-0.5200, valid loss-1.7501, acc-0.5162, test loss-1.7509, acc-0.5046\n",
      "Iter-72750, train loss-1.7896, acc-0.6000, valid loss-1.7501, acc-0.5160, test loss-1.7509, acc-0.5046\n",
      "Iter-72760, train loss-1.7019, acc-0.6200, valid loss-1.7500, acc-0.5160, test loss-1.7508, acc-0.5046\n",
      "Iter-72770, train loss-1.7758, acc-0.4800, valid loss-1.7500, acc-0.5160, test loss-1.7508, acc-0.5046\n",
      "Iter-72780, train loss-1.7676, acc-0.4600, valid loss-1.7499, acc-0.5160, test loss-1.7507, acc-0.5044\n",
      "Iter-72790, train loss-1.7335, acc-0.5400, valid loss-1.7499, acc-0.5160, test loss-1.7507, acc-0.5046\n",
      "Iter-72800, train loss-1.8790, acc-0.4000, valid loss-1.7499, acc-0.5160, test loss-1.7507, acc-0.5047\n",
      "Iter-72810, train loss-1.8850, acc-0.3800, valid loss-1.7498, acc-0.5158, test loss-1.7506, acc-0.5045\n",
      "Iter-72820, train loss-1.7656, acc-0.5600, valid loss-1.7498, acc-0.5160, test loss-1.7506, acc-0.5045\n",
      "Iter-72830, train loss-1.8102, acc-0.5000, valid loss-1.7497, acc-0.5158, test loss-1.7505, acc-0.5045\n",
      "Iter-72840, train loss-1.8228, acc-0.4400, valid loss-1.7497, acc-0.5156, test loss-1.7505, acc-0.5045\n",
      "Iter-72850, train loss-1.7714, acc-0.5600, valid loss-1.7496, acc-0.5156, test loss-1.7505, acc-0.5045\n",
      "Iter-72860, train loss-1.7655, acc-0.4600, valid loss-1.7496, acc-0.5156, test loss-1.7504, acc-0.5046\n",
      "Iter-72870, train loss-1.8221, acc-0.4000, valid loss-1.7496, acc-0.5156, test loss-1.7504, acc-0.5044\n",
      "Iter-72880, train loss-1.6083, acc-0.6000, valid loss-1.7495, acc-0.5156, test loss-1.7503, acc-0.5045\n",
      "Iter-72890, train loss-1.8773, acc-0.3600, valid loss-1.7495, acc-0.5158, test loss-1.7503, acc-0.5047\n",
      "Iter-72900, train loss-1.8545, acc-0.3600, valid loss-1.7494, acc-0.5158, test loss-1.7503, acc-0.5046\n",
      "Iter-72910, train loss-1.7468, acc-0.6000, valid loss-1.7494, acc-0.5158, test loss-1.7502, acc-0.5046\n",
      "Iter-72920, train loss-1.6963, acc-0.4600, valid loss-1.7494, acc-0.5158, test loss-1.7502, acc-0.5049\n",
      "Iter-72930, train loss-1.8271, acc-0.4600, valid loss-1.7493, acc-0.5158, test loss-1.7501, acc-0.5048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-72940, train loss-1.8071, acc-0.5600, valid loss-1.7493, acc-0.5158, test loss-1.7501, acc-0.5047\n",
      "Iter-72950, train loss-1.8125, acc-0.4200, valid loss-1.7492, acc-0.5158, test loss-1.7501, acc-0.5047\n",
      "Iter-72960, train loss-1.7425, acc-0.5800, valid loss-1.7492, acc-0.5158, test loss-1.7500, acc-0.5046\n",
      "Iter-72970, train loss-1.8692, acc-0.4200, valid loss-1.7492, acc-0.5158, test loss-1.7500, acc-0.5047\n",
      "Iter-72980, train loss-1.7341, acc-0.5200, valid loss-1.7491, acc-0.5158, test loss-1.7499, acc-0.5047\n",
      "Iter-72990, train loss-1.7852, acc-0.4400, valid loss-1.7491, acc-0.5156, test loss-1.7499, acc-0.5047\n",
      "Iter-73000, train loss-1.8893, acc-0.3800, valid loss-1.7490, acc-0.5156, test loss-1.7499, acc-0.5047\n",
      "Iter-73010, train loss-1.7698, acc-0.4600, valid loss-1.7490, acc-0.5158, test loss-1.7498, acc-0.5047\n",
      "Iter-73020, train loss-1.7585, acc-0.5200, valid loss-1.7490, acc-0.5156, test loss-1.7498, acc-0.5047\n",
      "Iter-73030, train loss-1.7810, acc-0.5200, valid loss-1.7489, acc-0.5156, test loss-1.7497, acc-0.5047\n",
      "Iter-73040, train loss-1.6721, acc-0.4800, valid loss-1.7489, acc-0.5156, test loss-1.7497, acc-0.5046\n",
      "Iter-73050, train loss-1.7551, acc-0.4800, valid loss-1.7488, acc-0.5156, test loss-1.7497, acc-0.5048\n",
      "Iter-73060, train loss-1.6643, acc-0.6200, valid loss-1.7488, acc-0.5156, test loss-1.7496, acc-0.5046\n",
      "Iter-73070, train loss-1.7069, acc-0.5400, valid loss-1.7488, acc-0.5156, test loss-1.7496, acc-0.5047\n",
      "Iter-73080, train loss-1.8645, acc-0.4200, valid loss-1.7487, acc-0.5156, test loss-1.7495, acc-0.5048\n",
      "Iter-73090, train loss-1.8070, acc-0.4800, valid loss-1.7487, acc-0.5158, test loss-1.7495, acc-0.5049\n",
      "Iter-73100, train loss-1.7190, acc-0.4800, valid loss-1.7486, acc-0.5158, test loss-1.7495, acc-0.5048\n",
      "Iter-73110, train loss-1.7926, acc-0.4400, valid loss-1.7486, acc-0.5158, test loss-1.7494, acc-0.5049\n",
      "Iter-73120, train loss-1.7217, acc-0.5000, valid loss-1.7485, acc-0.5158, test loss-1.7494, acc-0.5047\n",
      "Iter-73130, train loss-1.8024, acc-0.5200, valid loss-1.7485, acc-0.5158, test loss-1.7493, acc-0.5047\n",
      "Iter-73140, train loss-1.8451, acc-0.3800, valid loss-1.7485, acc-0.5158, test loss-1.7493, acc-0.5047\n",
      "Iter-73150, train loss-1.7952, acc-0.4200, valid loss-1.7484, acc-0.5160, test loss-1.7492, acc-0.5048\n",
      "Iter-73160, train loss-1.8203, acc-0.5000, valid loss-1.7484, acc-0.5158, test loss-1.7492, acc-0.5050\n",
      "Iter-73170, train loss-1.7131, acc-0.5400, valid loss-1.7483, acc-0.5158, test loss-1.7492, acc-0.5050\n",
      "Iter-73180, train loss-1.7112, acc-0.5400, valid loss-1.7483, acc-0.5160, test loss-1.7491, acc-0.5048\n",
      "Iter-73190, train loss-1.6855, acc-0.6400, valid loss-1.7483, acc-0.5158, test loss-1.7491, acc-0.5050\n",
      "Iter-73200, train loss-1.8060, acc-0.4800, valid loss-1.7482, acc-0.5158, test loss-1.7490, acc-0.5050\n",
      "Iter-73210, train loss-1.8487, acc-0.4200, valid loss-1.7482, acc-0.5158, test loss-1.7490, acc-0.5049\n",
      "Iter-73220, train loss-1.6943, acc-0.5600, valid loss-1.7481, acc-0.5158, test loss-1.7490, acc-0.5050\n",
      "Iter-73230, train loss-1.6457, acc-0.5600, valid loss-1.7481, acc-0.5158, test loss-1.7489, acc-0.5049\n",
      "Iter-73240, train loss-1.7656, acc-0.5000, valid loss-1.7481, acc-0.5156, test loss-1.7489, acc-0.5050\n",
      "Iter-73250, train loss-1.8095, acc-0.5000, valid loss-1.7480, acc-0.5156, test loss-1.7488, acc-0.5051\n",
      "Iter-73260, train loss-1.7269, acc-0.4800, valid loss-1.7480, acc-0.5156, test loss-1.7488, acc-0.5051\n",
      "Iter-73270, train loss-1.7285, acc-0.4800, valid loss-1.7479, acc-0.5156, test loss-1.7488, acc-0.5050\n",
      "Iter-73280, train loss-1.8154, acc-0.4200, valid loss-1.7479, acc-0.5156, test loss-1.7487, acc-0.5051\n",
      "Iter-73290, train loss-1.7220, acc-0.4800, valid loss-1.7479, acc-0.5156, test loss-1.7487, acc-0.5051\n",
      "Iter-73300, train loss-1.7716, acc-0.4200, valid loss-1.7478, acc-0.5156, test loss-1.7486, acc-0.5051\n",
      "Iter-73310, train loss-1.8006, acc-0.4000, valid loss-1.7478, acc-0.5156, test loss-1.7486, acc-0.5052\n",
      "Iter-73320, train loss-1.8772, acc-0.4000, valid loss-1.7477, acc-0.5156, test loss-1.7486, acc-0.5052\n",
      "Iter-73330, train loss-1.6139, acc-0.6000, valid loss-1.7477, acc-0.5156, test loss-1.7485, acc-0.5053\n",
      "Iter-73340, train loss-1.7920, acc-0.4600, valid loss-1.7477, acc-0.5156, test loss-1.7485, acc-0.5052\n",
      "Iter-73350, train loss-1.6321, acc-0.5800, valid loss-1.7476, acc-0.5156, test loss-1.7484, acc-0.5052\n",
      "Iter-73360, train loss-1.8287, acc-0.5000, valid loss-1.7476, acc-0.5156, test loss-1.7484, acc-0.5053\n",
      "Iter-73370, train loss-1.7797, acc-0.4400, valid loss-1.7475, acc-0.5156, test loss-1.7483, acc-0.5055\n",
      "Iter-73380, train loss-1.7723, acc-0.4600, valid loss-1.7475, acc-0.5156, test loss-1.7483, acc-0.5055\n",
      "Iter-73390, train loss-1.6912, acc-0.4600, valid loss-1.7475, acc-0.5156, test loss-1.7483, acc-0.5055\n",
      "Iter-73400, train loss-1.6937, acc-0.5800, valid loss-1.7474, acc-0.5156, test loss-1.7482, acc-0.5056\n",
      "Iter-73410, train loss-1.7695, acc-0.4600, valid loss-1.7474, acc-0.5158, test loss-1.7482, acc-0.5054\n",
      "Iter-73420, train loss-1.7402, acc-0.4200, valid loss-1.7473, acc-0.5160, test loss-1.7481, acc-0.5054\n",
      "Iter-73430, train loss-1.7942, acc-0.4800, valid loss-1.7473, acc-0.5160, test loss-1.7481, acc-0.5055\n",
      "Iter-73440, train loss-1.7668, acc-0.5000, valid loss-1.7473, acc-0.5160, test loss-1.7481, acc-0.5056\n",
      "Iter-73450, train loss-1.7489, acc-0.5600, valid loss-1.7472, acc-0.5160, test loss-1.7480, acc-0.5058\n",
      "Iter-73460, train loss-1.8615, acc-0.3800, valid loss-1.7472, acc-0.5158, test loss-1.7480, acc-0.5055\n",
      "Iter-73470, train loss-1.8180, acc-0.5000, valid loss-1.7471, acc-0.5160, test loss-1.7479, acc-0.5055\n",
      "Iter-73480, train loss-1.7992, acc-0.5400, valid loss-1.7471, acc-0.5160, test loss-1.7479, acc-0.5056\n",
      "Iter-73490, train loss-1.8011, acc-0.4600, valid loss-1.7470, acc-0.5160, test loss-1.7479, acc-0.5056\n",
      "Iter-73500, train loss-1.7938, acc-0.5000, valid loss-1.7470, acc-0.5160, test loss-1.7478, acc-0.5057\n",
      "Iter-73510, train loss-1.6886, acc-0.5400, valid loss-1.7470, acc-0.5158, test loss-1.7478, acc-0.5056\n",
      "Iter-73520, train loss-1.8411, acc-0.4600, valid loss-1.7469, acc-0.5158, test loss-1.7477, acc-0.5054\n",
      "Iter-73530, train loss-1.7467, acc-0.5200, valid loss-1.7469, acc-0.5156, test loss-1.7477, acc-0.5056\n",
      "Iter-73540, train loss-1.6872, acc-0.5600, valid loss-1.7468, acc-0.5156, test loss-1.7476, acc-0.5059\n",
      "Iter-73550, train loss-1.8011, acc-0.5400, valid loss-1.7468, acc-0.5156, test loss-1.7476, acc-0.5057\n",
      "Iter-73560, train loss-1.6898, acc-0.5600, valid loss-1.7468, acc-0.5156, test loss-1.7476, acc-0.5059\n",
      "Iter-73570, train loss-1.7826, acc-0.6000, valid loss-1.7467, acc-0.5156, test loss-1.7475, acc-0.5058\n",
      "Iter-73580, train loss-1.7059, acc-0.6000, valid loss-1.7467, acc-0.5156, test loss-1.7475, acc-0.5057\n",
      "Iter-73590, train loss-1.5884, acc-0.5800, valid loss-1.7466, acc-0.5160, test loss-1.7474, acc-0.5056\n",
      "Iter-73600, train loss-1.7789, acc-0.5400, valid loss-1.7466, acc-0.5160, test loss-1.7474, acc-0.5060\n",
      "Iter-73610, train loss-1.7924, acc-0.4800, valid loss-1.7466, acc-0.5158, test loss-1.7473, acc-0.5058\n",
      "Iter-73620, train loss-1.7857, acc-0.5200, valid loss-1.7465, acc-0.5158, test loss-1.7473, acc-0.5058\n",
      "Iter-73630, train loss-1.7929, acc-0.4800, valid loss-1.7465, acc-0.5160, test loss-1.7473, acc-0.5060\n",
      "Iter-73640, train loss-1.8688, acc-0.4000, valid loss-1.7464, acc-0.5160, test loss-1.7472, acc-0.5060\n",
      "Iter-73650, train loss-1.8256, acc-0.4400, valid loss-1.7464, acc-0.5160, test loss-1.7472, acc-0.5059\n",
      "Iter-73660, train loss-1.6801, acc-0.5800, valid loss-1.7463, acc-0.5160, test loss-1.7471, acc-0.5059\n",
      "Iter-73670, train loss-1.6685, acc-0.5200, valid loss-1.7463, acc-0.5160, test loss-1.7471, acc-0.5059\n",
      "Iter-73680, train loss-1.7991, acc-0.5400, valid loss-1.7463, acc-0.5160, test loss-1.7471, acc-0.5062\n",
      "Iter-73690, train loss-1.8064, acc-0.4200, valid loss-1.7462, acc-0.5160, test loss-1.7470, acc-0.5062\n",
      "Iter-73700, train loss-1.8857, acc-0.4400, valid loss-1.7462, acc-0.5160, test loss-1.7470, acc-0.5060\n",
      "Iter-73710, train loss-1.7491, acc-0.5400, valid loss-1.7461, acc-0.5160, test loss-1.7469, acc-0.5061\n",
      "Iter-73720, train loss-1.7045, acc-0.5400, valid loss-1.7461, acc-0.5160, test loss-1.7469, acc-0.5061\n",
      "Iter-73730, train loss-1.7785, acc-0.5200, valid loss-1.7461, acc-0.5160, test loss-1.7468, acc-0.5059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-73740, train loss-1.6651, acc-0.6200, valid loss-1.7460, acc-0.5160, test loss-1.7468, acc-0.5059\n",
      "Iter-73750, train loss-1.6856, acc-0.5600, valid loss-1.7460, acc-0.5160, test loss-1.7468, acc-0.5059\n",
      "Iter-73760, train loss-1.7902, acc-0.4600, valid loss-1.7459, acc-0.5160, test loss-1.7467, acc-0.5059\n",
      "Iter-73770, train loss-1.6599, acc-0.5600, valid loss-1.7459, acc-0.5160, test loss-1.7467, acc-0.5059\n",
      "Iter-73780, train loss-1.7683, acc-0.5200, valid loss-1.7459, acc-0.5160, test loss-1.7466, acc-0.5061\n",
      "Iter-73790, train loss-1.7982, acc-0.4600, valid loss-1.7458, acc-0.5160, test loss-1.7466, acc-0.5061\n",
      "Iter-73800, train loss-1.6660, acc-0.5400, valid loss-1.7458, acc-0.5160, test loss-1.7466, acc-0.5061\n",
      "Iter-73810, train loss-1.6883, acc-0.6000, valid loss-1.7457, acc-0.5160, test loss-1.7465, acc-0.5062\n",
      "Iter-73820, train loss-1.6857, acc-0.6600, valid loss-1.7457, acc-0.5160, test loss-1.7465, acc-0.5058\n",
      "Iter-73830, train loss-1.7408, acc-0.4800, valid loss-1.7456, acc-0.5160, test loss-1.7464, acc-0.5062\n",
      "Iter-73840, train loss-1.6880, acc-0.5000, valid loss-1.7456, acc-0.5158, test loss-1.7464, acc-0.5060\n",
      "Iter-73850, train loss-1.6980, acc-0.6200, valid loss-1.7455, acc-0.5158, test loss-1.7464, acc-0.5059\n",
      "Iter-73860, train loss-1.7744, acc-0.4200, valid loss-1.7455, acc-0.5160, test loss-1.7463, acc-0.5059\n",
      "Iter-73870, train loss-1.7620, acc-0.4800, valid loss-1.7455, acc-0.5162, test loss-1.7463, acc-0.5061\n",
      "Iter-73880, train loss-1.7815, acc-0.4600, valid loss-1.7454, acc-0.5162, test loss-1.7462, acc-0.5061\n",
      "Iter-73890, train loss-1.6911, acc-0.5800, valid loss-1.7454, acc-0.5162, test loss-1.7462, acc-0.5060\n",
      "Iter-73900, train loss-1.7620, acc-0.4800, valid loss-1.7453, acc-0.5162, test loss-1.7461, acc-0.5059\n",
      "Iter-73910, train loss-1.8141, acc-0.5200, valid loss-1.7453, acc-0.5162, test loss-1.7461, acc-0.5059\n",
      "Iter-73920, train loss-1.6753, acc-0.5400, valid loss-1.7453, acc-0.5162, test loss-1.7460, acc-0.5065\n",
      "Iter-73930, train loss-1.8091, acc-0.4200, valid loss-1.7452, acc-0.5162, test loss-1.7460, acc-0.5064\n",
      "Iter-73940, train loss-1.6303, acc-0.6000, valid loss-1.7452, acc-0.5162, test loss-1.7460, acc-0.5062\n",
      "Iter-73950, train loss-1.7248, acc-0.5000, valid loss-1.7451, acc-0.5162, test loss-1.7459, acc-0.5062\n",
      "Iter-73960, train loss-1.8585, acc-0.3600, valid loss-1.7451, acc-0.5162, test loss-1.7459, acc-0.5058\n",
      "Iter-73970, train loss-1.7011, acc-0.5000, valid loss-1.7450, acc-0.5162, test loss-1.7458, acc-0.5059\n",
      "Iter-73980, train loss-1.7065, acc-0.5800, valid loss-1.7450, acc-0.5162, test loss-1.7458, acc-0.5059\n",
      "Iter-73990, train loss-1.7841, acc-0.4800, valid loss-1.7450, acc-0.5162, test loss-1.7458, acc-0.5059\n",
      "Iter-74000, train loss-1.9294, acc-0.3800, valid loss-1.7449, acc-0.5162, test loss-1.7457, acc-0.5059\n",
      "Iter-74010, train loss-1.7688, acc-0.5400, valid loss-1.7449, acc-0.5162, test loss-1.7457, acc-0.5059\n",
      "Iter-74020, train loss-1.8276, acc-0.5200, valid loss-1.7448, acc-0.5162, test loss-1.7456, acc-0.5060\n",
      "Iter-74030, train loss-1.8235, acc-0.3800, valid loss-1.7448, acc-0.5160, test loss-1.7456, acc-0.5061\n",
      "Iter-74040, train loss-1.7121, acc-0.5600, valid loss-1.7448, acc-0.5160, test loss-1.7456, acc-0.5059\n",
      "Iter-74050, train loss-1.6503, acc-0.6000, valid loss-1.7447, acc-0.5160, test loss-1.7455, acc-0.5061\n",
      "Iter-74060, train loss-1.7852, acc-0.4800, valid loss-1.7447, acc-0.5158, test loss-1.7455, acc-0.5060\n",
      "Iter-74070, train loss-1.6586, acc-0.5800, valid loss-1.7446, acc-0.5160, test loss-1.7454, acc-0.5062\n",
      "Iter-74080, train loss-1.7664, acc-0.5000, valid loss-1.7446, acc-0.5158, test loss-1.7454, acc-0.5062\n",
      "Iter-74090, train loss-1.6490, acc-0.6200, valid loss-1.7446, acc-0.5158, test loss-1.7454, acc-0.5062\n",
      "Iter-74100, train loss-1.7898, acc-0.4800, valid loss-1.7445, acc-0.5158, test loss-1.7453, acc-0.5063\n",
      "Iter-74110, train loss-1.6535, acc-0.5000, valid loss-1.7445, acc-0.5160, test loss-1.7453, acc-0.5063\n",
      "Iter-74120, train loss-1.8013, acc-0.5200, valid loss-1.7444, acc-0.5160, test loss-1.7452, acc-0.5064\n",
      "Iter-74130, train loss-1.8402, acc-0.4000, valid loss-1.7444, acc-0.5160, test loss-1.7452, acc-0.5064\n",
      "Iter-74140, train loss-1.8415, acc-0.5200, valid loss-1.7444, acc-0.5160, test loss-1.7452, acc-0.5063\n",
      "Iter-74150, train loss-1.6940, acc-0.5200, valid loss-1.7443, acc-0.5160, test loss-1.7451, acc-0.5062\n",
      "Iter-74160, train loss-1.8741, acc-0.4400, valid loss-1.7443, acc-0.5160, test loss-1.7451, acc-0.5062\n",
      "Iter-74170, train loss-1.7325, acc-0.5800, valid loss-1.7442, acc-0.5160, test loss-1.7450, acc-0.5062\n",
      "Iter-74180, train loss-1.6610, acc-0.6400, valid loss-1.7442, acc-0.5164, test loss-1.7450, acc-0.5063\n",
      "Iter-74190, train loss-1.7960, acc-0.5600, valid loss-1.7442, acc-0.5162, test loss-1.7450, acc-0.5063\n",
      "Iter-74200, train loss-1.7227, acc-0.5600, valid loss-1.7441, acc-0.5164, test loss-1.7449, acc-0.5065\n",
      "Iter-74210, train loss-1.6429, acc-0.6000, valid loss-1.7441, acc-0.5164, test loss-1.7449, acc-0.5066\n",
      "Iter-74220, train loss-1.8174, acc-0.4800, valid loss-1.7440, acc-0.5164, test loss-1.7448, acc-0.5065\n",
      "Iter-74230, train loss-1.7809, acc-0.5000, valid loss-1.7440, acc-0.5164, test loss-1.7448, acc-0.5067\n",
      "Iter-74240, train loss-1.6940, acc-0.6200, valid loss-1.7440, acc-0.5164, test loss-1.7447, acc-0.5063\n",
      "Iter-74250, train loss-1.7991, acc-0.3800, valid loss-1.7439, acc-0.5164, test loss-1.7447, acc-0.5064\n",
      "Iter-74260, train loss-1.6283, acc-0.6400, valid loss-1.7439, acc-0.5162, test loss-1.7447, acc-0.5064\n",
      "Iter-74270, train loss-1.7753, acc-0.4600, valid loss-1.7438, acc-0.5164, test loss-1.7446, acc-0.5064\n",
      "Iter-74280, train loss-1.8209, acc-0.5000, valid loss-1.7438, acc-0.5162, test loss-1.7446, acc-0.5064\n",
      "Iter-74290, train loss-1.8044, acc-0.4000, valid loss-1.7438, acc-0.5162, test loss-1.7445, acc-0.5066\n",
      "Iter-74300, train loss-1.7318, acc-0.4800, valid loss-1.7437, acc-0.5162, test loss-1.7445, acc-0.5066\n",
      "Iter-74310, train loss-1.8045, acc-0.5200, valid loss-1.7437, acc-0.5164, test loss-1.7445, acc-0.5065\n",
      "Iter-74320, train loss-1.7838, acc-0.4800, valid loss-1.7436, acc-0.5164, test loss-1.7444, acc-0.5064\n",
      "Iter-74330, train loss-1.7708, acc-0.6000, valid loss-1.7436, acc-0.5166, test loss-1.7444, acc-0.5064\n",
      "Iter-74340, train loss-1.6558, acc-0.5600, valid loss-1.7436, acc-0.5166, test loss-1.7443, acc-0.5064\n",
      "Iter-74350, train loss-1.6285, acc-0.5800, valid loss-1.7435, acc-0.5166, test loss-1.7443, acc-0.5064\n",
      "Iter-74360, train loss-1.7422, acc-0.4800, valid loss-1.7435, acc-0.5166, test loss-1.7443, acc-0.5065\n",
      "Iter-74370, train loss-1.7711, acc-0.4600, valid loss-1.7434, acc-0.5166, test loss-1.7442, acc-0.5064\n",
      "Iter-74380, train loss-1.7468, acc-0.6400, valid loss-1.7434, acc-0.5166, test loss-1.7442, acc-0.5064\n",
      "Iter-74390, train loss-1.8227, acc-0.4200, valid loss-1.7433, acc-0.5166, test loss-1.7441, acc-0.5064\n",
      "Iter-74400, train loss-1.6603, acc-0.5400, valid loss-1.7433, acc-0.5166, test loss-1.7441, acc-0.5062\n",
      "Iter-74410, train loss-1.7970, acc-0.4800, valid loss-1.7433, acc-0.5166, test loss-1.7441, acc-0.5063\n",
      "Iter-74420, train loss-1.8346, acc-0.4800, valid loss-1.7432, acc-0.5166, test loss-1.7440, acc-0.5063\n",
      "Iter-74430, train loss-1.7876, acc-0.4400, valid loss-1.7432, acc-0.5166, test loss-1.7440, acc-0.5063\n",
      "Iter-74440, train loss-1.7112, acc-0.4800, valid loss-1.7431, acc-0.5166, test loss-1.7439, acc-0.5063\n",
      "Iter-74450, train loss-1.8479, acc-0.4400, valid loss-1.7431, acc-0.5166, test loss-1.7439, acc-0.5063\n",
      "Iter-74460, train loss-1.8448, acc-0.4600, valid loss-1.7431, acc-0.5164, test loss-1.7439, acc-0.5063\n",
      "Iter-74470, train loss-1.8388, acc-0.3800, valid loss-1.7430, acc-0.5166, test loss-1.7438, acc-0.5065\n",
      "Iter-74480, train loss-1.7012, acc-0.5000, valid loss-1.7430, acc-0.5166, test loss-1.7438, acc-0.5066\n",
      "Iter-74490, train loss-1.6153, acc-0.5600, valid loss-1.7429, acc-0.5166, test loss-1.7437, acc-0.5066\n",
      "Iter-74500, train loss-1.6778, acc-0.6200, valid loss-1.7429, acc-0.5164, test loss-1.7437, acc-0.5066\n",
      "Iter-74510, train loss-1.7711, acc-0.4400, valid loss-1.7428, acc-0.5164, test loss-1.7437, acc-0.5065\n",
      "Iter-74520, train loss-1.8427, acc-0.4200, valid loss-1.7428, acc-0.5164, test loss-1.7436, acc-0.5066\n",
      "Iter-74530, train loss-1.8367, acc-0.4000, valid loss-1.7428, acc-0.5166, test loss-1.7436, acc-0.5064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-74540, train loss-1.6431, acc-0.6800, valid loss-1.7427, acc-0.5166, test loss-1.7435, acc-0.5065\n",
      "Iter-74550, train loss-1.7926, acc-0.4600, valid loss-1.7427, acc-0.5168, test loss-1.7435, acc-0.5064\n",
      "Iter-74560, train loss-1.7312, acc-0.5600, valid loss-1.7426, acc-0.5166, test loss-1.7435, acc-0.5063\n",
      "Iter-74570, train loss-1.6901, acc-0.5200, valid loss-1.7426, acc-0.5166, test loss-1.7434, acc-0.5065\n",
      "Iter-74580, train loss-1.7161, acc-0.5600, valid loss-1.7425, acc-0.5166, test loss-1.7434, acc-0.5062\n",
      "Iter-74590, train loss-1.6330, acc-0.6800, valid loss-1.7425, acc-0.5166, test loss-1.7433, acc-0.5065\n",
      "Iter-74600, train loss-1.7794, acc-0.4000, valid loss-1.7425, acc-0.5166, test loss-1.7433, acc-0.5065\n",
      "Iter-74610, train loss-1.7905, acc-0.4200, valid loss-1.7424, acc-0.5168, test loss-1.7433, acc-0.5065\n",
      "Iter-74620, train loss-1.7648, acc-0.5600, valid loss-1.7424, acc-0.5170, test loss-1.7432, acc-0.5065\n",
      "Iter-74630, train loss-1.7103, acc-0.5800, valid loss-1.7423, acc-0.5166, test loss-1.7432, acc-0.5066\n",
      "Iter-74640, train loss-1.7096, acc-0.5200, valid loss-1.7423, acc-0.5170, test loss-1.7431, acc-0.5065\n",
      "Iter-74650, train loss-1.8106, acc-0.3000, valid loss-1.7423, acc-0.5170, test loss-1.7431, acc-0.5064\n",
      "Iter-74660, train loss-1.8325, acc-0.3800, valid loss-1.7422, acc-0.5168, test loss-1.7430, acc-0.5066\n",
      "Iter-74670, train loss-1.7518, acc-0.5400, valid loss-1.7422, acc-0.5166, test loss-1.7430, acc-0.5066\n",
      "Iter-74680, train loss-1.8227, acc-0.5200, valid loss-1.7421, acc-0.5166, test loss-1.7430, acc-0.5065\n",
      "Iter-74690, train loss-1.7249, acc-0.5400, valid loss-1.7421, acc-0.5166, test loss-1.7429, acc-0.5068\n",
      "Iter-74700, train loss-1.8488, acc-0.4400, valid loss-1.7421, acc-0.5166, test loss-1.7429, acc-0.5070\n",
      "Iter-74710, train loss-1.6497, acc-0.5600, valid loss-1.7420, acc-0.5166, test loss-1.7428, acc-0.5068\n",
      "Iter-74720, train loss-1.7695, acc-0.4800, valid loss-1.7420, acc-0.5166, test loss-1.7428, acc-0.5068\n",
      "Iter-74730, train loss-1.8161, acc-0.4400, valid loss-1.7419, acc-0.5166, test loss-1.7428, acc-0.5072\n",
      "Iter-74740, train loss-1.8682, acc-0.4800, valid loss-1.7419, acc-0.5166, test loss-1.7427, acc-0.5071\n",
      "Iter-74750, train loss-1.6890, acc-0.5600, valid loss-1.7419, acc-0.5166, test loss-1.7427, acc-0.5071\n",
      "Iter-74760, train loss-1.8275, acc-0.4200, valid loss-1.7418, acc-0.5166, test loss-1.7426, acc-0.5070\n",
      "Iter-74770, train loss-1.8335, acc-0.3600, valid loss-1.7418, acc-0.5166, test loss-1.7426, acc-0.5069\n",
      "Iter-74780, train loss-1.7819, acc-0.3800, valid loss-1.7417, acc-0.5166, test loss-1.7426, acc-0.5071\n",
      "Iter-74790, train loss-1.8008, acc-0.3800, valid loss-1.7417, acc-0.5166, test loss-1.7425, acc-0.5071\n",
      "Iter-74800, train loss-1.7561, acc-0.5000, valid loss-1.7417, acc-0.5166, test loss-1.7425, acc-0.5073\n",
      "Iter-74810, train loss-1.8620, acc-0.5200, valid loss-1.7416, acc-0.5166, test loss-1.7424, acc-0.5073\n",
      "Iter-74820, train loss-1.8809, acc-0.5000, valid loss-1.7416, acc-0.5164, test loss-1.7424, acc-0.5072\n",
      "Iter-74830, train loss-1.7859, acc-0.5200, valid loss-1.7415, acc-0.5164, test loss-1.7423, acc-0.5072\n",
      "Iter-74840, train loss-1.7599, acc-0.5000, valid loss-1.7415, acc-0.5164, test loss-1.7423, acc-0.5071\n",
      "Iter-74850, train loss-1.7055, acc-0.5600, valid loss-1.7415, acc-0.5164, test loss-1.7423, acc-0.5071\n",
      "Iter-74860, train loss-1.8920, acc-0.4600, valid loss-1.7414, acc-0.5162, test loss-1.7422, acc-0.5073\n",
      "Iter-74870, train loss-1.8080, acc-0.4000, valid loss-1.7414, acc-0.5162, test loss-1.7422, acc-0.5073\n",
      "Iter-74880, train loss-1.7054, acc-0.6000, valid loss-1.7413, acc-0.5162, test loss-1.7421, acc-0.5074\n",
      "Iter-74890, train loss-1.8249, acc-0.5400, valid loss-1.7413, acc-0.5164, test loss-1.7421, acc-0.5077\n",
      "Iter-74900, train loss-1.5936, acc-0.6400, valid loss-1.7413, acc-0.5162, test loss-1.7421, acc-0.5076\n",
      "Iter-74910, train loss-1.7401, acc-0.5200, valid loss-1.7412, acc-0.5162, test loss-1.7420, acc-0.5075\n",
      "Iter-74920, train loss-1.8842, acc-0.3400, valid loss-1.7412, acc-0.5164, test loss-1.7420, acc-0.5075\n",
      "Iter-74930, train loss-1.7411, acc-0.5200, valid loss-1.7411, acc-0.5162, test loss-1.7419, acc-0.5074\n",
      "Iter-74940, train loss-1.7394, acc-0.5800, valid loss-1.7411, acc-0.5164, test loss-1.7419, acc-0.5075\n",
      "Iter-74950, train loss-1.7855, acc-0.5200, valid loss-1.7410, acc-0.5162, test loss-1.7419, acc-0.5076\n",
      "Iter-74960, train loss-1.8288, acc-0.5000, valid loss-1.7410, acc-0.5162, test loss-1.7418, acc-0.5075\n",
      "Iter-74970, train loss-1.7735, acc-0.5200, valid loss-1.7410, acc-0.5162, test loss-1.7418, acc-0.5077\n",
      "Iter-74980, train loss-1.8020, acc-0.5000, valid loss-1.7409, acc-0.5164, test loss-1.7417, acc-0.5078\n",
      "Iter-74990, train loss-1.6965, acc-0.5400, valid loss-1.7409, acc-0.5166, test loss-1.7417, acc-0.5075\n",
      "Iter-75000, train loss-1.7548, acc-0.5400, valid loss-1.7409, acc-0.5166, test loss-1.7417, acc-0.5075\n",
      "Iter-75010, train loss-1.7131, acc-0.5400, valid loss-1.7408, acc-0.5166, test loss-1.7416, acc-0.5076\n",
      "Iter-75020, train loss-1.7298, acc-0.5800, valid loss-1.7408, acc-0.5166, test loss-1.7416, acc-0.5077\n",
      "Iter-75030, train loss-1.7002, acc-0.5400, valid loss-1.7407, acc-0.5166, test loss-1.7415, acc-0.5075\n",
      "Iter-75040, train loss-1.8307, acc-0.4800, valid loss-1.7407, acc-0.5166, test loss-1.7415, acc-0.5076\n",
      "Iter-75050, train loss-1.7484, acc-0.5600, valid loss-1.7406, acc-0.5166, test loss-1.7415, acc-0.5077\n",
      "Iter-75060, train loss-1.8320, acc-0.4600, valid loss-1.7406, acc-0.5166, test loss-1.7414, acc-0.5077\n",
      "Iter-75070, train loss-1.7903, acc-0.4400, valid loss-1.7406, acc-0.5166, test loss-1.7414, acc-0.5078\n",
      "Iter-75080, train loss-1.6792, acc-0.5400, valid loss-1.7405, acc-0.5166, test loss-1.7413, acc-0.5078\n",
      "Iter-75090, train loss-1.7325, acc-0.6400, valid loss-1.7405, acc-0.5166, test loss-1.7413, acc-0.5079\n",
      "Iter-75100, train loss-1.6749, acc-0.5800, valid loss-1.7404, acc-0.5166, test loss-1.7413, acc-0.5080\n",
      "Iter-75110, train loss-1.6763, acc-0.6600, valid loss-1.7404, acc-0.5168, test loss-1.7412, acc-0.5078\n",
      "Iter-75120, train loss-1.8105, acc-0.4000, valid loss-1.7404, acc-0.5168, test loss-1.7412, acc-0.5080\n",
      "Iter-75130, train loss-1.8216, acc-0.4200, valid loss-1.7403, acc-0.5168, test loss-1.7411, acc-0.5079\n",
      "Iter-75140, train loss-1.8167, acc-0.4800, valid loss-1.7403, acc-0.5168, test loss-1.7411, acc-0.5079\n",
      "Iter-75150, train loss-1.7584, acc-0.4600, valid loss-1.7402, acc-0.5168, test loss-1.7411, acc-0.5079\n",
      "Iter-75160, train loss-1.7258, acc-0.6200, valid loss-1.7402, acc-0.5168, test loss-1.7410, acc-0.5078\n",
      "Iter-75170, train loss-1.6837, acc-0.6200, valid loss-1.7401, acc-0.5168, test loss-1.7410, acc-0.5079\n",
      "Iter-75180, train loss-1.6545, acc-0.5800, valid loss-1.7401, acc-0.5168, test loss-1.7409, acc-0.5079\n",
      "Iter-75190, train loss-1.8919, acc-0.4000, valid loss-1.7401, acc-0.5168, test loss-1.7409, acc-0.5079\n",
      "Iter-75200, train loss-1.6271, acc-0.6200, valid loss-1.7400, acc-0.5166, test loss-1.7408, acc-0.5079\n",
      "Iter-75210, train loss-1.8225, acc-0.5000, valid loss-1.7400, acc-0.5166, test loss-1.7408, acc-0.5080\n",
      "Iter-75220, train loss-1.7098, acc-0.5200, valid loss-1.7400, acc-0.5164, test loss-1.7408, acc-0.5080\n",
      "Iter-75230, train loss-1.8248, acc-0.4600, valid loss-1.7399, acc-0.5166, test loss-1.7407, acc-0.5079\n",
      "Iter-75240, train loss-1.8591, acc-0.3200, valid loss-1.7399, acc-0.5166, test loss-1.7407, acc-0.5079\n",
      "Iter-75250, train loss-1.7537, acc-0.4600, valid loss-1.7398, acc-0.5164, test loss-1.7407, acc-0.5080\n",
      "Iter-75260, train loss-1.6640, acc-0.6000, valid loss-1.7398, acc-0.5166, test loss-1.7406, acc-0.5078\n",
      "Iter-75270, train loss-1.7683, acc-0.5000, valid loss-1.7398, acc-0.5166, test loss-1.7406, acc-0.5078\n",
      "Iter-75280, train loss-1.7517, acc-0.5000, valid loss-1.7397, acc-0.5166, test loss-1.7405, acc-0.5075\n",
      "Iter-75290, train loss-1.7079, acc-0.5600, valid loss-1.7397, acc-0.5166, test loss-1.7405, acc-0.5076\n",
      "Iter-75300, train loss-1.8599, acc-0.4400, valid loss-1.7396, acc-0.5166, test loss-1.7405, acc-0.5075\n",
      "Iter-75310, train loss-1.7924, acc-0.4600, valid loss-1.7396, acc-0.5168, test loss-1.7404, acc-0.5075\n",
      "Iter-75320, train loss-1.7320, acc-0.5000, valid loss-1.7396, acc-0.5168, test loss-1.7404, acc-0.5073\n",
      "Iter-75330, train loss-1.7980, acc-0.5000, valid loss-1.7395, acc-0.5170, test loss-1.7403, acc-0.5075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-75340, train loss-1.7938, acc-0.4800, valid loss-1.7395, acc-0.5172, test loss-1.7403, acc-0.5076\n",
      "Iter-75350, train loss-1.6655, acc-0.6200, valid loss-1.7394, acc-0.5168, test loss-1.7403, acc-0.5075\n",
      "Iter-75360, train loss-1.7649, acc-0.5200, valid loss-1.7394, acc-0.5168, test loss-1.7402, acc-0.5075\n",
      "Iter-75370, train loss-1.8046, acc-0.4200, valid loss-1.7394, acc-0.5170, test loss-1.7402, acc-0.5077\n",
      "Iter-75380, train loss-1.7349, acc-0.5000, valid loss-1.7393, acc-0.5174, test loss-1.7401, acc-0.5077\n",
      "Iter-75390, train loss-1.7808, acc-0.4600, valid loss-1.7393, acc-0.5168, test loss-1.7401, acc-0.5076\n",
      "Iter-75400, train loss-1.7011, acc-0.6000, valid loss-1.7392, acc-0.5168, test loss-1.7400, acc-0.5076\n",
      "Iter-75410, train loss-1.7756, acc-0.4400, valid loss-1.7392, acc-0.5170, test loss-1.7400, acc-0.5075\n",
      "Iter-75420, train loss-1.7690, acc-0.5200, valid loss-1.7392, acc-0.5168, test loss-1.7400, acc-0.5076\n",
      "Iter-75430, train loss-1.8583, acc-0.4400, valid loss-1.7391, acc-0.5168, test loss-1.7399, acc-0.5076\n",
      "Iter-75440, train loss-1.7682, acc-0.4800, valid loss-1.7391, acc-0.5168, test loss-1.7399, acc-0.5076\n",
      "Iter-75450, train loss-1.6882, acc-0.5200, valid loss-1.7390, acc-0.5170, test loss-1.7399, acc-0.5077\n",
      "Iter-75460, train loss-1.7435, acc-0.5200, valid loss-1.7390, acc-0.5168, test loss-1.7398, acc-0.5078\n",
      "Iter-75470, train loss-1.7394, acc-0.4600, valid loss-1.7390, acc-0.5168, test loss-1.7398, acc-0.5078\n",
      "Iter-75480, train loss-1.7966, acc-0.4600, valid loss-1.7389, acc-0.5172, test loss-1.7397, acc-0.5078\n",
      "Iter-75490, train loss-1.7415, acc-0.5400, valid loss-1.7389, acc-0.5172, test loss-1.7397, acc-0.5079\n",
      "Iter-75500, train loss-1.8018, acc-0.5000, valid loss-1.7388, acc-0.5174, test loss-1.7397, acc-0.5078\n",
      "Iter-75510, train loss-1.7136, acc-0.6000, valid loss-1.7388, acc-0.5176, test loss-1.7396, acc-0.5077\n",
      "Iter-75520, train loss-1.7266, acc-0.6000, valid loss-1.7388, acc-0.5176, test loss-1.7396, acc-0.5078\n",
      "Iter-75530, train loss-1.7029, acc-0.5200, valid loss-1.7387, acc-0.5172, test loss-1.7395, acc-0.5079\n",
      "Iter-75540, train loss-1.7606, acc-0.4400, valid loss-1.7387, acc-0.5170, test loss-1.7395, acc-0.5080\n",
      "Iter-75550, train loss-1.8175, acc-0.4200, valid loss-1.7386, acc-0.5170, test loss-1.7395, acc-0.5079\n",
      "Iter-75560, train loss-1.5971, acc-0.5600, valid loss-1.7386, acc-0.5172, test loss-1.7394, acc-0.5078\n",
      "Iter-75570, train loss-1.7259, acc-0.5000, valid loss-1.7386, acc-0.5174, test loss-1.7394, acc-0.5078\n",
      "Iter-75580, train loss-1.7728, acc-0.5600, valid loss-1.7385, acc-0.5172, test loss-1.7393, acc-0.5078\n",
      "Iter-75590, train loss-1.8184, acc-0.4000, valid loss-1.7385, acc-0.5172, test loss-1.7393, acc-0.5079\n",
      "Iter-75600, train loss-1.6911, acc-0.5600, valid loss-1.7384, acc-0.5176, test loss-1.7392, acc-0.5079\n",
      "Iter-75610, train loss-1.6856, acc-0.4800, valid loss-1.7384, acc-0.5176, test loss-1.7392, acc-0.5079\n",
      "Iter-75620, train loss-1.8199, acc-0.4000, valid loss-1.7383, acc-0.5176, test loss-1.7392, acc-0.5079\n",
      "Iter-75630, train loss-1.6734, acc-0.5400, valid loss-1.7383, acc-0.5176, test loss-1.7391, acc-0.5079\n",
      "Iter-75640, train loss-1.7724, acc-0.4800, valid loss-1.7383, acc-0.5176, test loss-1.7391, acc-0.5080\n",
      "Iter-75650, train loss-1.7671, acc-0.5400, valid loss-1.7382, acc-0.5174, test loss-1.7390, acc-0.5081\n",
      "Iter-75660, train loss-1.7178, acc-0.5400, valid loss-1.7382, acc-0.5174, test loss-1.7390, acc-0.5080\n",
      "Iter-75670, train loss-1.8045, acc-0.4200, valid loss-1.7381, acc-0.5172, test loss-1.7390, acc-0.5080\n",
      "Iter-75680, train loss-1.6102, acc-0.5200, valid loss-1.7381, acc-0.5170, test loss-1.7389, acc-0.5080\n",
      "Iter-75690, train loss-1.7416, acc-0.4800, valid loss-1.7381, acc-0.5174, test loss-1.7389, acc-0.5080\n",
      "Iter-75700, train loss-1.7383, acc-0.5200, valid loss-1.7380, acc-0.5172, test loss-1.7388, acc-0.5079\n",
      "Iter-75710, train loss-1.7683, acc-0.3800, valid loss-1.7380, acc-0.5172, test loss-1.7388, acc-0.5079\n",
      "Iter-75720, train loss-1.6542, acc-0.5600, valid loss-1.7379, acc-0.5172, test loss-1.7388, acc-0.5079\n",
      "Iter-75730, train loss-1.6954, acc-0.5600, valid loss-1.7379, acc-0.5170, test loss-1.7387, acc-0.5079\n",
      "Iter-75740, train loss-1.7470, acc-0.4800, valid loss-1.7379, acc-0.5172, test loss-1.7387, acc-0.5078\n",
      "Iter-75750, train loss-1.7547, acc-0.5000, valid loss-1.7378, acc-0.5172, test loss-1.7386, acc-0.5078\n",
      "Iter-75760, train loss-1.7553, acc-0.5400, valid loss-1.7378, acc-0.5172, test loss-1.7386, acc-0.5078\n",
      "Iter-75770, train loss-1.7533, acc-0.5400, valid loss-1.7377, acc-0.5172, test loss-1.7386, acc-0.5079\n",
      "Iter-75780, train loss-1.7256, acc-0.5000, valid loss-1.7377, acc-0.5172, test loss-1.7385, acc-0.5080\n",
      "Iter-75790, train loss-1.7229, acc-0.4800, valid loss-1.7377, acc-0.5172, test loss-1.7385, acc-0.5080\n",
      "Iter-75800, train loss-1.8299, acc-0.3400, valid loss-1.7376, acc-0.5174, test loss-1.7384, acc-0.5080\n",
      "Iter-75810, train loss-1.7956, acc-0.5000, valid loss-1.7376, acc-0.5172, test loss-1.7384, acc-0.5080\n",
      "Iter-75820, train loss-1.8054, acc-0.4600, valid loss-1.7375, acc-0.5174, test loss-1.7384, acc-0.5080\n",
      "Iter-75830, train loss-1.5789, acc-0.6600, valid loss-1.7375, acc-0.5174, test loss-1.7383, acc-0.5081\n",
      "Iter-75840, train loss-1.8577, acc-0.4000, valid loss-1.7375, acc-0.5174, test loss-1.7383, acc-0.5081\n",
      "Iter-75850, train loss-1.7548, acc-0.5000, valid loss-1.7374, acc-0.5170, test loss-1.7382, acc-0.5079\n",
      "Iter-75860, train loss-1.7537, acc-0.5000, valid loss-1.7374, acc-0.5174, test loss-1.7382, acc-0.5080\n",
      "Iter-75870, train loss-1.8467, acc-0.3800, valid loss-1.7373, acc-0.5172, test loss-1.7382, acc-0.5080\n",
      "Iter-75880, train loss-1.7399, acc-0.4400, valid loss-1.7373, acc-0.5172, test loss-1.7381, acc-0.5081\n",
      "Iter-75890, train loss-1.7048, acc-0.5600, valid loss-1.7372, acc-0.5172, test loss-1.7381, acc-0.5080\n",
      "Iter-75900, train loss-1.7622, acc-0.5200, valid loss-1.7372, acc-0.5172, test loss-1.7380, acc-0.5082\n",
      "Iter-75910, train loss-1.6883, acc-0.6000, valid loss-1.7372, acc-0.5170, test loss-1.7380, acc-0.5081\n",
      "Iter-75920, train loss-1.6342, acc-0.5600, valid loss-1.7371, acc-0.5170, test loss-1.7379, acc-0.5081\n",
      "Iter-75930, train loss-1.7414, acc-0.5400, valid loss-1.7371, acc-0.5170, test loss-1.7379, acc-0.5083\n",
      "Iter-75940, train loss-1.7584, acc-0.3600, valid loss-1.7370, acc-0.5170, test loss-1.7379, acc-0.5083\n",
      "Iter-75950, train loss-1.8472, acc-0.4800, valid loss-1.7370, acc-0.5170, test loss-1.7378, acc-0.5081\n",
      "Iter-75960, train loss-1.7770, acc-0.5400, valid loss-1.7370, acc-0.5172, test loss-1.7378, acc-0.5081\n",
      "Iter-75970, train loss-1.8360, acc-0.4800, valid loss-1.7369, acc-0.5172, test loss-1.7377, acc-0.5082\n",
      "Iter-75980, train loss-1.6277, acc-0.6400, valid loss-1.7369, acc-0.5170, test loss-1.7377, acc-0.5082\n",
      "Iter-75990, train loss-1.6866, acc-0.5000, valid loss-1.7369, acc-0.5172, test loss-1.7377, acc-0.5082\n",
      "Iter-76000, train loss-1.7307, acc-0.5200, valid loss-1.7368, acc-0.5174, test loss-1.7376, acc-0.5082\n",
      "Iter-76010, train loss-1.7737, acc-0.5400, valid loss-1.7368, acc-0.5172, test loss-1.7376, acc-0.5084\n",
      "Iter-76020, train loss-1.7727, acc-0.5000, valid loss-1.7367, acc-0.5170, test loss-1.7376, acc-0.5084\n",
      "Iter-76030, train loss-1.7729, acc-0.5400, valid loss-1.7367, acc-0.5172, test loss-1.7375, acc-0.5082\n",
      "Iter-76040, train loss-1.7095, acc-0.5800, valid loss-1.7367, acc-0.5174, test loss-1.7375, acc-0.5082\n",
      "Iter-76050, train loss-1.6557, acc-0.5800, valid loss-1.7366, acc-0.5176, test loss-1.7374, acc-0.5081\n",
      "Iter-76060, train loss-1.9250, acc-0.3800, valid loss-1.7366, acc-0.5176, test loss-1.7374, acc-0.5082\n",
      "Iter-76070, train loss-1.6965, acc-0.5400, valid loss-1.7365, acc-0.5182, test loss-1.7374, acc-0.5081\n",
      "Iter-76080, train loss-1.7568, acc-0.5400, valid loss-1.7365, acc-0.5180, test loss-1.7373, acc-0.5080\n",
      "Iter-76090, train loss-1.5517, acc-0.6600, valid loss-1.7365, acc-0.5180, test loss-1.7373, acc-0.5082\n",
      "Iter-76100, train loss-1.7816, acc-0.4000, valid loss-1.7364, acc-0.5180, test loss-1.7372, acc-0.5081\n",
      "Iter-76110, train loss-1.7622, acc-0.4000, valid loss-1.7364, acc-0.5180, test loss-1.7372, acc-0.5081\n",
      "Iter-76120, train loss-1.7242, acc-0.5200, valid loss-1.7363, acc-0.5182, test loss-1.7372, acc-0.5083\n",
      "Iter-76130, train loss-1.6505, acc-0.6200, valid loss-1.7363, acc-0.5182, test loss-1.7371, acc-0.5082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-76140, train loss-1.8102, acc-0.4800, valid loss-1.7363, acc-0.5184, test loss-1.7371, acc-0.5084\n",
      "Iter-76150, train loss-1.6875, acc-0.5800, valid loss-1.7362, acc-0.5182, test loss-1.7370, acc-0.5082\n",
      "Iter-76160, train loss-1.6788, acc-0.6200, valid loss-1.7362, acc-0.5182, test loss-1.7370, acc-0.5081\n",
      "Iter-76170, train loss-1.7333, acc-0.4400, valid loss-1.7361, acc-0.5182, test loss-1.7370, acc-0.5082\n",
      "Iter-76180, train loss-1.8499, acc-0.4800, valid loss-1.7361, acc-0.5184, test loss-1.7369, acc-0.5081\n",
      "Iter-76190, train loss-1.8464, acc-0.4000, valid loss-1.7361, acc-0.5182, test loss-1.7369, acc-0.5082\n",
      "Iter-76200, train loss-1.6406, acc-0.6200, valid loss-1.7360, acc-0.5182, test loss-1.7368, acc-0.5082\n",
      "Iter-76210, train loss-1.7174, acc-0.5200, valid loss-1.7360, acc-0.5182, test loss-1.7368, acc-0.5081\n",
      "Iter-76220, train loss-1.7151, acc-0.5600, valid loss-1.7359, acc-0.5182, test loss-1.7368, acc-0.5081\n",
      "Iter-76230, train loss-1.7162, acc-0.5600, valid loss-1.7359, acc-0.5184, test loss-1.7367, acc-0.5082\n",
      "Iter-76240, train loss-1.6723, acc-0.5000, valid loss-1.7358, acc-0.5184, test loss-1.7367, acc-0.5082\n",
      "Iter-76250, train loss-1.6008, acc-0.6800, valid loss-1.7358, acc-0.5184, test loss-1.7366, acc-0.5083\n",
      "Iter-76260, train loss-1.7139, acc-0.5600, valid loss-1.7358, acc-0.5184, test loss-1.7366, acc-0.5085\n",
      "Iter-76270, train loss-1.8238, acc-0.4400, valid loss-1.7357, acc-0.5184, test loss-1.7366, acc-0.5082\n",
      "Iter-76280, train loss-1.7405, acc-0.5400, valid loss-1.7357, acc-0.5184, test loss-1.7365, acc-0.5083\n",
      "Iter-76290, train loss-1.7739, acc-0.4600, valid loss-1.7356, acc-0.5184, test loss-1.7365, acc-0.5081\n",
      "Iter-76300, train loss-1.8266, acc-0.5000, valid loss-1.7356, acc-0.5184, test loss-1.7364, acc-0.5082\n",
      "Iter-76310, train loss-1.7805, acc-0.4200, valid loss-1.7356, acc-0.5186, test loss-1.7364, acc-0.5081\n",
      "Iter-76320, train loss-1.7539, acc-0.4600, valid loss-1.7355, acc-0.5184, test loss-1.7364, acc-0.5083\n",
      "Iter-76330, train loss-1.6854, acc-0.5800, valid loss-1.7355, acc-0.5184, test loss-1.7363, acc-0.5081\n",
      "Iter-76340, train loss-1.7530, acc-0.6000, valid loss-1.7355, acc-0.5182, test loss-1.7363, acc-0.5081\n",
      "Iter-76350, train loss-1.6821, acc-0.5800, valid loss-1.7354, acc-0.5182, test loss-1.7362, acc-0.5082\n",
      "Iter-76360, train loss-1.7683, acc-0.4000, valid loss-1.7354, acc-0.5182, test loss-1.7362, acc-0.5081\n",
      "Iter-76370, train loss-1.7682, acc-0.5400, valid loss-1.7353, acc-0.5182, test loss-1.7362, acc-0.5081\n",
      "Iter-76380, train loss-1.7713, acc-0.5400, valid loss-1.7353, acc-0.5184, test loss-1.7361, acc-0.5083\n",
      "Iter-76390, train loss-1.7167, acc-0.5600, valid loss-1.7353, acc-0.5184, test loss-1.7361, acc-0.5085\n",
      "Iter-76400, train loss-1.8583, acc-0.6000, valid loss-1.7352, acc-0.5186, test loss-1.7360, acc-0.5084\n",
      "Iter-76410, train loss-1.6942, acc-0.5400, valid loss-1.7352, acc-0.5184, test loss-1.7360, acc-0.5084\n",
      "Iter-76420, train loss-1.6354, acc-0.7000, valid loss-1.7351, acc-0.5184, test loss-1.7360, acc-0.5085\n",
      "Iter-76430, train loss-1.7524, acc-0.5000, valid loss-1.7351, acc-0.5182, test loss-1.7359, acc-0.5086\n",
      "Iter-76440, train loss-1.8210, acc-0.4600, valid loss-1.7350, acc-0.5182, test loss-1.7359, acc-0.5085\n",
      "Iter-76450, train loss-1.8156, acc-0.4600, valid loss-1.7350, acc-0.5182, test loss-1.7358, acc-0.5084\n",
      "Iter-76460, train loss-1.7599, acc-0.5400, valid loss-1.7350, acc-0.5186, test loss-1.7358, acc-0.5084\n",
      "Iter-76470, train loss-1.7819, acc-0.5200, valid loss-1.7349, acc-0.5186, test loss-1.7357, acc-0.5084\n",
      "Iter-76480, train loss-1.7490, acc-0.5400, valid loss-1.7349, acc-0.5188, test loss-1.7357, acc-0.5084\n",
      "Iter-76490, train loss-1.8419, acc-0.4600, valid loss-1.7349, acc-0.5186, test loss-1.7357, acc-0.5084\n",
      "Iter-76500, train loss-1.6415, acc-0.5400, valid loss-1.7348, acc-0.5186, test loss-1.7356, acc-0.5085\n",
      "Iter-76510, train loss-1.8157, acc-0.5800, valid loss-1.7348, acc-0.5186, test loss-1.7356, acc-0.5084\n",
      "Iter-76520, train loss-1.7209, acc-0.5600, valid loss-1.7347, acc-0.5186, test loss-1.7356, acc-0.5084\n",
      "Iter-76530, train loss-1.7621, acc-0.5200, valid loss-1.7347, acc-0.5186, test loss-1.7355, acc-0.5083\n",
      "Iter-76540, train loss-1.8878, acc-0.4000, valid loss-1.7347, acc-0.5188, test loss-1.7355, acc-0.5083\n",
      "Iter-76550, train loss-1.6891, acc-0.5600, valid loss-1.7346, acc-0.5190, test loss-1.7354, acc-0.5083\n",
      "Iter-76560, train loss-1.6921, acc-0.6000, valid loss-1.7346, acc-0.5190, test loss-1.7354, acc-0.5084\n",
      "Iter-76570, train loss-1.7764, acc-0.4000, valid loss-1.7345, acc-0.5192, test loss-1.7354, acc-0.5085\n",
      "Iter-76580, train loss-1.8532, acc-0.5000, valid loss-1.7345, acc-0.5190, test loss-1.7353, acc-0.5085\n",
      "Iter-76590, train loss-1.8380, acc-0.3600, valid loss-1.7345, acc-0.5190, test loss-1.7353, acc-0.5085\n",
      "Iter-76600, train loss-1.7397, acc-0.4600, valid loss-1.7344, acc-0.5190, test loss-1.7352, acc-0.5085\n",
      "Iter-76610, train loss-1.8222, acc-0.4800, valid loss-1.7344, acc-0.5194, test loss-1.7352, acc-0.5084\n",
      "Iter-76620, train loss-1.7799, acc-0.4600, valid loss-1.7343, acc-0.5192, test loss-1.7352, acc-0.5085\n",
      "Iter-76630, train loss-1.7354, acc-0.4800, valid loss-1.7343, acc-0.5192, test loss-1.7351, acc-0.5087\n",
      "Iter-76640, train loss-1.7943, acc-0.5200, valid loss-1.7343, acc-0.5190, test loss-1.7351, acc-0.5087\n",
      "Iter-76650, train loss-1.6611, acc-0.5600, valid loss-1.7342, acc-0.5192, test loss-1.7350, acc-0.5087\n",
      "Iter-76660, train loss-1.7557, acc-0.4600, valid loss-1.7342, acc-0.5190, test loss-1.7350, acc-0.5087\n",
      "Iter-76670, train loss-1.8410, acc-0.4600, valid loss-1.7341, acc-0.5192, test loss-1.7350, acc-0.5087\n",
      "Iter-76680, train loss-1.8028, acc-0.4200, valid loss-1.7341, acc-0.5196, test loss-1.7349, acc-0.5086\n",
      "Iter-76690, train loss-1.6711, acc-0.6400, valid loss-1.7341, acc-0.5196, test loss-1.7349, acc-0.5086\n",
      "Iter-76700, train loss-1.7272, acc-0.5000, valid loss-1.7340, acc-0.5196, test loss-1.7348, acc-0.5086\n",
      "Iter-76710, train loss-1.7809, acc-0.4200, valid loss-1.7340, acc-0.5196, test loss-1.7348, acc-0.5086\n",
      "Iter-76720, train loss-1.7022, acc-0.5600, valid loss-1.7339, acc-0.5196, test loss-1.7348, acc-0.5086\n",
      "Iter-76730, train loss-1.7551, acc-0.5600, valid loss-1.7339, acc-0.5196, test loss-1.7347, acc-0.5087\n",
      "Iter-76740, train loss-1.8085, acc-0.4800, valid loss-1.7339, acc-0.5194, test loss-1.7347, acc-0.5087\n",
      "Iter-76750, train loss-1.7769, acc-0.4600, valid loss-1.7338, acc-0.5194, test loss-1.7346, acc-0.5087\n",
      "Iter-76760, train loss-1.6682, acc-0.6200, valid loss-1.7338, acc-0.5192, test loss-1.7346, acc-0.5088\n",
      "Iter-76770, train loss-1.6789, acc-0.5400, valid loss-1.7338, acc-0.5192, test loss-1.7346, acc-0.5087\n",
      "Iter-76780, train loss-1.7685, acc-0.5200, valid loss-1.7337, acc-0.5194, test loss-1.7345, acc-0.5089\n",
      "Iter-76790, train loss-1.8699, acc-0.3400, valid loss-1.7337, acc-0.5192, test loss-1.7345, acc-0.5089\n",
      "Iter-76800, train loss-1.8333, acc-0.3800, valid loss-1.7336, acc-0.5196, test loss-1.7345, acc-0.5089\n",
      "Iter-76810, train loss-1.8246, acc-0.4400, valid loss-1.7336, acc-0.5194, test loss-1.7344, acc-0.5088\n",
      "Iter-76820, train loss-1.8314, acc-0.4200, valid loss-1.7336, acc-0.5194, test loss-1.7344, acc-0.5088\n",
      "Iter-76830, train loss-1.8158, acc-0.3800, valid loss-1.7335, acc-0.5194, test loss-1.7343, acc-0.5088\n",
      "Iter-76840, train loss-1.7421, acc-0.5600, valid loss-1.7335, acc-0.5194, test loss-1.7343, acc-0.5089\n",
      "Iter-76850, train loss-1.7316, acc-0.6000, valid loss-1.7334, acc-0.5198, test loss-1.7343, acc-0.5089\n",
      "Iter-76860, train loss-1.7273, acc-0.5800, valid loss-1.7334, acc-0.5198, test loss-1.7342, acc-0.5089\n",
      "Iter-76870, train loss-1.7557, acc-0.4000, valid loss-1.7334, acc-0.5198, test loss-1.7342, acc-0.5089\n",
      "Iter-76880, train loss-1.8673, acc-0.4200, valid loss-1.7333, acc-0.5192, test loss-1.7341, acc-0.5089\n",
      "Iter-76890, train loss-1.7515, acc-0.5400, valid loss-1.7333, acc-0.5198, test loss-1.7341, acc-0.5088\n",
      "Iter-76900, train loss-1.8295, acc-0.5000, valid loss-1.7333, acc-0.5196, test loss-1.7341, acc-0.5088\n",
      "Iter-76910, train loss-1.7711, acc-0.4400, valid loss-1.7332, acc-0.5196, test loss-1.7340, acc-0.5088\n",
      "Iter-76920, train loss-1.7056, acc-0.5600, valid loss-1.7332, acc-0.5194, test loss-1.7340, acc-0.5087\n",
      "Iter-76930, train loss-1.6438, acc-0.6000, valid loss-1.7331, acc-0.5196, test loss-1.7339, acc-0.5087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-76940, train loss-1.8599, acc-0.4600, valid loss-1.7331, acc-0.5198, test loss-1.7339, acc-0.5087\n",
      "Iter-76950, train loss-1.6996, acc-0.5400, valid loss-1.7331, acc-0.5198, test loss-1.7339, acc-0.5087\n",
      "Iter-76960, train loss-1.6813, acc-0.5200, valid loss-1.7330, acc-0.5196, test loss-1.7338, acc-0.5087\n",
      "Iter-76970, train loss-1.6577, acc-0.5600, valid loss-1.7330, acc-0.5200, test loss-1.7338, acc-0.5087\n",
      "Iter-76980, train loss-1.7694, acc-0.5200, valid loss-1.7329, acc-0.5198, test loss-1.7337, acc-0.5086\n",
      "Iter-76990, train loss-1.6906, acc-0.5400, valid loss-1.7329, acc-0.5198, test loss-1.7337, acc-0.5086\n",
      "Iter-77000, train loss-1.6355, acc-0.7600, valid loss-1.7329, acc-0.5198, test loss-1.7337, acc-0.5085\n",
      "Iter-77010, train loss-1.7201, acc-0.7000, valid loss-1.7328, acc-0.5198, test loss-1.7336, acc-0.5086\n",
      "Iter-77020, train loss-1.8139, acc-0.4800, valid loss-1.7328, acc-0.5198, test loss-1.7336, acc-0.5086\n",
      "Iter-77030, train loss-1.6898, acc-0.6200, valid loss-1.7327, acc-0.5198, test loss-1.7335, acc-0.5088\n",
      "Iter-77040, train loss-1.6783, acc-0.5400, valid loss-1.7327, acc-0.5196, test loss-1.7335, acc-0.5087\n",
      "Iter-77050, train loss-1.6909, acc-0.5400, valid loss-1.7327, acc-0.5196, test loss-1.7335, acc-0.5086\n",
      "Iter-77060, train loss-1.6465, acc-0.6200, valid loss-1.7326, acc-0.5196, test loss-1.7334, acc-0.5086\n",
      "Iter-77070, train loss-1.6175, acc-0.6400, valid loss-1.7326, acc-0.5200, test loss-1.7334, acc-0.5086\n",
      "Iter-77080, train loss-1.7814, acc-0.4600, valid loss-1.7325, acc-0.5194, test loss-1.7333, acc-0.5086\n",
      "Iter-77090, train loss-1.7948, acc-0.4000, valid loss-1.7325, acc-0.5200, test loss-1.7333, acc-0.5087\n",
      "Iter-77100, train loss-1.8141, acc-0.4800, valid loss-1.7325, acc-0.5198, test loss-1.7333, acc-0.5087\n",
      "Iter-77110, train loss-1.7326, acc-0.4800, valid loss-1.7324, acc-0.5198, test loss-1.7332, acc-0.5088\n",
      "Iter-77120, train loss-1.6697, acc-0.6600, valid loss-1.7324, acc-0.5200, test loss-1.7332, acc-0.5086\n",
      "Iter-77130, train loss-1.8320, acc-0.4000, valid loss-1.7323, acc-0.5198, test loss-1.7332, acc-0.5087\n",
      "Iter-77140, train loss-1.7013, acc-0.5000, valid loss-1.7323, acc-0.5196, test loss-1.7331, acc-0.5088\n",
      "Iter-77150, train loss-1.8235, acc-0.4200, valid loss-1.7323, acc-0.5202, test loss-1.7331, acc-0.5088\n",
      "Iter-77160, train loss-1.8136, acc-0.4000, valid loss-1.7322, acc-0.5202, test loss-1.7330, acc-0.5088\n",
      "Iter-77170, train loss-1.8214, acc-0.5400, valid loss-1.7322, acc-0.5202, test loss-1.7330, acc-0.5088\n",
      "Iter-77180, train loss-1.7607, acc-0.4600, valid loss-1.7321, acc-0.5202, test loss-1.7330, acc-0.5087\n",
      "Iter-77190, train loss-1.7056, acc-0.4800, valid loss-1.7321, acc-0.5202, test loss-1.7329, acc-0.5088\n",
      "Iter-77200, train loss-1.7743, acc-0.5200, valid loss-1.7321, acc-0.5202, test loss-1.7329, acc-0.5087\n",
      "Iter-77210, train loss-1.7191, acc-0.6400, valid loss-1.7320, acc-0.5202, test loss-1.7328, acc-0.5089\n",
      "Iter-77220, train loss-1.7131, acc-0.6200, valid loss-1.7320, acc-0.5202, test loss-1.7328, acc-0.5090\n",
      "Iter-77230, train loss-1.7465, acc-0.5200, valid loss-1.7319, acc-0.5208, test loss-1.7328, acc-0.5088\n",
      "Iter-77240, train loss-1.8483, acc-0.3600, valid loss-1.7319, acc-0.5204, test loss-1.7327, acc-0.5088\n",
      "Iter-77250, train loss-1.8975, acc-0.3400, valid loss-1.7319, acc-0.5202, test loss-1.7327, acc-0.5088\n",
      "Iter-77260, train loss-1.6200, acc-0.5800, valid loss-1.7318, acc-0.5202, test loss-1.7326, acc-0.5085\n",
      "Iter-77270, train loss-1.7562, acc-0.5400, valid loss-1.7318, acc-0.5200, test loss-1.7326, acc-0.5085\n",
      "Iter-77280, train loss-1.7156, acc-0.5400, valid loss-1.7317, acc-0.5202, test loss-1.7326, acc-0.5086\n",
      "Iter-77290, train loss-1.6584, acc-0.5800, valid loss-1.7317, acc-0.5200, test loss-1.7325, acc-0.5084\n",
      "Iter-77300, train loss-1.7697, acc-0.5400, valid loss-1.7317, acc-0.5206, test loss-1.7325, acc-0.5083\n",
      "Iter-77310, train loss-1.7028, acc-0.5400, valid loss-1.7316, acc-0.5204, test loss-1.7324, acc-0.5082\n",
      "Iter-77320, train loss-1.8023, acc-0.4600, valid loss-1.7316, acc-0.5202, test loss-1.7324, acc-0.5085\n",
      "Iter-77330, train loss-1.7592, acc-0.5000, valid loss-1.7315, acc-0.5202, test loss-1.7324, acc-0.5085\n",
      "Iter-77340, train loss-1.7506, acc-0.5400, valid loss-1.7315, acc-0.5204, test loss-1.7323, acc-0.5084\n",
      "Iter-77350, train loss-1.6927, acc-0.6200, valid loss-1.7315, acc-0.5200, test loss-1.7323, acc-0.5085\n",
      "Iter-77360, train loss-1.6818, acc-0.5000, valid loss-1.7314, acc-0.5202, test loss-1.7322, acc-0.5085\n",
      "Iter-77370, train loss-1.6393, acc-0.5800, valid loss-1.7314, acc-0.5200, test loss-1.7322, acc-0.5087\n",
      "Iter-77380, train loss-1.6502, acc-0.5000, valid loss-1.7313, acc-0.5206, test loss-1.7322, acc-0.5085\n",
      "Iter-77390, train loss-1.9275, acc-0.3800, valid loss-1.7313, acc-0.5206, test loss-1.7321, acc-0.5086\n",
      "Iter-77400, train loss-1.8339, acc-0.4400, valid loss-1.7313, acc-0.5202, test loss-1.7321, acc-0.5087\n",
      "Iter-77410, train loss-1.7651, acc-0.5400, valid loss-1.7312, acc-0.5202, test loss-1.7320, acc-0.5087\n",
      "Iter-77420, train loss-1.7543, acc-0.4600, valid loss-1.7312, acc-0.5202, test loss-1.7320, acc-0.5085\n",
      "Iter-77430, train loss-1.6965, acc-0.5600, valid loss-1.7312, acc-0.5204, test loss-1.7320, acc-0.5087\n",
      "Iter-77440, train loss-1.7164, acc-0.5600, valid loss-1.7311, acc-0.5204, test loss-1.7319, acc-0.5088\n",
      "Iter-77450, train loss-1.7195, acc-0.4600, valid loss-1.7311, acc-0.5202, test loss-1.7319, acc-0.5087\n",
      "Iter-77460, train loss-1.7445, acc-0.4800, valid loss-1.7310, acc-0.5204, test loss-1.7318, acc-0.5089\n",
      "Iter-77470, train loss-1.6951, acc-0.5400, valid loss-1.7310, acc-0.5204, test loss-1.7318, acc-0.5088\n",
      "Iter-77480, train loss-1.8110, acc-0.4000, valid loss-1.7310, acc-0.5202, test loss-1.7318, acc-0.5088\n",
      "Iter-77490, train loss-1.6364, acc-0.5200, valid loss-1.7309, acc-0.5200, test loss-1.7317, acc-0.5087\n",
      "Iter-77500, train loss-1.7005, acc-0.6600, valid loss-1.7309, acc-0.5200, test loss-1.7317, acc-0.5087\n",
      "Iter-77510, train loss-1.6974, acc-0.5800, valid loss-1.7308, acc-0.5202, test loss-1.7317, acc-0.5088\n",
      "Iter-77520, train loss-1.7021, acc-0.4400, valid loss-1.7308, acc-0.5202, test loss-1.7316, acc-0.5086\n",
      "Iter-77530, train loss-1.5903, acc-0.5400, valid loss-1.7308, acc-0.5202, test loss-1.7316, acc-0.5089\n",
      "Iter-77540, train loss-1.6986, acc-0.6200, valid loss-1.7307, acc-0.5202, test loss-1.7315, acc-0.5088\n",
      "Iter-77550, train loss-1.7396, acc-0.5200, valid loss-1.7307, acc-0.5202, test loss-1.7315, acc-0.5088\n",
      "Iter-77560, train loss-1.7834, acc-0.5000, valid loss-1.7306, acc-0.5202, test loss-1.7314, acc-0.5087\n",
      "Iter-77570, train loss-1.7190, acc-0.4800, valid loss-1.7306, acc-0.5206, test loss-1.7314, acc-0.5089\n",
      "Iter-77580, train loss-1.7056, acc-0.6000, valid loss-1.7306, acc-0.5204, test loss-1.7314, acc-0.5089\n",
      "Iter-77590, train loss-1.6741, acc-0.5600, valid loss-1.7305, acc-0.5208, test loss-1.7313, acc-0.5088\n",
      "Iter-77600, train loss-1.7630, acc-0.4400, valid loss-1.7305, acc-0.5206, test loss-1.7313, acc-0.5089\n",
      "Iter-77610, train loss-1.7975, acc-0.4400, valid loss-1.7304, acc-0.5202, test loss-1.7313, acc-0.5087\n",
      "Iter-77620, train loss-1.7627, acc-0.4400, valid loss-1.7304, acc-0.5202, test loss-1.7312, acc-0.5087\n",
      "Iter-77630, train loss-1.8966, acc-0.3600, valid loss-1.7304, acc-0.5204, test loss-1.7312, acc-0.5087\n",
      "Iter-77640, train loss-1.8473, acc-0.4200, valid loss-1.7303, acc-0.5206, test loss-1.7311, acc-0.5088\n",
      "Iter-77650, train loss-1.7359, acc-0.4600, valid loss-1.7303, acc-0.5206, test loss-1.7311, acc-0.5086\n",
      "Iter-77660, train loss-1.7878, acc-0.4200, valid loss-1.7302, acc-0.5204, test loss-1.7311, acc-0.5088\n",
      "Iter-77670, train loss-1.7849, acc-0.5400, valid loss-1.7302, acc-0.5208, test loss-1.7310, acc-0.5087\n",
      "Iter-77680, train loss-1.7541, acc-0.4600, valid loss-1.7302, acc-0.5208, test loss-1.7310, acc-0.5087\n",
      "Iter-77690, train loss-1.7527, acc-0.5000, valid loss-1.7301, acc-0.5206, test loss-1.7309, acc-0.5086\n",
      "Iter-77700, train loss-1.7240, acc-0.4800, valid loss-1.7301, acc-0.5208, test loss-1.7309, acc-0.5086\n",
      "Iter-77710, train loss-1.8581, acc-0.4600, valid loss-1.7301, acc-0.5208, test loss-1.7309, acc-0.5086\n",
      "Iter-77720, train loss-1.6928, acc-0.5600, valid loss-1.7300, acc-0.5206, test loss-1.7308, acc-0.5086\n",
      "Iter-77730, train loss-1.8543, acc-0.4200, valid loss-1.7300, acc-0.5208, test loss-1.7308, acc-0.5087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-77740, train loss-1.6369, acc-0.5200, valid loss-1.7299, acc-0.5204, test loss-1.7307, acc-0.5086\n",
      "Iter-77750, train loss-1.6589, acc-0.6200, valid loss-1.7299, acc-0.5204, test loss-1.7307, acc-0.5088\n",
      "Iter-77760, train loss-1.7030, acc-0.5200, valid loss-1.7299, acc-0.5206, test loss-1.7307, acc-0.5088\n",
      "Iter-77770, train loss-1.8559, acc-0.3000, valid loss-1.7298, acc-0.5204, test loss-1.7306, acc-0.5087\n",
      "Iter-77780, train loss-1.8651, acc-0.5000, valid loss-1.7298, acc-0.5204, test loss-1.7306, acc-0.5087\n",
      "Iter-77790, train loss-1.7180, acc-0.5200, valid loss-1.7297, acc-0.5204, test loss-1.7306, acc-0.5088\n",
      "Iter-77800, train loss-1.7793, acc-0.4000, valid loss-1.7297, acc-0.5206, test loss-1.7305, acc-0.5087\n",
      "Iter-77810, train loss-1.5943, acc-0.6000, valid loss-1.7297, acc-0.5206, test loss-1.7305, acc-0.5086\n",
      "Iter-77820, train loss-1.8861, acc-0.4200, valid loss-1.7296, acc-0.5206, test loss-1.7304, acc-0.5086\n",
      "Iter-77830, train loss-1.8762, acc-0.4200, valid loss-1.7296, acc-0.5208, test loss-1.7304, acc-0.5087\n",
      "Iter-77840, train loss-1.6857, acc-0.4800, valid loss-1.7295, acc-0.5208, test loss-1.7304, acc-0.5088\n",
      "Iter-77850, train loss-1.7979, acc-0.4800, valid loss-1.7295, acc-0.5208, test loss-1.7303, acc-0.5087\n",
      "Iter-77860, train loss-1.7979, acc-0.5000, valid loss-1.7295, acc-0.5206, test loss-1.7303, acc-0.5087\n",
      "Iter-77870, train loss-1.6637, acc-0.6400, valid loss-1.7294, acc-0.5208, test loss-1.7303, acc-0.5086\n",
      "Iter-77880, train loss-1.8086, acc-0.4200, valid loss-1.7294, acc-0.5208, test loss-1.7302, acc-0.5087\n",
      "Iter-77890, train loss-1.7133, acc-0.5800, valid loss-1.7293, acc-0.5206, test loss-1.7302, acc-0.5085\n",
      "Iter-77900, train loss-1.6940, acc-0.5000, valid loss-1.7293, acc-0.5206, test loss-1.7301, acc-0.5085\n",
      "Iter-77910, train loss-1.8063, acc-0.4800, valid loss-1.7293, acc-0.5206, test loss-1.7301, acc-0.5087\n",
      "Iter-77920, train loss-1.7811, acc-0.5200, valid loss-1.7292, acc-0.5208, test loss-1.7301, acc-0.5085\n",
      "Iter-77930, train loss-1.7117, acc-0.4800, valid loss-1.7292, acc-0.5206, test loss-1.7300, acc-0.5084\n",
      "Iter-77940, train loss-1.7491, acc-0.4400, valid loss-1.7291, acc-0.5208, test loss-1.7300, acc-0.5085\n",
      "Iter-77950, train loss-1.7145, acc-0.5200, valid loss-1.7291, acc-0.5208, test loss-1.7299, acc-0.5085\n",
      "Iter-77960, train loss-1.7311, acc-0.5000, valid loss-1.7291, acc-0.5208, test loss-1.7299, acc-0.5085\n",
      "Iter-77970, train loss-1.7299, acc-0.5400, valid loss-1.7290, acc-0.5208, test loss-1.7299, acc-0.5085\n",
      "Iter-77980, train loss-1.6936, acc-0.5600, valid loss-1.7290, acc-0.5208, test loss-1.7298, acc-0.5085\n",
      "Iter-77990, train loss-1.6513, acc-0.6600, valid loss-1.7290, acc-0.5208, test loss-1.7298, acc-0.5084\n",
      "Iter-78000, train loss-1.8011, acc-0.5600, valid loss-1.7289, acc-0.5210, test loss-1.7297, acc-0.5084\n",
      "Iter-78010, train loss-1.8433, acc-0.3800, valid loss-1.7289, acc-0.5210, test loss-1.7297, acc-0.5085\n",
      "Iter-78020, train loss-1.6217, acc-0.6600, valid loss-1.7288, acc-0.5210, test loss-1.7297, acc-0.5084\n",
      "Iter-78030, train loss-1.7975, acc-0.4200, valid loss-1.7288, acc-0.5208, test loss-1.7296, acc-0.5084\n",
      "Iter-78040, train loss-1.6593, acc-0.5600, valid loss-1.7287, acc-0.5212, test loss-1.7296, acc-0.5084\n",
      "Iter-78050, train loss-1.8052, acc-0.4200, valid loss-1.7287, acc-0.5212, test loss-1.7295, acc-0.5084\n",
      "Iter-78060, train loss-1.8102, acc-0.5000, valid loss-1.7287, acc-0.5216, test loss-1.7295, acc-0.5083\n",
      "Iter-78070, train loss-1.6940, acc-0.5600, valid loss-1.7286, acc-0.5216, test loss-1.7295, acc-0.5083\n",
      "Iter-78080, train loss-1.7316, acc-0.5600, valid loss-1.7286, acc-0.5214, test loss-1.7294, acc-0.5083\n",
      "Iter-78090, train loss-1.7653, acc-0.5000, valid loss-1.7285, acc-0.5214, test loss-1.7294, acc-0.5084\n",
      "Iter-78100, train loss-1.8602, acc-0.4200, valid loss-1.7285, acc-0.5212, test loss-1.7293, acc-0.5085\n",
      "Iter-78110, train loss-1.7939, acc-0.4600, valid loss-1.7285, acc-0.5212, test loss-1.7293, acc-0.5085\n",
      "Iter-78120, train loss-1.7289, acc-0.4400, valid loss-1.7284, acc-0.5212, test loss-1.7293, acc-0.5086\n",
      "Iter-78130, train loss-1.7549, acc-0.4800, valid loss-1.7284, acc-0.5212, test loss-1.7292, acc-0.5087\n",
      "Iter-78140, train loss-1.7555, acc-0.5600, valid loss-1.7284, acc-0.5212, test loss-1.7292, acc-0.5088\n",
      "Iter-78150, train loss-1.7837, acc-0.4600, valid loss-1.7283, acc-0.5212, test loss-1.7292, acc-0.5088\n",
      "Iter-78160, train loss-1.7507, acc-0.5600, valid loss-1.7283, acc-0.5212, test loss-1.7291, acc-0.5087\n",
      "Iter-78170, train loss-1.8589, acc-0.3200, valid loss-1.7282, acc-0.5214, test loss-1.7291, acc-0.5088\n",
      "Iter-78180, train loss-1.8600, acc-0.3600, valid loss-1.7282, acc-0.5214, test loss-1.7290, acc-0.5087\n",
      "Iter-78190, train loss-1.7076, acc-0.5800, valid loss-1.7282, acc-0.5216, test loss-1.7290, acc-0.5087\n",
      "Iter-78200, train loss-1.7209, acc-0.5400, valid loss-1.7281, acc-0.5216, test loss-1.7290, acc-0.5089\n",
      "Iter-78210, train loss-1.8135, acc-0.4800, valid loss-1.7281, acc-0.5216, test loss-1.7289, acc-0.5089\n",
      "Iter-78220, train loss-1.8437, acc-0.4200, valid loss-1.7281, acc-0.5216, test loss-1.7289, acc-0.5089\n",
      "Iter-78230, train loss-1.7726, acc-0.5200, valid loss-1.7280, acc-0.5214, test loss-1.7288, acc-0.5089\n",
      "Iter-78240, train loss-1.6937, acc-0.4600, valid loss-1.7280, acc-0.5216, test loss-1.7288, acc-0.5090\n",
      "Iter-78250, train loss-1.8034, acc-0.4600, valid loss-1.7279, acc-0.5216, test loss-1.7288, acc-0.5090\n",
      "Iter-78260, train loss-1.8797, acc-0.3600, valid loss-1.7279, acc-0.5216, test loss-1.7287, acc-0.5088\n",
      "Iter-78270, train loss-1.8224, acc-0.4200, valid loss-1.7279, acc-0.5216, test loss-1.7287, acc-0.5088\n",
      "Iter-78280, train loss-1.7264, acc-0.5200, valid loss-1.7278, acc-0.5216, test loss-1.7286, acc-0.5087\n",
      "Iter-78290, train loss-1.6950, acc-0.5400, valid loss-1.7278, acc-0.5216, test loss-1.7286, acc-0.5086\n",
      "Iter-78300, train loss-1.7794, acc-0.5000, valid loss-1.7277, acc-0.5214, test loss-1.7286, acc-0.5086\n",
      "Iter-78310, train loss-1.5400, acc-0.7200, valid loss-1.7277, acc-0.5214, test loss-1.7285, acc-0.5087\n",
      "Iter-78320, train loss-1.6982, acc-0.5400, valid loss-1.7277, acc-0.5214, test loss-1.7285, acc-0.5087\n",
      "Iter-78330, train loss-1.6765, acc-0.5600, valid loss-1.7276, acc-0.5214, test loss-1.7285, acc-0.5086\n",
      "Iter-78340, train loss-1.7520, acc-0.4600, valid loss-1.7276, acc-0.5214, test loss-1.7284, acc-0.5087\n",
      "Iter-78350, train loss-1.6079, acc-0.6400, valid loss-1.7275, acc-0.5214, test loss-1.7284, acc-0.5086\n",
      "Iter-78360, train loss-1.6368, acc-0.6400, valid loss-1.7275, acc-0.5212, test loss-1.7283, acc-0.5087\n",
      "Iter-78370, train loss-1.6153, acc-0.5400, valid loss-1.7275, acc-0.5212, test loss-1.7283, acc-0.5088\n",
      "Iter-78380, train loss-1.7719, acc-0.5000, valid loss-1.7274, acc-0.5212, test loss-1.7283, acc-0.5087\n",
      "Iter-78390, train loss-1.7662, acc-0.5600, valid loss-1.7274, acc-0.5214, test loss-1.7282, acc-0.5087\n",
      "Iter-78400, train loss-1.6939, acc-0.5200, valid loss-1.7273, acc-0.5212, test loss-1.7282, acc-0.5086\n",
      "Iter-78410, train loss-1.8118, acc-0.4600, valid loss-1.7273, acc-0.5214, test loss-1.7281, acc-0.5086\n",
      "Iter-78420, train loss-1.6821, acc-0.5000, valid loss-1.7273, acc-0.5212, test loss-1.7281, acc-0.5087\n",
      "Iter-78430, train loss-1.5974, acc-0.5400, valid loss-1.7272, acc-0.5210, test loss-1.7281, acc-0.5086\n",
      "Iter-78440, train loss-1.6919, acc-0.5000, valid loss-1.7272, acc-0.5212, test loss-1.7280, acc-0.5088\n",
      "Iter-78450, train loss-1.8095, acc-0.4400, valid loss-1.7271, acc-0.5210, test loss-1.7280, acc-0.5088\n",
      "Iter-78460, train loss-1.8361, acc-0.4200, valid loss-1.7271, acc-0.5212, test loss-1.7279, acc-0.5088\n",
      "Iter-78470, train loss-1.8126, acc-0.4000, valid loss-1.7271, acc-0.5212, test loss-1.7279, acc-0.5088\n",
      "Iter-78480, train loss-1.7591, acc-0.5200, valid loss-1.7270, acc-0.5212, test loss-1.7279, acc-0.5090\n",
      "Iter-78490, train loss-1.6939, acc-0.4600, valid loss-1.7270, acc-0.5212, test loss-1.7278, acc-0.5090\n",
      "Iter-78500, train loss-1.7582, acc-0.5200, valid loss-1.7270, acc-0.5216, test loss-1.7278, acc-0.5090\n",
      "Iter-78510, train loss-1.9019, acc-0.3800, valid loss-1.7269, acc-0.5216, test loss-1.7277, acc-0.5091\n",
      "Iter-78520, train loss-1.6960, acc-0.4800, valid loss-1.7269, acc-0.5216, test loss-1.7277, acc-0.5091\n",
      "Iter-78530, train loss-1.7856, acc-0.4000, valid loss-1.7268, acc-0.5216, test loss-1.7277, acc-0.5091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-78540, train loss-1.7559, acc-0.4800, valid loss-1.7268, acc-0.5218, test loss-1.7276, acc-0.5091\n",
      "Iter-78550, train loss-1.7474, acc-0.5400, valid loss-1.7268, acc-0.5216, test loss-1.7276, acc-0.5092\n",
      "Iter-78560, train loss-1.7833, acc-0.5200, valid loss-1.7267, acc-0.5216, test loss-1.7276, acc-0.5092\n",
      "Iter-78570, train loss-1.7609, acc-0.4000, valid loss-1.7267, acc-0.5216, test loss-1.7275, acc-0.5091\n",
      "Iter-78580, train loss-1.7384, acc-0.4800, valid loss-1.7267, acc-0.5216, test loss-1.7275, acc-0.5092\n",
      "Iter-78590, train loss-1.7001, acc-0.5000, valid loss-1.7266, acc-0.5218, test loss-1.7274, acc-0.5091\n",
      "Iter-78600, train loss-1.7430, acc-0.6000, valid loss-1.7266, acc-0.5216, test loss-1.7274, acc-0.5091\n",
      "Iter-78610, train loss-1.7229, acc-0.5200, valid loss-1.7265, acc-0.5214, test loss-1.7274, acc-0.5092\n",
      "Iter-78620, train loss-1.7636, acc-0.4600, valid loss-1.7265, acc-0.5216, test loss-1.7273, acc-0.5091\n",
      "Iter-78630, train loss-1.7461, acc-0.5200, valid loss-1.7265, acc-0.5216, test loss-1.7273, acc-0.5091\n",
      "Iter-78640, train loss-1.7308, acc-0.5600, valid loss-1.7264, acc-0.5218, test loss-1.7272, acc-0.5090\n",
      "Iter-78650, train loss-1.7529, acc-0.5400, valid loss-1.7264, acc-0.5218, test loss-1.7272, acc-0.5090\n",
      "Iter-78660, train loss-1.7539, acc-0.5000, valid loss-1.7263, acc-0.5218, test loss-1.7272, acc-0.5089\n",
      "Iter-78670, train loss-1.7343, acc-0.5600, valid loss-1.7263, acc-0.5220, test loss-1.7271, acc-0.5089\n",
      "Iter-78680, train loss-1.7624, acc-0.4800, valid loss-1.7263, acc-0.5220, test loss-1.7271, acc-0.5089\n",
      "Iter-78690, train loss-1.7763, acc-0.5600, valid loss-1.7262, acc-0.5220, test loss-1.7270, acc-0.5089\n",
      "Iter-78700, train loss-1.6445, acc-0.6200, valid loss-1.7262, acc-0.5220, test loss-1.7270, acc-0.5089\n",
      "Iter-78710, train loss-1.7507, acc-0.5000, valid loss-1.7261, acc-0.5220, test loss-1.7270, acc-0.5090\n",
      "Iter-78720, train loss-1.7724, acc-0.4000, valid loss-1.7261, acc-0.5216, test loss-1.7269, acc-0.5090\n",
      "Iter-78730, train loss-1.6342, acc-0.5600, valid loss-1.7261, acc-0.5216, test loss-1.7269, acc-0.5091\n",
      "Iter-78740, train loss-1.7044, acc-0.5200, valid loss-1.7260, acc-0.5218, test loss-1.7269, acc-0.5090\n",
      "Iter-78750, train loss-1.7381, acc-0.4800, valid loss-1.7260, acc-0.5218, test loss-1.7268, acc-0.5091\n",
      "Iter-78760, train loss-1.7418, acc-0.5600, valid loss-1.7260, acc-0.5218, test loss-1.7268, acc-0.5091\n",
      "Iter-78770, train loss-1.7302, acc-0.4400, valid loss-1.7259, acc-0.5220, test loss-1.7267, acc-0.5091\n",
      "Iter-78780, train loss-1.7504, acc-0.5200, valid loss-1.7259, acc-0.5216, test loss-1.7267, acc-0.5091\n",
      "Iter-78790, train loss-1.5395, acc-0.7200, valid loss-1.7258, acc-0.5216, test loss-1.7267, acc-0.5091\n",
      "Iter-78800, train loss-1.7159, acc-0.4800, valid loss-1.7258, acc-0.5216, test loss-1.7266, acc-0.5090\n",
      "Iter-78810, train loss-1.6687, acc-0.5600, valid loss-1.7258, acc-0.5216, test loss-1.7266, acc-0.5091\n",
      "Iter-78820, train loss-1.7122, acc-0.5600, valid loss-1.7257, acc-0.5216, test loss-1.7265, acc-0.5089\n",
      "Iter-78830, train loss-1.7187, acc-0.4800, valid loss-1.7257, acc-0.5214, test loss-1.7265, acc-0.5090\n",
      "Iter-78840, train loss-1.7741, acc-0.5400, valid loss-1.7256, acc-0.5214, test loss-1.7265, acc-0.5091\n",
      "Iter-78850, train loss-1.6814, acc-0.5000, valid loss-1.7256, acc-0.5214, test loss-1.7264, acc-0.5091\n",
      "Iter-78860, train loss-1.7316, acc-0.5200, valid loss-1.7256, acc-0.5216, test loss-1.7264, acc-0.5090\n",
      "Iter-78870, train loss-1.7689, acc-0.4000, valid loss-1.7255, acc-0.5216, test loss-1.7263, acc-0.5091\n",
      "Iter-78880, train loss-1.7155, acc-0.5200, valid loss-1.7255, acc-0.5214, test loss-1.7263, acc-0.5091\n",
      "Iter-78890, train loss-1.7103, acc-0.5200, valid loss-1.7254, acc-0.5214, test loss-1.7263, acc-0.5091\n",
      "Iter-78900, train loss-1.7572, acc-0.5000, valid loss-1.7254, acc-0.5214, test loss-1.7262, acc-0.5091\n",
      "Iter-78910, train loss-1.9353, acc-0.3600, valid loss-1.7254, acc-0.5218, test loss-1.7262, acc-0.5091\n",
      "Iter-78920, train loss-1.8693, acc-0.4400, valid loss-1.7253, acc-0.5216, test loss-1.7262, acc-0.5092\n",
      "Iter-78930, train loss-1.6660, acc-0.5800, valid loss-1.7253, acc-0.5214, test loss-1.7261, acc-0.5091\n",
      "Iter-78940, train loss-1.6952, acc-0.5200, valid loss-1.7253, acc-0.5216, test loss-1.7261, acc-0.5092\n",
      "Iter-78950, train loss-1.7960, acc-0.4800, valid loss-1.7252, acc-0.5218, test loss-1.7260, acc-0.5091\n",
      "Iter-78960, train loss-1.8325, acc-0.3800, valid loss-1.7252, acc-0.5218, test loss-1.7260, acc-0.5091\n",
      "Iter-78970, train loss-1.7562, acc-0.4800, valid loss-1.7251, acc-0.5218, test loss-1.7260, acc-0.5091\n",
      "Iter-78980, train loss-1.7441, acc-0.5600, valid loss-1.7251, acc-0.5216, test loss-1.7259, acc-0.5094\n",
      "Iter-78990, train loss-1.6778, acc-0.6400, valid loss-1.7251, acc-0.5214, test loss-1.7259, acc-0.5094\n",
      "Iter-79000, train loss-1.7751, acc-0.5400, valid loss-1.7250, acc-0.5216, test loss-1.7259, acc-0.5094\n",
      "Iter-79010, train loss-1.7033, acc-0.6000, valid loss-1.7250, acc-0.5218, test loss-1.7258, acc-0.5094\n",
      "Iter-79020, train loss-1.6501, acc-0.5200, valid loss-1.7250, acc-0.5218, test loss-1.7258, acc-0.5092\n",
      "Iter-79030, train loss-1.8023, acc-0.4200, valid loss-1.7249, acc-0.5218, test loss-1.7257, acc-0.5093\n",
      "Iter-79040, train loss-1.6673, acc-0.5400, valid loss-1.7249, acc-0.5214, test loss-1.7257, acc-0.5092\n",
      "Iter-79050, train loss-1.7060, acc-0.5600, valid loss-1.7248, acc-0.5214, test loss-1.7257, acc-0.5092\n",
      "Iter-79060, train loss-1.6702, acc-0.5400, valid loss-1.7248, acc-0.5214, test loss-1.7256, acc-0.5092\n",
      "Iter-79070, train loss-1.7517, acc-0.4200, valid loss-1.7248, acc-0.5216, test loss-1.7256, acc-0.5091\n",
      "Iter-79080, train loss-1.7019, acc-0.5400, valid loss-1.7247, acc-0.5218, test loss-1.7255, acc-0.5092\n",
      "Iter-79090, train loss-1.6425, acc-0.5000, valid loss-1.7247, acc-0.5220, test loss-1.7255, acc-0.5092\n",
      "Iter-79100, train loss-1.7675, acc-0.5000, valid loss-1.7246, acc-0.5220, test loss-1.7255, acc-0.5091\n",
      "Iter-79110, train loss-1.6897, acc-0.5200, valid loss-1.7246, acc-0.5218, test loss-1.7254, acc-0.5092\n",
      "Iter-79120, train loss-1.8904, acc-0.3400, valid loss-1.7246, acc-0.5216, test loss-1.7254, acc-0.5093\n",
      "Iter-79130, train loss-1.6556, acc-0.5400, valid loss-1.7245, acc-0.5218, test loss-1.7254, acc-0.5093\n",
      "Iter-79140, train loss-1.5991, acc-0.6600, valid loss-1.7245, acc-0.5220, test loss-1.7253, acc-0.5093\n",
      "Iter-79150, train loss-1.6529, acc-0.6000, valid loss-1.7245, acc-0.5218, test loss-1.7253, acc-0.5093\n",
      "Iter-79160, train loss-1.7679, acc-0.5000, valid loss-1.7244, acc-0.5218, test loss-1.7252, acc-0.5094\n",
      "Iter-79170, train loss-1.7601, acc-0.5000, valid loss-1.7244, acc-0.5218, test loss-1.7252, acc-0.5093\n",
      "Iter-79180, train loss-1.7646, acc-0.4800, valid loss-1.7243, acc-0.5218, test loss-1.7252, acc-0.5094\n",
      "Iter-79190, train loss-1.7766, acc-0.4800, valid loss-1.7243, acc-0.5220, test loss-1.7251, acc-0.5094\n",
      "Iter-79200, train loss-1.7728, acc-0.4400, valid loss-1.7243, acc-0.5220, test loss-1.7251, acc-0.5096\n",
      "Iter-79210, train loss-1.9072, acc-0.3400, valid loss-1.7242, acc-0.5220, test loss-1.7250, acc-0.5096\n",
      "Iter-79220, train loss-1.8367, acc-0.4200, valid loss-1.7242, acc-0.5220, test loss-1.7250, acc-0.5096\n",
      "Iter-79230, train loss-1.7476, acc-0.5200, valid loss-1.7241, acc-0.5220, test loss-1.7250, acc-0.5096\n",
      "Iter-79240, train loss-1.7375, acc-0.4200, valid loss-1.7241, acc-0.5220, test loss-1.7249, acc-0.5096\n",
      "Iter-79250, train loss-1.6989, acc-0.5000, valid loss-1.7241, acc-0.5220, test loss-1.7249, acc-0.5096\n",
      "Iter-79260, train loss-1.7992, acc-0.5200, valid loss-1.7240, acc-0.5220, test loss-1.7249, acc-0.5095\n",
      "Iter-79270, train loss-1.7875, acc-0.4800, valid loss-1.7240, acc-0.5220, test loss-1.7248, acc-0.5096\n",
      "Iter-79280, train loss-1.7144, acc-0.5800, valid loss-1.7239, acc-0.5220, test loss-1.7248, acc-0.5096\n",
      "Iter-79290, train loss-1.7909, acc-0.4600, valid loss-1.7239, acc-0.5220, test loss-1.7247, acc-0.5096\n",
      "Iter-79300, train loss-1.8069, acc-0.4000, valid loss-1.7239, acc-0.5218, test loss-1.7247, acc-0.5096\n",
      "Iter-79310, train loss-1.7687, acc-0.4800, valid loss-1.7238, acc-0.5220, test loss-1.7247, acc-0.5096\n",
      "Iter-79320, train loss-1.7180, acc-0.5600, valid loss-1.7238, acc-0.5220, test loss-1.7246, acc-0.5096\n",
      "Iter-79330, train loss-1.7237, acc-0.4800, valid loss-1.7237, acc-0.5222, test loss-1.7246, acc-0.5097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79340, train loss-1.8016, acc-0.4600, valid loss-1.7237, acc-0.5222, test loss-1.7246, acc-0.5097\n",
      "Iter-79350, train loss-1.7867, acc-0.5400, valid loss-1.7237, acc-0.5222, test loss-1.7245, acc-0.5097\n",
      "Iter-79360, train loss-1.7963, acc-0.4600, valid loss-1.7236, acc-0.5222, test loss-1.7245, acc-0.5097\n",
      "Iter-79370, train loss-1.8430, acc-0.4600, valid loss-1.7236, acc-0.5222, test loss-1.7244, acc-0.5097\n",
      "Iter-79380, train loss-1.7398, acc-0.5800, valid loss-1.7236, acc-0.5222, test loss-1.7244, acc-0.5096\n",
      "Iter-79390, train loss-1.7959, acc-0.4400, valid loss-1.7235, acc-0.5222, test loss-1.7244, acc-0.5096\n",
      "Iter-79400, train loss-1.7775, acc-0.5000, valid loss-1.7235, acc-0.5222, test loss-1.7243, acc-0.5096\n",
      "Iter-79410, train loss-1.7186, acc-0.5000, valid loss-1.7234, acc-0.5222, test loss-1.7243, acc-0.5097\n",
      "Iter-79420, train loss-1.8079, acc-0.4800, valid loss-1.7234, acc-0.5224, test loss-1.7242, acc-0.5097\n",
      "Iter-79430, train loss-1.7324, acc-0.5000, valid loss-1.7234, acc-0.5224, test loss-1.7242, acc-0.5097\n",
      "Iter-79440, train loss-1.7956, acc-0.4400, valid loss-1.7233, acc-0.5224, test loss-1.7242, acc-0.5097\n",
      "Iter-79450, train loss-1.7520, acc-0.4600, valid loss-1.7233, acc-0.5224, test loss-1.7241, acc-0.5097\n",
      "Iter-79460, train loss-1.7942, acc-0.5200, valid loss-1.7233, acc-0.5222, test loss-1.7241, acc-0.5097\n",
      "Iter-79470, train loss-1.8896, acc-0.3600, valid loss-1.7232, acc-0.5226, test loss-1.7241, acc-0.5097\n",
      "Iter-79480, train loss-1.8293, acc-0.3400, valid loss-1.7232, acc-0.5226, test loss-1.7240, acc-0.5097\n",
      "Iter-79490, train loss-1.7924, acc-0.4400, valid loss-1.7231, acc-0.5226, test loss-1.7240, acc-0.5097\n",
      "Iter-79500, train loss-1.8480, acc-0.3600, valid loss-1.7231, acc-0.5224, test loss-1.7239, acc-0.5098\n",
      "Iter-79510, train loss-1.7261, acc-0.4800, valid loss-1.7231, acc-0.5226, test loss-1.7239, acc-0.5097\n",
      "Iter-79520, train loss-1.7938, acc-0.4600, valid loss-1.7230, acc-0.5222, test loss-1.7239, acc-0.5097\n",
      "Iter-79530, train loss-1.7147, acc-0.5600, valid loss-1.7230, acc-0.5222, test loss-1.7238, acc-0.5097\n",
      "Iter-79540, train loss-1.6659, acc-0.5000, valid loss-1.7230, acc-0.5222, test loss-1.7238, acc-0.5097\n",
      "Iter-79550, train loss-1.7315, acc-0.4800, valid loss-1.7229, acc-0.5222, test loss-1.7238, acc-0.5096\n",
      "Iter-79560, train loss-1.7136, acc-0.5200, valid loss-1.7229, acc-0.5222, test loss-1.7237, acc-0.5095\n",
      "Iter-79570, train loss-1.6543, acc-0.5800, valid loss-1.7228, acc-0.5220, test loss-1.7237, acc-0.5096\n",
      "Iter-79580, train loss-1.8138, acc-0.4200, valid loss-1.7228, acc-0.5222, test loss-1.7236, acc-0.5096\n",
      "Iter-79590, train loss-1.6864, acc-0.4600, valid loss-1.7228, acc-0.5218, test loss-1.7236, acc-0.5096\n",
      "Iter-79600, train loss-1.6792, acc-0.5400, valid loss-1.7227, acc-0.5220, test loss-1.7236, acc-0.5096\n",
      "Iter-79610, train loss-1.6544, acc-0.5800, valid loss-1.7227, acc-0.5218, test loss-1.7235, acc-0.5096\n",
      "Iter-79620, train loss-1.6033, acc-0.5200, valid loss-1.7227, acc-0.5222, test loss-1.7235, acc-0.5096\n",
      "Iter-79630, train loss-1.7705, acc-0.5000, valid loss-1.7226, acc-0.5216, test loss-1.7235, acc-0.5097\n",
      "Iter-79640, train loss-1.7397, acc-0.5000, valid loss-1.7226, acc-0.5216, test loss-1.7234, acc-0.5098\n",
      "Iter-79650, train loss-1.7850, acc-0.5200, valid loss-1.7226, acc-0.5216, test loss-1.7234, acc-0.5099\n",
      "Iter-79660, train loss-1.7049, acc-0.6200, valid loss-1.7225, acc-0.5216, test loss-1.7233, acc-0.5099\n",
      "Iter-79670, train loss-1.8145, acc-0.4600, valid loss-1.7225, acc-0.5216, test loss-1.7233, acc-0.5099\n",
      "Iter-79680, train loss-1.6873, acc-0.5000, valid loss-1.7224, acc-0.5218, test loss-1.7233, acc-0.5100\n",
      "Iter-79690, train loss-1.8185, acc-0.4800, valid loss-1.7224, acc-0.5218, test loss-1.7232, acc-0.5100\n",
      "Iter-79700, train loss-1.7750, acc-0.4600, valid loss-1.7224, acc-0.5222, test loss-1.7232, acc-0.5100\n",
      "Iter-79710, train loss-1.8094, acc-0.4200, valid loss-1.7223, acc-0.5220, test loss-1.7231, acc-0.5100\n",
      "Iter-79720, train loss-1.7119, acc-0.5800, valid loss-1.7223, acc-0.5220, test loss-1.7231, acc-0.5100\n",
      "Iter-79730, train loss-1.7276, acc-0.4600, valid loss-1.7223, acc-0.5220, test loss-1.7231, acc-0.5100\n",
      "Iter-79740, train loss-1.7012, acc-0.4400, valid loss-1.7222, acc-0.5218, test loss-1.7230, acc-0.5100\n",
      "Iter-79750, train loss-1.6607, acc-0.5400, valid loss-1.7222, acc-0.5220, test loss-1.7230, acc-0.5100\n",
      "Iter-79760, train loss-1.7730, acc-0.5800, valid loss-1.7222, acc-0.5218, test loss-1.7230, acc-0.5099\n",
      "Iter-79770, train loss-1.7521, acc-0.5600, valid loss-1.7221, acc-0.5224, test loss-1.7229, acc-0.5099\n",
      "Iter-79780, train loss-1.7731, acc-0.4600, valid loss-1.7221, acc-0.5220, test loss-1.7229, acc-0.5099\n",
      "Iter-79790, train loss-1.7164, acc-0.5400, valid loss-1.7220, acc-0.5226, test loss-1.7228, acc-0.5100\n",
      "Iter-79800, train loss-1.7019, acc-0.5200, valid loss-1.7220, acc-0.5226, test loss-1.7228, acc-0.5100\n",
      "Iter-79810, train loss-1.8170, acc-0.4800, valid loss-1.7220, acc-0.5224, test loss-1.7228, acc-0.5100\n",
      "Iter-79820, train loss-1.7999, acc-0.4800, valid loss-1.7219, acc-0.5228, test loss-1.7227, acc-0.5098\n",
      "Iter-79830, train loss-1.7716, acc-0.3200, valid loss-1.7219, acc-0.5228, test loss-1.7227, acc-0.5098\n",
      "Iter-79840, train loss-1.8230, acc-0.4200, valid loss-1.7219, acc-0.5226, test loss-1.7227, acc-0.5100\n",
      "Iter-79850, train loss-1.7481, acc-0.5400, valid loss-1.7218, acc-0.5228, test loss-1.7226, acc-0.5101\n",
      "Iter-79860, train loss-1.6551, acc-0.6200, valid loss-1.7218, acc-0.5230, test loss-1.7226, acc-0.5099\n",
      "Iter-79870, train loss-1.6839, acc-0.5200, valid loss-1.7217, acc-0.5230, test loss-1.7225, acc-0.5099\n",
      "Iter-79880, train loss-1.7312, acc-0.5000, valid loss-1.7217, acc-0.5232, test loss-1.7225, acc-0.5098\n",
      "Iter-79890, train loss-1.8303, acc-0.4000, valid loss-1.7217, acc-0.5232, test loss-1.7225, acc-0.5099\n",
      "Iter-79900, train loss-1.7121, acc-0.5600, valid loss-1.7216, acc-0.5232, test loss-1.7224, acc-0.5099\n",
      "Iter-79910, train loss-1.7480, acc-0.4600, valid loss-1.7216, acc-0.5228, test loss-1.7224, acc-0.5099\n",
      "Iter-79920, train loss-1.6304, acc-0.5400, valid loss-1.7215, acc-0.5228, test loss-1.7224, acc-0.5099\n",
      "Iter-79930, train loss-1.6559, acc-0.5800, valid loss-1.7215, acc-0.5228, test loss-1.7223, acc-0.5100\n",
      "Iter-79940, train loss-1.7388, acc-0.4600, valid loss-1.7215, acc-0.5230, test loss-1.7223, acc-0.5099\n",
      "Iter-79950, train loss-1.7344, acc-0.5200, valid loss-1.7214, acc-0.5230, test loss-1.7223, acc-0.5097\n",
      "Iter-79960, train loss-1.7342, acc-0.5600, valid loss-1.7214, acc-0.5228, test loss-1.7222, acc-0.5097\n",
      "Iter-79970, train loss-1.7488, acc-0.4800, valid loss-1.7214, acc-0.5226, test loss-1.7222, acc-0.5098\n",
      "Iter-79980, train loss-1.7733, acc-0.6000, valid loss-1.7213, acc-0.5224, test loss-1.7221, acc-0.5097\n",
      "Iter-79990, train loss-1.6889, acc-0.5200, valid loss-1.7213, acc-0.5222, test loss-1.7221, acc-0.5100\n",
      "Iter-80000, train loss-1.7298, acc-0.4600, valid loss-1.7213, acc-0.5220, test loss-1.7221, acc-0.5101\n",
      "Iter-80010, train loss-1.7873, acc-0.4400, valid loss-1.7212, acc-0.5222, test loss-1.7220, acc-0.5100\n",
      "Iter-80020, train loss-1.8108, acc-0.4200, valid loss-1.7212, acc-0.5224, test loss-1.7220, acc-0.5101\n",
      "Iter-80030, train loss-1.7102, acc-0.5400, valid loss-1.7211, acc-0.5224, test loss-1.7219, acc-0.5101\n",
      "Iter-80040, train loss-1.6609, acc-0.5600, valid loss-1.7211, acc-0.5224, test loss-1.7219, acc-0.5100\n",
      "Iter-80050, train loss-1.7915, acc-0.4600, valid loss-1.7211, acc-0.5224, test loss-1.7219, acc-0.5102\n",
      "Iter-80060, train loss-1.7204, acc-0.5400, valid loss-1.7210, acc-0.5222, test loss-1.7218, acc-0.5102\n",
      "Iter-80070, train loss-1.9063, acc-0.3400, valid loss-1.7210, acc-0.5222, test loss-1.7218, acc-0.5102\n",
      "Iter-80080, train loss-1.8246, acc-0.4600, valid loss-1.7209, acc-0.5224, test loss-1.7218, acc-0.5102\n",
      "Iter-80090, train loss-1.6795, acc-0.6000, valid loss-1.7209, acc-0.5222, test loss-1.7217, acc-0.5103\n",
      "Iter-80100, train loss-1.6923, acc-0.5800, valid loss-1.7209, acc-0.5226, test loss-1.7217, acc-0.5103\n",
      "Iter-80110, train loss-1.6674, acc-0.5400, valid loss-1.7208, acc-0.5226, test loss-1.7216, acc-0.5103\n",
      "Iter-80120, train loss-1.6701, acc-0.6200, valid loss-1.7208, acc-0.5222, test loss-1.7216, acc-0.5103\n",
      "Iter-80130, train loss-1.7679, acc-0.4400, valid loss-1.7208, acc-0.5222, test loss-1.7216, acc-0.5103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-80140, train loss-1.7190, acc-0.5400, valid loss-1.7207, acc-0.5226, test loss-1.7215, acc-0.5103\n",
      "Iter-80150, train loss-1.7116, acc-0.5400, valid loss-1.7207, acc-0.5222, test loss-1.7215, acc-0.5102\n",
      "Iter-80160, train loss-1.8050, acc-0.4600, valid loss-1.7206, acc-0.5222, test loss-1.7214, acc-0.5102\n",
      "Iter-80170, train loss-1.7879, acc-0.4000, valid loss-1.7206, acc-0.5226, test loss-1.7214, acc-0.5103\n",
      "Iter-80180, train loss-1.7204, acc-0.5600, valid loss-1.7206, acc-0.5226, test loss-1.7214, acc-0.5103\n",
      "Iter-80190, train loss-1.5875, acc-0.7000, valid loss-1.7205, acc-0.5226, test loss-1.7213, acc-0.5103\n",
      "Iter-80200, train loss-1.8045, acc-0.5000, valid loss-1.7205, acc-0.5226, test loss-1.7213, acc-0.5103\n",
      "Iter-80210, train loss-1.7636, acc-0.4600, valid loss-1.7205, acc-0.5228, test loss-1.7213, acc-0.5104\n",
      "Iter-80220, train loss-1.8036, acc-0.4400, valid loss-1.7204, acc-0.5224, test loss-1.7212, acc-0.5104\n",
      "Iter-80230, train loss-1.8229, acc-0.4000, valid loss-1.7204, acc-0.5224, test loss-1.7212, acc-0.5104\n",
      "Iter-80240, train loss-1.8013, acc-0.4800, valid loss-1.7203, acc-0.5226, test loss-1.7211, acc-0.5104\n",
      "Iter-80250, train loss-1.7207, acc-0.5400, valid loss-1.7203, acc-0.5226, test loss-1.7211, acc-0.5104\n",
      "Iter-80260, train loss-1.7251, acc-0.4400, valid loss-1.7203, acc-0.5226, test loss-1.7211, acc-0.5103\n",
      "Iter-80270, train loss-1.8794, acc-0.4400, valid loss-1.7202, acc-0.5226, test loss-1.7210, acc-0.5103\n",
      "Iter-80280, train loss-1.7147, acc-0.5400, valid loss-1.7202, acc-0.5226, test loss-1.7210, acc-0.5102\n",
      "Iter-80290, train loss-1.6783, acc-0.5800, valid loss-1.7202, acc-0.5226, test loss-1.7210, acc-0.5102\n",
      "Iter-80300, train loss-1.8266, acc-0.4600, valid loss-1.7201, acc-0.5230, test loss-1.7209, acc-0.5102\n",
      "Iter-80310, train loss-1.6150, acc-0.5800, valid loss-1.7201, acc-0.5228, test loss-1.7209, acc-0.5102\n",
      "Iter-80320, train loss-1.8424, acc-0.4400, valid loss-1.7200, acc-0.5228, test loss-1.7209, acc-0.5102\n",
      "Iter-80330, train loss-1.6092, acc-0.5800, valid loss-1.7200, acc-0.5230, test loss-1.7208, acc-0.5103\n",
      "Iter-80340, train loss-1.6830, acc-0.5200, valid loss-1.7200, acc-0.5230, test loss-1.7208, acc-0.5104\n",
      "Iter-80350, train loss-1.6317, acc-0.6200, valid loss-1.7199, acc-0.5228, test loss-1.7207, acc-0.5105\n",
      "Iter-80360, train loss-1.7504, acc-0.4800, valid loss-1.7199, acc-0.5230, test loss-1.7207, acc-0.5106\n",
      "Iter-80370, train loss-1.7507, acc-0.5000, valid loss-1.7199, acc-0.5230, test loss-1.7207, acc-0.5105\n",
      "Iter-80380, train loss-1.6204, acc-0.6200, valid loss-1.7198, acc-0.5230, test loss-1.7206, acc-0.5105\n",
      "Iter-80390, train loss-1.7729, acc-0.4600, valid loss-1.7198, acc-0.5228, test loss-1.7206, acc-0.5104\n",
      "Iter-80400, train loss-1.6581, acc-0.5800, valid loss-1.7197, acc-0.5228, test loss-1.7206, acc-0.5104\n",
      "Iter-80410, train loss-1.6403, acc-0.6600, valid loss-1.7197, acc-0.5228, test loss-1.7205, acc-0.5105\n",
      "Iter-80420, train loss-1.7802, acc-0.4400, valid loss-1.7197, acc-0.5228, test loss-1.7205, acc-0.5105\n",
      "Iter-80430, train loss-1.7143, acc-0.5800, valid loss-1.7196, acc-0.5228, test loss-1.7204, acc-0.5104\n",
      "Iter-80440, train loss-1.5729, acc-0.6400, valid loss-1.7196, acc-0.5228, test loss-1.7204, acc-0.5105\n",
      "Iter-80450, train loss-1.8164, acc-0.4600, valid loss-1.7195, acc-0.5230, test loss-1.7204, acc-0.5104\n",
      "Iter-80460, train loss-1.6884, acc-0.4600, valid loss-1.7195, acc-0.5230, test loss-1.7203, acc-0.5104\n",
      "Iter-80470, train loss-1.6135, acc-0.6400, valid loss-1.7195, acc-0.5230, test loss-1.7203, acc-0.5104\n",
      "Iter-80480, train loss-1.7571, acc-0.5400, valid loss-1.7194, acc-0.5230, test loss-1.7202, acc-0.5105\n",
      "Iter-80490, train loss-1.6319, acc-0.5200, valid loss-1.7194, acc-0.5224, test loss-1.7202, acc-0.5105\n",
      "Iter-80500, train loss-1.7994, acc-0.4200, valid loss-1.7194, acc-0.5226, test loss-1.7202, acc-0.5104\n",
      "Iter-80510, train loss-1.7696, acc-0.4200, valid loss-1.7193, acc-0.5220, test loss-1.7201, acc-0.5105\n",
      "Iter-80520, train loss-1.7630, acc-0.4800, valid loss-1.7193, acc-0.5220, test loss-1.7201, acc-0.5104\n",
      "Iter-80530, train loss-1.7664, acc-0.5400, valid loss-1.7192, acc-0.5222, test loss-1.7201, acc-0.5104\n",
      "Iter-80540, train loss-1.7560, acc-0.5000, valid loss-1.7192, acc-0.5224, test loss-1.7200, acc-0.5105\n",
      "Iter-80550, train loss-1.7737, acc-0.5200, valid loss-1.7192, acc-0.5224, test loss-1.7200, acc-0.5104\n",
      "Iter-80560, train loss-1.7667, acc-0.4600, valid loss-1.7191, acc-0.5224, test loss-1.7199, acc-0.5105\n",
      "Iter-80570, train loss-1.7030, acc-0.5000, valid loss-1.7191, acc-0.5230, test loss-1.7199, acc-0.5105\n",
      "Iter-80580, train loss-1.7894, acc-0.4600, valid loss-1.7191, acc-0.5234, test loss-1.7199, acc-0.5105\n",
      "Iter-80590, train loss-1.7323, acc-0.5200, valid loss-1.7190, acc-0.5230, test loss-1.7198, acc-0.5104\n",
      "Iter-80600, train loss-1.6290, acc-0.6000, valid loss-1.7190, acc-0.5228, test loss-1.7198, acc-0.5104\n",
      "Iter-80610, train loss-1.8074, acc-0.4400, valid loss-1.7189, acc-0.5230, test loss-1.7198, acc-0.5103\n",
      "Iter-80620, train loss-1.6965, acc-0.5200, valid loss-1.7189, acc-0.5230, test loss-1.7197, acc-0.5104\n",
      "Iter-80630, train loss-1.7176, acc-0.4600, valid loss-1.7189, acc-0.5234, test loss-1.7197, acc-0.5104\n",
      "Iter-80640, train loss-1.6640, acc-0.5600, valid loss-1.7188, acc-0.5228, test loss-1.7196, acc-0.5103\n",
      "Iter-80650, train loss-1.6364, acc-0.5800, valid loss-1.7188, acc-0.5228, test loss-1.7196, acc-0.5103\n",
      "Iter-80660, train loss-1.8368, acc-0.3400, valid loss-1.7187, acc-0.5226, test loss-1.7196, acc-0.5103\n",
      "Iter-80670, train loss-1.8940, acc-0.3800, valid loss-1.7187, acc-0.5228, test loss-1.7195, acc-0.5104\n",
      "Iter-80680, train loss-1.7656, acc-0.4600, valid loss-1.7187, acc-0.5230, test loss-1.7195, acc-0.5104\n",
      "Iter-80690, train loss-1.7150, acc-0.4600, valid loss-1.7186, acc-0.5232, test loss-1.7195, acc-0.5104\n",
      "Iter-80700, train loss-1.6946, acc-0.5200, valid loss-1.7186, acc-0.5228, test loss-1.7194, acc-0.5103\n",
      "Iter-80710, train loss-1.6907, acc-0.5800, valid loss-1.7186, acc-0.5232, test loss-1.7194, acc-0.5105\n",
      "Iter-80720, train loss-1.8243, acc-0.3600, valid loss-1.7185, acc-0.5232, test loss-1.7193, acc-0.5104\n",
      "Iter-80730, train loss-1.6226, acc-0.6400, valid loss-1.7185, acc-0.5230, test loss-1.7193, acc-0.5104\n",
      "Iter-80740, train loss-1.7253, acc-0.4600, valid loss-1.7184, acc-0.5228, test loss-1.7193, acc-0.5104\n",
      "Iter-80750, train loss-1.7739, acc-0.4800, valid loss-1.7184, acc-0.5230, test loss-1.7192, acc-0.5104\n",
      "Iter-80760, train loss-1.7153, acc-0.4000, valid loss-1.7184, acc-0.5230, test loss-1.7192, acc-0.5105\n",
      "Iter-80770, train loss-1.8748, acc-0.4200, valid loss-1.7183, acc-0.5228, test loss-1.7192, acc-0.5105\n",
      "Iter-80780, train loss-1.6943, acc-0.5200, valid loss-1.7183, acc-0.5234, test loss-1.7191, acc-0.5106\n",
      "Iter-80790, train loss-1.7441, acc-0.4000, valid loss-1.7183, acc-0.5230, test loss-1.7191, acc-0.5104\n",
      "Iter-80800, train loss-1.6591, acc-0.5000, valid loss-1.7182, acc-0.5228, test loss-1.7190, acc-0.5105\n",
      "Iter-80810, train loss-1.7811, acc-0.5400, valid loss-1.7182, acc-0.5230, test loss-1.7190, acc-0.5104\n",
      "Iter-80820, train loss-1.7357, acc-0.4400, valid loss-1.7181, acc-0.5228, test loss-1.7190, acc-0.5103\n",
      "Iter-80830, train loss-1.7807, acc-0.4800, valid loss-1.7181, acc-0.5230, test loss-1.7189, acc-0.5104\n",
      "Iter-80840, train loss-1.7279, acc-0.5200, valid loss-1.7181, acc-0.5230, test loss-1.7189, acc-0.5103\n",
      "Iter-80850, train loss-1.6146, acc-0.6000, valid loss-1.7180, acc-0.5232, test loss-1.7189, acc-0.5104\n",
      "Iter-80860, train loss-1.7206, acc-0.4800, valid loss-1.7180, acc-0.5230, test loss-1.7188, acc-0.5104\n",
      "Iter-80870, train loss-1.7396, acc-0.4800, valid loss-1.7179, acc-0.5232, test loss-1.7188, acc-0.5103\n",
      "Iter-80880, train loss-1.6310, acc-0.5200, valid loss-1.7179, acc-0.5234, test loss-1.7187, acc-0.5103\n",
      "Iter-80890, train loss-1.9237, acc-0.3800, valid loss-1.7179, acc-0.5232, test loss-1.7187, acc-0.5104\n",
      "Iter-80900, train loss-1.8466, acc-0.3800, valid loss-1.7178, acc-0.5232, test loss-1.7187, acc-0.5106\n",
      "Iter-80910, train loss-1.6691, acc-0.6200, valid loss-1.7178, acc-0.5232, test loss-1.7186, acc-0.5104\n",
      "Iter-80920, train loss-1.7605, acc-0.5000, valid loss-1.7178, acc-0.5232, test loss-1.7186, acc-0.5103\n",
      "Iter-80930, train loss-1.5696, acc-0.5600, valid loss-1.7177, acc-0.5234, test loss-1.7186, acc-0.5103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-80940, train loss-1.7013, acc-0.4800, valid loss-1.7177, acc-0.5236, test loss-1.7185, acc-0.5104\n",
      "Iter-80950, train loss-1.6432, acc-0.5600, valid loss-1.7176, acc-0.5238, test loss-1.7185, acc-0.5105\n",
      "Iter-80960, train loss-1.8032, acc-0.5200, valid loss-1.7176, acc-0.5238, test loss-1.7184, acc-0.5107\n",
      "Iter-80970, train loss-1.7540, acc-0.5200, valid loss-1.7176, acc-0.5238, test loss-1.7184, acc-0.5108\n",
      "Iter-80980, train loss-1.7778, acc-0.5200, valid loss-1.7175, acc-0.5236, test loss-1.7184, acc-0.5106\n",
      "Iter-80990, train loss-1.7533, acc-0.5600, valid loss-1.7175, acc-0.5238, test loss-1.7183, acc-0.5106\n",
      "Iter-81000, train loss-1.7284, acc-0.5000, valid loss-1.7175, acc-0.5236, test loss-1.7183, acc-0.5108\n",
      "Iter-81010, train loss-1.7405, acc-0.5000, valid loss-1.7174, acc-0.5234, test loss-1.7183, acc-0.5107\n",
      "Iter-81020, train loss-1.6764, acc-0.5400, valid loss-1.7174, acc-0.5234, test loss-1.7182, acc-0.5109\n",
      "Iter-81030, train loss-1.6465, acc-0.5400, valid loss-1.7173, acc-0.5234, test loss-1.7182, acc-0.5110\n",
      "Iter-81040, train loss-1.7256, acc-0.5000, valid loss-1.7173, acc-0.5234, test loss-1.7181, acc-0.5110\n",
      "Iter-81050, train loss-1.6903, acc-0.5200, valid loss-1.7173, acc-0.5234, test loss-1.7181, acc-0.5111\n",
      "Iter-81060, train loss-1.6476, acc-0.5000, valid loss-1.7172, acc-0.5232, test loss-1.7181, acc-0.5111\n",
      "Iter-81070, train loss-1.7685, acc-0.4600, valid loss-1.7172, acc-0.5232, test loss-1.7180, acc-0.5111\n",
      "Iter-81080, train loss-1.6477, acc-0.5600, valid loss-1.7172, acc-0.5232, test loss-1.7180, acc-0.5110\n",
      "Iter-81090, train loss-1.7049, acc-0.5000, valid loss-1.7171, acc-0.5234, test loss-1.7179, acc-0.5110\n",
      "Iter-81100, train loss-1.8566, acc-0.4400, valid loss-1.7171, acc-0.5236, test loss-1.7179, acc-0.5112\n",
      "Iter-81110, train loss-1.7586, acc-0.4800, valid loss-1.7170, acc-0.5234, test loss-1.7179, acc-0.5110\n",
      "Iter-81120, train loss-1.7864, acc-0.4600, valid loss-1.7170, acc-0.5232, test loss-1.7178, acc-0.5110\n",
      "Iter-81130, train loss-1.7837, acc-0.4800, valid loss-1.7170, acc-0.5232, test loss-1.7178, acc-0.5111\n",
      "Iter-81140, train loss-1.7570, acc-0.4600, valid loss-1.7169, acc-0.5234, test loss-1.7178, acc-0.5109\n",
      "Iter-81150, train loss-1.8079, acc-0.4600, valid loss-1.7169, acc-0.5236, test loss-1.7177, acc-0.5111\n",
      "Iter-81160, train loss-1.7268, acc-0.5200, valid loss-1.7169, acc-0.5236, test loss-1.7177, acc-0.5110\n",
      "Iter-81170, train loss-1.7225, acc-0.5200, valid loss-1.7168, acc-0.5232, test loss-1.7176, acc-0.5110\n",
      "Iter-81180, train loss-1.7660, acc-0.5600, valid loss-1.7168, acc-0.5234, test loss-1.7176, acc-0.5110\n",
      "Iter-81190, train loss-1.6563, acc-0.5600, valid loss-1.7167, acc-0.5236, test loss-1.7176, acc-0.5111\n",
      "Iter-81200, train loss-1.7980, acc-0.5000, valid loss-1.7167, acc-0.5236, test loss-1.7175, acc-0.5111\n",
      "Iter-81210, train loss-1.8295, acc-0.4800, valid loss-1.7167, acc-0.5238, test loss-1.7175, acc-0.5111\n",
      "Iter-81220, train loss-1.7403, acc-0.4800, valid loss-1.7166, acc-0.5238, test loss-1.7175, acc-0.5113\n",
      "Iter-81230, train loss-1.7859, acc-0.4800, valid loss-1.7166, acc-0.5240, test loss-1.7174, acc-0.5114\n",
      "Iter-81240, train loss-1.7826, acc-0.4800, valid loss-1.7165, acc-0.5240, test loss-1.7174, acc-0.5113\n",
      "Iter-81250, train loss-1.6240, acc-0.6000, valid loss-1.7165, acc-0.5240, test loss-1.7173, acc-0.5112\n",
      "Iter-81260, train loss-1.7418, acc-0.5600, valid loss-1.7165, acc-0.5240, test loss-1.7173, acc-0.5112\n",
      "Iter-81270, train loss-1.7192, acc-0.6000, valid loss-1.7164, acc-0.5242, test loss-1.7173, acc-0.5112\n",
      "Iter-81280, train loss-1.7243, acc-0.5000, valid loss-1.7164, acc-0.5242, test loss-1.7172, acc-0.5112\n",
      "Iter-81290, train loss-1.6972, acc-0.4400, valid loss-1.7164, acc-0.5244, test loss-1.7172, acc-0.5112\n",
      "Iter-81300, train loss-1.6327, acc-0.5600, valid loss-1.7163, acc-0.5242, test loss-1.7172, acc-0.5109\n",
      "Iter-81310, train loss-1.6618, acc-0.6200, valid loss-1.7163, acc-0.5242, test loss-1.7171, acc-0.5109\n",
      "Iter-81320, train loss-1.7194, acc-0.5200, valid loss-1.7162, acc-0.5238, test loss-1.7171, acc-0.5111\n",
      "Iter-81330, train loss-1.6546, acc-0.5400, valid loss-1.7162, acc-0.5240, test loss-1.7170, acc-0.5112\n",
      "Iter-81340, train loss-1.6767, acc-0.6000, valid loss-1.7162, acc-0.5240, test loss-1.7170, acc-0.5113\n",
      "Iter-81350, train loss-1.7705, acc-0.4600, valid loss-1.7161, acc-0.5244, test loss-1.7170, acc-0.5112\n",
      "Iter-81360, train loss-1.5709, acc-0.6000, valid loss-1.7161, acc-0.5242, test loss-1.7169, acc-0.5111\n",
      "Iter-81370, train loss-1.5517, acc-0.6000, valid loss-1.7160, acc-0.5242, test loss-1.7169, acc-0.5109\n",
      "Iter-81380, train loss-1.6027, acc-0.5000, valid loss-1.7160, acc-0.5238, test loss-1.7169, acc-0.5109\n",
      "Iter-81390, train loss-1.7573, acc-0.5000, valid loss-1.7160, acc-0.5238, test loss-1.7168, acc-0.5107\n",
      "Iter-81400, train loss-1.6953, acc-0.5400, valid loss-1.7159, acc-0.5238, test loss-1.7168, acc-0.5108\n",
      "Iter-81410, train loss-1.7341, acc-0.5600, valid loss-1.7159, acc-0.5240, test loss-1.7167, acc-0.5108\n",
      "Iter-81420, train loss-1.6977, acc-0.6000, valid loss-1.7159, acc-0.5240, test loss-1.7167, acc-0.5109\n",
      "Iter-81430, train loss-1.6969, acc-0.5000, valid loss-1.7158, acc-0.5238, test loss-1.7167, acc-0.5110\n",
      "Iter-81440, train loss-1.7233, acc-0.5200, valid loss-1.7158, acc-0.5240, test loss-1.7166, acc-0.5110\n",
      "Iter-81450, train loss-1.6268, acc-0.5800, valid loss-1.7158, acc-0.5238, test loss-1.7166, acc-0.5110\n",
      "Iter-81460, train loss-1.8944, acc-0.4000, valid loss-1.7157, acc-0.5238, test loss-1.7166, acc-0.5109\n",
      "Iter-81470, train loss-1.6651, acc-0.5600, valid loss-1.7157, acc-0.5240, test loss-1.7165, acc-0.5108\n",
      "Iter-81480, train loss-1.8029, acc-0.4200, valid loss-1.7156, acc-0.5240, test loss-1.7165, acc-0.5109\n",
      "Iter-81490, train loss-1.6705, acc-0.5400, valid loss-1.7156, acc-0.5240, test loss-1.7165, acc-0.5107\n",
      "Iter-81500, train loss-1.7011, acc-0.6000, valid loss-1.7156, acc-0.5240, test loss-1.7164, acc-0.5109\n",
      "Iter-81510, train loss-1.8256, acc-0.5000, valid loss-1.7155, acc-0.5242, test loss-1.7164, acc-0.5111\n",
      "Iter-81520, train loss-1.6589, acc-0.5200, valid loss-1.7155, acc-0.5240, test loss-1.7163, acc-0.5111\n",
      "Iter-81530, train loss-1.8673, acc-0.4200, valid loss-1.7155, acc-0.5242, test loss-1.7163, acc-0.5111\n",
      "Iter-81540, train loss-1.6275, acc-0.5600, valid loss-1.7154, acc-0.5242, test loss-1.7163, acc-0.5111\n",
      "Iter-81550, train loss-1.6310, acc-0.5400, valid loss-1.7154, acc-0.5242, test loss-1.7162, acc-0.5111\n",
      "Iter-81560, train loss-1.7615, acc-0.5600, valid loss-1.7154, acc-0.5240, test loss-1.7162, acc-0.5112\n",
      "Iter-81570, train loss-1.8006, acc-0.5000, valid loss-1.7153, acc-0.5240, test loss-1.7162, acc-0.5112\n",
      "Iter-81580, train loss-1.6455, acc-0.6400, valid loss-1.7153, acc-0.5240, test loss-1.7161, acc-0.5111\n",
      "Iter-81590, train loss-1.7105, acc-0.5400, valid loss-1.7152, acc-0.5240, test loss-1.7161, acc-0.5111\n",
      "Iter-81600, train loss-1.6871, acc-0.5200, valid loss-1.7152, acc-0.5242, test loss-1.7160, acc-0.5113\n",
      "Iter-81610, train loss-1.7712, acc-0.5200, valid loss-1.7152, acc-0.5242, test loss-1.7160, acc-0.5112\n",
      "Iter-81620, train loss-1.7777, acc-0.4800, valid loss-1.7151, acc-0.5240, test loss-1.7160, acc-0.5111\n",
      "Iter-81630, train loss-1.7444, acc-0.4800, valid loss-1.7151, acc-0.5242, test loss-1.7159, acc-0.5111\n",
      "Iter-81640, train loss-1.5707, acc-0.6000, valid loss-1.7151, acc-0.5244, test loss-1.7159, acc-0.5110\n",
      "Iter-81650, train loss-1.6978, acc-0.5400, valid loss-1.7150, acc-0.5244, test loss-1.7159, acc-0.5111\n",
      "Iter-81660, train loss-1.8286, acc-0.4200, valid loss-1.7150, acc-0.5242, test loss-1.7158, acc-0.5112\n",
      "Iter-81670, train loss-1.6214, acc-0.5400, valid loss-1.7149, acc-0.5244, test loss-1.7158, acc-0.5112\n",
      "Iter-81680, train loss-1.7089, acc-0.5200, valid loss-1.7149, acc-0.5244, test loss-1.7158, acc-0.5111\n",
      "Iter-81690, train loss-1.7915, acc-0.5000, valid loss-1.7149, acc-0.5242, test loss-1.7157, acc-0.5111\n",
      "Iter-81700, train loss-1.7265, acc-0.4400, valid loss-1.7148, acc-0.5242, test loss-1.7157, acc-0.5110\n",
      "Iter-81710, train loss-1.6855, acc-0.5200, valid loss-1.7148, acc-0.5240, test loss-1.7156, acc-0.5111\n",
      "Iter-81720, train loss-1.7001, acc-0.5400, valid loss-1.7148, acc-0.5242, test loss-1.7156, acc-0.5110\n",
      "Iter-81730, train loss-1.7039, acc-0.5000, valid loss-1.7147, acc-0.5242, test loss-1.7156, acc-0.5110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-81740, train loss-1.7935, acc-0.5400, valid loss-1.7147, acc-0.5240, test loss-1.7155, acc-0.5109\n",
      "Iter-81750, train loss-1.6941, acc-0.5000, valid loss-1.7146, acc-0.5242, test loss-1.7155, acc-0.5109\n",
      "Iter-81760, train loss-1.8135, acc-0.4600, valid loss-1.7146, acc-0.5242, test loss-1.7155, acc-0.5112\n",
      "Iter-81770, train loss-1.5967, acc-0.6800, valid loss-1.7146, acc-0.5242, test loss-1.7154, acc-0.5111\n",
      "Iter-81780, train loss-1.7259, acc-0.4200, valid loss-1.7145, acc-0.5242, test loss-1.7154, acc-0.5112\n",
      "Iter-81790, train loss-1.7265, acc-0.5200, valid loss-1.7145, acc-0.5240, test loss-1.7153, acc-0.5110\n",
      "Iter-81800, train loss-1.6034, acc-0.6400, valid loss-1.7144, acc-0.5242, test loss-1.7153, acc-0.5111\n",
      "Iter-81810, train loss-1.8240, acc-0.4400, valid loss-1.7144, acc-0.5242, test loss-1.7153, acc-0.5111\n",
      "Iter-81820, train loss-1.6606, acc-0.5000, valid loss-1.7144, acc-0.5242, test loss-1.7152, acc-0.5113\n",
      "Iter-81830, train loss-1.6998, acc-0.6000, valid loss-1.7143, acc-0.5244, test loss-1.7152, acc-0.5112\n",
      "Iter-81840, train loss-1.7396, acc-0.4400, valid loss-1.7143, acc-0.5240, test loss-1.7152, acc-0.5112\n",
      "Iter-81850, train loss-1.7269, acc-0.5600, valid loss-1.7143, acc-0.5242, test loss-1.7151, acc-0.5111\n",
      "Iter-81860, train loss-1.7232, acc-0.5000, valid loss-1.7142, acc-0.5242, test loss-1.7151, acc-0.5114\n",
      "Iter-81870, train loss-1.7510, acc-0.5400, valid loss-1.7142, acc-0.5244, test loss-1.7150, acc-0.5112\n",
      "Iter-81880, train loss-1.6907, acc-0.5600, valid loss-1.7141, acc-0.5242, test loss-1.7150, acc-0.5111\n",
      "Iter-81890, train loss-1.7346, acc-0.5000, valid loss-1.7141, acc-0.5242, test loss-1.7150, acc-0.5111\n",
      "Iter-81900, train loss-1.7572, acc-0.4400, valid loss-1.7141, acc-0.5242, test loss-1.7149, acc-0.5111\n",
      "Iter-81910, train loss-1.5382, acc-0.6200, valid loss-1.7140, acc-0.5244, test loss-1.7149, acc-0.5110\n",
      "Iter-81920, train loss-1.6727, acc-0.5800, valid loss-1.7140, acc-0.5246, test loss-1.7149, acc-0.5110\n",
      "Iter-81930, train loss-1.6146, acc-0.6400, valid loss-1.7140, acc-0.5246, test loss-1.7148, acc-0.5110\n",
      "Iter-81940, train loss-1.6711, acc-0.6000, valid loss-1.7139, acc-0.5246, test loss-1.7148, acc-0.5111\n",
      "Iter-81950, train loss-1.6711, acc-0.5600, valid loss-1.7139, acc-0.5248, test loss-1.7148, acc-0.5113\n",
      "Iter-81960, train loss-1.6971, acc-0.4800, valid loss-1.7138, acc-0.5244, test loss-1.7147, acc-0.5110\n",
      "Iter-81970, train loss-1.7034, acc-0.5000, valid loss-1.7138, acc-0.5244, test loss-1.7147, acc-0.5111\n",
      "Iter-81980, train loss-1.6761, acc-0.5400, valid loss-1.7138, acc-0.5242, test loss-1.7146, acc-0.5111\n",
      "Iter-81990, train loss-1.7743, acc-0.4200, valid loss-1.7137, acc-0.5244, test loss-1.7146, acc-0.5111\n",
      "Iter-82000, train loss-1.8991, acc-0.4400, valid loss-1.7137, acc-0.5242, test loss-1.7146, acc-0.5109\n",
      "Iter-82010, train loss-1.7273, acc-0.4800, valid loss-1.7137, acc-0.5246, test loss-1.7145, acc-0.5110\n",
      "Iter-82020, train loss-1.7525, acc-0.4400, valid loss-1.7136, acc-0.5246, test loss-1.7145, acc-0.5111\n",
      "Iter-82030, train loss-1.8238, acc-0.5000, valid loss-1.7136, acc-0.5246, test loss-1.7145, acc-0.5109\n",
      "Iter-82040, train loss-1.7688, acc-0.4600, valid loss-1.7136, acc-0.5244, test loss-1.7144, acc-0.5111\n",
      "Iter-82050, train loss-1.6960, acc-0.5200, valid loss-1.7135, acc-0.5244, test loss-1.7144, acc-0.5113\n",
      "Iter-82060, train loss-1.6371, acc-0.5800, valid loss-1.7135, acc-0.5246, test loss-1.7143, acc-0.5111\n",
      "Iter-82070, train loss-1.6644, acc-0.5800, valid loss-1.7134, acc-0.5244, test loss-1.7143, acc-0.5111\n",
      "Iter-82080, train loss-1.6639, acc-0.4600, valid loss-1.7134, acc-0.5244, test loss-1.7143, acc-0.5112\n",
      "Iter-82090, train loss-1.7497, acc-0.4400, valid loss-1.7134, acc-0.5244, test loss-1.7142, acc-0.5112\n",
      "Iter-82100, train loss-1.7190, acc-0.6000, valid loss-1.7133, acc-0.5244, test loss-1.7142, acc-0.5112\n",
      "Iter-82110, train loss-1.7299, acc-0.4600, valid loss-1.7133, acc-0.5246, test loss-1.7142, acc-0.5112\n",
      "Iter-82120, train loss-1.6957, acc-0.5600, valid loss-1.7133, acc-0.5246, test loss-1.7141, acc-0.5111\n",
      "Iter-82130, train loss-1.7109, acc-0.5000, valid loss-1.7132, acc-0.5248, test loss-1.7141, acc-0.5112\n",
      "Iter-82140, train loss-1.6900, acc-0.4800, valid loss-1.7132, acc-0.5244, test loss-1.7141, acc-0.5112\n",
      "Iter-82150, train loss-1.6770, acc-0.5800, valid loss-1.7132, acc-0.5246, test loss-1.7140, acc-0.5112\n",
      "Iter-82160, train loss-1.7071, acc-0.5400, valid loss-1.7131, acc-0.5244, test loss-1.7140, acc-0.5112\n",
      "Iter-82170, train loss-1.6553, acc-0.5800, valid loss-1.7131, acc-0.5246, test loss-1.7139, acc-0.5112\n",
      "Iter-82180, train loss-1.6776, acc-0.5600, valid loss-1.7130, acc-0.5246, test loss-1.7139, acc-0.5111\n",
      "Iter-82190, train loss-1.7132, acc-0.5000, valid loss-1.7130, acc-0.5246, test loss-1.7139, acc-0.5111\n",
      "Iter-82200, train loss-1.7501, acc-0.6000, valid loss-1.7130, acc-0.5246, test loss-1.7138, acc-0.5112\n",
      "Iter-82210, train loss-1.8079, acc-0.4200, valid loss-1.7129, acc-0.5246, test loss-1.7138, acc-0.5111\n",
      "Iter-82220, train loss-1.6846, acc-0.5600, valid loss-1.7129, acc-0.5246, test loss-1.7137, acc-0.5111\n",
      "Iter-82230, train loss-1.8006, acc-0.4600, valid loss-1.7128, acc-0.5250, test loss-1.7137, acc-0.5112\n",
      "Iter-82240, train loss-1.7176, acc-0.4800, valid loss-1.7128, acc-0.5248, test loss-1.7137, acc-0.5112\n",
      "Iter-82250, train loss-1.7565, acc-0.5200, valid loss-1.7128, acc-0.5248, test loss-1.7136, acc-0.5114\n",
      "Iter-82260, train loss-1.8011, acc-0.4200, valid loss-1.7127, acc-0.5248, test loss-1.7136, acc-0.5115\n",
      "Iter-82270, train loss-1.6759, acc-0.5600, valid loss-1.7127, acc-0.5250, test loss-1.7136, acc-0.5116\n",
      "Iter-82280, train loss-1.7913, acc-0.4400, valid loss-1.7127, acc-0.5250, test loss-1.7135, acc-0.5116\n",
      "Iter-82290, train loss-1.8013, acc-0.4600, valid loss-1.7126, acc-0.5248, test loss-1.7135, acc-0.5115\n",
      "Iter-82300, train loss-1.7183, acc-0.5200, valid loss-1.7126, acc-0.5248, test loss-1.7134, acc-0.5115\n",
      "Iter-82310, train loss-1.7469, acc-0.4400, valid loss-1.7125, acc-0.5250, test loss-1.7134, acc-0.5116\n",
      "Iter-82320, train loss-1.8323, acc-0.4200, valid loss-1.7125, acc-0.5250, test loss-1.7134, acc-0.5116\n",
      "Iter-82330, train loss-1.7157, acc-0.5600, valid loss-1.7125, acc-0.5248, test loss-1.7133, acc-0.5117\n",
      "Iter-82340, train loss-1.7968, acc-0.4400, valid loss-1.7124, acc-0.5246, test loss-1.7133, acc-0.5118\n",
      "Iter-82350, train loss-1.7690, acc-0.5200, valid loss-1.7124, acc-0.5248, test loss-1.7133, acc-0.5117\n",
      "Iter-82360, train loss-1.7808, acc-0.5000, valid loss-1.7124, acc-0.5248, test loss-1.7132, acc-0.5116\n",
      "Iter-82370, train loss-1.7119, acc-0.5800, valid loss-1.7123, acc-0.5248, test loss-1.7132, acc-0.5118\n",
      "Iter-82380, train loss-1.7514, acc-0.5400, valid loss-1.7123, acc-0.5248, test loss-1.7132, acc-0.5117\n",
      "Iter-82390, train loss-1.7975, acc-0.4400, valid loss-1.7123, acc-0.5252, test loss-1.7131, acc-0.5120\n",
      "Iter-82400, train loss-1.7378, acc-0.4800, valid loss-1.7122, acc-0.5252, test loss-1.7131, acc-0.5120\n",
      "Iter-82410, train loss-1.8128, acc-0.4800, valid loss-1.7122, acc-0.5250, test loss-1.7130, acc-0.5122\n",
      "Iter-82420, train loss-1.7921, acc-0.4600, valid loss-1.7122, acc-0.5252, test loss-1.7130, acc-0.5122\n",
      "Iter-82430, train loss-1.6690, acc-0.5600, valid loss-1.7121, acc-0.5250, test loss-1.7130, acc-0.5122\n",
      "Iter-82440, train loss-1.6678, acc-0.5800, valid loss-1.7121, acc-0.5252, test loss-1.7129, acc-0.5122\n",
      "Iter-82450, train loss-1.6661, acc-0.5000, valid loss-1.7120, acc-0.5252, test loss-1.7129, acc-0.5122\n",
      "Iter-82460, train loss-1.8027, acc-0.5600, valid loss-1.7120, acc-0.5254, test loss-1.7129, acc-0.5123\n",
      "Iter-82470, train loss-1.6039, acc-0.6000, valid loss-1.7120, acc-0.5252, test loss-1.7128, acc-0.5121\n",
      "Iter-82480, train loss-1.7590, acc-0.6000, valid loss-1.7119, acc-0.5252, test loss-1.7128, acc-0.5122\n",
      "Iter-82490, train loss-1.8623, acc-0.4400, valid loss-1.7119, acc-0.5256, test loss-1.7128, acc-0.5124\n",
      "Iter-82500, train loss-1.6426, acc-0.5200, valid loss-1.7119, acc-0.5256, test loss-1.7127, acc-0.5126\n",
      "Iter-82510, train loss-1.6556, acc-0.5600, valid loss-1.7118, acc-0.5258, test loss-1.7127, acc-0.5124\n",
      "Iter-82520, train loss-1.7087, acc-0.5800, valid loss-1.7118, acc-0.5256, test loss-1.7126, acc-0.5126\n",
      "Iter-82530, train loss-1.7731, acc-0.4600, valid loss-1.7118, acc-0.5254, test loss-1.7126, acc-0.5128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-82540, train loss-1.7069, acc-0.4800, valid loss-1.7117, acc-0.5254, test loss-1.7126, acc-0.5126\n",
      "Iter-82550, train loss-1.7191, acc-0.5400, valid loss-1.7117, acc-0.5252, test loss-1.7125, acc-0.5126\n",
      "Iter-82560, train loss-1.6221, acc-0.6800, valid loss-1.7116, acc-0.5254, test loss-1.7125, acc-0.5124\n",
      "Iter-82570, train loss-1.7703, acc-0.5600, valid loss-1.7116, acc-0.5252, test loss-1.7125, acc-0.5126\n",
      "Iter-82580, train loss-1.7753, acc-0.4600, valid loss-1.7116, acc-0.5252, test loss-1.7124, acc-0.5124\n",
      "Iter-82590, train loss-1.7006, acc-0.5000, valid loss-1.7115, acc-0.5252, test loss-1.7124, acc-0.5122\n",
      "Iter-82600, train loss-1.6896, acc-0.5400, valid loss-1.7115, acc-0.5252, test loss-1.7124, acc-0.5123\n",
      "Iter-82610, train loss-1.7884, acc-0.5000, valid loss-1.7115, acc-0.5252, test loss-1.7123, acc-0.5124\n",
      "Iter-82620, train loss-1.8193, acc-0.4600, valid loss-1.7114, acc-0.5252, test loss-1.7123, acc-0.5127\n",
      "Iter-82630, train loss-1.7823, acc-0.5000, valid loss-1.7114, acc-0.5252, test loss-1.7122, acc-0.5127\n",
      "Iter-82640, train loss-1.5538, acc-0.5400, valid loss-1.7114, acc-0.5252, test loss-1.7122, acc-0.5126\n",
      "Iter-82650, train loss-1.7811, acc-0.4200, valid loss-1.7113, acc-0.5254, test loss-1.7122, acc-0.5125\n",
      "Iter-82660, train loss-1.7402, acc-0.4800, valid loss-1.7113, acc-0.5254, test loss-1.7121, acc-0.5124\n",
      "Iter-82670, train loss-1.7245, acc-0.5000, valid loss-1.7112, acc-0.5252, test loss-1.7121, acc-0.5123\n",
      "Iter-82680, train loss-1.7888, acc-0.4800, valid loss-1.7112, acc-0.5252, test loss-1.7121, acc-0.5122\n",
      "Iter-82690, train loss-1.7236, acc-0.5400, valid loss-1.7112, acc-0.5252, test loss-1.7120, acc-0.5121\n",
      "Iter-82700, train loss-1.7755, acc-0.4000, valid loss-1.7111, acc-0.5256, test loss-1.7120, acc-0.5123\n",
      "Iter-82710, train loss-1.7534, acc-0.4600, valid loss-1.7111, acc-0.5254, test loss-1.7120, acc-0.5122\n",
      "Iter-82720, train loss-1.8375, acc-0.5000, valid loss-1.7111, acc-0.5254, test loss-1.7119, acc-0.5122\n",
      "Iter-82730, train loss-1.8022, acc-0.5400, valid loss-1.7110, acc-0.5254, test loss-1.7119, acc-0.5123\n",
      "Iter-82740, train loss-1.6517, acc-0.5400, valid loss-1.7110, acc-0.5256, test loss-1.7118, acc-0.5123\n",
      "Iter-82750, train loss-1.7599, acc-0.4200, valid loss-1.7110, acc-0.5256, test loss-1.7118, acc-0.5123\n",
      "Iter-82760, train loss-1.6549, acc-0.5200, valid loss-1.7109, acc-0.5254, test loss-1.7118, acc-0.5124\n",
      "Iter-82770, train loss-1.8045, acc-0.4600, valid loss-1.7109, acc-0.5254, test loss-1.7117, acc-0.5123\n",
      "Iter-82780, train loss-1.8508, acc-0.3800, valid loss-1.7109, acc-0.5254, test loss-1.7117, acc-0.5123\n",
      "Iter-82790, train loss-1.6916, acc-0.4800, valid loss-1.7108, acc-0.5252, test loss-1.7117, acc-0.5123\n",
      "Iter-82800, train loss-1.6629, acc-0.5200, valid loss-1.7108, acc-0.5252, test loss-1.7116, acc-0.5125\n",
      "Iter-82810, train loss-1.8084, acc-0.4400, valid loss-1.7107, acc-0.5254, test loss-1.7116, acc-0.5124\n",
      "Iter-82820, train loss-1.7371, acc-0.5200, valid loss-1.7107, acc-0.5258, test loss-1.7116, acc-0.5124\n",
      "Iter-82830, train loss-1.7499, acc-0.4400, valid loss-1.7107, acc-0.5256, test loss-1.7115, acc-0.5125\n",
      "Iter-82840, train loss-1.8103, acc-0.3800, valid loss-1.7106, acc-0.5256, test loss-1.7115, acc-0.5125\n",
      "Iter-82850, train loss-1.7665, acc-0.5000, valid loss-1.7106, acc-0.5254, test loss-1.7114, acc-0.5125\n",
      "Iter-82860, train loss-1.7945, acc-0.4000, valid loss-1.7106, acc-0.5256, test loss-1.7114, acc-0.5125\n",
      "Iter-82870, train loss-1.8145, acc-0.3800, valid loss-1.7105, acc-0.5258, test loss-1.7114, acc-0.5125\n",
      "Iter-82880, train loss-1.6699, acc-0.5200, valid loss-1.7105, acc-0.5256, test loss-1.7113, acc-0.5125\n",
      "Iter-82890, train loss-1.6798, acc-0.5600, valid loss-1.7104, acc-0.5260, test loss-1.7113, acc-0.5125\n",
      "Iter-82900, train loss-1.7312, acc-0.5000, valid loss-1.7104, acc-0.5258, test loss-1.7113, acc-0.5127\n",
      "Iter-82910, train loss-1.7563, acc-0.4600, valid loss-1.7104, acc-0.5258, test loss-1.7112, acc-0.5126\n",
      "Iter-82920, train loss-1.7061, acc-0.5800, valid loss-1.7103, acc-0.5256, test loss-1.7112, acc-0.5127\n",
      "Iter-82930, train loss-1.7953, acc-0.4400, valid loss-1.7103, acc-0.5256, test loss-1.7112, acc-0.5128\n",
      "Iter-82940, train loss-1.6257, acc-0.6000, valid loss-1.7103, acc-0.5256, test loss-1.7111, acc-0.5128\n",
      "Iter-82950, train loss-1.7749, acc-0.4600, valid loss-1.7102, acc-0.5254, test loss-1.7111, acc-0.5128\n",
      "Iter-82960, train loss-1.6665, acc-0.5600, valid loss-1.7102, acc-0.5256, test loss-1.7110, acc-0.5129\n",
      "Iter-82970, train loss-1.7461, acc-0.4600, valid loss-1.7102, acc-0.5256, test loss-1.7110, acc-0.5126\n",
      "Iter-82980, train loss-1.7828, acc-0.4600, valid loss-1.7101, acc-0.5254, test loss-1.7110, acc-0.5126\n",
      "Iter-82990, train loss-1.7575, acc-0.4200, valid loss-1.7101, acc-0.5254, test loss-1.7109, acc-0.5126\n",
      "Iter-83000, train loss-1.7758, acc-0.4800, valid loss-1.7100, acc-0.5256, test loss-1.7109, acc-0.5126\n",
      "Iter-83010, train loss-1.7076, acc-0.5400, valid loss-1.7100, acc-0.5256, test loss-1.7109, acc-0.5126\n",
      "Iter-83020, train loss-1.6319, acc-0.6000, valid loss-1.7100, acc-0.5258, test loss-1.7108, acc-0.5126\n",
      "Iter-83030, train loss-1.7468, acc-0.5400, valid loss-1.7099, acc-0.5258, test loss-1.7108, acc-0.5126\n",
      "Iter-83040, train loss-1.8513, acc-0.4000, valid loss-1.7099, acc-0.5258, test loss-1.7107, acc-0.5125\n",
      "Iter-83050, train loss-1.7314, acc-0.6000, valid loss-1.7099, acc-0.5258, test loss-1.7107, acc-0.5124\n",
      "Iter-83060, train loss-1.6485, acc-0.6600, valid loss-1.7098, acc-0.5260, test loss-1.7107, acc-0.5126\n",
      "Iter-83070, train loss-1.6263, acc-0.5400, valid loss-1.7098, acc-0.5260, test loss-1.7106, acc-0.5125\n",
      "Iter-83080, train loss-1.7286, acc-0.3800, valid loss-1.7097, acc-0.5260, test loss-1.7106, acc-0.5128\n",
      "Iter-83090, train loss-1.6891, acc-0.5000, valid loss-1.7097, acc-0.5260, test loss-1.7106, acc-0.5130\n",
      "Iter-83100, train loss-1.8051, acc-0.4800, valid loss-1.7097, acc-0.5260, test loss-1.7105, acc-0.5128\n",
      "Iter-83110, train loss-1.7929, acc-0.4800, valid loss-1.7096, acc-0.5260, test loss-1.7105, acc-0.5129\n",
      "Iter-83120, train loss-1.7268, acc-0.5000, valid loss-1.7096, acc-0.5260, test loss-1.7105, acc-0.5130\n",
      "Iter-83130, train loss-1.6087, acc-0.6200, valid loss-1.7095, acc-0.5262, test loss-1.7104, acc-0.5130\n",
      "Iter-83140, train loss-1.7113, acc-0.5000, valid loss-1.7095, acc-0.5260, test loss-1.7104, acc-0.5130\n",
      "Iter-83150, train loss-1.7953, acc-0.4600, valid loss-1.7095, acc-0.5260, test loss-1.7103, acc-0.5129\n",
      "Iter-83160, train loss-1.7578, acc-0.5000, valid loss-1.7094, acc-0.5258, test loss-1.7103, acc-0.5130\n",
      "Iter-83170, train loss-1.6192, acc-0.6000, valid loss-1.7094, acc-0.5258, test loss-1.7103, acc-0.5129\n",
      "Iter-83180, train loss-1.6861, acc-0.5600, valid loss-1.7094, acc-0.5258, test loss-1.7102, acc-0.5131\n",
      "Iter-83190, train loss-1.5514, acc-0.6200, valid loss-1.7093, acc-0.5258, test loss-1.7102, acc-0.5130\n",
      "Iter-83200, train loss-1.6225, acc-0.5800, valid loss-1.7093, acc-0.5258, test loss-1.7102, acc-0.5130\n",
      "Iter-83210, train loss-1.6690, acc-0.6200, valid loss-1.7093, acc-0.5258, test loss-1.7101, acc-0.5130\n",
      "Iter-83220, train loss-1.6106, acc-0.4400, valid loss-1.7092, acc-0.5258, test loss-1.7101, acc-0.5131\n",
      "Iter-83230, train loss-1.6885, acc-0.5800, valid loss-1.7092, acc-0.5258, test loss-1.7100, acc-0.5130\n",
      "Iter-83240, train loss-1.7719, acc-0.5600, valid loss-1.7092, acc-0.5258, test loss-1.7100, acc-0.5132\n",
      "Iter-83250, train loss-1.6642, acc-0.6000, valid loss-1.7091, acc-0.5262, test loss-1.7100, acc-0.5131\n",
      "Iter-83260, train loss-1.7444, acc-0.5400, valid loss-1.7091, acc-0.5260, test loss-1.7099, acc-0.5132\n",
      "Iter-83270, train loss-1.6948, acc-0.5200, valid loss-1.7090, acc-0.5258, test loss-1.7099, acc-0.5131\n",
      "Iter-83280, train loss-1.6824, acc-0.5600, valid loss-1.7090, acc-0.5258, test loss-1.7099, acc-0.5134\n",
      "Iter-83290, train loss-1.7237, acc-0.5200, valid loss-1.7090, acc-0.5260, test loss-1.7098, acc-0.5134\n",
      "Iter-83300, train loss-1.5755, acc-0.6400, valid loss-1.7089, acc-0.5258, test loss-1.7098, acc-0.5133\n",
      "Iter-83310, train loss-1.7795, acc-0.4400, valid loss-1.7089, acc-0.5258, test loss-1.7098, acc-0.5135\n",
      "Iter-83320, train loss-1.7572, acc-0.4400, valid loss-1.7089, acc-0.5258, test loss-1.7097, acc-0.5134\n",
      "Iter-83330, train loss-1.8318, acc-0.4400, valid loss-1.7088, acc-0.5262, test loss-1.7097, acc-0.5134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-83340, train loss-1.6504, acc-0.5600, valid loss-1.7088, acc-0.5262, test loss-1.7096, acc-0.5134\n",
      "Iter-83350, train loss-1.7048, acc-0.4600, valid loss-1.7088, acc-0.5262, test loss-1.7096, acc-0.5135\n",
      "Iter-83360, train loss-1.6108, acc-0.6200, valid loss-1.7087, acc-0.5262, test loss-1.7096, acc-0.5135\n",
      "Iter-83370, train loss-1.7178, acc-0.5600, valid loss-1.7087, acc-0.5262, test loss-1.7095, acc-0.5134\n",
      "Iter-83380, train loss-1.6475, acc-0.5600, valid loss-1.7086, acc-0.5262, test loss-1.7095, acc-0.5133\n",
      "Iter-83390, train loss-1.6261, acc-0.6000, valid loss-1.7086, acc-0.5260, test loss-1.7095, acc-0.5134\n",
      "Iter-83400, train loss-1.6874, acc-0.5200, valid loss-1.7086, acc-0.5260, test loss-1.7094, acc-0.5134\n",
      "Iter-83410, train loss-1.8353, acc-0.3600, valid loss-1.7085, acc-0.5260, test loss-1.7094, acc-0.5136\n",
      "Iter-83420, train loss-1.7318, acc-0.4400, valid loss-1.7085, acc-0.5260, test loss-1.7094, acc-0.5135\n",
      "Iter-83430, train loss-1.6805, acc-0.5600, valid loss-1.7085, acc-0.5260, test loss-1.7093, acc-0.5135\n",
      "Iter-83440, train loss-1.6974, acc-0.5600, valid loss-1.7084, acc-0.5258, test loss-1.7093, acc-0.5134\n",
      "Iter-83450, train loss-1.6834, acc-0.4800, valid loss-1.7084, acc-0.5258, test loss-1.7093, acc-0.5134\n",
      "Iter-83460, train loss-1.5801, acc-0.6200, valid loss-1.7083, acc-0.5260, test loss-1.7092, acc-0.5132\n",
      "Iter-83470, train loss-1.6617, acc-0.6000, valid loss-1.7083, acc-0.5260, test loss-1.7092, acc-0.5135\n",
      "Iter-83480, train loss-1.7672, acc-0.4400, valid loss-1.7083, acc-0.5260, test loss-1.7091, acc-0.5133\n",
      "Iter-83490, train loss-1.6920, acc-0.5400, valid loss-1.7082, acc-0.5262, test loss-1.7091, acc-0.5133\n",
      "Iter-83500, train loss-1.8264, acc-0.4400, valid loss-1.7082, acc-0.5262, test loss-1.7091, acc-0.5135\n",
      "Iter-83510, train loss-1.7627, acc-0.5200, valid loss-1.7082, acc-0.5262, test loss-1.7090, acc-0.5135\n",
      "Iter-83520, train loss-1.6627, acc-0.5200, valid loss-1.7081, acc-0.5262, test loss-1.7090, acc-0.5133\n",
      "Iter-83530, train loss-1.8139, acc-0.4400, valid loss-1.7081, acc-0.5262, test loss-1.7090, acc-0.5133\n",
      "Iter-83540, train loss-1.7114, acc-0.5400, valid loss-1.7081, acc-0.5262, test loss-1.7089, acc-0.5133\n",
      "Iter-83550, train loss-1.7905, acc-0.4200, valid loss-1.7080, acc-0.5262, test loss-1.7089, acc-0.5133\n",
      "Iter-83560, train loss-1.5838, acc-0.5800, valid loss-1.7080, acc-0.5262, test loss-1.7089, acc-0.5133\n",
      "Iter-83570, train loss-1.7734, acc-0.4400, valid loss-1.7079, acc-0.5262, test loss-1.7088, acc-0.5132\n",
      "Iter-83580, train loss-1.8173, acc-0.4200, valid loss-1.7079, acc-0.5262, test loss-1.7088, acc-0.5132\n",
      "Iter-83590, train loss-1.7093, acc-0.5000, valid loss-1.7079, acc-0.5262, test loss-1.7088, acc-0.5133\n",
      "Iter-83600, train loss-1.7683, acc-0.4600, valid loss-1.7078, acc-0.5262, test loss-1.7087, acc-0.5134\n",
      "Iter-83610, train loss-1.7166, acc-0.4000, valid loss-1.7078, acc-0.5264, test loss-1.7087, acc-0.5135\n",
      "Iter-83620, train loss-1.7718, acc-0.4800, valid loss-1.7078, acc-0.5266, test loss-1.7087, acc-0.5134\n",
      "Iter-83630, train loss-1.6340, acc-0.5600, valid loss-1.7077, acc-0.5264, test loss-1.7086, acc-0.5134\n",
      "Iter-83640, train loss-1.7340, acc-0.5200, valid loss-1.7077, acc-0.5264, test loss-1.7086, acc-0.5134\n",
      "Iter-83650, train loss-1.8559, acc-0.3600, valid loss-1.7077, acc-0.5266, test loss-1.7085, acc-0.5134\n",
      "Iter-83660, train loss-1.7338, acc-0.4600, valid loss-1.7076, acc-0.5266, test loss-1.7085, acc-0.5134\n",
      "Iter-83670, train loss-1.7175, acc-0.4200, valid loss-1.7076, acc-0.5266, test loss-1.7085, acc-0.5133\n",
      "Iter-83680, train loss-1.6562, acc-0.5800, valid loss-1.7075, acc-0.5266, test loss-1.7084, acc-0.5134\n",
      "Iter-83690, train loss-1.7652, acc-0.4200, valid loss-1.7075, acc-0.5268, test loss-1.7084, acc-0.5133\n",
      "Iter-83700, train loss-1.7571, acc-0.5200, valid loss-1.7075, acc-0.5268, test loss-1.7084, acc-0.5133\n",
      "Iter-83710, train loss-1.6813, acc-0.4800, valid loss-1.7074, acc-0.5268, test loss-1.7083, acc-0.5133\n",
      "Iter-83720, train loss-1.7227, acc-0.5000, valid loss-1.7074, acc-0.5268, test loss-1.7083, acc-0.5134\n",
      "Iter-83730, train loss-1.7326, acc-0.4600, valid loss-1.7074, acc-0.5270, test loss-1.7083, acc-0.5135\n",
      "Iter-83740, train loss-1.7492, acc-0.5200, valid loss-1.7073, acc-0.5266, test loss-1.7082, acc-0.5136\n",
      "Iter-83750, train loss-1.6933, acc-0.4600, valid loss-1.7073, acc-0.5268, test loss-1.7082, acc-0.5135\n",
      "Iter-83760, train loss-1.6664, acc-0.5800, valid loss-1.7073, acc-0.5266, test loss-1.7081, acc-0.5136\n",
      "Iter-83770, train loss-1.6593, acc-0.5000, valid loss-1.7072, acc-0.5266, test loss-1.7081, acc-0.5137\n",
      "Iter-83780, train loss-1.7697, acc-0.5400, valid loss-1.7072, acc-0.5268, test loss-1.7081, acc-0.5138\n",
      "Iter-83790, train loss-1.7804, acc-0.4200, valid loss-1.7072, acc-0.5268, test loss-1.7080, acc-0.5137\n",
      "Iter-83800, train loss-1.6663, acc-0.5400, valid loss-1.7071, acc-0.5268, test loss-1.7080, acc-0.5137\n",
      "Iter-83810, train loss-1.7777, acc-0.4000, valid loss-1.7071, acc-0.5268, test loss-1.7080, acc-0.5137\n",
      "Iter-83820, train loss-1.7052, acc-0.4800, valid loss-1.7071, acc-0.5268, test loss-1.7079, acc-0.5136\n",
      "Iter-83830, train loss-1.7489, acc-0.5000, valid loss-1.7070, acc-0.5268, test loss-1.7079, acc-0.5137\n",
      "Iter-83840, train loss-1.7437, acc-0.5000, valid loss-1.7070, acc-0.5268, test loss-1.7079, acc-0.5138\n",
      "Iter-83850, train loss-1.7083, acc-0.4400, valid loss-1.7069, acc-0.5268, test loss-1.7078, acc-0.5136\n",
      "Iter-83860, train loss-1.7996, acc-0.3800, valid loss-1.7069, acc-0.5268, test loss-1.7078, acc-0.5135\n",
      "Iter-83870, train loss-1.7091, acc-0.6000, valid loss-1.7069, acc-0.5270, test loss-1.7078, acc-0.5138\n",
      "Iter-83880, train loss-1.8630, acc-0.4800, valid loss-1.7068, acc-0.5270, test loss-1.7077, acc-0.5138\n",
      "Iter-83890, train loss-1.6939, acc-0.5400, valid loss-1.7068, acc-0.5270, test loss-1.7077, acc-0.5135\n",
      "Iter-83900, train loss-1.6953, acc-0.5200, valid loss-1.7068, acc-0.5270, test loss-1.7076, acc-0.5136\n",
      "Iter-83910, train loss-1.6397, acc-0.5200, valid loss-1.7067, acc-0.5270, test loss-1.7076, acc-0.5135\n",
      "Iter-83920, train loss-1.7522, acc-0.5000, valid loss-1.7067, acc-0.5270, test loss-1.7076, acc-0.5136\n",
      "Iter-83930, train loss-1.6441, acc-0.6200, valid loss-1.7067, acc-0.5268, test loss-1.7075, acc-0.5137\n",
      "Iter-83940, train loss-1.6911, acc-0.6000, valid loss-1.7066, acc-0.5268, test loss-1.7075, acc-0.5139\n",
      "Iter-83950, train loss-1.7035, acc-0.6000, valid loss-1.7066, acc-0.5268, test loss-1.7075, acc-0.5139\n",
      "Iter-83960, train loss-1.7144, acc-0.5400, valid loss-1.7066, acc-0.5268, test loss-1.7074, acc-0.5139\n",
      "Iter-83970, train loss-1.6970, acc-0.4800, valid loss-1.7065, acc-0.5268, test loss-1.7074, acc-0.5138\n",
      "Iter-83980, train loss-1.6924, acc-0.5600, valid loss-1.7065, acc-0.5268, test loss-1.7074, acc-0.5137\n",
      "Iter-83990, train loss-1.6509, acc-0.6200, valid loss-1.7064, acc-0.5270, test loss-1.7073, acc-0.5137\n",
      "Iter-84000, train loss-1.8488, acc-0.3200, valid loss-1.7064, acc-0.5266, test loss-1.7073, acc-0.5137\n",
      "Iter-84010, train loss-1.7202, acc-0.4200, valid loss-1.7064, acc-0.5268, test loss-1.7073, acc-0.5137\n",
      "Iter-84020, train loss-1.7090, acc-0.5600, valid loss-1.7063, acc-0.5268, test loss-1.7072, acc-0.5138\n",
      "Iter-84030, train loss-1.6286, acc-0.5800, valid loss-1.7063, acc-0.5268, test loss-1.7072, acc-0.5139\n",
      "Iter-84040, train loss-1.7602, acc-0.4800, valid loss-1.7063, acc-0.5270, test loss-1.7071, acc-0.5136\n",
      "Iter-84050, train loss-1.7033, acc-0.5000, valid loss-1.7062, acc-0.5270, test loss-1.7071, acc-0.5136\n",
      "Iter-84060, train loss-1.7651, acc-0.4000, valid loss-1.7062, acc-0.5270, test loss-1.7071, acc-0.5136\n",
      "Iter-84070, train loss-1.7141, acc-0.4800, valid loss-1.7061, acc-0.5268, test loss-1.7070, acc-0.5138\n",
      "Iter-84080, train loss-1.7574, acc-0.4600, valid loss-1.7061, acc-0.5268, test loss-1.7070, acc-0.5137\n",
      "Iter-84090, train loss-1.5952, acc-0.6600, valid loss-1.7061, acc-0.5272, test loss-1.7070, acc-0.5138\n",
      "Iter-84100, train loss-1.7407, acc-0.5400, valid loss-1.7060, acc-0.5270, test loss-1.7069, acc-0.5138\n",
      "Iter-84110, train loss-1.8776, acc-0.4200, valid loss-1.7060, acc-0.5266, test loss-1.7069, acc-0.5138\n",
      "Iter-84120, train loss-1.6795, acc-0.5200, valid loss-1.7060, acc-0.5268, test loss-1.7069, acc-0.5138\n",
      "Iter-84130, train loss-1.6627, acc-0.6200, valid loss-1.7059, acc-0.5266, test loss-1.7068, acc-0.5137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-84140, train loss-1.6123, acc-0.5800, valid loss-1.7059, acc-0.5266, test loss-1.7068, acc-0.5137\n",
      "Iter-84150, train loss-1.6964, acc-0.5400, valid loss-1.7059, acc-0.5266, test loss-1.7068, acc-0.5137\n",
      "Iter-84160, train loss-1.6747, acc-0.5600, valid loss-1.7058, acc-0.5268, test loss-1.7067, acc-0.5138\n",
      "Iter-84170, train loss-1.7442, acc-0.4200, valid loss-1.7058, acc-0.5270, test loss-1.7067, acc-0.5138\n",
      "Iter-84180, train loss-1.7713, acc-0.5000, valid loss-1.7057, acc-0.5268, test loss-1.7066, acc-0.5139\n",
      "Iter-84190, train loss-1.8078, acc-0.4600, valid loss-1.7057, acc-0.5266, test loss-1.7066, acc-0.5137\n",
      "Iter-84200, train loss-1.7099, acc-0.4800, valid loss-1.7057, acc-0.5264, test loss-1.7066, acc-0.5138\n",
      "Iter-84210, train loss-1.6289, acc-0.6600, valid loss-1.7056, acc-0.5264, test loss-1.7065, acc-0.5138\n",
      "Iter-84220, train loss-1.5972, acc-0.6600, valid loss-1.7056, acc-0.5266, test loss-1.7065, acc-0.5139\n",
      "Iter-84230, train loss-1.8337, acc-0.5200, valid loss-1.7056, acc-0.5266, test loss-1.7065, acc-0.5138\n",
      "Iter-84240, train loss-1.7523, acc-0.4400, valid loss-1.7055, acc-0.5266, test loss-1.7064, acc-0.5138\n",
      "Iter-84250, train loss-1.7741, acc-0.5200, valid loss-1.7055, acc-0.5266, test loss-1.7064, acc-0.5139\n",
      "Iter-84260, train loss-1.6627, acc-0.6000, valid loss-1.7054, acc-0.5268, test loss-1.7064, acc-0.5139\n",
      "Iter-84270, train loss-1.6690, acc-0.5200, valid loss-1.7054, acc-0.5268, test loss-1.7063, acc-0.5139\n",
      "Iter-84280, train loss-1.7830, acc-0.4200, valid loss-1.7054, acc-0.5270, test loss-1.7063, acc-0.5138\n",
      "Iter-84290, train loss-1.6358, acc-0.5600, valid loss-1.7053, acc-0.5272, test loss-1.7062, acc-0.5138\n",
      "Iter-84300, train loss-1.7411, acc-0.5200, valid loss-1.7053, acc-0.5270, test loss-1.7062, acc-0.5138\n",
      "Iter-84310, train loss-1.6902, acc-0.5400, valid loss-1.7053, acc-0.5270, test loss-1.7062, acc-0.5138\n",
      "Iter-84320, train loss-1.8301, acc-0.3400, valid loss-1.7052, acc-0.5272, test loss-1.7061, acc-0.5137\n",
      "Iter-84330, train loss-1.6352, acc-0.6000, valid loss-1.7052, acc-0.5270, test loss-1.7061, acc-0.5137\n",
      "Iter-84340, train loss-1.7339, acc-0.4800, valid loss-1.7052, acc-0.5270, test loss-1.7061, acc-0.5138\n",
      "Iter-84350, train loss-1.6731, acc-0.6000, valid loss-1.7051, acc-0.5270, test loss-1.7060, acc-0.5137\n",
      "Iter-84360, train loss-1.6563, acc-0.6600, valid loss-1.7051, acc-0.5270, test loss-1.7060, acc-0.5138\n",
      "Iter-84370, train loss-1.7453, acc-0.4600, valid loss-1.7050, acc-0.5270, test loss-1.7060, acc-0.5139\n",
      "Iter-84380, train loss-1.7220, acc-0.5400, valid loss-1.7050, acc-0.5270, test loss-1.7059, acc-0.5140\n",
      "Iter-84390, train loss-1.6742, acc-0.5800, valid loss-1.7050, acc-0.5270, test loss-1.7059, acc-0.5141\n",
      "Iter-84400, train loss-1.7929, acc-0.5600, valid loss-1.7049, acc-0.5272, test loss-1.7059, acc-0.5141\n",
      "Iter-84410, train loss-1.6714, acc-0.5600, valid loss-1.7049, acc-0.5266, test loss-1.7058, acc-0.5140\n",
      "Iter-84420, train loss-1.8901, acc-0.3800, valid loss-1.7049, acc-0.5268, test loss-1.7058, acc-0.5140\n",
      "Iter-84430, train loss-1.6468, acc-0.6400, valid loss-1.7048, acc-0.5268, test loss-1.7057, acc-0.5141\n",
      "Iter-84440, train loss-1.6633, acc-0.5600, valid loss-1.7048, acc-0.5268, test loss-1.7057, acc-0.5142\n",
      "Iter-84450, train loss-1.8055, acc-0.4400, valid loss-1.7048, acc-0.5268, test loss-1.7057, acc-0.5140\n",
      "Iter-84460, train loss-1.7020, acc-0.5200, valid loss-1.7047, acc-0.5268, test loss-1.7056, acc-0.5144\n",
      "Iter-84470, train loss-1.7604, acc-0.4400, valid loss-1.7047, acc-0.5270, test loss-1.7056, acc-0.5142\n",
      "Iter-84480, train loss-1.7684, acc-0.4200, valid loss-1.7047, acc-0.5268, test loss-1.7056, acc-0.5143\n",
      "Iter-84490, train loss-1.6428, acc-0.5200, valid loss-1.7046, acc-0.5268, test loss-1.7055, acc-0.5141\n",
      "Iter-84500, train loss-1.7975, acc-0.4200, valid loss-1.7046, acc-0.5268, test loss-1.7055, acc-0.5142\n",
      "Iter-84510, train loss-1.7289, acc-0.4400, valid loss-1.7045, acc-0.5268, test loss-1.7055, acc-0.5142\n",
      "Iter-84520, train loss-1.7466, acc-0.5200, valid loss-1.7045, acc-0.5268, test loss-1.7054, acc-0.5140\n",
      "Iter-84530, train loss-1.7091, acc-0.4400, valid loss-1.7045, acc-0.5270, test loss-1.7054, acc-0.5142\n",
      "Iter-84540, train loss-1.6763, acc-0.4800, valid loss-1.7044, acc-0.5268, test loss-1.7053, acc-0.5143\n",
      "Iter-84550, train loss-1.6859, acc-0.6400, valid loss-1.7044, acc-0.5268, test loss-1.7053, acc-0.5143\n",
      "Iter-84560, train loss-1.7533, acc-0.5200, valid loss-1.7044, acc-0.5266, test loss-1.7053, acc-0.5141\n",
      "Iter-84570, train loss-1.7181, acc-0.4400, valid loss-1.7043, acc-0.5266, test loss-1.7052, acc-0.5141\n",
      "Iter-84580, train loss-1.7734, acc-0.5400, valid loss-1.7043, acc-0.5266, test loss-1.7052, acc-0.5142\n",
      "Iter-84590, train loss-1.6490, acc-0.6400, valid loss-1.7043, acc-0.5266, test loss-1.7052, acc-0.5141\n",
      "Iter-84600, train loss-1.8099, acc-0.4600, valid loss-1.7042, acc-0.5268, test loss-1.7051, acc-0.5142\n",
      "Iter-84610, train loss-1.7706, acc-0.5000, valid loss-1.7042, acc-0.5272, test loss-1.7051, acc-0.5144\n",
      "Iter-84620, train loss-1.7471, acc-0.4000, valid loss-1.7041, acc-0.5274, test loss-1.7051, acc-0.5144\n",
      "Iter-84630, train loss-1.7239, acc-0.5200, valid loss-1.7041, acc-0.5274, test loss-1.7050, acc-0.5144\n",
      "Iter-84640, train loss-1.7541, acc-0.5400, valid loss-1.7041, acc-0.5274, test loss-1.7050, acc-0.5143\n",
      "Iter-84650, train loss-1.7642, acc-0.4400, valid loss-1.7040, acc-0.5272, test loss-1.7050, acc-0.5143\n",
      "Iter-84660, train loss-1.7359, acc-0.4400, valid loss-1.7040, acc-0.5272, test loss-1.7049, acc-0.5144\n",
      "Iter-84670, train loss-1.6508, acc-0.5000, valid loss-1.7040, acc-0.5270, test loss-1.7049, acc-0.5143\n",
      "Iter-84680, train loss-1.7808, acc-0.5000, valid loss-1.7039, acc-0.5272, test loss-1.7048, acc-0.5143\n",
      "Iter-84690, train loss-1.6213, acc-0.6200, valid loss-1.7039, acc-0.5270, test loss-1.7048, acc-0.5143\n",
      "Iter-84700, train loss-1.7722, acc-0.4200, valid loss-1.7039, acc-0.5270, test loss-1.7048, acc-0.5144\n",
      "Iter-84710, train loss-1.7056, acc-0.6200, valid loss-1.7038, acc-0.5270, test loss-1.7047, acc-0.5143\n",
      "Iter-84720, train loss-1.6692, acc-0.6200, valid loss-1.7038, acc-0.5274, test loss-1.7047, acc-0.5145\n",
      "Iter-84730, train loss-1.8756, acc-0.3800, valid loss-1.7038, acc-0.5276, test loss-1.7047, acc-0.5143\n",
      "Iter-84740, train loss-1.6465, acc-0.5800, valid loss-1.7037, acc-0.5276, test loss-1.7046, acc-0.5143\n",
      "Iter-84750, train loss-1.7036, acc-0.4600, valid loss-1.7037, acc-0.5274, test loss-1.7046, acc-0.5146\n",
      "Iter-84760, train loss-1.8030, acc-0.4800, valid loss-1.7036, acc-0.5276, test loss-1.7046, acc-0.5145\n",
      "Iter-84770, train loss-1.6671, acc-0.5400, valid loss-1.7036, acc-0.5276, test loss-1.7045, acc-0.5145\n",
      "Iter-84780, train loss-1.7134, acc-0.5800, valid loss-1.7036, acc-0.5276, test loss-1.7045, acc-0.5144\n",
      "Iter-84790, train loss-1.6980, acc-0.5600, valid loss-1.7035, acc-0.5276, test loss-1.7044, acc-0.5145\n",
      "Iter-84800, train loss-1.7363, acc-0.5000, valid loss-1.7035, acc-0.5276, test loss-1.7044, acc-0.5146\n",
      "Iter-84810, train loss-1.7726, acc-0.4800, valid loss-1.7035, acc-0.5276, test loss-1.7044, acc-0.5145\n",
      "Iter-84820, train loss-1.7463, acc-0.5000, valid loss-1.7034, acc-0.5274, test loss-1.7043, acc-0.5145\n",
      "Iter-84830, train loss-1.7635, acc-0.4800, valid loss-1.7034, acc-0.5272, test loss-1.7043, acc-0.5145\n",
      "Iter-84840, train loss-1.6276, acc-0.6000, valid loss-1.7034, acc-0.5272, test loss-1.7043, acc-0.5145\n",
      "Iter-84850, train loss-1.5716, acc-0.6200, valid loss-1.7033, acc-0.5272, test loss-1.7042, acc-0.5145\n",
      "Iter-84860, train loss-1.7037, acc-0.5200, valid loss-1.7033, acc-0.5272, test loss-1.7042, acc-0.5146\n",
      "Iter-84870, train loss-1.6736, acc-0.6200, valid loss-1.7033, acc-0.5272, test loss-1.7042, acc-0.5146\n",
      "Iter-84880, train loss-1.6639, acc-0.5600, valid loss-1.7032, acc-0.5272, test loss-1.7041, acc-0.5147\n",
      "Iter-84890, train loss-1.5052, acc-0.7000, valid loss-1.7032, acc-0.5272, test loss-1.7041, acc-0.5146\n",
      "Iter-84900, train loss-1.8580, acc-0.3600, valid loss-1.7032, acc-0.5272, test loss-1.7041, acc-0.5146\n",
      "Iter-84910, train loss-1.6703, acc-0.5600, valid loss-1.7031, acc-0.5274, test loss-1.7040, acc-0.5145\n",
      "Iter-84920, train loss-1.6547, acc-0.5200, valid loss-1.7031, acc-0.5274, test loss-1.7040, acc-0.5147\n",
      "Iter-84930, train loss-1.8396, acc-0.4200, valid loss-1.7031, acc-0.5276, test loss-1.7040, acc-0.5146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-84940, train loss-1.6256, acc-0.5600, valid loss-1.7030, acc-0.5276, test loss-1.7039, acc-0.5145\n",
      "Iter-84950, train loss-1.8389, acc-0.3800, valid loss-1.7030, acc-0.5276, test loss-1.7039, acc-0.5145\n",
      "Iter-84960, train loss-1.7684, acc-0.4600, valid loss-1.7029, acc-0.5274, test loss-1.7038, acc-0.5145\n",
      "Iter-84970, train loss-1.6871, acc-0.4800, valid loss-1.7029, acc-0.5276, test loss-1.7038, acc-0.5145\n",
      "Iter-84980, train loss-1.7449, acc-0.5400, valid loss-1.7029, acc-0.5276, test loss-1.7038, acc-0.5147\n",
      "Iter-84990, train loss-1.6552, acc-0.5000, valid loss-1.7028, acc-0.5276, test loss-1.7037, acc-0.5147\n",
      "Iter-85000, train loss-1.7551, acc-0.4200, valid loss-1.7028, acc-0.5274, test loss-1.7037, acc-0.5147\n",
      "Iter-85010, train loss-1.6169, acc-0.6000, valid loss-1.7028, acc-0.5274, test loss-1.7037, acc-0.5146\n",
      "Iter-85020, train loss-1.6355, acc-0.5600, valid loss-1.7027, acc-0.5274, test loss-1.7036, acc-0.5146\n",
      "Iter-85030, train loss-1.7087, acc-0.4800, valid loss-1.7027, acc-0.5274, test loss-1.7036, acc-0.5146\n",
      "Iter-85040, train loss-1.7960, acc-0.4400, valid loss-1.7027, acc-0.5274, test loss-1.7036, acc-0.5146\n",
      "Iter-85050, train loss-1.6935, acc-0.5600, valid loss-1.7026, acc-0.5274, test loss-1.7035, acc-0.5146\n",
      "Iter-85060, train loss-1.8208, acc-0.3600, valid loss-1.7026, acc-0.5274, test loss-1.7035, acc-0.5146\n",
      "Iter-85070, train loss-1.7826, acc-0.4800, valid loss-1.7025, acc-0.5270, test loss-1.7035, acc-0.5146\n",
      "Iter-85080, train loss-1.7739, acc-0.4000, valid loss-1.7025, acc-0.5276, test loss-1.7034, acc-0.5146\n",
      "Iter-85090, train loss-1.7405, acc-0.5400, valid loss-1.7025, acc-0.5274, test loss-1.7034, acc-0.5146\n",
      "Iter-85100, train loss-1.7276, acc-0.4600, valid loss-1.7024, acc-0.5274, test loss-1.7034, acc-0.5146\n",
      "Iter-85110, train loss-1.7014, acc-0.5600, valid loss-1.7024, acc-0.5272, test loss-1.7033, acc-0.5146\n",
      "Iter-85120, train loss-1.7813, acc-0.4400, valid loss-1.7024, acc-0.5272, test loss-1.7033, acc-0.5148\n",
      "Iter-85130, train loss-1.7416, acc-0.5000, valid loss-1.7023, acc-0.5274, test loss-1.7032, acc-0.5148\n",
      "Iter-85140, train loss-1.6857, acc-0.5800, valid loss-1.7023, acc-0.5276, test loss-1.7032, acc-0.5147\n",
      "Iter-85150, train loss-1.7960, acc-0.5000, valid loss-1.7023, acc-0.5274, test loss-1.7032, acc-0.5148\n",
      "Iter-85160, train loss-1.6835, acc-0.6000, valid loss-1.7022, acc-0.5274, test loss-1.7031, acc-0.5148\n",
      "Iter-85170, train loss-1.7989, acc-0.4800, valid loss-1.7022, acc-0.5272, test loss-1.7031, acc-0.5149\n",
      "Iter-85180, train loss-1.7629, acc-0.3800, valid loss-1.7022, acc-0.5274, test loss-1.7031, acc-0.5147\n",
      "Iter-85190, train loss-1.7438, acc-0.5400, valid loss-1.7021, acc-0.5274, test loss-1.7030, acc-0.5149\n",
      "Iter-85200, train loss-1.7144, acc-0.5600, valid loss-1.7021, acc-0.5274, test loss-1.7030, acc-0.5146\n",
      "Iter-85210, train loss-1.7290, acc-0.5600, valid loss-1.7021, acc-0.5274, test loss-1.7030, acc-0.5148\n",
      "Iter-85220, train loss-1.7275, acc-0.4600, valid loss-1.7020, acc-0.5274, test loss-1.7029, acc-0.5147\n",
      "Iter-85230, train loss-1.6195, acc-0.6200, valid loss-1.7020, acc-0.5274, test loss-1.7029, acc-0.5147\n",
      "Iter-85240, train loss-1.7784, acc-0.4400, valid loss-1.7019, acc-0.5274, test loss-1.7029, acc-0.5146\n",
      "Iter-85250, train loss-1.7870, acc-0.5200, valid loss-1.7019, acc-0.5276, test loss-1.7028, acc-0.5146\n",
      "Iter-85260, train loss-1.5989, acc-0.6400, valid loss-1.7019, acc-0.5276, test loss-1.7028, acc-0.5145\n",
      "Iter-85270, train loss-1.7471, acc-0.4600, valid loss-1.7018, acc-0.5276, test loss-1.7027, acc-0.5146\n",
      "Iter-85280, train loss-1.6134, acc-0.5600, valid loss-1.7018, acc-0.5278, test loss-1.7027, acc-0.5148\n",
      "Iter-85290, train loss-1.6290, acc-0.5400, valid loss-1.7018, acc-0.5278, test loss-1.7027, acc-0.5148\n",
      "Iter-85300, train loss-1.8082, acc-0.5000, valid loss-1.7017, acc-0.5278, test loss-1.7026, acc-0.5147\n",
      "Iter-85310, train loss-1.6733, acc-0.6000, valid loss-1.7017, acc-0.5276, test loss-1.7026, acc-0.5148\n",
      "Iter-85320, train loss-1.7645, acc-0.4600, valid loss-1.7016, acc-0.5278, test loss-1.7026, acc-0.5148\n",
      "Iter-85330, train loss-1.7857, acc-0.4800, valid loss-1.7016, acc-0.5276, test loss-1.7025, acc-0.5148\n",
      "Iter-85340, train loss-1.7907, acc-0.5400, valid loss-1.7016, acc-0.5278, test loss-1.7025, acc-0.5148\n",
      "Iter-85350, train loss-1.7572, acc-0.4400, valid loss-1.7015, acc-0.5276, test loss-1.7025, acc-0.5148\n",
      "Iter-85360, train loss-1.7500, acc-0.4800, valid loss-1.7015, acc-0.5276, test loss-1.7024, acc-0.5147\n",
      "Iter-85370, train loss-1.6321, acc-0.6200, valid loss-1.7015, acc-0.5276, test loss-1.7024, acc-0.5149\n",
      "Iter-85380, train loss-1.7571, acc-0.4600, valid loss-1.7014, acc-0.5278, test loss-1.7023, acc-0.5148\n",
      "Iter-85390, train loss-1.7161, acc-0.5000, valid loss-1.7014, acc-0.5276, test loss-1.7023, acc-0.5147\n",
      "Iter-85400, train loss-1.7129, acc-0.5400, valid loss-1.7014, acc-0.5276, test loss-1.7023, acc-0.5148\n",
      "Iter-85410, train loss-1.6519, acc-0.5800, valid loss-1.7013, acc-0.5276, test loss-1.7022, acc-0.5148\n",
      "Iter-85420, train loss-1.7516, acc-0.5200, valid loss-1.7013, acc-0.5276, test loss-1.7022, acc-0.5148\n",
      "Iter-85430, train loss-1.5610, acc-0.6200, valid loss-1.7012, acc-0.5276, test loss-1.7022, acc-0.5146\n",
      "Iter-85440, train loss-1.7901, acc-0.4000, valid loss-1.7012, acc-0.5276, test loss-1.7021, acc-0.5147\n",
      "Iter-85450, train loss-1.7434, acc-0.4800, valid loss-1.7012, acc-0.5274, test loss-1.7021, acc-0.5148\n",
      "Iter-85460, train loss-1.7184, acc-0.5600, valid loss-1.7011, acc-0.5276, test loss-1.7021, acc-0.5146\n",
      "Iter-85470, train loss-1.7246, acc-0.4200, valid loss-1.7011, acc-0.5278, test loss-1.7020, acc-0.5148\n",
      "Iter-85480, train loss-1.6230, acc-0.6400, valid loss-1.7011, acc-0.5274, test loss-1.7020, acc-0.5149\n",
      "Iter-85490, train loss-1.5916, acc-0.5000, valid loss-1.7010, acc-0.5274, test loss-1.7019, acc-0.5148\n",
      "Iter-85500, train loss-1.6337, acc-0.5800, valid loss-1.7010, acc-0.5274, test loss-1.7019, acc-0.5147\n",
      "Iter-85510, train loss-1.8361, acc-0.4000, valid loss-1.7010, acc-0.5276, test loss-1.7019, acc-0.5147\n",
      "Iter-85520, train loss-1.6384, acc-0.5400, valid loss-1.7009, acc-0.5278, test loss-1.7018, acc-0.5147\n",
      "Iter-85530, train loss-1.7285, acc-0.5200, valid loss-1.7009, acc-0.5278, test loss-1.7018, acc-0.5146\n",
      "Iter-85540, train loss-1.7269, acc-0.4800, valid loss-1.7008, acc-0.5278, test loss-1.7018, acc-0.5148\n",
      "Iter-85550, train loss-1.7449, acc-0.4600, valid loss-1.7008, acc-0.5278, test loss-1.7017, acc-0.5147\n",
      "Iter-85560, train loss-1.5874, acc-0.5600, valid loss-1.7008, acc-0.5274, test loss-1.7017, acc-0.5146\n",
      "Iter-85570, train loss-1.7393, acc-0.4600, valid loss-1.7007, acc-0.5276, test loss-1.7017, acc-0.5146\n",
      "Iter-85580, train loss-1.7638, acc-0.4800, valid loss-1.7007, acc-0.5274, test loss-1.7016, acc-0.5146\n",
      "Iter-85590, train loss-1.7395, acc-0.5200, valid loss-1.7007, acc-0.5274, test loss-1.7016, acc-0.5146\n",
      "Iter-85600, train loss-1.6720, acc-0.5200, valid loss-1.7006, acc-0.5274, test loss-1.7016, acc-0.5146\n",
      "Iter-85610, train loss-1.7331, acc-0.4400, valid loss-1.7006, acc-0.5278, test loss-1.7015, acc-0.5146\n",
      "Iter-85620, train loss-1.7000, acc-0.6000, valid loss-1.7006, acc-0.5278, test loss-1.7015, acc-0.5148\n",
      "Iter-85630, train loss-1.6834, acc-0.4600, valid loss-1.7005, acc-0.5274, test loss-1.7014, acc-0.5146\n",
      "Iter-85640, train loss-1.7583, acc-0.4400, valid loss-1.7005, acc-0.5270, test loss-1.7014, acc-0.5146\n",
      "Iter-85650, train loss-1.6803, acc-0.5600, valid loss-1.7005, acc-0.5272, test loss-1.7014, acc-0.5146\n",
      "Iter-85660, train loss-1.7105, acc-0.5000, valid loss-1.7004, acc-0.5274, test loss-1.7013, acc-0.5145\n",
      "Iter-85670, train loss-1.7175, acc-0.5400, valid loss-1.7004, acc-0.5274, test loss-1.7013, acc-0.5146\n",
      "Iter-85680, train loss-1.8052, acc-0.4400, valid loss-1.7004, acc-0.5274, test loss-1.7013, acc-0.5148\n",
      "Iter-85690, train loss-1.7437, acc-0.6000, valid loss-1.7003, acc-0.5274, test loss-1.7012, acc-0.5149\n",
      "Iter-85700, train loss-1.7024, acc-0.5400, valid loss-1.7003, acc-0.5272, test loss-1.7012, acc-0.5145\n",
      "Iter-85710, train loss-1.7071, acc-0.5000, valid loss-1.7002, acc-0.5274, test loss-1.7012, acc-0.5144\n",
      "Iter-85720, train loss-1.7472, acc-0.5000, valid loss-1.7002, acc-0.5274, test loss-1.7011, acc-0.5145\n",
      "Iter-85730, train loss-1.6700, acc-0.5400, valid loss-1.7002, acc-0.5278, test loss-1.7011, acc-0.5149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-85740, train loss-1.6281, acc-0.5800, valid loss-1.7001, acc-0.5276, test loss-1.7011, acc-0.5148\n",
      "Iter-85750, train loss-1.6832, acc-0.5800, valid loss-1.7001, acc-0.5278, test loss-1.7010, acc-0.5148\n",
      "Iter-85760, train loss-1.6413, acc-0.5400, valid loss-1.7001, acc-0.5278, test loss-1.7010, acc-0.5149\n",
      "Iter-85770, train loss-1.6943, acc-0.6000, valid loss-1.7000, acc-0.5276, test loss-1.7010, acc-0.5149\n",
      "Iter-85780, train loss-1.7712, acc-0.5400, valid loss-1.7000, acc-0.5276, test loss-1.7009, acc-0.5152\n",
      "Iter-85790, train loss-1.6569, acc-0.5400, valid loss-1.7000, acc-0.5282, test loss-1.7009, acc-0.5151\n",
      "Iter-85800, train loss-1.8450, acc-0.4400, valid loss-1.6999, acc-0.5282, test loss-1.7008, acc-0.5152\n",
      "Iter-85810, train loss-1.7247, acc-0.4600, valid loss-1.6999, acc-0.5284, test loss-1.7008, acc-0.5152\n",
      "Iter-85820, train loss-1.7075, acc-0.4800, valid loss-1.6998, acc-0.5282, test loss-1.7008, acc-0.5152\n",
      "Iter-85830, train loss-1.7148, acc-0.4600, valid loss-1.6998, acc-0.5284, test loss-1.7007, acc-0.5153\n",
      "Iter-85840, train loss-1.6458, acc-0.6200, valid loss-1.6998, acc-0.5280, test loss-1.7007, acc-0.5151\n",
      "Iter-85850, train loss-1.7024, acc-0.5200, valid loss-1.6997, acc-0.5282, test loss-1.7007, acc-0.5151\n",
      "Iter-85860, train loss-1.5553, acc-0.6400, valid loss-1.6997, acc-0.5282, test loss-1.7006, acc-0.5149\n",
      "Iter-85870, train loss-1.6685, acc-0.4600, valid loss-1.6997, acc-0.5286, test loss-1.7006, acc-0.5150\n",
      "Iter-85880, train loss-1.7660, acc-0.5600, valid loss-1.6996, acc-0.5284, test loss-1.7006, acc-0.5150\n",
      "Iter-85890, train loss-1.8438, acc-0.4600, valid loss-1.6996, acc-0.5282, test loss-1.7005, acc-0.5151\n",
      "Iter-85900, train loss-1.7043, acc-0.5000, valid loss-1.6996, acc-0.5284, test loss-1.7005, acc-0.5152\n",
      "Iter-85910, train loss-1.7063, acc-0.4600, valid loss-1.6995, acc-0.5280, test loss-1.7005, acc-0.5156\n",
      "Iter-85920, train loss-1.7062, acc-0.4800, valid loss-1.6995, acc-0.5282, test loss-1.7004, acc-0.5155\n",
      "Iter-85930, train loss-1.6174, acc-0.5400, valid loss-1.6995, acc-0.5280, test loss-1.7004, acc-0.5155\n",
      "Iter-85940, train loss-1.5925, acc-0.6200, valid loss-1.6994, acc-0.5284, test loss-1.7003, acc-0.5153\n",
      "Iter-85950, train loss-1.7136, acc-0.5800, valid loss-1.6994, acc-0.5284, test loss-1.7003, acc-0.5154\n",
      "Iter-85960, train loss-1.8403, acc-0.3600, valid loss-1.6994, acc-0.5284, test loss-1.7003, acc-0.5153\n",
      "Iter-85970, train loss-1.6385, acc-0.6000, valid loss-1.6993, acc-0.5282, test loss-1.7002, acc-0.5155\n",
      "Iter-85980, train loss-1.7591, acc-0.5400, valid loss-1.6993, acc-0.5282, test loss-1.7002, acc-0.5156\n",
      "Iter-85990, train loss-1.6837, acc-0.4800, valid loss-1.6992, acc-0.5284, test loss-1.7002, acc-0.5156\n",
      "Iter-86000, train loss-1.6690, acc-0.5800, valid loss-1.6992, acc-0.5284, test loss-1.7001, acc-0.5156\n",
      "Iter-86010, train loss-1.8616, acc-0.3800, valid loss-1.6992, acc-0.5282, test loss-1.7001, acc-0.5155\n",
      "Iter-86020, train loss-1.5652, acc-0.5800, valid loss-1.6991, acc-0.5286, test loss-1.7001, acc-0.5156\n",
      "Iter-86030, train loss-1.7233, acc-0.5600, valid loss-1.6991, acc-0.5286, test loss-1.7000, acc-0.5154\n",
      "Iter-86040, train loss-1.6904, acc-0.4800, valid loss-1.6991, acc-0.5286, test loss-1.7000, acc-0.5155\n",
      "Iter-86050, train loss-1.6103, acc-0.6400, valid loss-1.6990, acc-0.5286, test loss-1.7000, acc-0.5155\n",
      "Iter-86060, train loss-1.6176, acc-0.6400, valid loss-1.6990, acc-0.5286, test loss-1.6999, acc-0.5155\n",
      "Iter-86070, train loss-1.6172, acc-0.5800, valid loss-1.6990, acc-0.5286, test loss-1.6999, acc-0.5156\n",
      "Iter-86080, train loss-1.7925, acc-0.4400, valid loss-1.6989, acc-0.5288, test loss-1.6999, acc-0.5157\n",
      "Iter-86090, train loss-1.7340, acc-0.4800, valid loss-1.6989, acc-0.5284, test loss-1.6998, acc-0.5157\n",
      "Iter-86100, train loss-1.6891, acc-0.5000, valid loss-1.6989, acc-0.5284, test loss-1.6998, acc-0.5155\n",
      "Iter-86110, train loss-1.8310, acc-0.4200, valid loss-1.6988, acc-0.5284, test loss-1.6997, acc-0.5154\n",
      "Iter-86120, train loss-1.7554, acc-0.5200, valid loss-1.6988, acc-0.5284, test loss-1.6997, acc-0.5154\n",
      "Iter-86130, train loss-1.7944, acc-0.4200, valid loss-1.6988, acc-0.5284, test loss-1.6997, acc-0.5155\n",
      "Iter-86140, train loss-1.7660, acc-0.5000, valid loss-1.6987, acc-0.5284, test loss-1.6996, acc-0.5155\n",
      "Iter-86150, train loss-1.6627, acc-0.5000, valid loss-1.6987, acc-0.5284, test loss-1.6996, acc-0.5155\n",
      "Iter-86160, train loss-1.7406, acc-0.5400, valid loss-1.6987, acc-0.5284, test loss-1.6996, acc-0.5156\n",
      "Iter-86170, train loss-1.6857, acc-0.5600, valid loss-1.6986, acc-0.5284, test loss-1.6995, acc-0.5156\n",
      "Iter-86180, train loss-1.6889, acc-0.5400, valid loss-1.6986, acc-0.5284, test loss-1.6995, acc-0.5156\n",
      "Iter-86190, train loss-1.7432, acc-0.4200, valid loss-1.6985, acc-0.5284, test loss-1.6995, acc-0.5157\n",
      "Iter-86200, train loss-1.6202, acc-0.6200, valid loss-1.6985, acc-0.5284, test loss-1.6994, acc-0.5155\n",
      "Iter-86210, train loss-1.8396, acc-0.4600, valid loss-1.6985, acc-0.5284, test loss-1.6994, acc-0.5154\n",
      "Iter-86220, train loss-1.6421, acc-0.5600, valid loss-1.6984, acc-0.5284, test loss-1.6994, acc-0.5154\n",
      "Iter-86230, train loss-1.7255, acc-0.4800, valid loss-1.6984, acc-0.5284, test loss-1.6993, acc-0.5154\n",
      "Iter-86240, train loss-1.6880, acc-0.5800, valid loss-1.6984, acc-0.5284, test loss-1.6993, acc-0.5154\n",
      "Iter-86250, train loss-1.8060, acc-0.4800, valid loss-1.6983, acc-0.5284, test loss-1.6993, acc-0.5154\n",
      "Iter-86260, train loss-1.6702, acc-0.5200, valid loss-1.6983, acc-0.5284, test loss-1.6992, acc-0.5155\n",
      "Iter-86270, train loss-1.7309, acc-0.5000, valid loss-1.6983, acc-0.5284, test loss-1.6992, acc-0.5155\n",
      "Iter-86280, train loss-1.6727, acc-0.4600, valid loss-1.6982, acc-0.5286, test loss-1.6992, acc-0.5156\n",
      "Iter-86290, train loss-1.8096, acc-0.4000, valid loss-1.6982, acc-0.5286, test loss-1.6991, acc-0.5155\n",
      "Iter-86300, train loss-1.7522, acc-0.5400, valid loss-1.6982, acc-0.5286, test loss-1.6991, acc-0.5156\n",
      "Iter-86310, train loss-1.7150, acc-0.4000, valid loss-1.6981, acc-0.5286, test loss-1.6991, acc-0.5157\n",
      "Iter-86320, train loss-1.7534, acc-0.4800, valid loss-1.6981, acc-0.5286, test loss-1.6990, acc-0.5157\n",
      "Iter-86330, train loss-1.7078, acc-0.5400, valid loss-1.6981, acc-0.5288, test loss-1.6990, acc-0.5157\n",
      "Iter-86340, train loss-1.7200, acc-0.6000, valid loss-1.6980, acc-0.5288, test loss-1.6990, acc-0.5156\n",
      "Iter-86350, train loss-1.6813, acc-0.5400, valid loss-1.6980, acc-0.5288, test loss-1.6989, acc-0.5158\n",
      "Iter-86360, train loss-1.7493, acc-0.5400, valid loss-1.6979, acc-0.5288, test loss-1.6989, acc-0.5157\n",
      "Iter-86370, train loss-1.6642, acc-0.4600, valid loss-1.6979, acc-0.5286, test loss-1.6988, acc-0.5158\n",
      "Iter-86380, train loss-1.6759, acc-0.5200, valid loss-1.6979, acc-0.5286, test loss-1.6988, acc-0.5158\n",
      "Iter-86390, train loss-1.7229, acc-0.5400, valid loss-1.6978, acc-0.5286, test loss-1.6988, acc-0.5157\n",
      "Iter-86400, train loss-1.7316, acc-0.5200, valid loss-1.6978, acc-0.5286, test loss-1.6987, acc-0.5157\n",
      "Iter-86410, train loss-1.7354, acc-0.5000, valid loss-1.6978, acc-0.5286, test loss-1.6987, acc-0.5157\n",
      "Iter-86420, train loss-1.6121, acc-0.5400, valid loss-1.6977, acc-0.5288, test loss-1.6987, acc-0.5157\n",
      "Iter-86430, train loss-1.5973, acc-0.5800, valid loss-1.6977, acc-0.5288, test loss-1.6986, acc-0.5158\n",
      "Iter-86440, train loss-1.6705, acc-0.5600, valid loss-1.6977, acc-0.5288, test loss-1.6986, acc-0.5159\n",
      "Iter-86450, train loss-1.7904, acc-0.4400, valid loss-1.6976, acc-0.5288, test loss-1.6986, acc-0.5157\n",
      "Iter-86460, train loss-1.6851, acc-0.5600, valid loss-1.6976, acc-0.5288, test loss-1.6985, acc-0.5157\n",
      "Iter-86470, train loss-1.6441, acc-0.5600, valid loss-1.6976, acc-0.5288, test loss-1.6985, acc-0.5156\n",
      "Iter-86480, train loss-1.7942, acc-0.4200, valid loss-1.6975, acc-0.5288, test loss-1.6985, acc-0.5155\n",
      "Iter-86490, train loss-1.6896, acc-0.5800, valid loss-1.6975, acc-0.5288, test loss-1.6984, acc-0.5154\n",
      "Iter-86500, train loss-1.6659, acc-0.4200, valid loss-1.6975, acc-0.5290, test loss-1.6984, acc-0.5156\n",
      "Iter-86510, train loss-1.6006, acc-0.6000, valid loss-1.6974, acc-0.5290, test loss-1.6983, acc-0.5156\n",
      "Iter-86520, train loss-1.7857, acc-0.3600, valid loss-1.6974, acc-0.5290, test loss-1.6983, acc-0.5155\n",
      "Iter-86530, train loss-1.7498, acc-0.5200, valid loss-1.6973, acc-0.5292, test loss-1.6983, acc-0.5157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-86540, train loss-1.7515, acc-0.4600, valid loss-1.6973, acc-0.5292, test loss-1.6982, acc-0.5158\n",
      "Iter-86550, train loss-1.8880, acc-0.4400, valid loss-1.6973, acc-0.5292, test loss-1.6982, acc-0.5158\n",
      "Iter-86560, train loss-1.7060, acc-0.5400, valid loss-1.6972, acc-0.5292, test loss-1.6982, acc-0.5158\n",
      "Iter-86570, train loss-1.6963, acc-0.4000, valid loss-1.6972, acc-0.5290, test loss-1.6981, acc-0.5160\n",
      "Iter-86580, train loss-1.6747, acc-0.5600, valid loss-1.6972, acc-0.5290, test loss-1.6981, acc-0.5161\n",
      "Iter-86590, train loss-1.7119, acc-0.5600, valid loss-1.6971, acc-0.5290, test loss-1.6981, acc-0.5158\n",
      "Iter-86600, train loss-1.7904, acc-0.4800, valid loss-1.6971, acc-0.5294, test loss-1.6980, acc-0.5159\n",
      "Iter-86610, train loss-1.7605, acc-0.5400, valid loss-1.6971, acc-0.5294, test loss-1.6980, acc-0.5161\n",
      "Iter-86620, train loss-1.7595, acc-0.5400, valid loss-1.6970, acc-0.5296, test loss-1.6980, acc-0.5161\n",
      "Iter-86630, train loss-1.7979, acc-0.4600, valid loss-1.6970, acc-0.5296, test loss-1.6979, acc-0.5161\n",
      "Iter-86640, train loss-1.6236, acc-0.5800, valid loss-1.6970, acc-0.5294, test loss-1.6979, acc-0.5160\n",
      "Iter-86650, train loss-1.8096, acc-0.3800, valid loss-1.6969, acc-0.5294, test loss-1.6979, acc-0.5160\n",
      "Iter-86660, train loss-1.6790, acc-0.5200, valid loss-1.6969, acc-0.5294, test loss-1.6978, acc-0.5160\n",
      "Iter-86670, train loss-1.7075, acc-0.5200, valid loss-1.6969, acc-0.5294, test loss-1.6978, acc-0.5161\n",
      "Iter-86680, train loss-1.6794, acc-0.5600, valid loss-1.6968, acc-0.5292, test loss-1.6978, acc-0.5162\n",
      "Iter-86690, train loss-1.8175, acc-0.4000, valid loss-1.6968, acc-0.5294, test loss-1.6977, acc-0.5159\n",
      "Iter-86700, train loss-1.6722, acc-0.6200, valid loss-1.6968, acc-0.5292, test loss-1.6977, acc-0.5160\n",
      "Iter-86710, train loss-1.8627, acc-0.3800, valid loss-1.6967, acc-0.5290, test loss-1.6977, acc-0.5161\n",
      "Iter-86720, train loss-1.7554, acc-0.4400, valid loss-1.6967, acc-0.5294, test loss-1.6976, acc-0.5164\n",
      "Iter-86730, train loss-1.6896, acc-0.5000, valid loss-1.6967, acc-0.5296, test loss-1.6976, acc-0.5162\n",
      "Iter-86740, train loss-1.6822, acc-0.5800, valid loss-1.6966, acc-0.5294, test loss-1.6976, acc-0.5164\n",
      "Iter-86750, train loss-1.8026, acc-0.4400, valid loss-1.6966, acc-0.5296, test loss-1.6975, acc-0.5163\n",
      "Iter-86760, train loss-1.7622, acc-0.5400, valid loss-1.6965, acc-0.5294, test loss-1.6975, acc-0.5163\n",
      "Iter-86770, train loss-1.5911, acc-0.5200, valid loss-1.6965, acc-0.5294, test loss-1.6974, acc-0.5165\n",
      "Iter-86780, train loss-1.9007, acc-0.3800, valid loss-1.6965, acc-0.5296, test loss-1.6974, acc-0.5163\n",
      "Iter-86790, train loss-1.6520, acc-0.5000, valid loss-1.6964, acc-0.5296, test loss-1.6974, acc-0.5163\n",
      "Iter-86800, train loss-1.7696, acc-0.4800, valid loss-1.6964, acc-0.5294, test loss-1.6973, acc-0.5164\n",
      "Iter-86810, train loss-1.7629, acc-0.5200, valid loss-1.6964, acc-0.5294, test loss-1.6973, acc-0.5164\n",
      "Iter-86820, train loss-1.7476, acc-0.4800, valid loss-1.6963, acc-0.5292, test loss-1.6973, acc-0.5163\n",
      "Iter-86830, train loss-1.6906, acc-0.5400, valid loss-1.6963, acc-0.5292, test loss-1.6972, acc-0.5163\n",
      "Iter-86840, train loss-1.7313, acc-0.4800, valid loss-1.6963, acc-0.5292, test loss-1.6972, acc-0.5163\n",
      "Iter-86850, train loss-1.7020, acc-0.4800, valid loss-1.6962, acc-0.5292, test loss-1.6972, acc-0.5166\n",
      "Iter-86860, train loss-1.8167, acc-0.4800, valid loss-1.6962, acc-0.5294, test loss-1.6971, acc-0.5165\n",
      "Iter-86870, train loss-1.7332, acc-0.4600, valid loss-1.6962, acc-0.5294, test loss-1.6971, acc-0.5164\n",
      "Iter-86880, train loss-1.6068, acc-0.6200, valid loss-1.6961, acc-0.5294, test loss-1.6971, acc-0.5164\n",
      "Iter-86890, train loss-1.6171, acc-0.6400, valid loss-1.6961, acc-0.5296, test loss-1.6970, acc-0.5164\n",
      "Iter-86900, train loss-1.7549, acc-0.4800, valid loss-1.6960, acc-0.5290, test loss-1.6970, acc-0.5165\n",
      "Iter-86910, train loss-1.8000, acc-0.4200, valid loss-1.6960, acc-0.5292, test loss-1.6970, acc-0.5165\n",
      "Iter-86920, train loss-1.6181, acc-0.5800, valid loss-1.6960, acc-0.5294, test loss-1.6969, acc-0.5164\n",
      "Iter-86930, train loss-1.7556, acc-0.4400, valid loss-1.6959, acc-0.5292, test loss-1.6969, acc-0.5165\n",
      "Iter-86940, train loss-1.6862, acc-0.6400, valid loss-1.6959, acc-0.5292, test loss-1.6969, acc-0.5165\n",
      "Iter-86950, train loss-1.6187, acc-0.6000, valid loss-1.6959, acc-0.5290, test loss-1.6968, acc-0.5165\n",
      "Iter-86960, train loss-1.7186, acc-0.5200, valid loss-1.6958, acc-0.5294, test loss-1.6968, acc-0.5164\n",
      "Iter-86970, train loss-1.7265, acc-0.4400, valid loss-1.6958, acc-0.5296, test loss-1.6968, acc-0.5164\n",
      "Iter-86980, train loss-1.6486, acc-0.5600, valid loss-1.6958, acc-0.5292, test loss-1.6967, acc-0.5164\n",
      "Iter-86990, train loss-1.8018, acc-0.3800, valid loss-1.6957, acc-0.5294, test loss-1.6967, acc-0.5163\n",
      "Iter-87000, train loss-1.6939, acc-0.5200, valid loss-1.6957, acc-0.5292, test loss-1.6967, acc-0.5164\n",
      "Iter-87010, train loss-1.6540, acc-0.5600, valid loss-1.6957, acc-0.5292, test loss-1.6966, acc-0.5164\n",
      "Iter-87020, train loss-1.5888, acc-0.6400, valid loss-1.6956, acc-0.5290, test loss-1.6966, acc-0.5166\n",
      "Iter-87030, train loss-1.7111, acc-0.5000, valid loss-1.6956, acc-0.5290, test loss-1.6966, acc-0.5166\n",
      "Iter-87040, train loss-1.7543, acc-0.4800, valid loss-1.6956, acc-0.5292, test loss-1.6965, acc-0.5165\n",
      "Iter-87050, train loss-1.5602, acc-0.5800, valid loss-1.6955, acc-0.5292, test loss-1.6965, acc-0.5166\n",
      "Iter-87060, train loss-1.7026, acc-0.4400, valid loss-1.6955, acc-0.5292, test loss-1.6964, acc-0.5166\n",
      "Iter-87070, train loss-1.6806, acc-0.5600, valid loss-1.6954, acc-0.5292, test loss-1.6964, acc-0.5165\n",
      "Iter-87080, train loss-1.6894, acc-0.5800, valid loss-1.6954, acc-0.5294, test loss-1.6964, acc-0.5166\n",
      "Iter-87090, train loss-1.6741, acc-0.5600, valid loss-1.6954, acc-0.5292, test loss-1.6963, acc-0.5166\n",
      "Iter-87100, train loss-1.5803, acc-0.5400, valid loss-1.6953, acc-0.5292, test loss-1.6963, acc-0.5165\n",
      "Iter-87110, train loss-1.7306, acc-0.4800, valid loss-1.6953, acc-0.5292, test loss-1.6963, acc-0.5164\n",
      "Iter-87120, train loss-1.7107, acc-0.4800, valid loss-1.6953, acc-0.5292, test loss-1.6962, acc-0.5165\n",
      "Iter-87130, train loss-1.7376, acc-0.6000, valid loss-1.6952, acc-0.5294, test loss-1.6962, acc-0.5164\n",
      "Iter-87140, train loss-1.7406, acc-0.4800, valid loss-1.6952, acc-0.5290, test loss-1.6962, acc-0.5165\n",
      "Iter-87150, train loss-1.7175, acc-0.5000, valid loss-1.6952, acc-0.5292, test loss-1.6961, acc-0.5166\n",
      "Iter-87160, train loss-1.7772, acc-0.5000, valid loss-1.6951, acc-0.5292, test loss-1.6961, acc-0.5166\n",
      "Iter-87170, train loss-1.7319, acc-0.4800, valid loss-1.6951, acc-0.5294, test loss-1.6961, acc-0.5165\n",
      "Iter-87180, train loss-1.7521, acc-0.3600, valid loss-1.6951, acc-0.5296, test loss-1.6960, acc-0.5168\n",
      "Iter-87190, train loss-1.6197, acc-0.6400, valid loss-1.6950, acc-0.5294, test loss-1.6960, acc-0.5168\n",
      "Iter-87200, train loss-1.6938, acc-0.5600, valid loss-1.6950, acc-0.5298, test loss-1.6960, acc-0.5167\n",
      "Iter-87210, train loss-1.7079, acc-0.6200, valid loss-1.6950, acc-0.5296, test loss-1.6959, acc-0.5166\n",
      "Iter-87220, train loss-1.6837, acc-0.5200, valid loss-1.6949, acc-0.5296, test loss-1.6959, acc-0.5166\n",
      "Iter-87230, train loss-1.7380, acc-0.5600, valid loss-1.6949, acc-0.5296, test loss-1.6958, acc-0.5167\n",
      "Iter-87240, train loss-1.6698, acc-0.5200, valid loss-1.6949, acc-0.5298, test loss-1.6958, acc-0.5167\n",
      "Iter-87250, train loss-1.7348, acc-0.5200, valid loss-1.6948, acc-0.5298, test loss-1.6958, acc-0.5167\n",
      "Iter-87260, train loss-1.5942, acc-0.6000, valid loss-1.6948, acc-0.5298, test loss-1.6957, acc-0.5167\n",
      "Iter-87270, train loss-1.7195, acc-0.4800, valid loss-1.6948, acc-0.5298, test loss-1.6957, acc-0.5167\n",
      "Iter-87280, train loss-1.6744, acc-0.5400, valid loss-1.6947, acc-0.5298, test loss-1.6957, acc-0.5167\n",
      "Iter-87290, train loss-1.7014, acc-0.5800, valid loss-1.6947, acc-0.5298, test loss-1.6956, acc-0.5166\n",
      "Iter-87300, train loss-1.7513, acc-0.5800, valid loss-1.6947, acc-0.5298, test loss-1.6956, acc-0.5167\n",
      "Iter-87310, train loss-1.6083, acc-0.6600, valid loss-1.6946, acc-0.5296, test loss-1.6956, acc-0.5167\n",
      "Iter-87320, train loss-1.6347, acc-0.5600, valid loss-1.6946, acc-0.5296, test loss-1.6955, acc-0.5168\n",
      "Iter-87330, train loss-1.8404, acc-0.5400, valid loss-1.6946, acc-0.5296, test loss-1.6955, acc-0.5168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-87340, train loss-1.7648, acc-0.4600, valid loss-1.6945, acc-0.5292, test loss-1.6955, acc-0.5170\n",
      "Iter-87350, train loss-1.6747, acc-0.5600, valid loss-1.6945, acc-0.5294, test loss-1.6954, acc-0.5170\n",
      "Iter-87360, train loss-1.6982, acc-0.5600, valid loss-1.6944, acc-0.5296, test loss-1.6954, acc-0.5170\n",
      "Iter-87370, train loss-1.6286, acc-0.6800, valid loss-1.6944, acc-0.5296, test loss-1.6953, acc-0.5169\n",
      "Iter-87380, train loss-1.7351, acc-0.5400, valid loss-1.6944, acc-0.5298, test loss-1.6953, acc-0.5170\n",
      "Iter-87390, train loss-1.6345, acc-0.5800, valid loss-1.6943, acc-0.5296, test loss-1.6953, acc-0.5170\n",
      "Iter-87400, train loss-1.8154, acc-0.4800, valid loss-1.6943, acc-0.5296, test loss-1.6952, acc-0.5170\n",
      "Iter-87410, train loss-1.6522, acc-0.5800, valid loss-1.6943, acc-0.5298, test loss-1.6952, acc-0.5170\n",
      "Iter-87420, train loss-1.8066, acc-0.4000, valid loss-1.6942, acc-0.5300, test loss-1.6952, acc-0.5171\n",
      "Iter-87430, train loss-1.8123, acc-0.4600, valid loss-1.6942, acc-0.5298, test loss-1.6952, acc-0.5170\n",
      "Iter-87440, train loss-1.8175, acc-0.3800, valid loss-1.6942, acc-0.5298, test loss-1.6951, acc-0.5169\n",
      "Iter-87450, train loss-1.7525, acc-0.5200, valid loss-1.6941, acc-0.5300, test loss-1.6951, acc-0.5169\n",
      "Iter-87460, train loss-1.6910, acc-0.5200, valid loss-1.6941, acc-0.5298, test loss-1.6950, acc-0.5169\n",
      "Iter-87470, train loss-1.8300, acc-0.4600, valid loss-1.6941, acc-0.5296, test loss-1.6950, acc-0.5169\n",
      "Iter-87480, train loss-1.7460, acc-0.5200, valid loss-1.6940, acc-0.5294, test loss-1.6950, acc-0.5169\n",
      "Iter-87490, train loss-1.7660, acc-0.4200, valid loss-1.6940, acc-0.5294, test loss-1.6949, acc-0.5169\n",
      "Iter-87500, train loss-1.7998, acc-0.4800, valid loss-1.6940, acc-0.5294, test loss-1.6949, acc-0.5169\n",
      "Iter-87510, train loss-1.7960, acc-0.4200, valid loss-1.6939, acc-0.5294, test loss-1.6949, acc-0.5167\n",
      "Iter-87520, train loss-1.6890, acc-0.5200, valid loss-1.6939, acc-0.5294, test loss-1.6948, acc-0.5168\n",
      "Iter-87530, train loss-1.7597, acc-0.4600, valid loss-1.6939, acc-0.5296, test loss-1.6948, acc-0.5170\n",
      "Iter-87540, train loss-1.8418, acc-0.4200, valid loss-1.6938, acc-0.5294, test loss-1.6948, acc-0.5170\n",
      "Iter-87550, train loss-1.8305, acc-0.4200, valid loss-1.6938, acc-0.5298, test loss-1.6947, acc-0.5170\n",
      "Iter-87560, train loss-1.7626, acc-0.4800, valid loss-1.6938, acc-0.5294, test loss-1.6947, acc-0.5170\n",
      "Iter-87570, train loss-1.8274, acc-0.5400, valid loss-1.6937, acc-0.5294, test loss-1.6947, acc-0.5170\n",
      "Iter-87580, train loss-1.7456, acc-0.5200, valid loss-1.6937, acc-0.5294, test loss-1.6946, acc-0.5170\n",
      "Iter-87590, train loss-1.6370, acc-0.6200, valid loss-1.6937, acc-0.5294, test loss-1.6946, acc-0.5170\n",
      "Iter-87600, train loss-1.6954, acc-0.6400, valid loss-1.6936, acc-0.5298, test loss-1.6946, acc-0.5171\n",
      "Iter-87610, train loss-1.7273, acc-0.5000, valid loss-1.6936, acc-0.5298, test loss-1.6945, acc-0.5172\n",
      "Iter-87620, train loss-1.7093, acc-0.4600, valid loss-1.6936, acc-0.5296, test loss-1.6945, acc-0.5171\n",
      "Iter-87630, train loss-1.5313, acc-0.6800, valid loss-1.6935, acc-0.5294, test loss-1.6945, acc-0.5169\n",
      "Iter-87640, train loss-1.6900, acc-0.4800, valid loss-1.6935, acc-0.5294, test loss-1.6944, acc-0.5171\n",
      "Iter-87650, train loss-1.7221, acc-0.5800, valid loss-1.6934, acc-0.5296, test loss-1.6944, acc-0.5170\n",
      "Iter-87660, train loss-1.6719, acc-0.5400, valid loss-1.6934, acc-0.5296, test loss-1.6944, acc-0.5169\n",
      "Iter-87670, train loss-1.8689, acc-0.3600, valid loss-1.6934, acc-0.5294, test loss-1.6943, acc-0.5170\n",
      "Iter-87680, train loss-1.7229, acc-0.5200, valid loss-1.6933, acc-0.5292, test loss-1.6943, acc-0.5171\n",
      "Iter-87690, train loss-1.7536, acc-0.5000, valid loss-1.6933, acc-0.5294, test loss-1.6943, acc-0.5170\n",
      "Iter-87700, train loss-1.7418, acc-0.4400, valid loss-1.6933, acc-0.5294, test loss-1.6942, acc-0.5171\n",
      "Iter-87710, train loss-1.6435, acc-0.5800, valid loss-1.6932, acc-0.5294, test loss-1.6942, acc-0.5170\n",
      "Iter-87720, train loss-1.7429, acc-0.4600, valid loss-1.6932, acc-0.5294, test loss-1.6942, acc-0.5169\n",
      "Iter-87730, train loss-1.6275, acc-0.5800, valid loss-1.6932, acc-0.5294, test loss-1.6941, acc-0.5171\n",
      "Iter-87740, train loss-1.7246, acc-0.5200, valid loss-1.6931, acc-0.5294, test loss-1.6941, acc-0.5171\n",
      "Iter-87750, train loss-1.7428, acc-0.4600, valid loss-1.6931, acc-0.5296, test loss-1.6941, acc-0.5170\n",
      "Iter-87760, train loss-1.6191, acc-0.5800, valid loss-1.6931, acc-0.5296, test loss-1.6940, acc-0.5170\n",
      "Iter-87770, train loss-1.7781, acc-0.5200, valid loss-1.6930, acc-0.5294, test loss-1.6940, acc-0.5170\n",
      "Iter-87780, train loss-1.6743, acc-0.5600, valid loss-1.6930, acc-0.5294, test loss-1.6939, acc-0.5171\n",
      "Iter-87790, train loss-1.6386, acc-0.6200, valid loss-1.6930, acc-0.5292, test loss-1.6939, acc-0.5171\n",
      "Iter-87800, train loss-1.6182, acc-0.5800, valid loss-1.6929, acc-0.5294, test loss-1.6939, acc-0.5171\n",
      "Iter-87810, train loss-1.7494, acc-0.4800, valid loss-1.6929, acc-0.5296, test loss-1.6938, acc-0.5170\n",
      "Iter-87820, train loss-1.6972, acc-0.6200, valid loss-1.6929, acc-0.5298, test loss-1.6938, acc-0.5170\n",
      "Iter-87830, train loss-1.6622, acc-0.5600, valid loss-1.6928, acc-0.5296, test loss-1.6938, acc-0.5170\n",
      "Iter-87840, train loss-1.7762, acc-0.4400, valid loss-1.6928, acc-0.5296, test loss-1.6937, acc-0.5171\n",
      "Iter-87850, train loss-1.6310, acc-0.5400, valid loss-1.6927, acc-0.5296, test loss-1.6937, acc-0.5172\n",
      "Iter-87860, train loss-1.8005, acc-0.4200, valid loss-1.6927, acc-0.5296, test loss-1.6937, acc-0.5172\n",
      "Iter-87870, train loss-1.6810, acc-0.4200, valid loss-1.6927, acc-0.5296, test loss-1.6936, acc-0.5171\n",
      "Iter-87880, train loss-1.7267, acc-0.5800, valid loss-1.6926, acc-0.5298, test loss-1.6936, acc-0.5171\n",
      "Iter-87890, train loss-1.7258, acc-0.5200, valid loss-1.6926, acc-0.5296, test loss-1.6936, acc-0.5171\n",
      "Iter-87900, train loss-1.6221, acc-0.5800, valid loss-1.6926, acc-0.5294, test loss-1.6935, acc-0.5171\n",
      "Iter-87910, train loss-1.7106, acc-0.4800, valid loss-1.6925, acc-0.5294, test loss-1.6935, acc-0.5171\n",
      "Iter-87920, train loss-1.6587, acc-0.5200, valid loss-1.6925, acc-0.5296, test loss-1.6935, acc-0.5171\n",
      "Iter-87930, train loss-1.7770, acc-0.4800, valid loss-1.6925, acc-0.5292, test loss-1.6934, acc-0.5171\n",
      "Iter-87940, train loss-1.6519, acc-0.5800, valid loss-1.6924, acc-0.5300, test loss-1.6934, acc-0.5171\n",
      "Iter-87950, train loss-1.7554, acc-0.5400, valid loss-1.6924, acc-0.5298, test loss-1.6934, acc-0.5172\n",
      "Iter-87960, train loss-1.7898, acc-0.4600, valid loss-1.6924, acc-0.5296, test loss-1.6933, acc-0.5171\n",
      "Iter-87970, train loss-1.6228, acc-0.6600, valid loss-1.6923, acc-0.5294, test loss-1.6933, acc-0.5172\n",
      "Iter-87980, train loss-1.7233, acc-0.4600, valid loss-1.6923, acc-0.5294, test loss-1.6933, acc-0.5171\n",
      "Iter-87990, train loss-1.6614, acc-0.5800, valid loss-1.6923, acc-0.5294, test loss-1.6932, acc-0.5171\n",
      "Iter-88000, train loss-1.7557, acc-0.4000, valid loss-1.6922, acc-0.5298, test loss-1.6932, acc-0.5171\n",
      "Iter-88010, train loss-1.7625, acc-0.5000, valid loss-1.6922, acc-0.5298, test loss-1.6932, acc-0.5170\n",
      "Iter-88020, train loss-1.8042, acc-0.4600, valid loss-1.6921, acc-0.5298, test loss-1.6931, acc-0.5171\n",
      "Iter-88030, train loss-1.8059, acc-0.3600, valid loss-1.6921, acc-0.5294, test loss-1.6931, acc-0.5172\n",
      "Iter-88040, train loss-1.7915, acc-0.4400, valid loss-1.6921, acc-0.5292, test loss-1.6931, acc-0.5173\n",
      "Iter-88050, train loss-1.7667, acc-0.4800, valid loss-1.6920, acc-0.5292, test loss-1.6930, acc-0.5172\n",
      "Iter-88060, train loss-1.8135, acc-0.3600, valid loss-1.6920, acc-0.5292, test loss-1.6930, acc-0.5172\n",
      "Iter-88070, train loss-1.6550, acc-0.6200, valid loss-1.6920, acc-0.5296, test loss-1.6930, acc-0.5172\n",
      "Iter-88080, train loss-1.6091, acc-0.5600, valid loss-1.6920, acc-0.5298, test loss-1.6929, acc-0.5173\n",
      "Iter-88090, train loss-1.7683, acc-0.4200, valid loss-1.6919, acc-0.5298, test loss-1.6929, acc-0.5173\n",
      "Iter-88100, train loss-1.7471, acc-0.5000, valid loss-1.6919, acc-0.5296, test loss-1.6928, acc-0.5174\n",
      "Iter-88110, train loss-1.7678, acc-0.4200, valid loss-1.6918, acc-0.5296, test loss-1.6928, acc-0.5174\n",
      "Iter-88120, train loss-1.6530, acc-0.5000, valid loss-1.6918, acc-0.5296, test loss-1.6928, acc-0.5176\n",
      "Iter-88130, train loss-1.7806, acc-0.4800, valid loss-1.6918, acc-0.5296, test loss-1.6928, acc-0.5175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-88140, train loss-1.8261, acc-0.4000, valid loss-1.6918, acc-0.5296, test loss-1.6927, acc-0.5176\n",
      "Iter-88150, train loss-1.7283, acc-0.5200, valid loss-1.6917, acc-0.5296, test loss-1.6927, acc-0.5176\n",
      "Iter-88160, train loss-1.6152, acc-0.4800, valid loss-1.6917, acc-0.5296, test loss-1.6927, acc-0.5175\n",
      "Iter-88170, train loss-1.7410, acc-0.4200, valid loss-1.6916, acc-0.5298, test loss-1.6926, acc-0.5175\n",
      "Iter-88180, train loss-1.6392, acc-0.5400, valid loss-1.6916, acc-0.5298, test loss-1.6926, acc-0.5175\n",
      "Iter-88190, train loss-1.7894, acc-0.4400, valid loss-1.6916, acc-0.5298, test loss-1.6925, acc-0.5176\n",
      "Iter-88200, train loss-1.7949, acc-0.4400, valid loss-1.6915, acc-0.5298, test loss-1.6925, acc-0.5176\n",
      "Iter-88210, train loss-1.7400, acc-0.4600, valid loss-1.6915, acc-0.5300, test loss-1.6925, acc-0.5176\n",
      "Iter-88220, train loss-1.7141, acc-0.4800, valid loss-1.6915, acc-0.5296, test loss-1.6924, acc-0.5175\n",
      "Iter-88230, train loss-1.6384, acc-0.6000, valid loss-1.6914, acc-0.5296, test loss-1.6924, acc-0.5175\n",
      "Iter-88240, train loss-1.6336, acc-0.6000, valid loss-1.6914, acc-0.5294, test loss-1.6924, acc-0.5174\n",
      "Iter-88250, train loss-1.7195, acc-0.4600, valid loss-1.6914, acc-0.5294, test loss-1.6923, acc-0.5175\n",
      "Iter-88260, train loss-1.7594, acc-0.4000, valid loss-1.6913, acc-0.5294, test loss-1.6923, acc-0.5175\n",
      "Iter-88270, train loss-1.7354, acc-0.5000, valid loss-1.6913, acc-0.5294, test loss-1.6923, acc-0.5175\n",
      "Iter-88280, train loss-1.7481, acc-0.5000, valid loss-1.6913, acc-0.5294, test loss-1.6922, acc-0.5175\n",
      "Iter-88290, train loss-1.5940, acc-0.6200, valid loss-1.6912, acc-0.5294, test loss-1.6922, acc-0.5173\n",
      "Iter-88300, train loss-1.6479, acc-0.5800, valid loss-1.6912, acc-0.5296, test loss-1.6922, acc-0.5173\n",
      "Iter-88310, train loss-1.6820, acc-0.5600, valid loss-1.6912, acc-0.5298, test loss-1.6921, acc-0.5173\n",
      "Iter-88320, train loss-1.7955, acc-0.4600, valid loss-1.6911, acc-0.5298, test loss-1.6921, acc-0.5176\n",
      "Iter-88330, train loss-1.7452, acc-0.4800, valid loss-1.6911, acc-0.5296, test loss-1.6921, acc-0.5174\n",
      "Iter-88340, train loss-1.6558, acc-0.6000, valid loss-1.6911, acc-0.5296, test loss-1.6920, acc-0.5175\n",
      "Iter-88350, train loss-1.7412, acc-0.6400, valid loss-1.6910, acc-0.5294, test loss-1.6920, acc-0.5175\n",
      "Iter-88360, train loss-1.7884, acc-0.4600, valid loss-1.6910, acc-0.5294, test loss-1.6920, acc-0.5175\n",
      "Iter-88370, train loss-1.5848, acc-0.5600, valid loss-1.6910, acc-0.5294, test loss-1.6919, acc-0.5176\n",
      "Iter-88380, train loss-1.8719, acc-0.4200, valid loss-1.6909, acc-0.5292, test loss-1.6919, acc-0.5173\n",
      "Iter-88390, train loss-1.7252, acc-0.4400, valid loss-1.6909, acc-0.5294, test loss-1.6919, acc-0.5173\n",
      "Iter-88400, train loss-1.8003, acc-0.5000, valid loss-1.6909, acc-0.5294, test loss-1.6918, acc-0.5173\n",
      "Iter-88410, train loss-1.7545, acc-0.5400, valid loss-1.6908, acc-0.5294, test loss-1.6918, acc-0.5173\n",
      "Iter-88420, train loss-1.8059, acc-0.5000, valid loss-1.6908, acc-0.5294, test loss-1.6918, acc-0.5173\n",
      "Iter-88430, train loss-1.8054, acc-0.4200, valid loss-1.6907, acc-0.5292, test loss-1.6917, acc-0.5173\n",
      "Iter-88440, train loss-1.7153, acc-0.4400, valid loss-1.6907, acc-0.5290, test loss-1.6917, acc-0.5173\n",
      "Iter-88450, train loss-1.6023, acc-0.6200, valid loss-1.6907, acc-0.5292, test loss-1.6917, acc-0.5173\n",
      "Iter-88460, train loss-1.7496, acc-0.5000, valid loss-1.6907, acc-0.5294, test loss-1.6916, acc-0.5173\n",
      "Iter-88470, train loss-1.6738, acc-0.5600, valid loss-1.6906, acc-0.5292, test loss-1.6916, acc-0.5174\n",
      "Iter-88480, train loss-1.6619, acc-0.5400, valid loss-1.6906, acc-0.5294, test loss-1.6916, acc-0.5174\n",
      "Iter-88490, train loss-1.6940, acc-0.5000, valid loss-1.6906, acc-0.5290, test loss-1.6915, acc-0.5174\n",
      "Iter-88500, train loss-1.6905, acc-0.5000, valid loss-1.6905, acc-0.5290, test loss-1.6915, acc-0.5173\n",
      "Iter-88510, train loss-1.6629, acc-0.4800, valid loss-1.6905, acc-0.5290, test loss-1.6915, acc-0.5173\n",
      "Iter-88520, train loss-1.7578, acc-0.5600, valid loss-1.6904, acc-0.5290, test loss-1.6914, acc-0.5173\n",
      "Iter-88530, train loss-1.7467, acc-0.4800, valid loss-1.6904, acc-0.5290, test loss-1.6914, acc-0.5174\n",
      "Iter-88540, train loss-1.7210, acc-0.6200, valid loss-1.6904, acc-0.5290, test loss-1.6913, acc-0.5174\n",
      "Iter-88550, train loss-1.6208, acc-0.6200, valid loss-1.6903, acc-0.5294, test loss-1.6913, acc-0.5174\n",
      "Iter-88560, train loss-1.6382, acc-0.5200, valid loss-1.6903, acc-0.5296, test loss-1.6913, acc-0.5174\n",
      "Iter-88570, train loss-1.5791, acc-0.5200, valid loss-1.6903, acc-0.5298, test loss-1.6912, acc-0.5173\n",
      "Iter-88580, train loss-1.6750, acc-0.5400, valid loss-1.6902, acc-0.5296, test loss-1.6912, acc-0.5173\n",
      "Iter-88590, train loss-1.7222, acc-0.5200, valid loss-1.6902, acc-0.5298, test loss-1.6912, acc-0.5171\n",
      "Iter-88600, train loss-1.6116, acc-0.6400, valid loss-1.6902, acc-0.5296, test loss-1.6911, acc-0.5172\n",
      "Iter-88610, train loss-1.6646, acc-0.5800, valid loss-1.6901, acc-0.5296, test loss-1.6911, acc-0.5172\n",
      "Iter-88620, train loss-1.6986, acc-0.4400, valid loss-1.6901, acc-0.5298, test loss-1.6911, acc-0.5174\n",
      "Iter-88630, train loss-1.6715, acc-0.5600, valid loss-1.6901, acc-0.5300, test loss-1.6910, acc-0.5174\n",
      "Iter-88640, train loss-1.7629, acc-0.4400, valid loss-1.6900, acc-0.5294, test loss-1.6910, acc-0.5174\n",
      "Iter-88650, train loss-1.6268, acc-0.5800, valid loss-1.6900, acc-0.5294, test loss-1.6910, acc-0.5174\n",
      "Iter-88660, train loss-1.7063, acc-0.5000, valid loss-1.6900, acc-0.5294, test loss-1.6909, acc-0.5174\n",
      "Iter-88670, train loss-1.7210, acc-0.5400, valid loss-1.6899, acc-0.5292, test loss-1.6909, acc-0.5176\n",
      "Iter-88680, train loss-1.5790, acc-0.6000, valid loss-1.6899, acc-0.5292, test loss-1.6909, acc-0.5175\n",
      "Iter-88690, train loss-1.7392, acc-0.5400, valid loss-1.6899, acc-0.5292, test loss-1.6908, acc-0.5175\n",
      "Iter-88700, train loss-1.8172, acc-0.3800, valid loss-1.6898, acc-0.5294, test loss-1.6908, acc-0.5174\n",
      "Iter-88710, train loss-1.6734, acc-0.6000, valid loss-1.6898, acc-0.5296, test loss-1.6908, acc-0.5173\n",
      "Iter-88720, train loss-1.7420, acc-0.3800, valid loss-1.6898, acc-0.5298, test loss-1.6907, acc-0.5172\n",
      "Iter-88730, train loss-1.7224, acc-0.5600, valid loss-1.6897, acc-0.5298, test loss-1.6907, acc-0.5173\n",
      "Iter-88740, train loss-1.7495, acc-0.4600, valid loss-1.6897, acc-0.5298, test loss-1.6907, acc-0.5172\n",
      "Iter-88750, train loss-1.6489, acc-0.5600, valid loss-1.6896, acc-0.5294, test loss-1.6906, acc-0.5174\n",
      "Iter-88760, train loss-1.7237, acc-0.5600, valid loss-1.6896, acc-0.5292, test loss-1.6906, acc-0.5173\n",
      "Iter-88770, train loss-1.6320, acc-0.5400, valid loss-1.6896, acc-0.5292, test loss-1.6906, acc-0.5174\n",
      "Iter-88780, train loss-1.8172, acc-0.4000, valid loss-1.6895, acc-0.5292, test loss-1.6905, acc-0.5174\n",
      "Iter-88790, train loss-1.7425, acc-0.4600, valid loss-1.6895, acc-0.5294, test loss-1.6905, acc-0.5175\n",
      "Iter-88800, train loss-1.7170, acc-0.4600, valid loss-1.6895, acc-0.5294, test loss-1.6905, acc-0.5175\n",
      "Iter-88810, train loss-1.6059, acc-0.6600, valid loss-1.6894, acc-0.5294, test loss-1.6904, acc-0.5175\n",
      "Iter-88820, train loss-1.7964, acc-0.3800, valid loss-1.6894, acc-0.5294, test loss-1.6904, acc-0.5174\n",
      "Iter-88830, train loss-1.6037, acc-0.6000, valid loss-1.6894, acc-0.5294, test loss-1.6904, acc-0.5178\n",
      "Iter-88840, train loss-1.7305, acc-0.4600, valid loss-1.6893, acc-0.5294, test loss-1.6903, acc-0.5178\n",
      "Iter-88850, train loss-1.6949, acc-0.5600, valid loss-1.6893, acc-0.5296, test loss-1.6903, acc-0.5177\n",
      "Iter-88860, train loss-1.7642, acc-0.4800, valid loss-1.6893, acc-0.5294, test loss-1.6903, acc-0.5179\n",
      "Iter-88870, train loss-1.7069, acc-0.4800, valid loss-1.6892, acc-0.5294, test loss-1.6902, acc-0.5178\n",
      "Iter-88880, train loss-1.7503, acc-0.5200, valid loss-1.6892, acc-0.5294, test loss-1.6902, acc-0.5179\n",
      "Iter-88890, train loss-1.6935, acc-0.5200, valid loss-1.6892, acc-0.5294, test loss-1.6902, acc-0.5179\n",
      "Iter-88900, train loss-1.6418, acc-0.4800, valid loss-1.6891, acc-0.5294, test loss-1.6901, acc-0.5179\n",
      "Iter-88910, train loss-1.7561, acc-0.4200, valid loss-1.6891, acc-0.5294, test loss-1.6901, acc-0.5178\n",
      "Iter-88920, train loss-1.7439, acc-0.5000, valid loss-1.6891, acc-0.5294, test loss-1.6900, acc-0.5178\n",
      "Iter-88930, train loss-1.6763, acc-0.4600, valid loss-1.6890, acc-0.5294, test loss-1.6900, acc-0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-88940, train loss-1.6662, acc-0.5800, valid loss-1.6890, acc-0.5294, test loss-1.6900, acc-0.5178\n",
      "Iter-88950, train loss-1.7431, acc-0.4800, valid loss-1.6890, acc-0.5294, test loss-1.6899, acc-0.5178\n",
      "Iter-88960, train loss-1.7950, acc-0.4000, valid loss-1.6889, acc-0.5294, test loss-1.6899, acc-0.5177\n",
      "Iter-88970, train loss-1.8128, acc-0.4200, valid loss-1.6889, acc-0.5294, test loss-1.6899, acc-0.5177\n",
      "Iter-88980, train loss-1.5800, acc-0.6200, valid loss-1.6889, acc-0.5296, test loss-1.6898, acc-0.5176\n",
      "Iter-88990, train loss-1.6811, acc-0.5200, valid loss-1.6888, acc-0.5300, test loss-1.6898, acc-0.5175\n",
      "Iter-89000, train loss-1.7253, acc-0.5600, valid loss-1.6888, acc-0.5298, test loss-1.6898, acc-0.5177\n",
      "Iter-89010, train loss-1.6245, acc-0.5600, valid loss-1.6888, acc-0.5300, test loss-1.6897, acc-0.5177\n",
      "Iter-89020, train loss-1.7372, acc-0.3800, valid loss-1.6887, acc-0.5300, test loss-1.6897, acc-0.5177\n",
      "Iter-89030, train loss-1.6815, acc-0.6000, valid loss-1.6887, acc-0.5298, test loss-1.6897, acc-0.5177\n",
      "Iter-89040, train loss-1.7492, acc-0.5200, valid loss-1.6886, acc-0.5298, test loss-1.6896, acc-0.5176\n",
      "Iter-89050, train loss-1.7051, acc-0.5800, valid loss-1.6886, acc-0.5298, test loss-1.6896, acc-0.5177\n",
      "Iter-89060, train loss-1.6564, acc-0.5600, valid loss-1.6886, acc-0.5298, test loss-1.6896, acc-0.5177\n",
      "Iter-89070, train loss-1.7119, acc-0.5000, valid loss-1.6885, acc-0.5298, test loss-1.6895, acc-0.5176\n",
      "Iter-89080, train loss-1.6446, acc-0.5400, valid loss-1.6885, acc-0.5298, test loss-1.6895, acc-0.5175\n",
      "Iter-89090, train loss-1.6888, acc-0.5400, valid loss-1.6885, acc-0.5296, test loss-1.6895, acc-0.5175\n",
      "Iter-89100, train loss-1.5662, acc-0.6600, valid loss-1.6884, acc-0.5298, test loss-1.6894, acc-0.5173\n",
      "Iter-89110, train loss-1.5765, acc-0.5600, valid loss-1.6884, acc-0.5300, test loss-1.6894, acc-0.5174\n",
      "Iter-89120, train loss-1.7017, acc-0.5000, valid loss-1.6884, acc-0.5300, test loss-1.6894, acc-0.5174\n",
      "Iter-89130, train loss-1.7139, acc-0.5600, valid loss-1.6883, acc-0.5298, test loss-1.6893, acc-0.5174\n",
      "Iter-89140, train loss-1.8223, acc-0.4200, valid loss-1.6883, acc-0.5300, test loss-1.6893, acc-0.5175\n",
      "Iter-89150, train loss-1.7804, acc-0.4600, valid loss-1.6883, acc-0.5300, test loss-1.6893, acc-0.5176\n",
      "Iter-89160, train loss-1.8067, acc-0.4800, valid loss-1.6882, acc-0.5304, test loss-1.6892, acc-0.5177\n",
      "Iter-89170, train loss-1.6584, acc-0.6000, valid loss-1.6882, acc-0.5302, test loss-1.6892, acc-0.5175\n",
      "Iter-89180, train loss-1.8080, acc-0.5400, valid loss-1.6882, acc-0.5302, test loss-1.6891, acc-0.5174\n",
      "Iter-89190, train loss-1.5481, acc-0.6000, valid loss-1.6881, acc-0.5306, test loss-1.6891, acc-0.5175\n",
      "Iter-89200, train loss-1.6893, acc-0.5000, valid loss-1.6881, acc-0.5298, test loss-1.6891, acc-0.5176\n",
      "Iter-89210, train loss-1.8163, acc-0.5200, valid loss-1.6881, acc-0.5300, test loss-1.6890, acc-0.5178\n",
      "Iter-89220, train loss-1.6352, acc-0.6000, valid loss-1.6880, acc-0.5304, test loss-1.6890, acc-0.5178\n",
      "Iter-89230, train loss-1.7407, acc-0.5200, valid loss-1.6880, acc-0.5304, test loss-1.6890, acc-0.5178\n",
      "Iter-89240, train loss-1.6741, acc-0.5800, valid loss-1.6880, acc-0.5304, test loss-1.6889, acc-0.5177\n",
      "Iter-89250, train loss-1.8315, acc-0.4200, valid loss-1.6879, acc-0.5304, test loss-1.6889, acc-0.5176\n",
      "Iter-89260, train loss-1.6933, acc-0.5400, valid loss-1.6879, acc-0.5306, test loss-1.6889, acc-0.5175\n",
      "Iter-89270, train loss-1.7218, acc-0.5200, valid loss-1.6879, acc-0.5302, test loss-1.6888, acc-0.5177\n",
      "Iter-89280, train loss-1.6462, acc-0.5000, valid loss-1.6878, acc-0.5304, test loss-1.6888, acc-0.5176\n",
      "Iter-89290, train loss-1.6834, acc-0.5000, valid loss-1.6878, acc-0.5302, test loss-1.6888, acc-0.5178\n",
      "Iter-89300, train loss-1.6676, acc-0.5400, valid loss-1.6878, acc-0.5300, test loss-1.6887, acc-0.5178\n",
      "Iter-89310, train loss-1.7487, acc-0.5600, valid loss-1.6877, acc-0.5300, test loss-1.6887, acc-0.5178\n",
      "Iter-89320, train loss-1.6189, acc-0.6000, valid loss-1.6877, acc-0.5304, test loss-1.6887, acc-0.5175\n",
      "Iter-89330, train loss-1.7359, acc-0.5200, valid loss-1.6877, acc-0.5302, test loss-1.6886, acc-0.5176\n",
      "Iter-89340, train loss-1.6421, acc-0.5000, valid loss-1.6876, acc-0.5300, test loss-1.6886, acc-0.5176\n",
      "Iter-89350, train loss-1.7443, acc-0.5200, valid loss-1.6876, acc-0.5302, test loss-1.6886, acc-0.5176\n",
      "Iter-89360, train loss-1.6668, acc-0.6200, valid loss-1.6876, acc-0.5300, test loss-1.6885, acc-0.5179\n",
      "Iter-89370, train loss-1.6766, acc-0.5200, valid loss-1.6875, acc-0.5300, test loss-1.6885, acc-0.5177\n",
      "Iter-89380, train loss-1.6941, acc-0.5000, valid loss-1.6875, acc-0.5300, test loss-1.6885, acc-0.5177\n",
      "Iter-89390, train loss-1.7229, acc-0.5600, valid loss-1.6875, acc-0.5300, test loss-1.6884, acc-0.5177\n",
      "Iter-89400, train loss-1.6064, acc-0.5600, valid loss-1.6874, acc-0.5300, test loss-1.6884, acc-0.5181\n",
      "Iter-89410, train loss-1.7481, acc-0.5200, valid loss-1.6874, acc-0.5302, test loss-1.6884, acc-0.5180\n",
      "Iter-89420, train loss-1.8510, acc-0.4400, valid loss-1.6874, acc-0.5302, test loss-1.6883, acc-0.5180\n",
      "Iter-89430, train loss-1.6774, acc-0.5600, valid loss-1.6873, acc-0.5300, test loss-1.6883, acc-0.5180\n",
      "Iter-89440, train loss-1.6903, acc-0.5600, valid loss-1.6873, acc-0.5302, test loss-1.6883, acc-0.5181\n",
      "Iter-89450, train loss-1.6650, acc-0.4600, valid loss-1.6873, acc-0.5302, test loss-1.6882, acc-0.5182\n",
      "Iter-89460, train loss-1.6900, acc-0.5400, valid loss-1.6872, acc-0.5302, test loss-1.6882, acc-0.5181\n",
      "Iter-89470, train loss-1.6740, acc-0.5400, valid loss-1.6872, acc-0.5300, test loss-1.6882, acc-0.5181\n",
      "Iter-89480, train loss-1.7767, acc-0.3200, valid loss-1.6872, acc-0.5302, test loss-1.6881, acc-0.5180\n",
      "Iter-89490, train loss-1.7571, acc-0.4400, valid loss-1.6871, acc-0.5302, test loss-1.6881, acc-0.5181\n",
      "Iter-89500, train loss-1.7320, acc-0.4200, valid loss-1.6871, acc-0.5302, test loss-1.6881, acc-0.5181\n",
      "Iter-89510, train loss-1.7375, acc-0.4600, valid loss-1.6871, acc-0.5302, test loss-1.6880, acc-0.5181\n",
      "Iter-89520, train loss-1.6244, acc-0.5600, valid loss-1.6870, acc-0.5304, test loss-1.6880, acc-0.5181\n",
      "Iter-89530, train loss-1.6652, acc-0.5800, valid loss-1.6870, acc-0.5304, test loss-1.6880, acc-0.5182\n",
      "Iter-89540, train loss-1.6999, acc-0.4200, valid loss-1.6870, acc-0.5304, test loss-1.6879, acc-0.5181\n",
      "Iter-89550, train loss-1.5852, acc-0.6000, valid loss-1.6869, acc-0.5304, test loss-1.6879, acc-0.5181\n",
      "Iter-89560, train loss-1.6638, acc-0.4400, valid loss-1.6869, acc-0.5304, test loss-1.6879, acc-0.5180\n",
      "Iter-89570, train loss-1.6376, acc-0.5000, valid loss-1.6869, acc-0.5304, test loss-1.6878, acc-0.5181\n",
      "Iter-89580, train loss-1.5667, acc-0.5200, valid loss-1.6868, acc-0.5304, test loss-1.6878, acc-0.5181\n",
      "Iter-89590, train loss-1.6114, acc-0.6600, valid loss-1.6868, acc-0.5302, test loss-1.6878, acc-0.5181\n",
      "Iter-89600, train loss-1.6773, acc-0.5400, valid loss-1.6868, acc-0.5302, test loss-1.6877, acc-0.5180\n",
      "Iter-89610, train loss-1.6748, acc-0.6200, valid loss-1.6867, acc-0.5304, test loss-1.6877, acc-0.5181\n",
      "Iter-89620, train loss-1.8110, acc-0.4800, valid loss-1.6867, acc-0.5304, test loss-1.6877, acc-0.5181\n",
      "Iter-89630, train loss-1.7682, acc-0.5600, valid loss-1.6867, acc-0.5302, test loss-1.6876, acc-0.5181\n",
      "Iter-89640, train loss-1.8126, acc-0.4400, valid loss-1.6866, acc-0.5302, test loss-1.6876, acc-0.5181\n",
      "Iter-89650, train loss-1.8771, acc-0.3800, valid loss-1.6866, acc-0.5302, test loss-1.6876, acc-0.5179\n",
      "Iter-89660, train loss-1.6688, acc-0.6800, valid loss-1.6866, acc-0.5302, test loss-1.6875, acc-0.5180\n",
      "Iter-89670, train loss-1.7133, acc-0.5600, valid loss-1.6865, acc-0.5302, test loss-1.6875, acc-0.5179\n",
      "Iter-89680, train loss-1.6539, acc-0.5400, valid loss-1.6865, acc-0.5302, test loss-1.6875, acc-0.5180\n",
      "Iter-89690, train loss-1.6675, acc-0.5600, valid loss-1.6865, acc-0.5302, test loss-1.6874, acc-0.5178\n",
      "Iter-89700, train loss-1.7838, acc-0.4400, valid loss-1.6864, acc-0.5302, test loss-1.6874, acc-0.5181\n",
      "Iter-89710, train loss-1.7218, acc-0.4600, valid loss-1.6864, acc-0.5302, test loss-1.6874, acc-0.5180\n",
      "Iter-89720, train loss-1.7497, acc-0.5400, valid loss-1.6864, acc-0.5302, test loss-1.6873, acc-0.5180\n",
      "Iter-89730, train loss-1.6472, acc-0.5600, valid loss-1.6863, acc-0.5302, test loss-1.6873, acc-0.5180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-89740, train loss-1.7137, acc-0.5400, valid loss-1.6863, acc-0.5304, test loss-1.6873, acc-0.5181\n",
      "Iter-89750, train loss-1.6151, acc-0.5200, valid loss-1.6863, acc-0.5304, test loss-1.6872, acc-0.5180\n",
      "Iter-89760, train loss-1.7506, acc-0.4600, valid loss-1.6862, acc-0.5304, test loss-1.6872, acc-0.5180\n",
      "Iter-89770, train loss-1.7382, acc-0.4800, valid loss-1.6862, acc-0.5304, test loss-1.6872, acc-0.5181\n",
      "Iter-89780, train loss-1.6997, acc-0.5800, valid loss-1.6862, acc-0.5304, test loss-1.6871, acc-0.5181\n",
      "Iter-89790, train loss-1.7562, acc-0.3600, valid loss-1.6861, acc-0.5304, test loss-1.6871, acc-0.5182\n",
      "Iter-89800, train loss-1.7034, acc-0.5000, valid loss-1.6861, acc-0.5304, test loss-1.6871, acc-0.5180\n",
      "Iter-89810, train loss-1.7149, acc-0.5000, valid loss-1.6861, acc-0.5304, test loss-1.6870, acc-0.5179\n",
      "Iter-89820, train loss-1.6679, acc-0.5400, valid loss-1.6860, acc-0.5306, test loss-1.6870, acc-0.5179\n",
      "Iter-89830, train loss-1.6975, acc-0.6000, valid loss-1.6860, acc-0.5306, test loss-1.6870, acc-0.5179\n",
      "Iter-89840, train loss-1.6911, acc-0.4800, valid loss-1.6860, acc-0.5304, test loss-1.6869, acc-0.5180\n",
      "Iter-89850, train loss-1.7124, acc-0.4600, valid loss-1.6859, acc-0.5306, test loss-1.6869, acc-0.5179\n",
      "Iter-89860, train loss-1.8011, acc-0.3800, valid loss-1.6859, acc-0.5306, test loss-1.6869, acc-0.5179\n",
      "Iter-89870, train loss-1.6622, acc-0.6000, valid loss-1.6859, acc-0.5308, test loss-1.6868, acc-0.5181\n",
      "Iter-89880, train loss-1.7009, acc-0.5800, valid loss-1.6858, acc-0.5306, test loss-1.6868, acc-0.5180\n",
      "Iter-89890, train loss-1.7977, acc-0.4600, valid loss-1.6858, acc-0.5306, test loss-1.6868, acc-0.5180\n",
      "Iter-89900, train loss-1.5980, acc-0.6200, valid loss-1.6858, acc-0.5304, test loss-1.6867, acc-0.5178\n",
      "Iter-89910, train loss-1.6828, acc-0.6000, valid loss-1.6857, acc-0.5306, test loss-1.6867, acc-0.5178\n",
      "Iter-89920, train loss-1.6608, acc-0.5600, valid loss-1.6857, acc-0.5306, test loss-1.6867, acc-0.5180\n",
      "Iter-89930, train loss-1.6576, acc-0.5400, valid loss-1.6857, acc-0.5306, test loss-1.6866, acc-0.5179\n",
      "Iter-89940, train loss-1.7062, acc-0.5800, valid loss-1.6856, acc-0.5306, test loss-1.6866, acc-0.5179\n",
      "Iter-89950, train loss-1.7335, acc-0.4200, valid loss-1.6856, acc-0.5306, test loss-1.6866, acc-0.5180\n",
      "Iter-89960, train loss-1.6488, acc-0.5800, valid loss-1.6856, acc-0.5306, test loss-1.6865, acc-0.5180\n",
      "Iter-89970, train loss-1.7171, acc-0.5400, valid loss-1.6855, acc-0.5306, test loss-1.6865, acc-0.5179\n",
      "Iter-89980, train loss-1.7575, acc-0.4200, valid loss-1.6855, acc-0.5306, test loss-1.6865, acc-0.5181\n",
      "Iter-89990, train loss-1.7537, acc-0.4400, valid loss-1.6855, acc-0.5306, test loss-1.6864, acc-0.5180\n",
      "Iter-90000, train loss-1.7493, acc-0.5200, valid loss-1.6854, acc-0.5306, test loss-1.6864, acc-0.5181\n",
      "Iter-90010, train loss-1.7567, acc-0.4400, valid loss-1.6854, acc-0.5306, test loss-1.6864, acc-0.5180\n",
      "Iter-90020, train loss-1.7366, acc-0.4400, valid loss-1.6854, acc-0.5306, test loss-1.6863, acc-0.5180\n",
      "Iter-90030, train loss-1.7530, acc-0.4600, valid loss-1.6853, acc-0.5306, test loss-1.6863, acc-0.5179\n",
      "Iter-90040, train loss-1.6426, acc-0.5200, valid loss-1.6853, acc-0.5306, test loss-1.6863, acc-0.5180\n",
      "Iter-90050, train loss-1.6498, acc-0.5200, valid loss-1.6853, acc-0.5306, test loss-1.6862, acc-0.5180\n",
      "Iter-90060, train loss-1.6054, acc-0.5400, valid loss-1.6852, acc-0.5306, test loss-1.6862, acc-0.5181\n",
      "Iter-90070, train loss-1.7198, acc-0.5000, valid loss-1.6852, acc-0.5306, test loss-1.6861, acc-0.5182\n",
      "Iter-90080, train loss-1.6731, acc-0.5600, valid loss-1.6852, acc-0.5306, test loss-1.6861, acc-0.5182\n",
      "Iter-90090, train loss-1.6813, acc-0.4600, valid loss-1.6851, acc-0.5306, test loss-1.6861, acc-0.5183\n",
      "Iter-90100, train loss-1.9138, acc-0.3400, valid loss-1.6851, acc-0.5306, test loss-1.6860, acc-0.5181\n",
      "Iter-90110, train loss-1.6431, acc-0.5800, valid loss-1.6851, acc-0.5306, test loss-1.6860, acc-0.5181\n",
      "Iter-90120, train loss-1.6572, acc-0.6400, valid loss-1.6850, acc-0.5306, test loss-1.6860, acc-0.5181\n",
      "Iter-90130, train loss-1.6843, acc-0.5000, valid loss-1.6850, acc-0.5306, test loss-1.6859, acc-0.5181\n",
      "Iter-90140, train loss-1.8087, acc-0.3800, valid loss-1.6849, acc-0.5306, test loss-1.6859, acc-0.5181\n",
      "Iter-90150, train loss-1.5958, acc-0.5400, valid loss-1.6849, acc-0.5306, test loss-1.6859, acc-0.5182\n",
      "Iter-90160, train loss-1.6961, acc-0.4200, valid loss-1.6849, acc-0.5306, test loss-1.6858, acc-0.5181\n",
      "Iter-90170, train loss-1.6648, acc-0.5400, valid loss-1.6848, acc-0.5306, test loss-1.6858, acc-0.5182\n",
      "Iter-90180, train loss-1.8217, acc-0.4600, valid loss-1.6848, acc-0.5306, test loss-1.6858, acc-0.5182\n",
      "Iter-90190, train loss-1.6335, acc-0.5600, valid loss-1.6848, acc-0.5306, test loss-1.6857, acc-0.5182\n",
      "Iter-90200, train loss-1.7137, acc-0.4200, valid loss-1.6847, acc-0.5306, test loss-1.6857, acc-0.5182\n",
      "Iter-90210, train loss-1.6381, acc-0.5000, valid loss-1.6847, acc-0.5306, test loss-1.6857, acc-0.5181\n",
      "Iter-90220, train loss-1.8578, acc-0.3400, valid loss-1.6847, acc-0.5306, test loss-1.6856, acc-0.5183\n",
      "Iter-90230, train loss-1.7190, acc-0.4800, valid loss-1.6846, acc-0.5306, test loss-1.6856, acc-0.5182\n",
      "Iter-90240, train loss-1.6594, acc-0.4600, valid loss-1.6846, acc-0.5306, test loss-1.6856, acc-0.5181\n",
      "Iter-90250, train loss-1.7672, acc-0.4800, valid loss-1.6846, acc-0.5306, test loss-1.6855, acc-0.5183\n",
      "Iter-90260, train loss-1.7220, acc-0.4400, valid loss-1.6845, acc-0.5306, test loss-1.6855, acc-0.5182\n",
      "Iter-90270, train loss-1.8614, acc-0.4600, valid loss-1.6845, acc-0.5306, test loss-1.6855, acc-0.5182\n",
      "Iter-90280, train loss-1.7212, acc-0.5000, valid loss-1.6845, acc-0.5306, test loss-1.6854, acc-0.5184\n",
      "Iter-90290, train loss-1.6641, acc-0.5400, valid loss-1.6844, acc-0.5306, test loss-1.6854, acc-0.5183\n",
      "Iter-90300, train loss-1.6053, acc-0.6000, valid loss-1.6844, acc-0.5306, test loss-1.6854, acc-0.5183\n",
      "Iter-90310, train loss-1.6940, acc-0.5000, valid loss-1.6844, acc-0.5304, test loss-1.6853, acc-0.5183\n",
      "Iter-90320, train loss-1.6710, acc-0.5400, valid loss-1.6843, acc-0.5302, test loss-1.6853, acc-0.5183\n",
      "Iter-90330, train loss-1.6866, acc-0.5000, valid loss-1.6843, acc-0.5304, test loss-1.6853, acc-0.5182\n",
      "Iter-90340, train loss-1.6751, acc-0.5600, valid loss-1.6843, acc-0.5302, test loss-1.6852, acc-0.5181\n",
      "Iter-90350, train loss-1.6269, acc-0.5000, valid loss-1.6842, acc-0.5302, test loss-1.6852, acc-0.5182\n",
      "Iter-90360, train loss-1.6802, acc-0.6000, valid loss-1.6842, acc-0.5302, test loss-1.6852, acc-0.5182\n",
      "Iter-90370, train loss-1.7086, acc-0.4800, valid loss-1.6842, acc-0.5302, test loss-1.6851, acc-0.5183\n",
      "Iter-90380, train loss-1.6192, acc-0.6000, valid loss-1.6841, acc-0.5304, test loss-1.6851, acc-0.5184\n",
      "Iter-90390, train loss-1.7839, acc-0.4600, valid loss-1.6841, acc-0.5306, test loss-1.6851, acc-0.5183\n",
      "Iter-90400, train loss-1.6879, acc-0.5400, valid loss-1.6841, acc-0.5306, test loss-1.6850, acc-0.5183\n",
      "Iter-90410, train loss-1.6681, acc-0.6000, valid loss-1.6840, acc-0.5306, test loss-1.6850, acc-0.5184\n",
      "Iter-90420, train loss-1.8008, acc-0.5000, valid loss-1.6840, acc-0.5304, test loss-1.6850, acc-0.5184\n",
      "Iter-90430, train loss-1.6231, acc-0.7200, valid loss-1.6840, acc-0.5302, test loss-1.6849, acc-0.5185\n",
      "Iter-90440, train loss-1.7118, acc-0.5200, valid loss-1.6839, acc-0.5302, test loss-1.6849, acc-0.5185\n",
      "Iter-90450, train loss-1.6485, acc-0.6000, valid loss-1.6839, acc-0.5302, test loss-1.6849, acc-0.5184\n",
      "Iter-90460, train loss-1.5935, acc-0.5800, valid loss-1.6839, acc-0.5306, test loss-1.6848, acc-0.5185\n",
      "Iter-90470, train loss-1.8737, acc-0.4200, valid loss-1.6838, acc-0.5304, test loss-1.6848, acc-0.5184\n",
      "Iter-90480, train loss-1.8023, acc-0.4600, valid loss-1.6838, acc-0.5304, test loss-1.6848, acc-0.5183\n",
      "Iter-90490, train loss-1.7487, acc-0.5000, valid loss-1.6838, acc-0.5306, test loss-1.6847, acc-0.5183\n",
      "Iter-90500, train loss-1.7798, acc-0.4600, valid loss-1.6837, acc-0.5306, test loss-1.6847, acc-0.5183\n",
      "Iter-90510, train loss-1.5996, acc-0.5800, valid loss-1.6837, acc-0.5306, test loss-1.6847, acc-0.5183\n",
      "Iter-90520, train loss-1.7950, acc-0.4400, valid loss-1.6837, acc-0.5304, test loss-1.6846, acc-0.5183\n",
      "Iter-90530, train loss-1.7453, acc-0.4400, valid loss-1.6836, acc-0.5304, test loss-1.6846, acc-0.5182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90540, train loss-1.6790, acc-0.5400, valid loss-1.6836, acc-0.5304, test loss-1.6846, acc-0.5182\n",
      "Iter-90550, train loss-1.7378, acc-0.5200, valid loss-1.6836, acc-0.5306, test loss-1.6845, acc-0.5184\n",
      "Iter-90560, train loss-1.5742, acc-0.6000, valid loss-1.6835, acc-0.5306, test loss-1.6845, acc-0.5183\n",
      "Iter-90570, train loss-1.5731, acc-0.6600, valid loss-1.6835, acc-0.5306, test loss-1.6845, acc-0.5183\n",
      "Iter-90580, train loss-1.5928, acc-0.6000, valid loss-1.6835, acc-0.5304, test loss-1.6844, acc-0.5182\n",
      "Iter-90590, train loss-1.7729, acc-0.4400, valid loss-1.6834, acc-0.5304, test loss-1.6844, acc-0.5185\n",
      "Iter-90600, train loss-1.8503, acc-0.4000, valid loss-1.6834, acc-0.5308, test loss-1.6844, acc-0.5183\n",
      "Iter-90610, train loss-1.6592, acc-0.6000, valid loss-1.6834, acc-0.5306, test loss-1.6843, acc-0.5185\n",
      "Iter-90620, train loss-1.6619, acc-0.5400, valid loss-1.6833, acc-0.5306, test loss-1.6843, acc-0.5185\n",
      "Iter-90630, train loss-1.5662, acc-0.5400, valid loss-1.6833, acc-0.5306, test loss-1.6843, acc-0.5185\n",
      "Iter-90640, train loss-1.6436, acc-0.5200, valid loss-1.6833, acc-0.5306, test loss-1.6842, acc-0.5186\n",
      "Iter-90650, train loss-1.7623, acc-0.4200, valid loss-1.6832, acc-0.5306, test loss-1.6842, acc-0.5186\n",
      "Iter-90660, train loss-1.7493, acc-0.5000, valid loss-1.6832, acc-0.5306, test loss-1.6842, acc-0.5187\n",
      "Iter-90670, train loss-1.7568, acc-0.5000, valid loss-1.6832, acc-0.5306, test loss-1.6841, acc-0.5188\n",
      "Iter-90680, train loss-1.7505, acc-0.4800, valid loss-1.6832, acc-0.5306, test loss-1.6841, acc-0.5188\n",
      "Iter-90690, train loss-1.7243, acc-0.5000, valid loss-1.6831, acc-0.5306, test loss-1.6841, acc-0.5189\n",
      "Iter-90700, train loss-1.8113, acc-0.3600, valid loss-1.6831, acc-0.5306, test loss-1.6840, acc-0.5188\n",
      "Iter-90710, train loss-1.7310, acc-0.5400, valid loss-1.6831, acc-0.5306, test loss-1.6840, acc-0.5190\n",
      "Iter-90720, train loss-1.7104, acc-0.5200, valid loss-1.6830, acc-0.5306, test loss-1.6840, acc-0.5189\n",
      "Iter-90730, train loss-1.6939, acc-0.4800, valid loss-1.6830, acc-0.5306, test loss-1.6839, acc-0.5189\n",
      "Iter-90740, train loss-1.7404, acc-0.4800, valid loss-1.6830, acc-0.5306, test loss-1.6839, acc-0.5189\n",
      "Iter-90750, train loss-1.6822, acc-0.5600, valid loss-1.6829, acc-0.5306, test loss-1.6839, acc-0.5187\n",
      "Iter-90760, train loss-1.7338, acc-0.5200, valid loss-1.6829, acc-0.5306, test loss-1.6838, acc-0.5188\n",
      "Iter-90770, train loss-1.6339, acc-0.6200, valid loss-1.6829, acc-0.5306, test loss-1.6838, acc-0.5187\n",
      "Iter-90780, train loss-1.6611, acc-0.5600, valid loss-1.6828, acc-0.5304, test loss-1.6838, acc-0.5188\n",
      "Iter-90790, train loss-1.6995, acc-0.5600, valid loss-1.6828, acc-0.5304, test loss-1.6837, acc-0.5188\n",
      "Iter-90800, train loss-1.7342, acc-0.5800, valid loss-1.6828, acc-0.5304, test loss-1.6837, acc-0.5187\n",
      "Iter-90810, train loss-1.8076, acc-0.4400, valid loss-1.6827, acc-0.5304, test loss-1.6837, acc-0.5187\n",
      "Iter-90820, train loss-1.5943, acc-0.5000, valid loss-1.6827, acc-0.5304, test loss-1.6837, acc-0.5187\n",
      "Iter-90830, train loss-1.7582, acc-0.4600, valid loss-1.6827, acc-0.5302, test loss-1.6836, acc-0.5187\n",
      "Iter-90840, train loss-1.6921, acc-0.4800, valid loss-1.6826, acc-0.5300, test loss-1.6836, acc-0.5187\n",
      "Iter-90850, train loss-1.6740, acc-0.5200, valid loss-1.6826, acc-0.5304, test loss-1.6836, acc-0.5188\n",
      "Iter-90860, train loss-1.6433, acc-0.5400, valid loss-1.6826, acc-0.5300, test loss-1.6835, acc-0.5188\n",
      "Iter-90870, train loss-1.7237, acc-0.5200, valid loss-1.6825, acc-0.5304, test loss-1.6835, acc-0.5189\n",
      "Iter-90880, train loss-1.5976, acc-0.6400, valid loss-1.6825, acc-0.5306, test loss-1.6835, acc-0.5189\n",
      "Iter-90890, train loss-1.7959, acc-0.5000, valid loss-1.6825, acc-0.5306, test loss-1.6834, acc-0.5189\n",
      "Iter-90900, train loss-1.5752, acc-0.6600, valid loss-1.6824, acc-0.5306, test loss-1.6834, acc-0.5189\n",
      "Iter-90910, train loss-1.7605, acc-0.5000, valid loss-1.6824, acc-0.5306, test loss-1.6834, acc-0.5191\n",
      "Iter-90920, train loss-1.6678, acc-0.5800, valid loss-1.6824, acc-0.5306, test loss-1.6833, acc-0.5191\n",
      "Iter-90930, train loss-1.7334, acc-0.5000, valid loss-1.6823, acc-0.5308, test loss-1.6833, acc-0.5190\n",
      "Iter-90940, train loss-1.5540, acc-0.6000, valid loss-1.6823, acc-0.5306, test loss-1.6833, acc-0.5190\n",
      "Iter-90950, train loss-1.7023, acc-0.5000, valid loss-1.6823, acc-0.5308, test loss-1.6832, acc-0.5189\n",
      "Iter-90960, train loss-1.5224, acc-0.6000, valid loss-1.6822, acc-0.5310, test loss-1.6832, acc-0.5190\n",
      "Iter-90970, train loss-1.6997, acc-0.5800, valid loss-1.6822, acc-0.5314, test loss-1.6832, acc-0.5190\n",
      "Iter-90980, train loss-1.7798, acc-0.5000, valid loss-1.6822, acc-0.5314, test loss-1.6831, acc-0.5190\n",
      "Iter-90990, train loss-1.6951, acc-0.5600, valid loss-1.6821, acc-0.5316, test loss-1.6831, acc-0.5190\n",
      "Iter-91000, train loss-1.7329, acc-0.6000, valid loss-1.6821, acc-0.5318, test loss-1.6831, acc-0.5190\n",
      "Iter-91010, train loss-1.7353, acc-0.5800, valid loss-1.6821, acc-0.5318, test loss-1.6830, acc-0.5190\n",
      "Iter-91020, train loss-1.6893, acc-0.4800, valid loss-1.6820, acc-0.5318, test loss-1.6830, acc-0.5190\n",
      "Iter-91030, train loss-1.8166, acc-0.3600, valid loss-1.6820, acc-0.5318, test loss-1.6830, acc-0.5191\n",
      "Iter-91040, train loss-1.5821, acc-0.6400, valid loss-1.6820, acc-0.5318, test loss-1.6829, acc-0.5191\n",
      "Iter-91050, train loss-1.6592, acc-0.5400, valid loss-1.6819, acc-0.5318, test loss-1.6829, acc-0.5192\n",
      "Iter-91060, train loss-1.7335, acc-0.4800, valid loss-1.6819, acc-0.5318, test loss-1.6829, acc-0.5193\n",
      "Iter-91070, train loss-1.6510, acc-0.5400, valid loss-1.6819, acc-0.5318, test loss-1.6828, acc-0.5194\n",
      "Iter-91080, train loss-1.8427, acc-0.3600, valid loss-1.6818, acc-0.5318, test loss-1.6828, acc-0.5192\n",
      "Iter-91090, train loss-1.6346, acc-0.5800, valid loss-1.6818, acc-0.5316, test loss-1.6828, acc-0.5192\n",
      "Iter-91100, train loss-1.6113, acc-0.5600, valid loss-1.6818, acc-0.5316, test loss-1.6827, acc-0.5192\n",
      "Iter-91110, train loss-1.5847, acc-0.5400, valid loss-1.6817, acc-0.5316, test loss-1.6827, acc-0.5195\n",
      "Iter-91120, train loss-1.7390, acc-0.5200, valid loss-1.6817, acc-0.5316, test loss-1.6827, acc-0.5196\n",
      "Iter-91130, train loss-1.7433, acc-0.5400, valid loss-1.6817, acc-0.5316, test loss-1.6826, acc-0.5196\n",
      "Iter-91140, train loss-1.7651, acc-0.4400, valid loss-1.6816, acc-0.5316, test loss-1.6826, acc-0.5196\n",
      "Iter-91150, train loss-1.6757, acc-0.5200, valid loss-1.6816, acc-0.5316, test loss-1.6826, acc-0.5196\n",
      "Iter-91160, train loss-1.6912, acc-0.5200, valid loss-1.6816, acc-0.5316, test loss-1.6825, acc-0.5196\n",
      "Iter-91170, train loss-1.7028, acc-0.5000, valid loss-1.6815, acc-0.5316, test loss-1.6825, acc-0.5196\n",
      "Iter-91180, train loss-1.7820, acc-0.5000, valid loss-1.6815, acc-0.5316, test loss-1.6825, acc-0.5196\n",
      "Iter-91190, train loss-1.7343, acc-0.5200, valid loss-1.6815, acc-0.5318, test loss-1.6824, acc-0.5195\n",
      "Iter-91200, train loss-1.6248, acc-0.5400, valid loss-1.6814, acc-0.5316, test loss-1.6824, acc-0.5196\n",
      "Iter-91210, train loss-1.7157, acc-0.5600, valid loss-1.6814, acc-0.5316, test loss-1.6824, acc-0.5196\n",
      "Iter-91220, train loss-1.6478, acc-0.5800, valid loss-1.6814, acc-0.5316, test loss-1.6823, acc-0.5196\n",
      "Iter-91230, train loss-1.6526, acc-0.5400, valid loss-1.6813, acc-0.5316, test loss-1.6823, acc-0.5196\n",
      "Iter-91240, train loss-1.6382, acc-0.6200, valid loss-1.6813, acc-0.5316, test loss-1.6823, acc-0.5196\n",
      "Iter-91250, train loss-1.7152, acc-0.4600, valid loss-1.6813, acc-0.5318, test loss-1.6822, acc-0.5197\n",
      "Iter-91260, train loss-1.7297, acc-0.5400, valid loss-1.6812, acc-0.5320, test loss-1.6822, acc-0.5196\n",
      "Iter-91270, train loss-1.7009, acc-0.5600, valid loss-1.6812, acc-0.5318, test loss-1.6822, acc-0.5195\n",
      "Iter-91280, train loss-1.6471, acc-0.6000, valid loss-1.6812, acc-0.5318, test loss-1.6821, acc-0.5196\n",
      "Iter-91290, train loss-1.7482, acc-0.4600, valid loss-1.6811, acc-0.5318, test loss-1.6821, acc-0.5196\n",
      "Iter-91300, train loss-1.7256, acc-0.3800, valid loss-1.6811, acc-0.5320, test loss-1.6821, acc-0.5197\n",
      "Iter-91310, train loss-1.6451, acc-0.6000, valid loss-1.6811, acc-0.5320, test loss-1.6820, acc-0.5197\n",
      "Iter-91320, train loss-1.7184, acc-0.5000, valid loss-1.6810, acc-0.5320, test loss-1.6820, acc-0.5197\n",
      "Iter-91330, train loss-1.6287, acc-0.5000, valid loss-1.6810, acc-0.5322, test loss-1.6820, acc-0.5198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-91340, train loss-1.5679, acc-0.6200, valid loss-1.6810, acc-0.5322, test loss-1.6819, acc-0.5196\n",
      "Iter-91350, train loss-1.7378, acc-0.6200, valid loss-1.6809, acc-0.5322, test loss-1.6819, acc-0.5197\n",
      "Iter-91360, train loss-1.6601, acc-0.5600, valid loss-1.6809, acc-0.5322, test loss-1.6819, acc-0.5198\n",
      "Iter-91370, train loss-1.6289, acc-0.5400, valid loss-1.6809, acc-0.5322, test loss-1.6818, acc-0.5198\n",
      "Iter-91380, train loss-1.6901, acc-0.5200, valid loss-1.6808, acc-0.5322, test loss-1.6818, acc-0.5198\n",
      "Iter-91390, train loss-1.7001, acc-0.5200, valid loss-1.6808, acc-0.5324, test loss-1.6818, acc-0.5198\n",
      "Iter-91400, train loss-1.7597, acc-0.4800, valid loss-1.6808, acc-0.5324, test loss-1.6817, acc-0.5197\n",
      "Iter-91410, train loss-1.7251, acc-0.5600, valid loss-1.6807, acc-0.5326, test loss-1.6817, acc-0.5197\n",
      "Iter-91420, train loss-1.8336, acc-0.4200, valid loss-1.6807, acc-0.5326, test loss-1.6817, acc-0.5198\n",
      "Iter-91430, train loss-1.7457, acc-0.5400, valid loss-1.6807, acc-0.5328, test loss-1.6816, acc-0.5197\n",
      "Iter-91440, train loss-1.5162, acc-0.6200, valid loss-1.6806, acc-0.5324, test loss-1.6816, acc-0.5198\n",
      "Iter-91450, train loss-1.6199, acc-0.5200, valid loss-1.6806, acc-0.5324, test loss-1.6816, acc-0.5196\n",
      "Iter-91460, train loss-1.7262, acc-0.4400, valid loss-1.6806, acc-0.5324, test loss-1.6815, acc-0.5197\n",
      "Iter-91470, train loss-1.6651, acc-0.5600, valid loss-1.6805, acc-0.5324, test loss-1.6815, acc-0.5198\n",
      "Iter-91480, train loss-1.6721, acc-0.4000, valid loss-1.6805, acc-0.5324, test loss-1.6815, acc-0.5196\n",
      "Iter-91490, train loss-1.6562, acc-0.5400, valid loss-1.6805, acc-0.5326, test loss-1.6814, acc-0.5196\n",
      "Iter-91500, train loss-1.6678, acc-0.5600, valid loss-1.6804, acc-0.5324, test loss-1.6814, acc-0.5197\n",
      "Iter-91510, train loss-1.7473, acc-0.4200, valid loss-1.6804, acc-0.5326, test loss-1.6814, acc-0.5197\n",
      "Iter-91520, train loss-1.7855, acc-0.4600, valid loss-1.6804, acc-0.5326, test loss-1.6813, acc-0.5199\n",
      "Iter-91530, train loss-1.6715, acc-0.5600, valid loss-1.6803, acc-0.5330, test loss-1.6813, acc-0.5199\n",
      "Iter-91540, train loss-1.7890, acc-0.4600, valid loss-1.6803, acc-0.5326, test loss-1.6813, acc-0.5199\n",
      "Iter-91550, train loss-1.6434, acc-0.4400, valid loss-1.6803, acc-0.5326, test loss-1.6812, acc-0.5199\n",
      "Iter-91560, train loss-1.7448, acc-0.5200, valid loss-1.6802, acc-0.5328, test loss-1.6812, acc-0.5198\n",
      "Iter-91570, train loss-1.7282, acc-0.4800, valid loss-1.6802, acc-0.5330, test loss-1.6812, acc-0.5198\n",
      "Iter-91580, train loss-1.7246, acc-0.5000, valid loss-1.6802, acc-0.5330, test loss-1.6811, acc-0.5198\n",
      "Iter-91590, train loss-1.6580, acc-0.5000, valid loss-1.6801, acc-0.5326, test loss-1.6811, acc-0.5198\n",
      "Iter-91600, train loss-1.7789, acc-0.5000, valid loss-1.6801, acc-0.5328, test loss-1.6811, acc-0.5198\n",
      "Iter-91610, train loss-1.6970, acc-0.4600, valid loss-1.6801, acc-0.5330, test loss-1.6811, acc-0.5198\n",
      "Iter-91620, train loss-1.7451, acc-0.4600, valid loss-1.6800, acc-0.5330, test loss-1.6810, acc-0.5197\n",
      "Iter-91630, train loss-1.7310, acc-0.5200, valid loss-1.6800, acc-0.5330, test loss-1.6810, acc-0.5197\n",
      "Iter-91640, train loss-1.8273, acc-0.4600, valid loss-1.6800, acc-0.5328, test loss-1.6810, acc-0.5197\n",
      "Iter-91650, train loss-1.6694, acc-0.5600, valid loss-1.6799, acc-0.5330, test loss-1.6809, acc-0.5198\n",
      "Iter-91660, train loss-1.7193, acc-0.6400, valid loss-1.6799, acc-0.5330, test loss-1.6809, acc-0.5198\n",
      "Iter-91670, train loss-1.7109, acc-0.4800, valid loss-1.6799, acc-0.5330, test loss-1.6809, acc-0.5198\n",
      "Iter-91680, train loss-1.5837, acc-0.6600, valid loss-1.6798, acc-0.5328, test loss-1.6808, acc-0.5196\n",
      "Iter-91690, train loss-1.7203, acc-0.5200, valid loss-1.6798, acc-0.5330, test loss-1.6808, acc-0.5197\n",
      "Iter-91700, train loss-1.6648, acc-0.5000, valid loss-1.6797, acc-0.5328, test loss-1.6808, acc-0.5196\n",
      "Iter-91710, train loss-1.7011, acc-0.5200, valid loss-1.6797, acc-0.5328, test loss-1.6807, acc-0.5195\n",
      "Iter-91720, train loss-1.5233, acc-0.6200, valid loss-1.6797, acc-0.5328, test loss-1.6807, acc-0.5198\n",
      "Iter-91730, train loss-1.7112, acc-0.4400, valid loss-1.6797, acc-0.5328, test loss-1.6807, acc-0.5198\n",
      "Iter-91740, train loss-1.8056, acc-0.4400, valid loss-1.6796, acc-0.5326, test loss-1.6806, acc-0.5198\n",
      "Iter-91750, train loss-1.6238, acc-0.6000, valid loss-1.6796, acc-0.5328, test loss-1.6806, acc-0.5199\n",
      "Iter-91760, train loss-1.7334, acc-0.5000, valid loss-1.6796, acc-0.5328, test loss-1.6806, acc-0.5199\n",
      "Iter-91770, train loss-1.6591, acc-0.4400, valid loss-1.6795, acc-0.5328, test loss-1.6805, acc-0.5198\n",
      "Iter-91780, train loss-1.6845, acc-0.5400, valid loss-1.6795, acc-0.5328, test loss-1.6805, acc-0.5199\n",
      "Iter-91790, train loss-1.6760, acc-0.5000, valid loss-1.6795, acc-0.5328, test loss-1.6805, acc-0.5199\n",
      "Iter-91800, train loss-1.6874, acc-0.4600, valid loss-1.6794, acc-0.5328, test loss-1.6804, acc-0.5198\n",
      "Iter-91810, train loss-1.7172, acc-0.5800, valid loss-1.6794, acc-0.5328, test loss-1.6804, acc-0.5199\n",
      "Iter-91820, train loss-1.6166, acc-0.5200, valid loss-1.6794, acc-0.5328, test loss-1.6804, acc-0.5200\n",
      "Iter-91830, train loss-1.6665, acc-0.5200, valid loss-1.6793, acc-0.5328, test loss-1.6803, acc-0.5200\n",
      "Iter-91840, train loss-1.5668, acc-0.5600, valid loss-1.6793, acc-0.5328, test loss-1.6803, acc-0.5201\n",
      "Iter-91850, train loss-1.7254, acc-0.5200, valid loss-1.6793, acc-0.5330, test loss-1.6803, acc-0.5201\n",
      "Iter-91860, train loss-1.6136, acc-0.5200, valid loss-1.6792, acc-0.5330, test loss-1.6802, acc-0.5201\n",
      "Iter-91870, train loss-1.7043, acc-0.5200, valid loss-1.6792, acc-0.5330, test loss-1.6802, acc-0.5201\n",
      "Iter-91880, train loss-1.6020, acc-0.6400, valid loss-1.6792, acc-0.5330, test loss-1.6802, acc-0.5201\n",
      "Iter-91890, train loss-1.6835, acc-0.5200, valid loss-1.6791, acc-0.5330, test loss-1.6801, acc-0.5201\n",
      "Iter-91900, train loss-1.7904, acc-0.4800, valid loss-1.6791, acc-0.5332, test loss-1.6801, acc-0.5203\n",
      "Iter-91910, train loss-1.8307, acc-0.3800, valid loss-1.6791, acc-0.5334, test loss-1.6801, acc-0.5203\n",
      "Iter-91920, train loss-1.7000, acc-0.4600, valid loss-1.6790, acc-0.5332, test loss-1.6800, acc-0.5201\n",
      "Iter-91930, train loss-1.6992, acc-0.5800, valid loss-1.6790, acc-0.5332, test loss-1.6800, acc-0.5202\n",
      "Iter-91940, train loss-1.6369, acc-0.5200, valid loss-1.6790, acc-0.5334, test loss-1.6800, acc-0.5201\n",
      "Iter-91950, train loss-1.6301, acc-0.5800, valid loss-1.6789, acc-0.5334, test loss-1.6799, acc-0.5201\n",
      "Iter-91960, train loss-1.8029, acc-0.4200, valid loss-1.6789, acc-0.5332, test loss-1.6799, acc-0.5202\n",
      "Iter-91970, train loss-1.7837, acc-0.4400, valid loss-1.6789, acc-0.5330, test loss-1.6799, acc-0.5203\n",
      "Iter-91980, train loss-1.6225, acc-0.5000, valid loss-1.6788, acc-0.5330, test loss-1.6798, acc-0.5204\n",
      "Iter-91990, train loss-1.6780, acc-0.5800, valid loss-1.6788, acc-0.5330, test loss-1.6798, acc-0.5204\n",
      "Iter-92000, train loss-1.7312, acc-0.5400, valid loss-1.6788, acc-0.5330, test loss-1.6798, acc-0.5200\n",
      "Iter-92010, train loss-1.5890, acc-0.5800, valid loss-1.6788, acc-0.5330, test loss-1.6797, acc-0.5202\n",
      "Iter-92020, train loss-1.6756, acc-0.5800, valid loss-1.6787, acc-0.5332, test loss-1.6797, acc-0.5203\n",
      "Iter-92030, train loss-1.6655, acc-0.6000, valid loss-1.6787, acc-0.5332, test loss-1.6797, acc-0.5203\n",
      "Iter-92040, train loss-1.7258, acc-0.4200, valid loss-1.6786, acc-0.5332, test loss-1.6796, acc-0.5202\n",
      "Iter-92050, train loss-1.6595, acc-0.5200, valid loss-1.6786, acc-0.5332, test loss-1.6796, acc-0.5202\n",
      "Iter-92060, train loss-1.7022, acc-0.5000, valid loss-1.6786, acc-0.5332, test loss-1.6796, acc-0.5202\n",
      "Iter-92070, train loss-1.6760, acc-0.5800, valid loss-1.6785, acc-0.5332, test loss-1.6795, acc-0.5204\n",
      "Iter-92080, train loss-1.7726, acc-0.4600, valid loss-1.6785, acc-0.5332, test loss-1.6795, acc-0.5203\n",
      "Iter-92090, train loss-1.6907, acc-0.5000, valid loss-1.6785, acc-0.5330, test loss-1.6795, acc-0.5204\n",
      "Iter-92100, train loss-1.6809, acc-0.6200, valid loss-1.6785, acc-0.5330, test loss-1.6794, acc-0.5204\n",
      "Iter-92110, train loss-1.7348, acc-0.4000, valid loss-1.6784, acc-0.5330, test loss-1.6794, acc-0.5204\n",
      "Iter-92120, train loss-1.6524, acc-0.6000, valid loss-1.6784, acc-0.5330, test loss-1.6794, acc-0.5205\n",
      "Iter-92130, train loss-1.6023, acc-0.6200, valid loss-1.6784, acc-0.5332, test loss-1.6793, acc-0.5205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-92140, train loss-1.7971, acc-0.4000, valid loss-1.6783, acc-0.5332, test loss-1.6793, acc-0.5205\n",
      "Iter-92150, train loss-1.7046, acc-0.6200, valid loss-1.6783, acc-0.5332, test loss-1.6793, acc-0.5204\n",
      "Iter-92160, train loss-1.6592, acc-0.5200, valid loss-1.6783, acc-0.5332, test loss-1.6792, acc-0.5205\n",
      "Iter-92170, train loss-1.6290, acc-0.6000, valid loss-1.6782, acc-0.5332, test loss-1.6792, acc-0.5205\n",
      "Iter-92180, train loss-1.6403, acc-0.5200, valid loss-1.6782, acc-0.5332, test loss-1.6792, acc-0.5205\n",
      "Iter-92190, train loss-1.7535, acc-0.4400, valid loss-1.6782, acc-0.5328, test loss-1.6791, acc-0.5204\n",
      "Iter-92200, train loss-1.6340, acc-0.6600, valid loss-1.6781, acc-0.5328, test loss-1.6791, acc-0.5204\n",
      "Iter-92210, train loss-1.8195, acc-0.3400, valid loss-1.6781, acc-0.5326, test loss-1.6791, acc-0.5202\n",
      "Iter-92220, train loss-1.7575, acc-0.4200, valid loss-1.6781, acc-0.5326, test loss-1.6791, acc-0.5201\n",
      "Iter-92230, train loss-1.7033, acc-0.4400, valid loss-1.6780, acc-0.5326, test loss-1.6790, acc-0.5201\n",
      "Iter-92240, train loss-1.5933, acc-0.5800, valid loss-1.6780, acc-0.5324, test loss-1.6790, acc-0.5202\n",
      "Iter-92250, train loss-1.5726, acc-0.5600, valid loss-1.6780, acc-0.5322, test loss-1.6790, acc-0.5201\n",
      "Iter-92260, train loss-1.7773, acc-0.4000, valid loss-1.6779, acc-0.5322, test loss-1.6789, acc-0.5202\n",
      "Iter-92270, train loss-1.6835, acc-0.5600, valid loss-1.6779, acc-0.5322, test loss-1.6789, acc-0.5201\n",
      "Iter-92280, train loss-1.7554, acc-0.5200, valid loss-1.6779, acc-0.5322, test loss-1.6789, acc-0.5201\n",
      "Iter-92290, train loss-1.6928, acc-0.3400, valid loss-1.6778, acc-0.5322, test loss-1.6788, acc-0.5201\n",
      "Iter-92300, train loss-1.7279, acc-0.5200, valid loss-1.6778, acc-0.5322, test loss-1.6788, acc-0.5201\n",
      "Iter-92310, train loss-1.7508, acc-0.4600, valid loss-1.6778, acc-0.5322, test loss-1.6788, acc-0.5202\n",
      "Iter-92320, train loss-1.6581, acc-0.5600, valid loss-1.6777, acc-0.5324, test loss-1.6787, acc-0.5204\n",
      "Iter-92330, train loss-1.7412, acc-0.4600, valid loss-1.6777, acc-0.5324, test loss-1.6787, acc-0.5205\n",
      "Iter-92340, train loss-1.6429, acc-0.5600, valid loss-1.6777, acc-0.5324, test loss-1.6787, acc-0.5205\n",
      "Iter-92350, train loss-1.6913, acc-0.4400, valid loss-1.6776, acc-0.5328, test loss-1.6786, acc-0.5204\n",
      "Iter-92360, train loss-1.5753, acc-0.5800, valid loss-1.6776, acc-0.5330, test loss-1.6786, acc-0.5203\n",
      "Iter-92370, train loss-1.7066, acc-0.5000, valid loss-1.6776, acc-0.5330, test loss-1.6786, acc-0.5204\n",
      "Iter-92380, train loss-1.7459, acc-0.4200, valid loss-1.6775, acc-0.5328, test loss-1.6785, acc-0.5204\n",
      "Iter-92390, train loss-1.6848, acc-0.4800, valid loss-1.6775, acc-0.5330, test loss-1.6785, acc-0.5203\n",
      "Iter-92400, train loss-1.7529, acc-0.4000, valid loss-1.6775, acc-0.5330, test loss-1.6785, acc-0.5204\n",
      "Iter-92410, train loss-1.7459, acc-0.4800, valid loss-1.6774, acc-0.5328, test loss-1.6784, acc-0.5205\n",
      "Iter-92420, train loss-1.7448, acc-0.4400, valid loss-1.6774, acc-0.5330, test loss-1.6784, acc-0.5203\n",
      "Iter-92430, train loss-1.7355, acc-0.4800, valid loss-1.6774, acc-0.5330, test loss-1.6784, acc-0.5205\n",
      "Iter-92440, train loss-1.8029, acc-0.3800, valid loss-1.6774, acc-0.5330, test loss-1.6783, acc-0.5205\n",
      "Iter-92450, train loss-1.6356, acc-0.5400, valid loss-1.6773, acc-0.5330, test loss-1.6783, acc-0.5203\n",
      "Iter-92460, train loss-1.7759, acc-0.4200, valid loss-1.6773, acc-0.5330, test loss-1.6783, acc-0.5205\n",
      "Iter-92470, train loss-1.6917, acc-0.5800, valid loss-1.6773, acc-0.5328, test loss-1.6782, acc-0.5203\n",
      "Iter-92480, train loss-1.6931, acc-0.4600, valid loss-1.6772, acc-0.5328, test loss-1.6782, acc-0.5202\n",
      "Iter-92490, train loss-1.6714, acc-0.5600, valid loss-1.6772, acc-0.5330, test loss-1.6782, acc-0.5206\n",
      "Iter-92500, train loss-1.6646, acc-0.5400, valid loss-1.6772, acc-0.5330, test loss-1.6781, acc-0.5206\n",
      "Iter-92510, train loss-1.8140, acc-0.4400, valid loss-1.6771, acc-0.5330, test loss-1.6781, acc-0.5205\n",
      "Iter-92520, train loss-1.7080, acc-0.5200, valid loss-1.6771, acc-0.5328, test loss-1.6781, acc-0.5205\n",
      "Iter-92530, train loss-1.7741, acc-0.4200, valid loss-1.6771, acc-0.5328, test loss-1.6781, acc-0.5206\n",
      "Iter-92540, train loss-1.6347, acc-0.5200, valid loss-1.6770, acc-0.5328, test loss-1.6780, acc-0.5206\n",
      "Iter-92550, train loss-1.6315, acc-0.5800, valid loss-1.6770, acc-0.5328, test loss-1.6780, acc-0.5208\n",
      "Iter-92560, train loss-1.6306, acc-0.6200, valid loss-1.6770, acc-0.5328, test loss-1.6780, acc-0.5206\n",
      "Iter-92570, train loss-1.7121, acc-0.5200, valid loss-1.6769, acc-0.5330, test loss-1.6779, acc-0.5206\n",
      "Iter-92580, train loss-1.6202, acc-0.6400, valid loss-1.6769, acc-0.5330, test loss-1.6779, acc-0.5205\n",
      "Iter-92590, train loss-1.6794, acc-0.6000, valid loss-1.6769, acc-0.5330, test loss-1.6779, acc-0.5205\n",
      "Iter-92600, train loss-1.7637, acc-0.4400, valid loss-1.6768, acc-0.5330, test loss-1.6778, acc-0.5205\n",
      "Iter-92610, train loss-1.7383, acc-0.4600, valid loss-1.6768, acc-0.5330, test loss-1.6778, acc-0.5205\n",
      "Iter-92620, train loss-1.7300, acc-0.5800, valid loss-1.6768, acc-0.5330, test loss-1.6778, acc-0.5205\n",
      "Iter-92630, train loss-1.6578, acc-0.5800, valid loss-1.6767, acc-0.5324, test loss-1.6777, acc-0.5205\n",
      "Iter-92640, train loss-1.7826, acc-0.5000, valid loss-1.6767, acc-0.5326, test loss-1.6777, acc-0.5205\n",
      "Iter-92650, train loss-1.7858, acc-0.3800, valid loss-1.6767, acc-0.5328, test loss-1.6777, acc-0.5205\n",
      "Iter-92660, train loss-1.7810, acc-0.4400, valid loss-1.6766, acc-0.5324, test loss-1.6776, acc-0.5206\n",
      "Iter-92670, train loss-1.7584, acc-0.5800, valid loss-1.6766, acc-0.5328, test loss-1.6776, acc-0.5206\n",
      "Iter-92680, train loss-1.8010, acc-0.4400, valid loss-1.6766, acc-0.5328, test loss-1.6776, acc-0.5206\n",
      "Iter-92690, train loss-1.6473, acc-0.6000, valid loss-1.6765, acc-0.5326, test loss-1.6775, acc-0.5206\n",
      "Iter-92700, train loss-1.7813, acc-0.5200, valid loss-1.6765, acc-0.5328, test loss-1.6775, acc-0.5206\n",
      "Iter-92710, train loss-1.8089, acc-0.4400, valid loss-1.6765, acc-0.5328, test loss-1.6775, acc-0.5207\n",
      "Iter-92720, train loss-1.6960, acc-0.5400, valid loss-1.6765, acc-0.5330, test loss-1.6774, acc-0.5208\n",
      "Iter-92730, train loss-1.7010, acc-0.5200, valid loss-1.6764, acc-0.5332, test loss-1.6774, acc-0.5209\n",
      "Iter-92740, train loss-1.6564, acc-0.4600, valid loss-1.6764, acc-0.5332, test loss-1.6774, acc-0.5209\n",
      "Iter-92750, train loss-1.5605, acc-0.5600, valid loss-1.6763, acc-0.5326, test loss-1.6773, acc-0.5209\n",
      "Iter-92760, train loss-1.7501, acc-0.4800, valid loss-1.6763, acc-0.5328, test loss-1.6773, acc-0.5208\n",
      "Iter-92770, train loss-1.5851, acc-0.6200, valid loss-1.6763, acc-0.5328, test loss-1.6773, acc-0.5209\n",
      "Iter-92780, train loss-1.6717, acc-0.5400, valid loss-1.6762, acc-0.5326, test loss-1.6772, acc-0.5210\n",
      "Iter-92790, train loss-1.7229, acc-0.4800, valid loss-1.6762, acc-0.5326, test loss-1.6772, acc-0.5209\n",
      "Iter-92800, train loss-1.6704, acc-0.5400, valid loss-1.6762, acc-0.5326, test loss-1.6772, acc-0.5209\n",
      "Iter-92810, train loss-1.7138, acc-0.4400, valid loss-1.6762, acc-0.5324, test loss-1.6771, acc-0.5209\n",
      "Iter-92820, train loss-1.7567, acc-0.4800, valid loss-1.6761, acc-0.5326, test loss-1.6771, acc-0.5208\n",
      "Iter-92830, train loss-1.7706, acc-0.4600, valid loss-1.6761, acc-0.5328, test loss-1.6771, acc-0.5209\n",
      "Iter-92840, train loss-1.7522, acc-0.4800, valid loss-1.6761, acc-0.5328, test loss-1.6770, acc-0.5209\n",
      "Iter-92850, train loss-1.6702, acc-0.5000, valid loss-1.6760, acc-0.5328, test loss-1.6770, acc-0.5209\n",
      "Iter-92860, train loss-1.6492, acc-0.5800, valid loss-1.6760, acc-0.5328, test loss-1.6770, acc-0.5209\n",
      "Iter-92870, train loss-1.7222, acc-0.4800, valid loss-1.6760, acc-0.5330, test loss-1.6769, acc-0.5209\n",
      "Iter-92880, train loss-1.6264, acc-0.5400, valid loss-1.6759, acc-0.5330, test loss-1.6769, acc-0.5209\n",
      "Iter-92890, train loss-1.6639, acc-0.6200, valid loss-1.6759, acc-0.5330, test loss-1.6769, acc-0.5209\n",
      "Iter-92900, train loss-1.6082, acc-0.5600, valid loss-1.6759, acc-0.5330, test loss-1.6768, acc-0.5209\n",
      "Iter-92910, train loss-1.7924, acc-0.5800, valid loss-1.6758, acc-0.5328, test loss-1.6768, acc-0.5209\n",
      "Iter-92920, train loss-1.6555, acc-0.6400, valid loss-1.6758, acc-0.5326, test loss-1.6768, acc-0.5209\n",
      "Iter-92930, train loss-1.6906, acc-0.4600, valid loss-1.6758, acc-0.5326, test loss-1.6767, acc-0.5209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-92940, train loss-1.6409, acc-0.5400, valid loss-1.6757, acc-0.5326, test loss-1.6767, acc-0.5209\n",
      "Iter-92950, train loss-1.7324, acc-0.5000, valid loss-1.6757, acc-0.5326, test loss-1.6767, acc-0.5210\n",
      "Iter-92960, train loss-1.7453, acc-0.5400, valid loss-1.6757, acc-0.5326, test loss-1.6767, acc-0.5209\n",
      "Iter-92970, train loss-1.7513, acc-0.4600, valid loss-1.6756, acc-0.5326, test loss-1.6766, acc-0.5209\n",
      "Iter-92980, train loss-1.7031, acc-0.4800, valid loss-1.6756, acc-0.5326, test loss-1.6766, acc-0.5209\n",
      "Iter-92990, train loss-1.8666, acc-0.4200, valid loss-1.6756, acc-0.5326, test loss-1.6766, acc-0.5209\n",
      "Iter-93000, train loss-1.7707, acc-0.4800, valid loss-1.6755, acc-0.5324, test loss-1.6765, acc-0.5209\n",
      "Iter-93010, train loss-1.5989, acc-0.5400, valid loss-1.6755, acc-0.5324, test loss-1.6765, acc-0.5210\n",
      "Iter-93020, train loss-1.7641, acc-0.4400, valid loss-1.6755, acc-0.5324, test loss-1.6765, acc-0.5210\n",
      "Iter-93030, train loss-1.6823, acc-0.5600, valid loss-1.6754, acc-0.5324, test loss-1.6764, acc-0.5209\n",
      "Iter-93040, train loss-1.6117, acc-0.5800, valid loss-1.6754, acc-0.5324, test loss-1.6764, acc-0.5209\n",
      "Iter-93050, train loss-1.7304, acc-0.4800, valid loss-1.6754, acc-0.5324, test loss-1.6764, acc-0.5209\n",
      "Iter-93060, train loss-1.6343, acc-0.4800, valid loss-1.6753, acc-0.5322, test loss-1.6763, acc-0.5209\n",
      "Iter-93070, train loss-1.7825, acc-0.4000, valid loss-1.6753, acc-0.5324, test loss-1.6763, acc-0.5210\n",
      "Iter-93080, train loss-1.8185, acc-0.4200, valid loss-1.6753, acc-0.5324, test loss-1.6763, acc-0.5210\n",
      "Iter-93090, train loss-1.7005, acc-0.4000, valid loss-1.6752, acc-0.5322, test loss-1.6762, acc-0.5210\n",
      "Iter-93100, train loss-1.7028, acc-0.5000, valid loss-1.6752, acc-0.5326, test loss-1.6762, acc-0.5211\n",
      "Iter-93110, train loss-1.6773, acc-0.5200, valid loss-1.6752, acc-0.5324, test loss-1.6762, acc-0.5210\n",
      "Iter-93120, train loss-1.6761, acc-0.5000, valid loss-1.6751, acc-0.5322, test loss-1.6761, acc-0.5211\n",
      "Iter-93130, train loss-1.6940, acc-0.5600, valid loss-1.6751, acc-0.5324, test loss-1.6761, acc-0.5212\n",
      "Iter-93140, train loss-1.6209, acc-0.4800, valid loss-1.6751, acc-0.5324, test loss-1.6761, acc-0.5211\n",
      "Iter-93150, train loss-1.7274, acc-0.4400, valid loss-1.6750, acc-0.5322, test loss-1.6760, acc-0.5212\n",
      "Iter-93160, train loss-1.7113, acc-0.4000, valid loss-1.6750, acc-0.5322, test loss-1.6760, acc-0.5214\n",
      "Iter-93170, train loss-1.6932, acc-0.5200, valid loss-1.6750, acc-0.5322, test loss-1.6760, acc-0.5211\n",
      "Iter-93180, train loss-1.7542, acc-0.4800, valid loss-1.6749, acc-0.5320, test loss-1.6759, acc-0.5211\n",
      "Iter-93190, train loss-1.7308, acc-0.5000, valid loss-1.6749, acc-0.5322, test loss-1.6759, acc-0.5212\n",
      "Iter-93200, train loss-1.7326, acc-0.4600, valid loss-1.6749, acc-0.5326, test loss-1.6759, acc-0.5211\n",
      "Iter-93210, train loss-1.7005, acc-0.5000, valid loss-1.6748, acc-0.5326, test loss-1.6758, acc-0.5210\n",
      "Iter-93220, train loss-1.7776, acc-0.4400, valid loss-1.6748, acc-0.5326, test loss-1.6758, acc-0.5211\n",
      "Iter-93230, train loss-1.5833, acc-0.6600, valid loss-1.6748, acc-0.5326, test loss-1.6758, acc-0.5212\n",
      "Iter-93240, train loss-1.7132, acc-0.4800, valid loss-1.6747, acc-0.5326, test loss-1.6758, acc-0.5211\n",
      "Iter-93250, train loss-1.6217, acc-0.5200, valid loss-1.6747, acc-0.5326, test loss-1.6757, acc-0.5212\n",
      "Iter-93260, train loss-1.7537, acc-0.5000, valid loss-1.6747, acc-0.5326, test loss-1.6757, acc-0.5211\n",
      "Iter-93270, train loss-1.8418, acc-0.4400, valid loss-1.6747, acc-0.5326, test loss-1.6757, acc-0.5212\n",
      "Iter-93280, train loss-1.6547, acc-0.5000, valid loss-1.6746, acc-0.5326, test loss-1.6756, acc-0.5211\n",
      "Iter-93290, train loss-1.6568, acc-0.4400, valid loss-1.6746, acc-0.5326, test loss-1.6756, acc-0.5211\n",
      "Iter-93300, train loss-1.6287, acc-0.5600, valid loss-1.6746, acc-0.5326, test loss-1.6756, acc-0.5211\n",
      "Iter-93310, train loss-1.6545, acc-0.5000, valid loss-1.6745, acc-0.5326, test loss-1.6755, acc-0.5212\n",
      "Iter-93320, train loss-1.7440, acc-0.4800, valid loss-1.6745, acc-0.5326, test loss-1.6755, acc-0.5212\n",
      "Iter-93330, train loss-1.5739, acc-0.5000, valid loss-1.6745, acc-0.5326, test loss-1.6755, acc-0.5212\n",
      "Iter-93340, train loss-1.7757, acc-0.4000, valid loss-1.6744, acc-0.5328, test loss-1.6754, acc-0.5212\n",
      "Iter-93350, train loss-1.8254, acc-0.4600, valid loss-1.6744, acc-0.5328, test loss-1.6754, acc-0.5211\n",
      "Iter-93360, train loss-1.6762, acc-0.6000, valid loss-1.6744, acc-0.5326, test loss-1.6754, acc-0.5211\n",
      "Iter-93370, train loss-1.5917, acc-0.5800, valid loss-1.6743, acc-0.5326, test loss-1.6753, acc-0.5210\n",
      "Iter-93380, train loss-1.7171, acc-0.5200, valid loss-1.6743, acc-0.5326, test loss-1.6753, acc-0.5211\n",
      "Iter-93390, train loss-1.6887, acc-0.4400, valid loss-1.6743, acc-0.5326, test loss-1.6753, acc-0.5214\n",
      "Iter-93400, train loss-1.6238, acc-0.5000, valid loss-1.6742, acc-0.5326, test loss-1.6752, acc-0.5213\n",
      "Iter-93410, train loss-1.6793, acc-0.5200, valid loss-1.6742, acc-0.5326, test loss-1.6752, acc-0.5213\n",
      "Iter-93420, train loss-1.7217, acc-0.4800, valid loss-1.6742, acc-0.5326, test loss-1.6752, acc-0.5213\n",
      "Iter-93430, train loss-1.7595, acc-0.4000, valid loss-1.6741, acc-0.5326, test loss-1.6751, acc-0.5214\n",
      "Iter-93440, train loss-1.5994, acc-0.5000, valid loss-1.6741, acc-0.5328, test loss-1.6751, acc-0.5213\n",
      "Iter-93450, train loss-1.7316, acc-0.5000, valid loss-1.6741, acc-0.5326, test loss-1.6751, acc-0.5212\n",
      "Iter-93460, train loss-1.7669, acc-0.4200, valid loss-1.6740, acc-0.5328, test loss-1.6750, acc-0.5212\n",
      "Iter-93470, train loss-1.6426, acc-0.5600, valid loss-1.6740, acc-0.5326, test loss-1.6750, acc-0.5212\n",
      "Iter-93480, train loss-1.7498, acc-0.5200, valid loss-1.6740, acc-0.5326, test loss-1.6750, acc-0.5211\n",
      "Iter-93490, train loss-1.5965, acc-0.6000, valid loss-1.6739, acc-0.5326, test loss-1.6749, acc-0.5211\n",
      "Iter-93500, train loss-1.5936, acc-0.6000, valid loss-1.6739, acc-0.5326, test loss-1.6749, acc-0.5211\n",
      "Iter-93510, train loss-1.7310, acc-0.5400, valid loss-1.6739, acc-0.5326, test loss-1.6749, acc-0.5212\n",
      "Iter-93520, train loss-1.6383, acc-0.6400, valid loss-1.6738, acc-0.5326, test loss-1.6748, acc-0.5212\n",
      "Iter-93530, train loss-1.6462, acc-0.5400, valid loss-1.6738, acc-0.5330, test loss-1.6748, acc-0.5212\n",
      "Iter-93540, train loss-1.6613, acc-0.5000, valid loss-1.6738, acc-0.5330, test loss-1.6748, acc-0.5213\n",
      "Iter-93550, train loss-1.6751, acc-0.5000, valid loss-1.6737, acc-0.5330, test loss-1.6747, acc-0.5211\n",
      "Iter-93560, train loss-1.7006, acc-0.4800, valid loss-1.6737, acc-0.5330, test loss-1.6747, acc-0.5211\n",
      "Iter-93570, train loss-1.7440, acc-0.6600, valid loss-1.6737, acc-0.5330, test loss-1.6747, acc-0.5211\n",
      "Iter-93580, train loss-1.6535, acc-0.5200, valid loss-1.6736, acc-0.5330, test loss-1.6746, acc-0.5211\n",
      "Iter-93590, train loss-1.7082, acc-0.4600, valid loss-1.6736, acc-0.5330, test loss-1.6746, acc-0.5212\n",
      "Iter-93600, train loss-1.6493, acc-0.5600, valid loss-1.6736, acc-0.5332, test loss-1.6746, acc-0.5211\n",
      "Iter-93610, train loss-1.7349, acc-0.4600, valid loss-1.6735, acc-0.5330, test loss-1.6745, acc-0.5211\n",
      "Iter-93620, train loss-1.7663, acc-0.4800, valid loss-1.6735, acc-0.5330, test loss-1.6745, acc-0.5210\n",
      "Iter-93630, train loss-1.6290, acc-0.6000, valid loss-1.6735, acc-0.5330, test loss-1.6745, acc-0.5211\n",
      "Iter-93640, train loss-1.7020, acc-0.5200, valid loss-1.6734, acc-0.5330, test loss-1.6745, acc-0.5210\n",
      "Iter-93650, train loss-1.7348, acc-0.4400, valid loss-1.6734, acc-0.5328, test loss-1.6744, acc-0.5211\n",
      "Iter-93660, train loss-1.6270, acc-0.6000, valid loss-1.6734, acc-0.5330, test loss-1.6744, acc-0.5212\n",
      "Iter-93670, train loss-1.6713, acc-0.5600, valid loss-1.6734, acc-0.5330, test loss-1.6744, acc-0.5213\n",
      "Iter-93680, train loss-1.6142, acc-0.4800, valid loss-1.6733, acc-0.5332, test loss-1.6743, acc-0.5210\n",
      "Iter-93690, train loss-1.6746, acc-0.5000, valid loss-1.6733, acc-0.5332, test loss-1.6743, acc-0.5210\n",
      "Iter-93700, train loss-1.5951, acc-0.6000, valid loss-1.6732, acc-0.5334, test loss-1.6743, acc-0.5211\n",
      "Iter-93710, train loss-1.5875, acc-0.6000, valid loss-1.6732, acc-0.5334, test loss-1.6742, acc-0.5211\n",
      "Iter-93720, train loss-1.7249, acc-0.5000, valid loss-1.6732, acc-0.5334, test loss-1.6742, acc-0.5211\n",
      "Iter-93730, train loss-1.6854, acc-0.5800, valid loss-1.6732, acc-0.5336, test loss-1.6742, acc-0.5210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-93740, train loss-1.6789, acc-0.5000, valid loss-1.6731, acc-0.5336, test loss-1.6741, acc-0.5211\n",
      "Iter-93750, train loss-1.6950, acc-0.4600, valid loss-1.6731, acc-0.5338, test loss-1.6741, acc-0.5213\n",
      "Iter-93760, train loss-1.6418, acc-0.6200, valid loss-1.6731, acc-0.5336, test loss-1.6741, acc-0.5212\n",
      "Iter-93770, train loss-1.7431, acc-0.4600, valid loss-1.6730, acc-0.5336, test loss-1.6740, acc-0.5212\n",
      "Iter-93780, train loss-1.7421, acc-0.5200, valid loss-1.6730, acc-0.5336, test loss-1.6740, acc-0.5212\n",
      "Iter-93790, train loss-1.6898, acc-0.5200, valid loss-1.6730, acc-0.5336, test loss-1.6740, acc-0.5213\n",
      "Iter-93800, train loss-1.7088, acc-0.4600, valid loss-1.6729, acc-0.5336, test loss-1.6739, acc-0.5212\n",
      "Iter-93810, train loss-1.6540, acc-0.4600, valid loss-1.6729, acc-0.5336, test loss-1.6739, acc-0.5214\n",
      "Iter-93820, train loss-1.5950, acc-0.6600, valid loss-1.6729, acc-0.5334, test loss-1.6739, acc-0.5214\n",
      "Iter-93830, train loss-1.6563, acc-0.6400, valid loss-1.6728, acc-0.5338, test loss-1.6738, acc-0.5214\n",
      "Iter-93840, train loss-1.5747, acc-0.6600, valid loss-1.6728, acc-0.5336, test loss-1.6738, acc-0.5217\n",
      "Iter-93850, train loss-1.6556, acc-0.5200, valid loss-1.6728, acc-0.5334, test loss-1.6738, acc-0.5216\n",
      "Iter-93860, train loss-1.7296, acc-0.4800, valid loss-1.6727, acc-0.5334, test loss-1.6737, acc-0.5215\n",
      "Iter-93870, train loss-1.7301, acc-0.5400, valid loss-1.6727, acc-0.5334, test loss-1.6737, acc-0.5217\n",
      "Iter-93880, train loss-1.6556, acc-0.5000, valid loss-1.6727, acc-0.5334, test loss-1.6737, acc-0.5218\n",
      "Iter-93890, train loss-1.6197, acc-0.6200, valid loss-1.6726, acc-0.5334, test loss-1.6737, acc-0.5218\n",
      "Iter-93900, train loss-1.7258, acc-0.5000, valid loss-1.6726, acc-0.5336, test loss-1.6736, acc-0.5220\n",
      "Iter-93910, train loss-1.6079, acc-0.6000, valid loss-1.6726, acc-0.5332, test loss-1.6736, acc-0.5221\n",
      "Iter-93920, train loss-1.7272, acc-0.4800, valid loss-1.6725, acc-0.5334, test loss-1.6736, acc-0.5220\n",
      "Iter-93930, train loss-1.6962, acc-0.5600, valid loss-1.6725, acc-0.5334, test loss-1.6735, acc-0.5220\n",
      "Iter-93940, train loss-1.6863, acc-0.6000, valid loss-1.6725, acc-0.5334, test loss-1.6735, acc-0.5221\n",
      "Iter-93950, train loss-1.6479, acc-0.4800, valid loss-1.6724, acc-0.5332, test loss-1.6735, acc-0.5222\n",
      "Iter-93960, train loss-1.7655, acc-0.5800, valid loss-1.6724, acc-0.5334, test loss-1.6734, acc-0.5220\n",
      "Iter-93970, train loss-1.6015, acc-0.5000, valid loss-1.6724, acc-0.5334, test loss-1.6734, acc-0.5220\n",
      "Iter-93980, train loss-1.7477, acc-0.5200, valid loss-1.6723, acc-0.5334, test loss-1.6734, acc-0.5220\n",
      "Iter-93990, train loss-1.7249, acc-0.5200, valid loss-1.6723, acc-0.5334, test loss-1.6733, acc-0.5220\n",
      "Iter-94000, train loss-1.6820, acc-0.5200, valid loss-1.6723, acc-0.5334, test loss-1.6733, acc-0.5221\n",
      "Iter-94010, train loss-1.6151, acc-0.5600, valid loss-1.6722, acc-0.5334, test loss-1.6733, acc-0.5222\n",
      "Iter-94020, train loss-1.6821, acc-0.5400, valid loss-1.6722, acc-0.5334, test loss-1.6732, acc-0.5221\n",
      "Iter-94030, train loss-1.7454, acc-0.5400, valid loss-1.6722, acc-0.5332, test loss-1.6732, acc-0.5221\n",
      "Iter-94040, train loss-1.6419, acc-0.5200, valid loss-1.6722, acc-0.5332, test loss-1.6732, acc-0.5222\n",
      "Iter-94050, train loss-1.6925, acc-0.4800, valid loss-1.6721, acc-0.5330, test loss-1.6731, acc-0.5221\n",
      "Iter-94060, train loss-1.8157, acc-0.4200, valid loss-1.6721, acc-0.5334, test loss-1.6731, acc-0.5221\n",
      "Iter-94070, train loss-1.6724, acc-0.5000, valid loss-1.6721, acc-0.5334, test loss-1.6731, acc-0.5221\n",
      "Iter-94080, train loss-1.7860, acc-0.4600, valid loss-1.6720, acc-0.5336, test loss-1.6730, acc-0.5221\n",
      "Iter-94090, train loss-1.6571, acc-0.5000, valid loss-1.6720, acc-0.5338, test loss-1.6730, acc-0.5222\n",
      "Iter-94100, train loss-1.6504, acc-0.5600, valid loss-1.6720, acc-0.5338, test loss-1.6730, acc-0.5221\n",
      "Iter-94110, train loss-1.7113, acc-0.5600, valid loss-1.6719, acc-0.5338, test loss-1.6729, acc-0.5223\n",
      "Iter-94120, train loss-1.7148, acc-0.5200, valid loss-1.6719, acc-0.5336, test loss-1.6729, acc-0.5225\n",
      "Iter-94130, train loss-1.7737, acc-0.4400, valid loss-1.6719, acc-0.5336, test loss-1.6729, acc-0.5225\n",
      "Iter-94140, train loss-1.8585, acc-0.3800, valid loss-1.6718, acc-0.5336, test loss-1.6728, acc-0.5224\n",
      "Iter-94150, train loss-1.6231, acc-0.4800, valid loss-1.6718, acc-0.5336, test loss-1.6728, acc-0.5223\n",
      "Iter-94160, train loss-1.6531, acc-0.5800, valid loss-1.6718, acc-0.5338, test loss-1.6728, acc-0.5223\n",
      "Iter-94170, train loss-1.6475, acc-0.6000, valid loss-1.6717, acc-0.5338, test loss-1.6728, acc-0.5221\n",
      "Iter-94180, train loss-1.7305, acc-0.4600, valid loss-1.6717, acc-0.5338, test loss-1.6727, acc-0.5221\n",
      "Iter-94190, train loss-1.6070, acc-0.5800, valid loss-1.6717, acc-0.5336, test loss-1.6727, acc-0.5222\n",
      "Iter-94200, train loss-1.8015, acc-0.4200, valid loss-1.6716, acc-0.5338, test loss-1.6727, acc-0.5221\n",
      "Iter-94210, train loss-1.7545, acc-0.5000, valid loss-1.6716, acc-0.5340, test loss-1.6726, acc-0.5223\n",
      "Iter-94220, train loss-1.6394, acc-0.5400, valid loss-1.6716, acc-0.5336, test loss-1.6726, acc-0.5223\n",
      "Iter-94230, train loss-1.6306, acc-0.5200, valid loss-1.6715, acc-0.5336, test loss-1.6726, acc-0.5223\n",
      "Iter-94240, train loss-1.6271, acc-0.6000, valid loss-1.6715, acc-0.5336, test loss-1.6725, acc-0.5223\n",
      "Iter-94250, train loss-1.6183, acc-0.4200, valid loss-1.6715, acc-0.5338, test loss-1.6725, acc-0.5223\n",
      "Iter-94260, train loss-1.6846, acc-0.6000, valid loss-1.6714, acc-0.5336, test loss-1.6725, acc-0.5223\n",
      "Iter-94270, train loss-1.6334, acc-0.6400, valid loss-1.6714, acc-0.5336, test loss-1.6724, acc-0.5223\n",
      "Iter-94280, train loss-1.6235, acc-0.5400, valid loss-1.6714, acc-0.5336, test loss-1.6724, acc-0.5222\n",
      "Iter-94290, train loss-1.6413, acc-0.5400, valid loss-1.6713, acc-0.5336, test loss-1.6724, acc-0.5223\n",
      "Iter-94300, train loss-1.7243, acc-0.4800, valid loss-1.6713, acc-0.5334, test loss-1.6723, acc-0.5223\n",
      "Iter-94310, train loss-1.7720, acc-0.4200, valid loss-1.6713, acc-0.5338, test loss-1.6723, acc-0.5223\n",
      "Iter-94320, train loss-1.6379, acc-0.6000, valid loss-1.6712, acc-0.5336, test loss-1.6723, acc-0.5222\n",
      "Iter-94330, train loss-1.6390, acc-0.5400, valid loss-1.6712, acc-0.5334, test loss-1.6722, acc-0.5222\n",
      "Iter-94340, train loss-1.6064, acc-0.5800, valid loss-1.6712, acc-0.5336, test loss-1.6722, acc-0.5222\n",
      "Iter-94350, train loss-1.6092, acc-0.5000, valid loss-1.6711, acc-0.5334, test loss-1.6722, acc-0.5222\n",
      "Iter-94360, train loss-1.7655, acc-0.5200, valid loss-1.6711, acc-0.5334, test loss-1.6721, acc-0.5223\n",
      "Iter-94370, train loss-1.7437, acc-0.4800, valid loss-1.6711, acc-0.5336, test loss-1.6721, acc-0.5221\n",
      "Iter-94380, train loss-1.6748, acc-0.5600, valid loss-1.6711, acc-0.5336, test loss-1.6721, acc-0.5221\n",
      "Iter-94390, train loss-1.5584, acc-0.6400, valid loss-1.6710, acc-0.5338, test loss-1.6721, acc-0.5221\n",
      "Iter-94400, train loss-1.7341, acc-0.5200, valid loss-1.6710, acc-0.5336, test loss-1.6720, acc-0.5221\n",
      "Iter-94410, train loss-1.7446, acc-0.5200, valid loss-1.6710, acc-0.5336, test loss-1.6720, acc-0.5221\n",
      "Iter-94420, train loss-1.6462, acc-0.5800, valid loss-1.6709, acc-0.5338, test loss-1.6720, acc-0.5221\n",
      "Iter-94430, train loss-1.7666, acc-0.5000, valid loss-1.6709, acc-0.5336, test loss-1.6719, acc-0.5221\n",
      "Iter-94440, train loss-1.5545, acc-0.5800, valid loss-1.6709, acc-0.5336, test loss-1.6719, acc-0.5222\n",
      "Iter-94450, train loss-1.6663, acc-0.5000, valid loss-1.6708, acc-0.5336, test loss-1.6719, acc-0.5224\n",
      "Iter-94460, train loss-1.6754, acc-0.5400, valid loss-1.6708, acc-0.5336, test loss-1.6718, acc-0.5224\n",
      "Iter-94470, train loss-1.6715, acc-0.4600, valid loss-1.6708, acc-0.5336, test loss-1.6718, acc-0.5225\n",
      "Iter-94480, train loss-1.6390, acc-0.5800, valid loss-1.6707, acc-0.5336, test loss-1.6718, acc-0.5225\n",
      "Iter-94490, train loss-1.7662, acc-0.4800, valid loss-1.6707, acc-0.5336, test loss-1.6717, acc-0.5224\n",
      "Iter-94500, train loss-1.7550, acc-0.3600, valid loss-1.6707, acc-0.5338, test loss-1.6717, acc-0.5226\n",
      "Iter-94510, train loss-1.5960, acc-0.5800, valid loss-1.6706, acc-0.5338, test loss-1.6717, acc-0.5225\n",
      "Iter-94520, train loss-1.6570, acc-0.5400, valid loss-1.6706, acc-0.5336, test loss-1.6716, acc-0.5224\n",
      "Iter-94530, train loss-1.6020, acc-0.5400, valid loss-1.6706, acc-0.5338, test loss-1.6716, acc-0.5224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-94540, train loss-1.7446, acc-0.5200, valid loss-1.6705, acc-0.5338, test loss-1.6716, acc-0.5226\n",
      "Iter-94550, train loss-1.7386, acc-0.4000, valid loss-1.6705, acc-0.5338, test loss-1.6716, acc-0.5227\n",
      "Iter-94560, train loss-1.7340, acc-0.5000, valid loss-1.6705, acc-0.5336, test loss-1.6715, acc-0.5222\n",
      "Iter-94570, train loss-1.8281, acc-0.4600, valid loss-1.6705, acc-0.5338, test loss-1.6715, acc-0.5225\n",
      "Iter-94580, train loss-1.6191, acc-0.5600, valid loss-1.6704, acc-0.5340, test loss-1.6715, acc-0.5224\n",
      "Iter-94590, train loss-1.6892, acc-0.4400, valid loss-1.6704, acc-0.5338, test loss-1.6714, acc-0.5226\n",
      "Iter-94600, train loss-1.7399, acc-0.4800, valid loss-1.6704, acc-0.5338, test loss-1.6714, acc-0.5225\n",
      "Iter-94610, train loss-1.7449, acc-0.4200, valid loss-1.6703, acc-0.5338, test loss-1.6714, acc-0.5226\n",
      "Iter-94620, train loss-1.6839, acc-0.4200, valid loss-1.6703, acc-0.5338, test loss-1.6713, acc-0.5226\n",
      "Iter-94630, train loss-1.7517, acc-0.5000, valid loss-1.6703, acc-0.5338, test loss-1.6713, acc-0.5226\n",
      "Iter-94640, train loss-1.7344, acc-0.5400, valid loss-1.6702, acc-0.5336, test loss-1.6713, acc-0.5226\n",
      "Iter-94650, train loss-1.6461, acc-0.5400, valid loss-1.6702, acc-0.5340, test loss-1.6712, acc-0.5226\n",
      "Iter-94660, train loss-1.8350, acc-0.4400, valid loss-1.6702, acc-0.5338, test loss-1.6712, acc-0.5226\n",
      "Iter-94670, train loss-1.6617, acc-0.5200, valid loss-1.6701, acc-0.5336, test loss-1.6712, acc-0.5227\n",
      "Iter-94680, train loss-1.5798, acc-0.5600, valid loss-1.6701, acc-0.5338, test loss-1.6711, acc-0.5226\n",
      "Iter-94690, train loss-1.6332, acc-0.5200, valid loss-1.6701, acc-0.5338, test loss-1.6711, acc-0.5225\n",
      "Iter-94700, train loss-1.7651, acc-0.4400, valid loss-1.6700, acc-0.5340, test loss-1.6711, acc-0.5227\n",
      "Iter-94710, train loss-1.6925, acc-0.5600, valid loss-1.6700, acc-0.5340, test loss-1.6710, acc-0.5226\n",
      "Iter-94720, train loss-1.5856, acc-0.5800, valid loss-1.6700, acc-0.5340, test loss-1.6710, acc-0.5226\n",
      "Iter-94730, train loss-1.5204, acc-0.6400, valid loss-1.6699, acc-0.5340, test loss-1.6710, acc-0.5226\n",
      "Iter-94740, train loss-1.5807, acc-0.5800, valid loss-1.6699, acc-0.5338, test loss-1.6709, acc-0.5226\n",
      "Iter-94750, train loss-1.6123, acc-0.5600, valid loss-1.6699, acc-0.5340, test loss-1.6709, acc-0.5225\n",
      "Iter-94760, train loss-1.7383, acc-0.5000, valid loss-1.6698, acc-0.5342, test loss-1.6709, acc-0.5226\n",
      "Iter-94770, train loss-1.6950, acc-0.5600, valid loss-1.6698, acc-0.5340, test loss-1.6708, acc-0.5225\n",
      "Iter-94780, train loss-1.7394, acc-0.4200, valid loss-1.6698, acc-0.5340, test loss-1.6708, acc-0.5226\n",
      "Iter-94790, train loss-1.8133, acc-0.5000, valid loss-1.6697, acc-0.5340, test loss-1.6708, acc-0.5225\n",
      "Iter-94800, train loss-1.6883, acc-0.4800, valid loss-1.6697, acc-0.5340, test loss-1.6708, acc-0.5226\n",
      "Iter-94810, train loss-1.7266, acc-0.4800, valid loss-1.6697, acc-0.5340, test loss-1.6707, acc-0.5226\n",
      "Iter-94820, train loss-1.5825, acc-0.5800, valid loss-1.6697, acc-0.5338, test loss-1.6707, acc-0.5225\n",
      "Iter-94830, train loss-1.6780, acc-0.6200, valid loss-1.6696, acc-0.5336, test loss-1.6707, acc-0.5226\n",
      "Iter-94840, train loss-1.7141, acc-0.4800, valid loss-1.6696, acc-0.5336, test loss-1.6706, acc-0.5227\n",
      "Iter-94850, train loss-1.8167, acc-0.4800, valid loss-1.6696, acc-0.5336, test loss-1.6706, acc-0.5227\n",
      "Iter-94860, train loss-1.6170, acc-0.6200, valid loss-1.6695, acc-0.5336, test loss-1.6706, acc-0.5227\n",
      "Iter-94870, train loss-1.7337, acc-0.4600, valid loss-1.6695, acc-0.5336, test loss-1.6705, acc-0.5226\n",
      "Iter-94880, train loss-1.6514, acc-0.5400, valid loss-1.6695, acc-0.5336, test loss-1.6705, acc-0.5227\n",
      "Iter-94890, train loss-1.7949, acc-0.5000, valid loss-1.6694, acc-0.5338, test loss-1.6705, acc-0.5227\n",
      "Iter-94900, train loss-1.7964, acc-0.5000, valid loss-1.6694, acc-0.5338, test loss-1.6704, acc-0.5227\n",
      "Iter-94910, train loss-1.6386, acc-0.5800, valid loss-1.6694, acc-0.5336, test loss-1.6704, acc-0.5228\n",
      "Iter-94920, train loss-1.6829, acc-0.5800, valid loss-1.6693, acc-0.5338, test loss-1.6704, acc-0.5226\n",
      "Iter-94930, train loss-1.6160, acc-0.5200, valid loss-1.6693, acc-0.5338, test loss-1.6703, acc-0.5227\n",
      "Iter-94940, train loss-1.6328, acc-0.5600, valid loss-1.6693, acc-0.5340, test loss-1.6703, acc-0.5226\n",
      "Iter-94950, train loss-1.6348, acc-0.5600, valid loss-1.6693, acc-0.5340, test loss-1.6703, acc-0.5229\n",
      "Iter-94960, train loss-1.5866, acc-0.5000, valid loss-1.6692, acc-0.5340, test loss-1.6702, acc-0.5229\n",
      "Iter-94970, train loss-1.7481, acc-0.4400, valid loss-1.6692, acc-0.5340, test loss-1.6702, acc-0.5229\n",
      "Iter-94980, train loss-1.6681, acc-0.5400, valid loss-1.6692, acc-0.5342, test loss-1.6702, acc-0.5230\n",
      "Iter-94990, train loss-1.8501, acc-0.3600, valid loss-1.6691, acc-0.5342, test loss-1.6702, acc-0.5230\n",
      "Iter-95000, train loss-1.6892, acc-0.5400, valid loss-1.6691, acc-0.5340, test loss-1.6701, acc-0.5229\n",
      "Iter-95010, train loss-1.6579, acc-0.5800, valid loss-1.6691, acc-0.5342, test loss-1.6701, acc-0.5229\n",
      "Iter-95020, train loss-1.6188, acc-0.5600, valid loss-1.6690, acc-0.5342, test loss-1.6701, acc-0.5230\n",
      "Iter-95030, train loss-1.6334, acc-0.5800, valid loss-1.6690, acc-0.5340, test loss-1.6700, acc-0.5230\n",
      "Iter-95040, train loss-1.7236, acc-0.4400, valid loss-1.6690, acc-0.5340, test loss-1.6700, acc-0.5231\n",
      "Iter-95050, train loss-1.6491, acc-0.6400, valid loss-1.6689, acc-0.5340, test loss-1.6700, acc-0.5231\n",
      "Iter-95060, train loss-1.7230, acc-0.4400, valid loss-1.6689, acc-0.5342, test loss-1.6699, acc-0.5232\n",
      "Iter-95070, train loss-1.8450, acc-0.4000, valid loss-1.6689, acc-0.5342, test loss-1.6699, acc-0.5231\n",
      "Iter-95080, train loss-1.6419, acc-0.5600, valid loss-1.6688, acc-0.5342, test loss-1.6699, acc-0.5230\n",
      "Iter-95090, train loss-1.7613, acc-0.5600, valid loss-1.6688, acc-0.5342, test loss-1.6698, acc-0.5231\n",
      "Iter-95100, train loss-1.6859, acc-0.5600, valid loss-1.6688, acc-0.5340, test loss-1.6698, acc-0.5231\n",
      "Iter-95110, train loss-1.6037, acc-0.6400, valid loss-1.6687, acc-0.5340, test loss-1.6698, acc-0.5231\n",
      "Iter-95120, train loss-1.6045, acc-0.4800, valid loss-1.6687, acc-0.5340, test loss-1.6697, acc-0.5231\n",
      "Iter-95130, train loss-1.6901, acc-0.5400, valid loss-1.6687, acc-0.5340, test loss-1.6697, acc-0.5231\n",
      "Iter-95140, train loss-1.6697, acc-0.5000, valid loss-1.6686, acc-0.5342, test loss-1.6697, acc-0.5232\n",
      "Iter-95150, train loss-1.6579, acc-0.6000, valid loss-1.6686, acc-0.5344, test loss-1.6696, acc-0.5232\n",
      "Iter-95160, train loss-1.7232, acc-0.5000, valid loss-1.6686, acc-0.5342, test loss-1.6696, acc-0.5232\n",
      "Iter-95170, train loss-1.5973, acc-0.6600, valid loss-1.6686, acc-0.5342, test loss-1.6696, acc-0.5232\n",
      "Iter-95180, train loss-1.7168, acc-0.5200, valid loss-1.6685, acc-0.5342, test loss-1.6695, acc-0.5232\n",
      "Iter-95190, train loss-1.4816, acc-0.5400, valid loss-1.6685, acc-0.5342, test loss-1.6695, acc-0.5232\n",
      "Iter-95200, train loss-1.5361, acc-0.6400, valid loss-1.6685, acc-0.5342, test loss-1.6695, acc-0.5235\n",
      "Iter-95210, train loss-1.6594, acc-0.4800, valid loss-1.6684, acc-0.5340, test loss-1.6694, acc-0.5234\n",
      "Iter-95220, train loss-1.5113, acc-0.6200, valid loss-1.6684, acc-0.5340, test loss-1.6694, acc-0.5233\n",
      "Iter-95230, train loss-1.7042, acc-0.5200, valid loss-1.6684, acc-0.5338, test loss-1.6694, acc-0.5235\n",
      "Iter-95240, train loss-1.7927, acc-0.4600, valid loss-1.6683, acc-0.5340, test loss-1.6693, acc-0.5234\n",
      "Iter-95250, train loss-1.6059, acc-0.5200, valid loss-1.6683, acc-0.5340, test loss-1.6693, acc-0.5235\n",
      "Iter-95260, train loss-1.7087, acc-0.4400, valid loss-1.6683, acc-0.5340, test loss-1.6693, acc-0.5235\n",
      "Iter-95270, train loss-1.7234, acc-0.4800, valid loss-1.6682, acc-0.5340, test loss-1.6693, acc-0.5233\n",
      "Iter-95280, train loss-1.7462, acc-0.6000, valid loss-1.6682, acc-0.5338, test loss-1.6692, acc-0.5233\n",
      "Iter-95290, train loss-1.7604, acc-0.4400, valid loss-1.6682, acc-0.5338, test loss-1.6692, acc-0.5233\n",
      "Iter-95300, train loss-1.7082, acc-0.5600, valid loss-1.6681, acc-0.5338, test loss-1.6692, acc-0.5232\n",
      "Iter-95310, train loss-1.7676, acc-0.4800, valid loss-1.6681, acc-0.5340, test loss-1.6691, acc-0.5234\n",
      "Iter-95320, train loss-1.7609, acc-0.5200, valid loss-1.6681, acc-0.5342, test loss-1.6691, acc-0.5235\n",
      "Iter-95330, train loss-1.7265, acc-0.5400, valid loss-1.6680, acc-0.5344, test loss-1.6691, acc-0.5233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-95340, train loss-1.7059, acc-0.5600, valid loss-1.6680, acc-0.5342, test loss-1.6690, acc-0.5233\n",
      "Iter-95350, train loss-1.5371, acc-0.6600, valid loss-1.6680, acc-0.5342, test loss-1.6690, acc-0.5234\n",
      "Iter-95360, train loss-1.6400, acc-0.6200, valid loss-1.6679, acc-0.5342, test loss-1.6690, acc-0.5234\n",
      "Iter-95370, train loss-1.6972, acc-0.5000, valid loss-1.6679, acc-0.5342, test loss-1.6689, acc-0.5234\n",
      "Iter-95380, train loss-1.6652, acc-0.5200, valid loss-1.6679, acc-0.5342, test loss-1.6689, acc-0.5234\n",
      "Iter-95390, train loss-1.6763, acc-0.5400, valid loss-1.6679, acc-0.5342, test loss-1.6689, acc-0.5234\n",
      "Iter-95400, train loss-1.6793, acc-0.5600, valid loss-1.6678, acc-0.5342, test loss-1.6689, acc-0.5234\n",
      "Iter-95410, train loss-1.7600, acc-0.4600, valid loss-1.6678, acc-0.5342, test loss-1.6688, acc-0.5234\n",
      "Iter-95420, train loss-1.8464, acc-0.4600, valid loss-1.6678, acc-0.5340, test loss-1.6688, acc-0.5234\n",
      "Iter-95430, train loss-1.7175, acc-0.4600, valid loss-1.6677, acc-0.5340, test loss-1.6688, acc-0.5234\n",
      "Iter-95440, train loss-1.7564, acc-0.4400, valid loss-1.6677, acc-0.5342, test loss-1.6687, acc-0.5233\n",
      "Iter-95450, train loss-1.6009, acc-0.5600, valid loss-1.6677, acc-0.5340, test loss-1.6687, acc-0.5233\n",
      "Iter-95460, train loss-1.6659, acc-0.5400, valid loss-1.6676, acc-0.5342, test loss-1.6687, acc-0.5233\n",
      "Iter-95470, train loss-1.6940, acc-0.5000, valid loss-1.6676, acc-0.5342, test loss-1.6686, acc-0.5235\n",
      "Iter-95480, train loss-1.7273, acc-0.5600, valid loss-1.6676, acc-0.5342, test loss-1.6686, acc-0.5233\n",
      "Iter-95490, train loss-1.5902, acc-0.5600, valid loss-1.6675, acc-0.5342, test loss-1.6686, acc-0.5235\n",
      "Iter-95500, train loss-1.6655, acc-0.4800, valid loss-1.6675, acc-0.5342, test loss-1.6685, acc-0.5236\n",
      "Iter-95510, train loss-1.7123, acc-0.5200, valid loss-1.6675, acc-0.5342, test loss-1.6685, acc-0.5234\n",
      "Iter-95520, train loss-1.5763, acc-0.5800, valid loss-1.6674, acc-0.5342, test loss-1.6685, acc-0.5235\n",
      "Iter-95530, train loss-1.6089, acc-0.6000, valid loss-1.6674, acc-0.5342, test loss-1.6684, acc-0.5235\n",
      "Iter-95540, train loss-1.6638, acc-0.5200, valid loss-1.6674, acc-0.5342, test loss-1.6684, acc-0.5235\n",
      "Iter-95550, train loss-1.6576, acc-0.4800, valid loss-1.6673, acc-0.5342, test loss-1.6684, acc-0.5235\n",
      "Iter-95560, train loss-1.6652, acc-0.5400, valid loss-1.6673, acc-0.5342, test loss-1.6683, acc-0.5235\n",
      "Iter-95570, train loss-1.5572, acc-0.6200, valid loss-1.6673, acc-0.5342, test loss-1.6683, acc-0.5235\n",
      "Iter-95580, train loss-1.7634, acc-0.4600, valid loss-1.6673, acc-0.5342, test loss-1.6683, acc-0.5235\n",
      "Iter-95590, train loss-1.6817, acc-0.5400, valid loss-1.6672, acc-0.5340, test loss-1.6682, acc-0.5235\n",
      "Iter-95600, train loss-1.7080, acc-0.5800, valid loss-1.6672, acc-0.5342, test loss-1.6682, acc-0.5234\n",
      "Iter-95610, train loss-1.6581, acc-0.5400, valid loss-1.6672, acc-0.5342, test loss-1.6682, acc-0.5234\n",
      "Iter-95620, train loss-1.7175, acc-0.4600, valid loss-1.6671, acc-0.5342, test loss-1.6681, acc-0.5235\n",
      "Iter-95630, train loss-1.6649, acc-0.5200, valid loss-1.6671, acc-0.5342, test loss-1.6681, acc-0.5234\n",
      "Iter-95640, train loss-1.6145, acc-0.6000, valid loss-1.6671, acc-0.5342, test loss-1.6681, acc-0.5234\n",
      "Iter-95650, train loss-1.6583, acc-0.5200, valid loss-1.6670, acc-0.5340, test loss-1.6681, acc-0.5234\n",
      "Iter-95660, train loss-1.7102, acc-0.4600, valid loss-1.6670, acc-0.5340, test loss-1.6680, acc-0.5234\n",
      "Iter-95670, train loss-1.6594, acc-0.6400, valid loss-1.6670, acc-0.5340, test loss-1.6680, acc-0.5234\n",
      "Iter-95680, train loss-1.6404, acc-0.5600, valid loss-1.6669, acc-0.5340, test loss-1.6680, acc-0.5237\n",
      "Iter-95690, train loss-1.7349, acc-0.4400, valid loss-1.6669, acc-0.5340, test loss-1.6679, acc-0.5235\n",
      "Iter-95700, train loss-1.5912, acc-0.5600, valid loss-1.6669, acc-0.5340, test loss-1.6679, acc-0.5235\n",
      "Iter-95710, train loss-1.6599, acc-0.5000, valid loss-1.6668, acc-0.5338, test loss-1.6679, acc-0.5235\n",
      "Iter-95720, train loss-1.7926, acc-0.4200, valid loss-1.6668, acc-0.5340, test loss-1.6678, acc-0.5234\n",
      "Iter-95730, train loss-1.6562, acc-0.5600, valid loss-1.6668, acc-0.5342, test loss-1.6678, acc-0.5235\n",
      "Iter-95740, train loss-1.5993, acc-0.5000, valid loss-1.6667, acc-0.5342, test loss-1.6678, acc-0.5235\n",
      "Iter-95750, train loss-1.6707, acc-0.4800, valid loss-1.6667, acc-0.5342, test loss-1.6677, acc-0.5235\n",
      "Iter-95760, train loss-1.7526, acc-0.5200, valid loss-1.6667, acc-0.5342, test loss-1.6677, acc-0.5235\n",
      "Iter-95770, train loss-1.7283, acc-0.5000, valid loss-1.6666, acc-0.5342, test loss-1.6677, acc-0.5235\n",
      "Iter-95780, train loss-1.6783, acc-0.5000, valid loss-1.6666, acc-0.5342, test loss-1.6676, acc-0.5234\n",
      "Iter-95790, train loss-1.7135, acc-0.5000, valid loss-1.6666, acc-0.5342, test loss-1.6676, acc-0.5234\n",
      "Iter-95800, train loss-1.6911, acc-0.4800, valid loss-1.6666, acc-0.5342, test loss-1.6676, acc-0.5233\n",
      "Iter-95810, train loss-1.7179, acc-0.4600, valid loss-1.6665, acc-0.5342, test loss-1.6675, acc-0.5233\n",
      "Iter-95820, train loss-1.6848, acc-0.5000, valid loss-1.6665, acc-0.5342, test loss-1.6675, acc-0.5233\n",
      "Iter-95830, train loss-1.6461, acc-0.5600, valid loss-1.6665, acc-0.5342, test loss-1.6675, acc-0.5233\n",
      "Iter-95840, train loss-1.7822, acc-0.4400, valid loss-1.6664, acc-0.5346, test loss-1.6674, acc-0.5233\n",
      "Iter-95850, train loss-1.7943, acc-0.4600, valid loss-1.6664, acc-0.5346, test loss-1.6674, acc-0.5234\n",
      "Iter-95860, train loss-1.6735, acc-0.4600, valid loss-1.6664, acc-0.5348, test loss-1.6674, acc-0.5235\n",
      "Iter-95870, train loss-1.7781, acc-0.4600, valid loss-1.6664, acc-0.5348, test loss-1.6674, acc-0.5236\n",
      "Iter-95880, train loss-1.6574, acc-0.5800, valid loss-1.6663, acc-0.5350, test loss-1.6673, acc-0.5238\n",
      "Iter-95890, train loss-1.6180, acc-0.6600, valid loss-1.6663, acc-0.5346, test loss-1.6673, acc-0.5238\n",
      "Iter-95900, train loss-1.6285, acc-0.6000, valid loss-1.6663, acc-0.5346, test loss-1.6673, acc-0.5239\n",
      "Iter-95910, train loss-1.5727, acc-0.5600, valid loss-1.6662, acc-0.5348, test loss-1.6672, acc-0.5238\n",
      "Iter-95920, train loss-1.6811, acc-0.5200, valid loss-1.6662, acc-0.5348, test loss-1.6672, acc-0.5239\n",
      "Iter-95930, train loss-1.6435, acc-0.5200, valid loss-1.6662, acc-0.5346, test loss-1.6672, acc-0.5239\n",
      "Iter-95940, train loss-1.6382, acc-0.5200, valid loss-1.6661, acc-0.5348, test loss-1.6671, acc-0.5240\n",
      "Iter-95950, train loss-1.6871, acc-0.5000, valid loss-1.6661, acc-0.5348, test loss-1.6671, acc-0.5240\n",
      "Iter-95960, train loss-1.6198, acc-0.5400, valid loss-1.6661, acc-0.5348, test loss-1.6671, acc-0.5239\n",
      "Iter-95970, train loss-1.6674, acc-0.5400, valid loss-1.6661, acc-0.5348, test loss-1.6670, acc-0.5239\n",
      "Iter-95980, train loss-1.7409, acc-0.4200, valid loss-1.6660, acc-0.5346, test loss-1.6670, acc-0.5240\n",
      "Iter-95990, train loss-1.6190, acc-0.5800, valid loss-1.6660, acc-0.5348, test loss-1.6670, acc-0.5236\n",
      "Iter-96000, train loss-1.7585, acc-0.4400, valid loss-1.6660, acc-0.5348, test loss-1.6669, acc-0.5236\n",
      "Iter-96010, train loss-1.7265, acc-0.5800, valid loss-1.6659, acc-0.5348, test loss-1.6669, acc-0.5237\n",
      "Iter-96020, train loss-1.6909, acc-0.4800, valid loss-1.6659, acc-0.5350, test loss-1.6669, acc-0.5235\n",
      "Iter-96030, train loss-1.7762, acc-0.4800, valid loss-1.6659, acc-0.5346, test loss-1.6668, acc-0.5235\n",
      "Iter-96040, train loss-1.7767, acc-0.5600, valid loss-1.6658, acc-0.5348, test loss-1.6668, acc-0.5237\n",
      "Iter-96050, train loss-1.5723, acc-0.6600, valid loss-1.6658, acc-0.5350, test loss-1.6668, acc-0.5237\n",
      "Iter-96060, train loss-1.6150, acc-0.5200, valid loss-1.6658, acc-0.5348, test loss-1.6668, acc-0.5238\n",
      "Iter-96070, train loss-1.6786, acc-0.5000, valid loss-1.6657, acc-0.5348, test loss-1.6667, acc-0.5238\n",
      "Iter-96080, train loss-1.7303, acc-0.5800, valid loss-1.6657, acc-0.5350, test loss-1.6667, acc-0.5237\n",
      "Iter-96090, train loss-1.6643, acc-0.6400, valid loss-1.6657, acc-0.5350, test loss-1.6667, acc-0.5237\n",
      "Iter-96100, train loss-1.6832, acc-0.4800, valid loss-1.6656, acc-0.5348, test loss-1.6666, acc-0.5239\n",
      "Iter-96110, train loss-1.7223, acc-0.4800, valid loss-1.6656, acc-0.5348, test loss-1.6666, acc-0.5237\n",
      "Iter-96120, train loss-1.6399, acc-0.5800, valid loss-1.6656, acc-0.5348, test loss-1.6666, acc-0.5237\n",
      "Iter-96130, train loss-1.7550, acc-0.4800, valid loss-1.6655, acc-0.5350, test loss-1.6665, acc-0.5237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-96140, train loss-1.7510, acc-0.4600, valid loss-1.6655, acc-0.5350, test loss-1.6665, acc-0.5236\n",
      "Iter-96150, train loss-1.5909, acc-0.6200, valid loss-1.6655, acc-0.5348, test loss-1.6665, acc-0.5236\n",
      "Iter-96160, train loss-1.6115, acc-0.5600, valid loss-1.6655, acc-0.5346, test loss-1.6664, acc-0.5238\n",
      "Iter-96170, train loss-1.6234, acc-0.5400, valid loss-1.6654, acc-0.5346, test loss-1.6664, acc-0.5238\n",
      "Iter-96180, train loss-1.6067, acc-0.6200, valid loss-1.6654, acc-0.5350, test loss-1.6664, acc-0.5237\n",
      "Iter-96190, train loss-1.7269, acc-0.6000, valid loss-1.6654, acc-0.5350, test loss-1.6663, acc-0.5237\n",
      "Iter-96200, train loss-1.7396, acc-0.5000, valid loss-1.6653, acc-0.5348, test loss-1.6663, acc-0.5236\n",
      "Iter-96210, train loss-1.5897, acc-0.5800, valid loss-1.6653, acc-0.5348, test loss-1.6663, acc-0.5237\n",
      "Iter-96220, train loss-1.7625, acc-0.5200, valid loss-1.6653, acc-0.5350, test loss-1.6662, acc-0.5236\n",
      "Iter-96230, train loss-1.7326, acc-0.5200, valid loss-1.6652, acc-0.5350, test loss-1.6662, acc-0.5236\n",
      "Iter-96240, train loss-1.4904, acc-0.5800, valid loss-1.6652, acc-0.5356, test loss-1.6662, acc-0.5234\n",
      "Iter-96250, train loss-1.6555, acc-0.6000, valid loss-1.6652, acc-0.5352, test loss-1.6662, acc-0.5234\n",
      "Iter-96260, train loss-1.6111, acc-0.6400, valid loss-1.6651, acc-0.5356, test loss-1.6661, acc-0.5234\n",
      "Iter-96270, train loss-1.6399, acc-0.6000, valid loss-1.6651, acc-0.5354, test loss-1.6661, acc-0.5234\n",
      "Iter-96280, train loss-1.7245, acc-0.5000, valid loss-1.6651, acc-0.5354, test loss-1.6661, acc-0.5236\n",
      "Iter-96290, train loss-1.6551, acc-0.6000, valid loss-1.6651, acc-0.5352, test loss-1.6660, acc-0.5235\n",
      "Iter-96300, train loss-1.6070, acc-0.5800, valid loss-1.6650, acc-0.5350, test loss-1.6660, acc-0.5236\n",
      "Iter-96310, train loss-1.6266, acc-0.4800, valid loss-1.6650, acc-0.5350, test loss-1.6660, acc-0.5236\n",
      "Iter-96320, train loss-1.6839, acc-0.5400, valid loss-1.6650, acc-0.5348, test loss-1.6659, acc-0.5236\n",
      "Iter-96330, train loss-1.7266, acc-0.4400, valid loss-1.6649, acc-0.5348, test loss-1.6659, acc-0.5236\n",
      "Iter-96340, train loss-1.6140, acc-0.6200, valid loss-1.6649, acc-0.5348, test loss-1.6659, acc-0.5237\n",
      "Iter-96350, train loss-1.6117, acc-0.5600, valid loss-1.6649, acc-0.5350, test loss-1.6658, acc-0.5235\n",
      "Iter-96360, train loss-1.6583, acc-0.5800, valid loss-1.6648, acc-0.5348, test loss-1.6658, acc-0.5235\n",
      "Iter-96370, train loss-1.6625, acc-0.5600, valid loss-1.6648, acc-0.5350, test loss-1.6658, acc-0.5235\n",
      "Iter-96380, train loss-1.6921, acc-0.5400, valid loss-1.6648, acc-0.5350, test loss-1.6657, acc-0.5235\n",
      "Iter-96390, train loss-1.6231, acc-0.5800, valid loss-1.6647, acc-0.5350, test loss-1.6657, acc-0.5236\n",
      "Iter-96400, train loss-1.6970, acc-0.4800, valid loss-1.6647, acc-0.5350, test loss-1.6657, acc-0.5236\n",
      "Iter-96410, train loss-1.6583, acc-0.5400, valid loss-1.6647, acc-0.5348, test loss-1.6657, acc-0.5236\n",
      "Iter-96420, train loss-1.7309, acc-0.4800, valid loss-1.6646, acc-0.5350, test loss-1.6656, acc-0.5236\n",
      "Iter-96430, train loss-1.8057, acc-0.3600, valid loss-1.6646, acc-0.5348, test loss-1.6656, acc-0.5236\n",
      "Iter-96440, train loss-1.6897, acc-0.5400, valid loss-1.6646, acc-0.5350, test loss-1.6656, acc-0.5236\n",
      "Iter-96450, train loss-1.7186, acc-0.4800, valid loss-1.6645, acc-0.5350, test loss-1.6655, acc-0.5236\n",
      "Iter-96460, train loss-1.7796, acc-0.5000, valid loss-1.6645, acc-0.5348, test loss-1.6655, acc-0.5235\n",
      "Iter-96470, train loss-1.6452, acc-0.5200, valid loss-1.6645, acc-0.5348, test loss-1.6655, acc-0.5235\n",
      "Iter-96480, train loss-1.6773, acc-0.5400, valid loss-1.6644, acc-0.5348, test loss-1.6654, acc-0.5235\n",
      "Iter-96490, train loss-1.6291, acc-0.5200, valid loss-1.6644, acc-0.5348, test loss-1.6654, acc-0.5234\n",
      "Iter-96500, train loss-1.6452, acc-0.5600, valid loss-1.6644, acc-0.5348, test loss-1.6654, acc-0.5235\n",
      "Iter-96510, train loss-1.6448, acc-0.5400, valid loss-1.6643, acc-0.5350, test loss-1.6653, acc-0.5235\n",
      "Iter-96520, train loss-1.5596, acc-0.6200, valid loss-1.6643, acc-0.5350, test loss-1.6653, acc-0.5235\n",
      "Iter-96530, train loss-1.6890, acc-0.5400, valid loss-1.6643, acc-0.5350, test loss-1.6653, acc-0.5235\n",
      "Iter-96540, train loss-1.6170, acc-0.6200, valid loss-1.6642, acc-0.5350, test loss-1.6652, acc-0.5236\n",
      "Iter-96550, train loss-1.7536, acc-0.4200, valid loss-1.6642, acc-0.5350, test loss-1.6652, acc-0.5237\n",
      "Iter-96560, train loss-1.5381, acc-0.6000, valid loss-1.6642, acc-0.5350, test loss-1.6652, acc-0.5238\n",
      "Iter-96570, train loss-1.7014, acc-0.4000, valid loss-1.6641, acc-0.5352, test loss-1.6652, acc-0.5238\n",
      "Iter-96580, train loss-1.5883, acc-0.5800, valid loss-1.6641, acc-0.5352, test loss-1.6651, acc-0.5237\n",
      "Iter-96590, train loss-1.6181, acc-0.6400, valid loss-1.6641, acc-0.5350, test loss-1.6651, acc-0.5237\n",
      "Iter-96600, train loss-1.6730, acc-0.5400, valid loss-1.6641, acc-0.5354, test loss-1.6651, acc-0.5237\n",
      "Iter-96610, train loss-1.6296, acc-0.5400, valid loss-1.6640, acc-0.5354, test loss-1.6650, acc-0.5237\n",
      "Iter-96620, train loss-1.5745, acc-0.5400, valid loss-1.6640, acc-0.5352, test loss-1.6650, acc-0.5237\n",
      "Iter-96630, train loss-1.6350, acc-0.5000, valid loss-1.6640, acc-0.5352, test loss-1.6650, acc-0.5237\n",
      "Iter-96640, train loss-1.7292, acc-0.4600, valid loss-1.6639, acc-0.5352, test loss-1.6649, acc-0.5237\n",
      "Iter-96650, train loss-1.7663, acc-0.4200, valid loss-1.6639, acc-0.5354, test loss-1.6649, acc-0.5237\n",
      "Iter-96660, train loss-1.7103, acc-0.4800, valid loss-1.6639, acc-0.5354, test loss-1.6649, acc-0.5237\n",
      "Iter-96670, train loss-1.7056, acc-0.4200, valid loss-1.6638, acc-0.5352, test loss-1.6648, acc-0.5237\n",
      "Iter-96680, train loss-1.6421, acc-0.5200, valid loss-1.6638, acc-0.5352, test loss-1.6648, acc-0.5237\n",
      "Iter-96690, train loss-1.6671, acc-0.5200, valid loss-1.6638, acc-0.5352, test loss-1.6648, acc-0.5237\n",
      "Iter-96700, train loss-1.5690, acc-0.7000, valid loss-1.6637, acc-0.5352, test loss-1.6647, acc-0.5237\n",
      "Iter-96710, train loss-1.7069, acc-0.4800, valid loss-1.6637, acc-0.5352, test loss-1.6647, acc-0.5237\n",
      "Iter-96720, train loss-1.6200, acc-0.6200, valid loss-1.6637, acc-0.5352, test loss-1.6647, acc-0.5237\n",
      "Iter-96730, train loss-1.7851, acc-0.4400, valid loss-1.6636, acc-0.5354, test loss-1.6647, acc-0.5237\n",
      "Iter-96740, train loss-1.6464, acc-0.5800, valid loss-1.6636, acc-0.5354, test loss-1.6646, acc-0.5237\n",
      "Iter-96750, train loss-1.6938, acc-0.4800, valid loss-1.6636, acc-0.5354, test loss-1.6646, acc-0.5238\n",
      "Iter-96760, train loss-1.7249, acc-0.4600, valid loss-1.6635, acc-0.5352, test loss-1.6646, acc-0.5238\n",
      "Iter-96770, train loss-1.7236, acc-0.5400, valid loss-1.6635, acc-0.5352, test loss-1.6645, acc-0.5238\n",
      "Iter-96780, train loss-1.7046, acc-0.5400, valid loss-1.6635, acc-0.5354, test loss-1.6645, acc-0.5237\n",
      "Iter-96790, train loss-1.6569, acc-0.5200, valid loss-1.6635, acc-0.5354, test loss-1.6645, acc-0.5237\n",
      "Iter-96800, train loss-1.6947, acc-0.5000, valid loss-1.6634, acc-0.5354, test loss-1.6644, acc-0.5237\n",
      "Iter-96810, train loss-1.7083, acc-0.4800, valid loss-1.6634, acc-0.5352, test loss-1.6644, acc-0.5237\n",
      "Iter-96820, train loss-1.6994, acc-0.5400, valid loss-1.6634, acc-0.5354, test loss-1.6644, acc-0.5237\n",
      "Iter-96830, train loss-1.6175, acc-0.5000, valid loss-1.6633, acc-0.5352, test loss-1.6643, acc-0.5237\n",
      "Iter-96840, train loss-1.5922, acc-0.6200, valid loss-1.6633, acc-0.5354, test loss-1.6643, acc-0.5235\n",
      "Iter-96850, train loss-1.8542, acc-0.4200, valid loss-1.6633, acc-0.5354, test loss-1.6643, acc-0.5235\n",
      "Iter-96860, train loss-1.7402, acc-0.5000, valid loss-1.6632, acc-0.5354, test loss-1.6642, acc-0.5236\n",
      "Iter-96870, train loss-1.6647, acc-0.5200, valid loss-1.6632, acc-0.5354, test loss-1.6642, acc-0.5235\n",
      "Iter-96880, train loss-1.5027, acc-0.5800, valid loss-1.6632, acc-0.5354, test loss-1.6642, acc-0.5235\n",
      "Iter-96890, train loss-1.6490, acc-0.5200, valid loss-1.6631, acc-0.5354, test loss-1.6641, acc-0.5235\n",
      "Iter-96900, train loss-1.6797, acc-0.6000, valid loss-1.6631, acc-0.5354, test loss-1.6641, acc-0.5234\n",
      "Iter-96910, train loss-1.6780, acc-0.6000, valid loss-1.6631, acc-0.5352, test loss-1.6641, acc-0.5234\n",
      "Iter-96920, train loss-1.6029, acc-0.5000, valid loss-1.6631, acc-0.5352, test loss-1.6641, acc-0.5234\n",
      "Iter-96930, train loss-1.6852, acc-0.4400, valid loss-1.6630, acc-0.5352, test loss-1.6640, acc-0.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-96940, train loss-1.6672, acc-0.6400, valid loss-1.6630, acc-0.5350, test loss-1.6640, acc-0.5235\n",
      "Iter-96950, train loss-1.7050, acc-0.5000, valid loss-1.6630, acc-0.5350, test loss-1.6640, acc-0.5236\n",
      "Iter-96960, train loss-1.6921, acc-0.6600, valid loss-1.6629, acc-0.5350, test loss-1.6639, acc-0.5236\n",
      "Iter-96970, train loss-1.6930, acc-0.5400, valid loss-1.6629, acc-0.5350, test loss-1.6639, acc-0.5236\n",
      "Iter-96980, train loss-1.6752, acc-0.5600, valid loss-1.6629, acc-0.5348, test loss-1.6639, acc-0.5236\n",
      "Iter-96990, train loss-1.6535, acc-0.4600, valid loss-1.6628, acc-0.5350, test loss-1.6638, acc-0.5236\n",
      "Iter-97000, train loss-1.7335, acc-0.4800, valid loss-1.6628, acc-0.5350, test loss-1.6638, acc-0.5237\n",
      "Iter-97010, train loss-1.6973, acc-0.5000, valid loss-1.6628, acc-0.5350, test loss-1.6638, acc-0.5235\n",
      "Iter-97020, train loss-1.6260, acc-0.5800, valid loss-1.6627, acc-0.5352, test loss-1.6637, acc-0.5236\n",
      "Iter-97030, train loss-1.6075, acc-0.6400, valid loss-1.6627, acc-0.5350, test loss-1.6637, acc-0.5237\n",
      "Iter-97040, train loss-1.5822, acc-0.5600, valid loss-1.6627, acc-0.5350, test loss-1.6637, acc-0.5236\n",
      "Iter-97050, train loss-1.6762, acc-0.6000, valid loss-1.6627, acc-0.5350, test loss-1.6636, acc-0.5238\n",
      "Iter-97060, train loss-1.8267, acc-0.4200, valid loss-1.6626, acc-0.5352, test loss-1.6636, acc-0.5239\n",
      "Iter-97070, train loss-1.5983, acc-0.6000, valid loss-1.6626, acc-0.5352, test loss-1.6636, acc-0.5238\n",
      "Iter-97080, train loss-1.6014, acc-0.5200, valid loss-1.6626, acc-0.5352, test loss-1.6636, acc-0.5242\n",
      "Iter-97090, train loss-1.6681, acc-0.5400, valid loss-1.6625, acc-0.5352, test loss-1.6635, acc-0.5240\n",
      "Iter-97100, train loss-1.7764, acc-0.4400, valid loss-1.6625, acc-0.5352, test loss-1.6635, acc-0.5239\n",
      "Iter-97110, train loss-1.6753, acc-0.5200, valid loss-1.6625, acc-0.5350, test loss-1.6635, acc-0.5240\n",
      "Iter-97120, train loss-1.8006, acc-0.4400, valid loss-1.6624, acc-0.5350, test loss-1.6634, acc-0.5240\n",
      "Iter-97130, train loss-1.7020, acc-0.5200, valid loss-1.6624, acc-0.5350, test loss-1.6634, acc-0.5240\n",
      "Iter-97140, train loss-1.6432, acc-0.5200, valid loss-1.6624, acc-0.5350, test loss-1.6634, acc-0.5239\n",
      "Iter-97150, train loss-1.7316, acc-0.5400, valid loss-1.6623, acc-0.5350, test loss-1.6633, acc-0.5240\n",
      "Iter-97160, train loss-1.6926, acc-0.4800, valid loss-1.6623, acc-0.5350, test loss-1.6633, acc-0.5240\n",
      "Iter-97170, train loss-1.7116, acc-0.4800, valid loss-1.6623, acc-0.5350, test loss-1.6633, acc-0.5239\n",
      "Iter-97180, train loss-1.6782, acc-0.5200, valid loss-1.6622, acc-0.5350, test loss-1.6632, acc-0.5239\n",
      "Iter-97190, train loss-1.6497, acc-0.6200, valid loss-1.6622, acc-0.5352, test loss-1.6632, acc-0.5239\n",
      "Iter-97200, train loss-1.6845, acc-0.5400, valid loss-1.6622, acc-0.5350, test loss-1.6632, acc-0.5238\n",
      "Iter-97210, train loss-1.7854, acc-0.4800, valid loss-1.6622, acc-0.5350, test loss-1.6632, acc-0.5240\n",
      "Iter-97220, train loss-1.6083, acc-0.6200, valid loss-1.6621, acc-0.5352, test loss-1.6631, acc-0.5238\n",
      "Iter-97230, train loss-1.7397, acc-0.4400, valid loss-1.6621, acc-0.5352, test loss-1.6631, acc-0.5239\n",
      "Iter-97240, train loss-1.5603, acc-0.5800, valid loss-1.6621, acc-0.5352, test loss-1.6631, acc-0.5237\n",
      "Iter-97250, train loss-1.6215, acc-0.5400, valid loss-1.6620, acc-0.5352, test loss-1.6630, acc-0.5237\n",
      "Iter-97260, train loss-1.6519, acc-0.5400, valid loss-1.6620, acc-0.5352, test loss-1.6630, acc-0.5239\n",
      "Iter-97270, train loss-1.6660, acc-0.5200, valid loss-1.6620, acc-0.5352, test loss-1.6630, acc-0.5238\n",
      "Iter-97280, train loss-1.6940, acc-0.5800, valid loss-1.6619, acc-0.5350, test loss-1.6629, acc-0.5240\n",
      "Iter-97290, train loss-1.6669, acc-0.5000, valid loss-1.6619, acc-0.5352, test loss-1.6629, acc-0.5239\n",
      "Iter-97300, train loss-1.8212, acc-0.4000, valid loss-1.6619, acc-0.5352, test loss-1.6629, acc-0.5239\n",
      "Iter-97310, train loss-1.7071, acc-0.5200, valid loss-1.6618, acc-0.5352, test loss-1.6628, acc-0.5240\n",
      "Iter-97320, train loss-1.7483, acc-0.4800, valid loss-1.6618, acc-0.5352, test loss-1.6628, acc-0.5239\n",
      "Iter-97330, train loss-1.7937, acc-0.4800, valid loss-1.6618, acc-0.5352, test loss-1.6628, acc-0.5239\n",
      "Iter-97340, train loss-1.7429, acc-0.5000, valid loss-1.6618, acc-0.5352, test loss-1.6627, acc-0.5237\n",
      "Iter-97350, train loss-1.7529, acc-0.4600, valid loss-1.6617, acc-0.5352, test loss-1.6627, acc-0.5237\n",
      "Iter-97360, train loss-1.6470, acc-0.4800, valid loss-1.6617, acc-0.5352, test loss-1.6627, acc-0.5237\n",
      "Iter-97370, train loss-1.7285, acc-0.5000, valid loss-1.6617, acc-0.5352, test loss-1.6626, acc-0.5237\n",
      "Iter-97380, train loss-1.7540, acc-0.4200, valid loss-1.6616, acc-0.5352, test loss-1.6626, acc-0.5239\n",
      "Iter-97390, train loss-1.5794, acc-0.5800, valid loss-1.6616, acc-0.5352, test loss-1.6626, acc-0.5238\n",
      "Iter-97400, train loss-1.7153, acc-0.5000, valid loss-1.6616, acc-0.5350, test loss-1.6626, acc-0.5241\n",
      "Iter-97410, train loss-1.5541, acc-0.5600, valid loss-1.6615, acc-0.5350, test loss-1.6625, acc-0.5241\n",
      "Iter-97420, train loss-1.6565, acc-0.5200, valid loss-1.6615, acc-0.5350, test loss-1.6625, acc-0.5240\n",
      "Iter-97430, train loss-1.5338, acc-0.6600, valid loss-1.6615, acc-0.5352, test loss-1.6625, acc-0.5239\n",
      "Iter-97440, train loss-1.6476, acc-0.5200, valid loss-1.6614, acc-0.5352, test loss-1.6624, acc-0.5240\n",
      "Iter-97450, train loss-1.7067, acc-0.5600, valid loss-1.6614, acc-0.5350, test loss-1.6624, acc-0.5240\n",
      "Iter-97460, train loss-1.5667, acc-0.6200, valid loss-1.6614, acc-0.5352, test loss-1.6624, acc-0.5241\n",
      "Iter-97470, train loss-1.7161, acc-0.5600, valid loss-1.6613, acc-0.5352, test loss-1.6623, acc-0.5242\n",
      "Iter-97480, train loss-1.6386, acc-0.5400, valid loss-1.6613, acc-0.5352, test loss-1.6623, acc-0.5242\n",
      "Iter-97490, train loss-1.7227, acc-0.4800, valid loss-1.6613, acc-0.5352, test loss-1.6623, acc-0.5241\n",
      "Iter-97500, train loss-1.8623, acc-0.3600, valid loss-1.6612, acc-0.5352, test loss-1.6622, acc-0.5242\n",
      "Iter-97510, train loss-1.6055, acc-0.5600, valid loss-1.6612, acc-0.5352, test loss-1.6622, acc-0.5243\n",
      "Iter-97520, train loss-1.6690, acc-0.5600, valid loss-1.6612, acc-0.5352, test loss-1.6622, acc-0.5243\n",
      "Iter-97530, train loss-1.6785, acc-0.4600, valid loss-1.6612, acc-0.5352, test loss-1.6622, acc-0.5243\n",
      "Iter-97540, train loss-1.5960, acc-0.6000, valid loss-1.6611, acc-0.5350, test loss-1.6621, acc-0.5243\n",
      "Iter-97550, train loss-1.6816, acc-0.5800, valid loss-1.6611, acc-0.5350, test loss-1.6621, acc-0.5243\n",
      "Iter-97560, train loss-1.7347, acc-0.4400, valid loss-1.6611, acc-0.5352, test loss-1.6621, acc-0.5243\n",
      "Iter-97570, train loss-1.5617, acc-0.6200, valid loss-1.6610, acc-0.5352, test loss-1.6620, acc-0.5244\n",
      "Iter-97580, train loss-1.6471, acc-0.5400, valid loss-1.6610, acc-0.5354, test loss-1.6620, acc-0.5243\n",
      "Iter-97590, train loss-1.6444, acc-0.5200, valid loss-1.6610, acc-0.5354, test loss-1.6620, acc-0.5244\n",
      "Iter-97600, train loss-1.6103, acc-0.6000, valid loss-1.6609, acc-0.5352, test loss-1.6619, acc-0.5243\n",
      "Iter-97610, train loss-1.6830, acc-0.5200, valid loss-1.6609, acc-0.5352, test loss-1.6619, acc-0.5243\n",
      "Iter-97620, train loss-1.6949, acc-0.5000, valid loss-1.6609, acc-0.5352, test loss-1.6619, acc-0.5244\n",
      "Iter-97630, train loss-1.7612, acc-0.4000, valid loss-1.6609, acc-0.5354, test loss-1.6618, acc-0.5243\n",
      "Iter-97640, train loss-1.7398, acc-0.4400, valid loss-1.6608, acc-0.5354, test loss-1.6618, acc-0.5244\n",
      "Iter-97650, train loss-1.7857, acc-0.3600, valid loss-1.6608, acc-0.5352, test loss-1.6618, acc-0.5244\n",
      "Iter-97660, train loss-1.6588, acc-0.5200, valid loss-1.6608, acc-0.5352, test loss-1.6617, acc-0.5244\n",
      "Iter-97670, train loss-1.7008, acc-0.4800, valid loss-1.6607, acc-0.5352, test loss-1.6617, acc-0.5243\n",
      "Iter-97680, train loss-1.6339, acc-0.5600, valid loss-1.6607, acc-0.5352, test loss-1.6617, acc-0.5243\n",
      "Iter-97690, train loss-1.7944, acc-0.4600, valid loss-1.6607, acc-0.5350, test loss-1.6617, acc-0.5244\n",
      "Iter-97700, train loss-1.6070, acc-0.6000, valid loss-1.6606, acc-0.5350, test loss-1.6616, acc-0.5244\n",
      "Iter-97710, train loss-1.6780, acc-0.5800, valid loss-1.6606, acc-0.5350, test loss-1.6616, acc-0.5243\n",
      "Iter-97720, train loss-1.6456, acc-0.5800, valid loss-1.6606, acc-0.5352, test loss-1.6616, acc-0.5243\n",
      "Iter-97730, train loss-1.6643, acc-0.5000, valid loss-1.6605, acc-0.5352, test loss-1.6615, acc-0.5244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-97740, train loss-1.6666, acc-0.5200, valid loss-1.6605, acc-0.5354, test loss-1.6615, acc-0.5243\n",
      "Iter-97750, train loss-1.7444, acc-0.4000, valid loss-1.6605, acc-0.5354, test loss-1.6615, acc-0.5245\n",
      "Iter-97760, train loss-1.5991, acc-0.6200, valid loss-1.6604, acc-0.5354, test loss-1.6614, acc-0.5246\n",
      "Iter-97770, train loss-1.6313, acc-0.5000, valid loss-1.6604, acc-0.5354, test loss-1.6614, acc-0.5245\n",
      "Iter-97780, train loss-1.6438, acc-0.6000, valid loss-1.6604, acc-0.5352, test loss-1.6614, acc-0.5244\n",
      "Iter-97790, train loss-1.6225, acc-0.5400, valid loss-1.6604, acc-0.5354, test loss-1.6613, acc-0.5244\n",
      "Iter-97800, train loss-1.7372, acc-0.4600, valid loss-1.6603, acc-0.5354, test loss-1.6613, acc-0.5244\n",
      "Iter-97810, train loss-1.7952, acc-0.4000, valid loss-1.6603, acc-0.5354, test loss-1.6613, acc-0.5244\n",
      "Iter-97820, train loss-1.5708, acc-0.6200, valid loss-1.6603, acc-0.5352, test loss-1.6612, acc-0.5243\n",
      "Iter-97830, train loss-1.6586, acc-0.5800, valid loss-1.6602, acc-0.5354, test loss-1.6612, acc-0.5244\n",
      "Iter-97840, train loss-1.5706, acc-0.5400, valid loss-1.6602, acc-0.5354, test loss-1.6612, acc-0.5244\n",
      "Iter-97850, train loss-1.7842, acc-0.4400, valid loss-1.6602, acc-0.5356, test loss-1.6612, acc-0.5244\n",
      "Iter-97860, train loss-1.6503, acc-0.5800, valid loss-1.6601, acc-0.5354, test loss-1.6611, acc-0.5245\n",
      "Iter-97870, train loss-1.5684, acc-0.5800, valid loss-1.6601, acc-0.5358, test loss-1.6611, acc-0.5243\n",
      "Iter-97880, train loss-1.6696, acc-0.5600, valid loss-1.6601, acc-0.5360, test loss-1.6611, acc-0.5242\n",
      "Iter-97890, train loss-1.7395, acc-0.4800, valid loss-1.6600, acc-0.5356, test loss-1.6610, acc-0.5242\n",
      "Iter-97900, train loss-1.6552, acc-0.5600, valid loss-1.6600, acc-0.5356, test loss-1.6610, acc-0.5243\n",
      "Iter-97910, train loss-1.7377, acc-0.4400, valid loss-1.6600, acc-0.5358, test loss-1.6610, acc-0.5242\n",
      "Iter-97920, train loss-1.5956, acc-0.5600, valid loss-1.6599, acc-0.5354, test loss-1.6609, acc-0.5242\n",
      "Iter-97930, train loss-1.7001, acc-0.5000, valid loss-1.6599, acc-0.5354, test loss-1.6609, acc-0.5243\n",
      "Iter-97940, train loss-1.6400, acc-0.5600, valid loss-1.6599, acc-0.5356, test loss-1.6609, acc-0.5242\n",
      "Iter-97950, train loss-1.7394, acc-0.5000, valid loss-1.6599, acc-0.5356, test loss-1.6608, acc-0.5242\n",
      "Iter-97960, train loss-1.7626, acc-0.5000, valid loss-1.6598, acc-0.5354, test loss-1.6608, acc-0.5244\n",
      "Iter-97970, train loss-1.6581, acc-0.4400, valid loss-1.6598, acc-0.5352, test loss-1.6608, acc-0.5244\n",
      "Iter-97980, train loss-1.7495, acc-0.4600, valid loss-1.6598, acc-0.5354, test loss-1.6608, acc-0.5245\n",
      "Iter-97990, train loss-1.6641, acc-0.5600, valid loss-1.6597, acc-0.5354, test loss-1.6607, acc-0.5245\n",
      "Iter-98000, train loss-1.7700, acc-0.4400, valid loss-1.6597, acc-0.5356, test loss-1.6607, acc-0.5245\n",
      "Iter-98010, train loss-1.6230, acc-0.5600, valid loss-1.6597, acc-0.5358, test loss-1.6607, acc-0.5244\n",
      "Iter-98020, train loss-1.7252, acc-0.5200, valid loss-1.6596, acc-0.5358, test loss-1.6606, acc-0.5244\n",
      "Iter-98030, train loss-1.6938, acc-0.4400, valid loss-1.6596, acc-0.5360, test loss-1.6606, acc-0.5244\n",
      "Iter-98040, train loss-1.5949, acc-0.6000, valid loss-1.6596, acc-0.5360, test loss-1.6606, acc-0.5246\n",
      "Iter-98050, train loss-1.7075, acc-0.4600, valid loss-1.6596, acc-0.5360, test loss-1.6605, acc-0.5246\n",
      "Iter-98060, train loss-1.7764, acc-0.4600, valid loss-1.6595, acc-0.5360, test loss-1.6605, acc-0.5245\n",
      "Iter-98070, train loss-1.6622, acc-0.5200, valid loss-1.6595, acc-0.5358, test loss-1.6605, acc-0.5247\n",
      "Iter-98080, train loss-1.5701, acc-0.5600, valid loss-1.6595, acc-0.5358, test loss-1.6604, acc-0.5246\n",
      "Iter-98090, train loss-1.6572, acc-0.4200, valid loss-1.6594, acc-0.5360, test loss-1.6604, acc-0.5246\n",
      "Iter-98100, train loss-1.5749, acc-0.5400, valid loss-1.6594, acc-0.5360, test loss-1.6604, acc-0.5247\n",
      "Iter-98110, train loss-1.5973, acc-0.6200, valid loss-1.6594, acc-0.5360, test loss-1.6604, acc-0.5247\n",
      "Iter-98120, train loss-1.6824, acc-0.5200, valid loss-1.6593, acc-0.5362, test loss-1.6603, acc-0.5247\n",
      "Iter-98130, train loss-1.7174, acc-0.5000, valid loss-1.6593, acc-0.5362, test loss-1.6603, acc-0.5248\n",
      "Iter-98140, train loss-1.8342, acc-0.4000, valid loss-1.6593, acc-0.5364, test loss-1.6603, acc-0.5249\n",
      "Iter-98150, train loss-1.6510, acc-0.5400, valid loss-1.6593, acc-0.5364, test loss-1.6602, acc-0.5250\n",
      "Iter-98160, train loss-1.7830, acc-0.4800, valid loss-1.6592, acc-0.5362, test loss-1.6602, acc-0.5249\n",
      "Iter-98170, train loss-1.6650, acc-0.5400, valid loss-1.6592, acc-0.5364, test loss-1.6602, acc-0.5249\n",
      "Iter-98180, train loss-1.7581, acc-0.4800, valid loss-1.6592, acc-0.5364, test loss-1.6601, acc-0.5250\n",
      "Iter-98190, train loss-1.6819, acc-0.5200, valid loss-1.6591, acc-0.5364, test loss-1.6601, acc-0.5250\n",
      "Iter-98200, train loss-1.5940, acc-0.6000, valid loss-1.6591, acc-0.5362, test loss-1.6601, acc-0.5250\n",
      "Iter-98210, train loss-1.6554, acc-0.5400, valid loss-1.6591, acc-0.5362, test loss-1.6600, acc-0.5249\n",
      "Iter-98220, train loss-1.6278, acc-0.5200, valid loss-1.6590, acc-0.5360, test loss-1.6600, acc-0.5249\n",
      "Iter-98230, train loss-1.6905, acc-0.5400, valid loss-1.6590, acc-0.5360, test loss-1.6600, acc-0.5248\n",
      "Iter-98240, train loss-1.7791, acc-0.3600, valid loss-1.6590, acc-0.5360, test loss-1.6600, acc-0.5247\n",
      "Iter-98250, train loss-1.7511, acc-0.5000, valid loss-1.6589, acc-0.5358, test loss-1.6599, acc-0.5246\n",
      "Iter-98260, train loss-1.6485, acc-0.5400, valid loss-1.6589, acc-0.5360, test loss-1.6599, acc-0.5246\n",
      "Iter-98270, train loss-1.7710, acc-0.5600, valid loss-1.6589, acc-0.5360, test loss-1.6599, acc-0.5248\n",
      "Iter-98280, train loss-1.7685, acc-0.4200, valid loss-1.6589, acc-0.5360, test loss-1.6598, acc-0.5249\n",
      "Iter-98290, train loss-1.8036, acc-0.5000, valid loss-1.6588, acc-0.5362, test loss-1.6598, acc-0.5249\n",
      "Iter-98300, train loss-1.6063, acc-0.5600, valid loss-1.6588, acc-0.5362, test loss-1.6598, acc-0.5249\n",
      "Iter-98310, train loss-1.8558, acc-0.2800, valid loss-1.6588, acc-0.5362, test loss-1.6597, acc-0.5249\n",
      "Iter-98320, train loss-1.8188, acc-0.4400, valid loss-1.6587, acc-0.5362, test loss-1.6597, acc-0.5251\n",
      "Iter-98330, train loss-1.7233, acc-0.4400, valid loss-1.6587, acc-0.5360, test loss-1.6597, acc-0.5250\n",
      "Iter-98340, train loss-1.7031, acc-0.5200, valid loss-1.6587, acc-0.5360, test loss-1.6596, acc-0.5250\n",
      "Iter-98350, train loss-1.7361, acc-0.5600, valid loss-1.6586, acc-0.5362, test loss-1.6596, acc-0.5251\n",
      "Iter-98360, train loss-1.7795, acc-0.4600, valid loss-1.6586, acc-0.5362, test loss-1.6596, acc-0.5251\n",
      "Iter-98370, train loss-1.5698, acc-0.6200, valid loss-1.6586, acc-0.5362, test loss-1.6596, acc-0.5250\n",
      "Iter-98380, train loss-1.7168, acc-0.4800, valid loss-1.6585, acc-0.5360, test loss-1.6595, acc-0.5253\n",
      "Iter-98390, train loss-1.6910, acc-0.4400, valid loss-1.6585, acc-0.5360, test loss-1.6595, acc-0.5252\n",
      "Iter-98400, train loss-1.6356, acc-0.5600, valid loss-1.6585, acc-0.5360, test loss-1.6595, acc-0.5250\n",
      "Iter-98410, train loss-1.8282, acc-0.3600, valid loss-1.6585, acc-0.5360, test loss-1.6594, acc-0.5253\n",
      "Iter-98420, train loss-1.5789, acc-0.5600, valid loss-1.6584, acc-0.5360, test loss-1.6594, acc-0.5251\n",
      "Iter-98430, train loss-1.6988, acc-0.4800, valid loss-1.6584, acc-0.5358, test loss-1.6594, acc-0.5252\n",
      "Iter-98440, train loss-1.6014, acc-0.4600, valid loss-1.6584, acc-0.5358, test loss-1.6593, acc-0.5249\n",
      "Iter-98450, train loss-1.5951, acc-0.5600, valid loss-1.6583, acc-0.5362, test loss-1.6593, acc-0.5249\n",
      "Iter-98460, train loss-1.6592, acc-0.5400, valid loss-1.6583, acc-0.5360, test loss-1.6593, acc-0.5249\n",
      "Iter-98470, train loss-1.6139, acc-0.6600, valid loss-1.6583, acc-0.5360, test loss-1.6592, acc-0.5249\n",
      "Iter-98480, train loss-1.7036, acc-0.4800, valid loss-1.6582, acc-0.5360, test loss-1.6592, acc-0.5249\n",
      "Iter-98490, train loss-1.6948, acc-0.5600, valid loss-1.6582, acc-0.5360, test loss-1.6592, acc-0.5249\n",
      "Iter-98500, train loss-1.6351, acc-0.5800, valid loss-1.6582, acc-0.5360, test loss-1.6592, acc-0.5250\n",
      "Iter-98510, train loss-1.6796, acc-0.5600, valid loss-1.6582, acc-0.5358, test loss-1.6591, acc-0.5249\n",
      "Iter-98520, train loss-1.7143, acc-0.4200, valid loss-1.6581, acc-0.5358, test loss-1.6591, acc-0.5250\n",
      "Iter-98530, train loss-1.5804, acc-0.5400, valid loss-1.6581, acc-0.5358, test loss-1.6591, acc-0.5249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-98540, train loss-1.7941, acc-0.5000, valid loss-1.6581, acc-0.5358, test loss-1.6590, acc-0.5249\n",
      "Iter-98550, train loss-1.6244, acc-0.4600, valid loss-1.6580, acc-0.5362, test loss-1.6590, acc-0.5249\n",
      "Iter-98560, train loss-1.7568, acc-0.5000, valid loss-1.6580, acc-0.5360, test loss-1.6590, acc-0.5249\n",
      "Iter-98570, train loss-1.6327, acc-0.5800, valid loss-1.6580, acc-0.5358, test loss-1.6589, acc-0.5250\n",
      "Iter-98580, train loss-1.6058, acc-0.5800, valid loss-1.6579, acc-0.5356, test loss-1.6589, acc-0.5250\n",
      "Iter-98590, train loss-1.5269, acc-0.7400, valid loss-1.6579, acc-0.5354, test loss-1.6589, acc-0.5248\n",
      "Iter-98600, train loss-1.7075, acc-0.5200, valid loss-1.6579, acc-0.5360, test loss-1.6589, acc-0.5249\n",
      "Iter-98610, train loss-1.5534, acc-0.6200, valid loss-1.6579, acc-0.5360, test loss-1.6588, acc-0.5249\n",
      "Iter-98620, train loss-1.7385, acc-0.4400, valid loss-1.6578, acc-0.5358, test loss-1.6588, acc-0.5250\n",
      "Iter-98630, train loss-1.9262, acc-0.2800, valid loss-1.6578, acc-0.5356, test loss-1.6588, acc-0.5250\n",
      "Iter-98640, train loss-1.7741, acc-0.4200, valid loss-1.6578, acc-0.5358, test loss-1.6587, acc-0.5250\n",
      "Iter-98650, train loss-1.7853, acc-0.6000, valid loss-1.6577, acc-0.5356, test loss-1.6587, acc-0.5250\n",
      "Iter-98660, train loss-1.7031, acc-0.4800, valid loss-1.6577, acc-0.5358, test loss-1.6587, acc-0.5250\n",
      "Iter-98670, train loss-1.5957, acc-0.6200, valid loss-1.6577, acc-0.5358, test loss-1.6586, acc-0.5250\n",
      "Iter-98680, train loss-1.8001, acc-0.4800, valid loss-1.6576, acc-0.5358, test loss-1.6586, acc-0.5249\n",
      "Iter-98690, train loss-1.7252, acc-0.4400, valid loss-1.6576, acc-0.5358, test loss-1.6586, acc-0.5251\n",
      "Iter-98700, train loss-1.6864, acc-0.4400, valid loss-1.6576, acc-0.5358, test loss-1.6586, acc-0.5252\n",
      "Iter-98710, train loss-1.6164, acc-0.5800, valid loss-1.6576, acc-0.5356, test loss-1.6585, acc-0.5254\n",
      "Iter-98720, train loss-1.6433, acc-0.4800, valid loss-1.6575, acc-0.5356, test loss-1.6585, acc-0.5254\n",
      "Iter-98730, train loss-1.6380, acc-0.5600, valid loss-1.6575, acc-0.5358, test loss-1.6585, acc-0.5252\n",
      "Iter-98740, train loss-1.7307, acc-0.5000, valid loss-1.6575, acc-0.5358, test loss-1.6584, acc-0.5252\n",
      "Iter-98750, train loss-1.7290, acc-0.5200, valid loss-1.6574, acc-0.5358, test loss-1.6584, acc-0.5254\n",
      "Iter-98760, train loss-1.7619, acc-0.4200, valid loss-1.6574, acc-0.5358, test loss-1.6584, acc-0.5254\n",
      "Iter-98770, train loss-1.6047, acc-0.5600, valid loss-1.6574, acc-0.5358, test loss-1.6583, acc-0.5254\n",
      "Iter-98780, train loss-1.5558, acc-0.5800, valid loss-1.6573, acc-0.5360, test loss-1.6583, acc-0.5251\n",
      "Iter-98790, train loss-1.6383, acc-0.6200, valid loss-1.6573, acc-0.5358, test loss-1.6583, acc-0.5253\n",
      "Iter-98800, train loss-1.6293, acc-0.4800, valid loss-1.6573, acc-0.5358, test loss-1.6582, acc-0.5254\n",
      "Iter-98810, train loss-1.6033, acc-0.4600, valid loss-1.6572, acc-0.5358, test loss-1.6582, acc-0.5255\n",
      "Iter-98820, train loss-1.6297, acc-0.5400, valid loss-1.6572, acc-0.5358, test loss-1.6582, acc-0.5254\n",
      "Iter-98830, train loss-1.6372, acc-0.5200, valid loss-1.6572, acc-0.5358, test loss-1.6582, acc-0.5252\n",
      "Iter-98840, train loss-1.7126, acc-0.5400, valid loss-1.6571, acc-0.5358, test loss-1.6581, acc-0.5251\n",
      "Iter-98850, train loss-1.7679, acc-0.5400, valid loss-1.6571, acc-0.5358, test loss-1.6581, acc-0.5251\n",
      "Iter-98860, train loss-1.6927, acc-0.5800, valid loss-1.6571, acc-0.5358, test loss-1.6581, acc-0.5251\n",
      "Iter-98870, train loss-1.8176, acc-0.4200, valid loss-1.6571, acc-0.5358, test loss-1.6580, acc-0.5251\n",
      "Iter-98880, train loss-1.6699, acc-0.4800, valid loss-1.6570, acc-0.5358, test loss-1.6580, acc-0.5253\n",
      "Iter-98890, train loss-1.6909, acc-0.5200, valid loss-1.6570, acc-0.5360, test loss-1.6580, acc-0.5254\n",
      "Iter-98900, train loss-1.6190, acc-0.6400, valid loss-1.6570, acc-0.5358, test loss-1.6579, acc-0.5253\n",
      "Iter-98910, train loss-1.5874, acc-0.6200, valid loss-1.6569, acc-0.5358, test loss-1.6579, acc-0.5254\n",
      "Iter-98920, train loss-1.7046, acc-0.4600, valid loss-1.6569, acc-0.5358, test loss-1.6579, acc-0.5254\n",
      "Iter-98930, train loss-1.6798, acc-0.5800, valid loss-1.6569, acc-0.5358, test loss-1.6579, acc-0.5254\n",
      "Iter-98940, train loss-1.6638, acc-0.5800, valid loss-1.6568, acc-0.5358, test loss-1.6578, acc-0.5254\n",
      "Iter-98950, train loss-1.6331, acc-0.5400, valid loss-1.6568, acc-0.5358, test loss-1.6578, acc-0.5254\n",
      "Iter-98960, train loss-1.5865, acc-0.5400, valid loss-1.6568, acc-0.5356, test loss-1.6578, acc-0.5255\n",
      "Iter-98970, train loss-1.6145, acc-0.5800, valid loss-1.6567, acc-0.5356, test loss-1.6577, acc-0.5256\n",
      "Iter-98980, train loss-1.7339, acc-0.3800, valid loss-1.6567, acc-0.5356, test loss-1.6577, acc-0.5256\n",
      "Iter-98990, train loss-1.8171, acc-0.4400, valid loss-1.6567, acc-0.5356, test loss-1.6577, acc-0.5257\n",
      "Iter-99000, train loss-1.6642, acc-0.5000, valid loss-1.6567, acc-0.5356, test loss-1.6576, acc-0.5258\n",
      "Iter-99010, train loss-1.6869, acc-0.4200, valid loss-1.6566, acc-0.5356, test loss-1.6576, acc-0.5257\n",
      "Iter-99020, train loss-1.5987, acc-0.5600, valid loss-1.6566, acc-0.5358, test loss-1.6576, acc-0.5257\n",
      "Iter-99030, train loss-1.7056, acc-0.5600, valid loss-1.6566, acc-0.5358, test loss-1.6575, acc-0.5259\n",
      "Iter-99040, train loss-1.5608, acc-0.6200, valid loss-1.6565, acc-0.5354, test loss-1.6575, acc-0.5258\n",
      "Iter-99050, train loss-1.7135, acc-0.4200, valid loss-1.6565, acc-0.5354, test loss-1.6575, acc-0.5258\n",
      "Iter-99060, train loss-1.6806, acc-0.5000, valid loss-1.6565, acc-0.5358, test loss-1.6575, acc-0.5258\n",
      "Iter-99070, train loss-1.8237, acc-0.4200, valid loss-1.6564, acc-0.5358, test loss-1.6574, acc-0.5258\n",
      "Iter-99080, train loss-1.7300, acc-0.5000, valid loss-1.6564, acc-0.5356, test loss-1.6574, acc-0.5257\n",
      "Iter-99090, train loss-1.7781, acc-0.4600, valid loss-1.6564, acc-0.5356, test loss-1.6574, acc-0.5257\n",
      "Iter-99100, train loss-1.6156, acc-0.5400, valid loss-1.6564, acc-0.5358, test loss-1.6573, acc-0.5258\n",
      "Iter-99110, train loss-1.6535, acc-0.5600, valid loss-1.6563, acc-0.5356, test loss-1.6573, acc-0.5260\n",
      "Iter-99120, train loss-1.8170, acc-0.3600, valid loss-1.6563, acc-0.5356, test loss-1.6573, acc-0.5261\n",
      "Iter-99130, train loss-1.5261, acc-0.7800, valid loss-1.6563, acc-0.5356, test loss-1.6573, acc-0.5260\n",
      "Iter-99140, train loss-1.5746, acc-0.5400, valid loss-1.6562, acc-0.5356, test loss-1.6572, acc-0.5260\n",
      "Iter-99150, train loss-1.7252, acc-0.4800, valid loss-1.6562, acc-0.5356, test loss-1.6572, acc-0.5260\n",
      "Iter-99160, train loss-1.7166, acc-0.4800, valid loss-1.6562, acc-0.5354, test loss-1.6572, acc-0.5262\n",
      "Iter-99170, train loss-1.6651, acc-0.5400, valid loss-1.6561, acc-0.5356, test loss-1.6571, acc-0.5261\n",
      "Iter-99180, train loss-1.7245, acc-0.4600, valid loss-1.6561, acc-0.5354, test loss-1.6571, acc-0.5259\n",
      "Iter-99190, train loss-1.6338, acc-0.6000, valid loss-1.6561, acc-0.5356, test loss-1.6571, acc-0.5259\n",
      "Iter-99200, train loss-1.7196, acc-0.5000, valid loss-1.6560, acc-0.5354, test loss-1.6570, acc-0.5261\n",
      "Iter-99210, train loss-1.7010, acc-0.5200, valid loss-1.6560, acc-0.5356, test loss-1.6570, acc-0.5261\n",
      "Iter-99220, train loss-1.6608, acc-0.5200, valid loss-1.6560, acc-0.5356, test loss-1.6570, acc-0.5261\n",
      "Iter-99230, train loss-1.5847, acc-0.5600, valid loss-1.6560, acc-0.5356, test loss-1.6569, acc-0.5261\n",
      "Iter-99240, train loss-1.5936, acc-0.5800, valid loss-1.6559, acc-0.5356, test loss-1.6569, acc-0.5262\n",
      "Iter-99250, train loss-1.5916, acc-0.5600, valid loss-1.6559, acc-0.5356, test loss-1.6569, acc-0.5263\n",
      "Iter-99260, train loss-1.6160, acc-0.5200, valid loss-1.6559, acc-0.5358, test loss-1.6569, acc-0.5261\n",
      "Iter-99270, train loss-1.7047, acc-0.5200, valid loss-1.6558, acc-0.5358, test loss-1.6568, acc-0.5261\n",
      "Iter-99280, train loss-1.7221, acc-0.5000, valid loss-1.6558, acc-0.5360, test loss-1.6568, acc-0.5262\n",
      "Iter-99290, train loss-1.6253, acc-0.5800, valid loss-1.6558, acc-0.5358, test loss-1.6568, acc-0.5262\n",
      "Iter-99300, train loss-1.6760, acc-0.5800, valid loss-1.6557, acc-0.5356, test loss-1.6567, acc-0.5263\n",
      "Iter-99310, train loss-1.7428, acc-0.4200, valid loss-1.6557, acc-0.5358, test loss-1.6567, acc-0.5262\n",
      "Iter-99320, train loss-1.6097, acc-0.6000, valid loss-1.6557, acc-0.5356, test loss-1.6567, acc-0.5264\n",
      "Iter-99330, train loss-1.6845, acc-0.6000, valid loss-1.6556, acc-0.5354, test loss-1.6566, acc-0.5263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-99340, train loss-1.6456, acc-0.5000, valid loss-1.6556, acc-0.5354, test loss-1.6566, acc-0.5262\n",
      "Iter-99350, train loss-1.6510, acc-0.5200, valid loss-1.6556, acc-0.5354, test loss-1.6566, acc-0.5262\n",
      "Iter-99360, train loss-1.6550, acc-0.5600, valid loss-1.6555, acc-0.5356, test loss-1.6566, acc-0.5266\n",
      "Iter-99370, train loss-1.6770, acc-0.5000, valid loss-1.6555, acc-0.5352, test loss-1.6565, acc-0.5264\n",
      "Iter-99380, train loss-1.6005, acc-0.6400, valid loss-1.6555, acc-0.5354, test loss-1.6565, acc-0.5265\n",
      "Iter-99390, train loss-1.5987, acc-0.5200, valid loss-1.6555, acc-0.5356, test loss-1.6565, acc-0.5264\n",
      "Iter-99400, train loss-1.6357, acc-0.5600, valid loss-1.6554, acc-0.5354, test loss-1.6564, acc-0.5263\n",
      "Iter-99410, train loss-1.6914, acc-0.5200, valid loss-1.6554, acc-0.5354, test loss-1.6564, acc-0.5262\n",
      "Iter-99420, train loss-1.6545, acc-0.5200, valid loss-1.6554, acc-0.5354, test loss-1.6564, acc-0.5262\n",
      "Iter-99430, train loss-1.6553, acc-0.5200, valid loss-1.6553, acc-0.5354, test loss-1.6563, acc-0.5261\n",
      "Iter-99440, train loss-1.8029, acc-0.4000, valid loss-1.6553, acc-0.5354, test loss-1.6563, acc-0.5262\n",
      "Iter-99450, train loss-1.5863, acc-0.7200, valid loss-1.6553, acc-0.5354, test loss-1.6563, acc-0.5262\n",
      "Iter-99460, train loss-1.6838, acc-0.5400, valid loss-1.6552, acc-0.5354, test loss-1.6563, acc-0.5261\n",
      "Iter-99470, train loss-1.7066, acc-0.5400, valid loss-1.6552, acc-0.5354, test loss-1.6562, acc-0.5264\n",
      "Iter-99480, train loss-1.6925, acc-0.5600, valid loss-1.6552, acc-0.5354, test loss-1.6562, acc-0.5262\n",
      "Iter-99490, train loss-1.6731, acc-0.5400, valid loss-1.6552, acc-0.5354, test loss-1.6562, acc-0.5262\n",
      "Iter-99500, train loss-1.6291, acc-0.6800, valid loss-1.6551, acc-0.5354, test loss-1.6561, acc-0.5262\n",
      "Iter-99510, train loss-1.5468, acc-0.6000, valid loss-1.6551, acc-0.5354, test loss-1.6561, acc-0.5262\n",
      "Iter-99520, train loss-1.7279, acc-0.5200, valid loss-1.6551, acc-0.5354, test loss-1.6561, acc-0.5261\n",
      "Iter-99530, train loss-1.6266, acc-0.5600, valid loss-1.6550, acc-0.5352, test loss-1.6561, acc-0.5261\n",
      "Iter-99540, train loss-1.6471, acc-0.5600, valid loss-1.6550, acc-0.5348, test loss-1.6560, acc-0.5261\n",
      "Iter-99550, train loss-1.6710, acc-0.5000, valid loss-1.6550, acc-0.5348, test loss-1.6560, acc-0.5261\n",
      "Iter-99560, train loss-1.6901, acc-0.5200, valid loss-1.6550, acc-0.5348, test loss-1.6560, acc-0.5261\n",
      "Iter-99570, train loss-1.7220, acc-0.4400, valid loss-1.6549, acc-0.5348, test loss-1.6559, acc-0.5261\n",
      "Iter-99580, train loss-1.6968, acc-0.5400, valid loss-1.6549, acc-0.5346, test loss-1.6559, acc-0.5263\n",
      "Iter-99590, train loss-1.6991, acc-0.4800, valid loss-1.6549, acc-0.5346, test loss-1.6559, acc-0.5262\n",
      "Iter-99600, train loss-1.8098, acc-0.4800, valid loss-1.6548, acc-0.5346, test loss-1.6559, acc-0.5262\n",
      "Iter-99610, train loss-1.6541, acc-0.5400, valid loss-1.6548, acc-0.5346, test loss-1.6558, acc-0.5262\n",
      "Iter-99620, train loss-1.7085, acc-0.4400, valid loss-1.6548, acc-0.5346, test loss-1.6558, acc-0.5261\n",
      "Iter-99630, train loss-1.5575, acc-0.6000, valid loss-1.6547, acc-0.5346, test loss-1.6558, acc-0.5261\n",
      "Iter-99640, train loss-1.5992, acc-0.5200, valid loss-1.6547, acc-0.5346, test loss-1.6557, acc-0.5261\n",
      "Iter-99650, train loss-1.5991, acc-0.5200, valid loss-1.6547, acc-0.5346, test loss-1.6557, acc-0.5262\n",
      "Iter-99660, train loss-1.6469, acc-0.5200, valid loss-1.6546, acc-0.5346, test loss-1.6557, acc-0.5261\n",
      "Iter-99670, train loss-1.7094, acc-0.5600, valid loss-1.6546, acc-0.5344, test loss-1.6556, acc-0.5261\n",
      "Iter-99680, train loss-1.6624, acc-0.4800, valid loss-1.6546, acc-0.5346, test loss-1.6556, acc-0.5262\n",
      "Iter-99690, train loss-1.8082, acc-0.4200, valid loss-1.6546, acc-0.5346, test loss-1.6556, acc-0.5263\n",
      "Iter-99700, train loss-1.7924, acc-0.4200, valid loss-1.6545, acc-0.5346, test loss-1.6556, acc-0.5262\n",
      "Iter-99710, train loss-1.6060, acc-0.5200, valid loss-1.6545, acc-0.5346, test loss-1.6555, acc-0.5264\n",
      "Iter-99720, train loss-1.5224, acc-0.5800, valid loss-1.6545, acc-0.5346, test loss-1.6555, acc-0.5263\n",
      "Iter-99730, train loss-1.7465, acc-0.5000, valid loss-1.6544, acc-0.5348, test loss-1.6555, acc-0.5263\n",
      "Iter-99740, train loss-1.7945, acc-0.4600, valid loss-1.6544, acc-0.5348, test loss-1.6554, acc-0.5262\n",
      "Iter-99750, train loss-1.6408, acc-0.5600, valid loss-1.6544, acc-0.5346, test loss-1.6554, acc-0.5263\n",
      "Iter-99760, train loss-1.7235, acc-0.4600, valid loss-1.6543, acc-0.5344, test loss-1.6554, acc-0.5263\n",
      "Iter-99770, train loss-1.6984, acc-0.5200, valid loss-1.6543, acc-0.5346, test loss-1.6553, acc-0.5262\n",
      "Iter-99780, train loss-1.6542, acc-0.5200, valid loss-1.6543, acc-0.5346, test loss-1.6553, acc-0.5263\n",
      "Iter-99790, train loss-1.8239, acc-0.4400, valid loss-1.6543, acc-0.5346, test loss-1.6553, acc-0.5262\n",
      "Iter-99800, train loss-1.6519, acc-0.6200, valid loss-1.6542, acc-0.5344, test loss-1.6552, acc-0.5263\n",
      "Iter-99810, train loss-1.6421, acc-0.5600, valid loss-1.6542, acc-0.5346, test loss-1.6552, acc-0.5263\n",
      "Iter-99820, train loss-1.6817, acc-0.4800, valid loss-1.6542, acc-0.5346, test loss-1.6552, acc-0.5262\n",
      "Iter-99830, train loss-1.6281, acc-0.6000, valid loss-1.6541, acc-0.5346, test loss-1.6552, acc-0.5261\n",
      "Iter-99840, train loss-1.6548, acc-0.4200, valid loss-1.6541, acc-0.5346, test loss-1.6551, acc-0.5262\n",
      "Iter-99850, train loss-1.6037, acc-0.5600, valid loss-1.6541, acc-0.5346, test loss-1.6551, acc-0.5262\n",
      "Iter-99860, train loss-1.6737, acc-0.5600, valid loss-1.6540, acc-0.5348, test loss-1.6551, acc-0.5262\n",
      "Iter-99870, train loss-1.6519, acc-0.4800, valid loss-1.6540, acc-0.5346, test loss-1.6550, acc-0.5262\n",
      "Iter-99880, train loss-1.6721, acc-0.5200, valid loss-1.6540, acc-0.5348, test loss-1.6550, acc-0.5264\n",
      "Iter-99890, train loss-1.5443, acc-0.6000, valid loss-1.6540, acc-0.5346, test loss-1.6550, acc-0.5262\n",
      "Iter-99900, train loss-1.5888, acc-0.6200, valid loss-1.6539, acc-0.5348, test loss-1.6549, acc-0.5262\n",
      "Iter-99910, train loss-1.7764, acc-0.4200, valid loss-1.6539, acc-0.5346, test loss-1.6549, acc-0.5262\n",
      "Iter-99920, train loss-1.6889, acc-0.5000, valid loss-1.6539, acc-0.5346, test loss-1.6549, acc-0.5262\n",
      "Iter-99930, train loss-1.6296, acc-0.6400, valid loss-1.6538, acc-0.5346, test loss-1.6549, acc-0.5261\n",
      "Iter-99940, train loss-1.6910, acc-0.4800, valid loss-1.6538, acc-0.5348, test loss-1.6548, acc-0.5262\n",
      "Iter-99950, train loss-1.6889, acc-0.5200, valid loss-1.6538, acc-0.5348, test loss-1.6548, acc-0.5260\n",
      "Iter-99960, train loss-1.7671, acc-0.5200, valid loss-1.6538, acc-0.5348, test loss-1.6548, acc-0.5264\n",
      "Iter-99970, train loss-1.6109, acc-0.5800, valid loss-1.6537, acc-0.5348, test loss-1.6547, acc-0.5261\n",
      "Iter-99980, train loss-1.6699, acc-0.5600, valid loss-1.6537, acc-0.5352, test loss-1.6547, acc-0.5264\n",
      "Iter-99990, train loss-1.6320, acc-0.5200, valid loss-1.6537, acc-0.5352, test loss-1.6547, acc-0.5264\n",
      "Iter-100000, train loss-1.5977, acc-0.4800, valid loss-1.6536, acc-0.5350, test loss-1.6546, acc-0.5261\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1d61b9470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCoD/B2lXFRnxr30yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1d61b9320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

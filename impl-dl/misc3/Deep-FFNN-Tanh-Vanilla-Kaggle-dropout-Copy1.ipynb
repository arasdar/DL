{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "      <th>msd_track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41.08</td>\n",
       "      <td>6.579</td>\n",
       "      <td>4.307</td>\n",
       "      <td>3.421</td>\n",
       "      <td>3.192</td>\n",
       "      <td>2.076</td>\n",
       "      <td>2.179</td>\n",
       "      <td>2.052</td>\n",
       "      <td>1.794</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3470</td>\n",
       "      <td>-0.2463</td>\n",
       "      <td>-1.5470</td>\n",
       "      <td>0.17920</td>\n",
       "      <td>-1.1530</td>\n",
       "      <td>-0.7370</td>\n",
       "      <td>0.40750</td>\n",
       "      <td>-0.67190</td>\n",
       "      <td>-0.05147</td>\n",
       "      <td>TRPLTEM128F92E1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>60.80</td>\n",
       "      <td>5.973</td>\n",
       "      <td>4.344</td>\n",
       "      <td>3.261</td>\n",
       "      <td>2.835</td>\n",
       "      <td>2.725</td>\n",
       "      <td>2.446</td>\n",
       "      <td>1.884</td>\n",
       "      <td>1.962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3316</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>-1.4760</td>\n",
       "      <td>0.52700</td>\n",
       "      <td>-2.1960</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>-1.39000</td>\n",
       "      <td>0.22560</td>\n",
       "      <td>-0.72080</td>\n",
       "      <td>TRJWMBQ128F424155E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51.47</td>\n",
       "      <td>4.971</td>\n",
       "      <td>4.316</td>\n",
       "      <td>2.916</td>\n",
       "      <td>3.112</td>\n",
       "      <td>2.290</td>\n",
       "      <td>2.053</td>\n",
       "      <td>1.934</td>\n",
       "      <td>1.878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2803</td>\n",
       "      <td>-0.1603</td>\n",
       "      <td>-0.1355</td>\n",
       "      <td>1.03500</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>1.4890</td>\n",
       "      <td>0.02959</td>\n",
       "      <td>-0.13670</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>TRRZWMO12903CCFCC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>41.28</td>\n",
       "      <td>6.610</td>\n",
       "      <td>4.411</td>\n",
       "      <td>2.602</td>\n",
       "      <td>2.822</td>\n",
       "      <td>2.126</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1.973</td>\n",
       "      <td>1.945</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6930</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>-0.3953</td>\n",
       "      <td>0.26710</td>\n",
       "      <td>-1.0450</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.03724</td>\n",
       "      <td>1.04500</td>\n",
       "      <td>-0.20000</td>\n",
       "      <td>TRBZRUT12903CE6C04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54.17</td>\n",
       "      <td>8.945</td>\n",
       "      <td>4.685</td>\n",
       "      <td>4.208</td>\n",
       "      <td>3.154</td>\n",
       "      <td>3.527</td>\n",
       "      <td>2.733</td>\n",
       "      <td>2.202</td>\n",
       "      <td>2.686</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4690</td>\n",
       "      <td>-0.5449</td>\n",
       "      <td>-0.5622</td>\n",
       "      <td>-0.08968</td>\n",
       "      <td>-0.9823</td>\n",
       "      <td>-0.2445</td>\n",
       "      <td>-1.65800</td>\n",
       "      <td>-0.04825</td>\n",
       "      <td>-0.70950</td>\n",
       "      <td>TRLUJQF128F42AF5BF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9  \\\n",
       "0   1  41.08  6.579  4.307  3.421  3.192  2.076  2.179  2.052  1.794   \n",
       "1   2  60.80  5.973  4.344  3.261  2.835  2.725  2.446  1.884  1.962   \n",
       "2   3  51.47  4.971  4.316  2.916  3.112  2.290  2.053  1.934  1.878   \n",
       "3   4  41.28  6.610  4.411  2.602  2.822  2.126  1.984  1.973  1.945   \n",
       "4   5  54.17  8.945  4.685  4.208  3.154  3.527  2.733  2.202  2.686   \n",
       "\n",
       "          ...           att18   att19   att20    att21   att22   att23  \\\n",
       "0         ...          1.3470 -0.2463 -1.5470  0.17920 -1.1530 -0.7370   \n",
       "1         ...         -0.3316  0.3519 -1.4760  0.52700 -2.1960  1.5990   \n",
       "2         ...         -0.2803 -0.1603 -0.1355  1.03500  0.2370  1.4890   \n",
       "3         ...         -1.6930  1.0040 -0.3953  0.26710 -1.0450  0.4974   \n",
       "4         ...          2.4690 -0.5449 -0.5622 -0.08968 -0.9823 -0.2445   \n",
       "\n",
       "     att24    att25    att26        msd_track_id  \n",
       "0  0.40750 -0.67190 -0.05147  TRPLTEM128F92E1389  \n",
       "1 -1.39000  0.22560 -0.72080  TRJWMBQ128F424155E  \n",
       "2  0.02959 -0.13670  0.10820  TRRZWMO12903CCFCC2  \n",
       "3  0.03724  1.04500 -0.20000  TRBZRUT12903CE6C04  \n",
       "4 -1.65800 -0.04825 -0.70950  TRLUJQF128F42AF5BF  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att17</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.22</td>\n",
       "      <td>8.076</td>\n",
       "      <td>6.935</td>\n",
       "      <td>4.696</td>\n",
       "      <td>3.856</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.922</td>\n",
       "      <td>2.568</td>\n",
       "      <td>2.070</td>\n",
       "      <td>...</td>\n",
       "      <td>3.988</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>-2.2210</td>\n",
       "      <td>0.6453</td>\n",
       "      <td>-0.2923</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>-0.09179</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36.42</td>\n",
       "      <td>6.131</td>\n",
       "      <td>5.364</td>\n",
       "      <td>4.292</td>\n",
       "      <td>3.968</td>\n",
       "      <td>2.937</td>\n",
       "      <td>2.872</td>\n",
       "      <td>2.142</td>\n",
       "      <td>2.050</td>\n",
       "      <td>...</td>\n",
       "      <td>7.098</td>\n",
       "      <td>1.2290</td>\n",
       "      <td>0.5971</td>\n",
       "      <td>-1.0670</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>-1.8240</td>\n",
       "      <td>2.3130</td>\n",
       "      <td>-0.80890</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>-0.6225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70.01</td>\n",
       "      <td>5.496</td>\n",
       "      <td>4.698</td>\n",
       "      <td>3.699</td>\n",
       "      <td>3.258</td>\n",
       "      <td>2.293</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.034</td>\n",
       "      <td>...</td>\n",
       "      <td>4.449</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>1.6370</td>\n",
       "      <td>-1.0690</td>\n",
       "      <td>2.4160</td>\n",
       "      <td>-0.6299</td>\n",
       "      <td>1.4190</td>\n",
       "      <td>-0.81960</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>-0.5948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40.64</td>\n",
       "      <td>7.281</td>\n",
       "      <td>6.702</td>\n",
       "      <td>4.043</td>\n",
       "      <td>3.729</td>\n",
       "      <td>3.043</td>\n",
       "      <td>2.644</td>\n",
       "      <td>2.366</td>\n",
       "      <td>1.940</td>\n",
       "      <td>...</td>\n",
       "      <td>2.785</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>-1.1370</td>\n",
       "      <td>1.2750</td>\n",
       "      <td>1.7920</td>\n",
       "      <td>-2.1250</td>\n",
       "      <td>1.6090</td>\n",
       "      <td>-0.83230</td>\n",
       "      <td>-0.1998</td>\n",
       "      <td>-0.1218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38.85</td>\n",
       "      <td>7.118</td>\n",
       "      <td>5.703</td>\n",
       "      <td>4.825</td>\n",
       "      <td>4.088</td>\n",
       "      <td>3.823</td>\n",
       "      <td>3.254</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.193</td>\n",
       "      <td>...</td>\n",
       "      <td>4.536</td>\n",
       "      <td>2.1470</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>2.8050</td>\n",
       "      <td>0.2762</td>\n",
       "      <td>0.2504</td>\n",
       "      <td>1.04900</td>\n",
       "      <td>0.3447</td>\n",
       "      <td>-0.7689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9   ...    \\\n",
       "0   1  38.22  8.076  6.935  4.696  3.856  3.465  2.922  2.568  2.070   ...     \n",
       "1   2  36.42  6.131  5.364  4.292  3.968  2.937  2.872  2.142  2.050   ...     \n",
       "2   3  70.01  5.496  4.698  3.699  3.258  2.293  2.680  2.226  2.034   ...     \n",
       "3   4  40.64  7.281  6.702  4.043  3.729  3.043  2.644  2.366  1.940   ...     \n",
       "4   5  38.85  7.118  5.703  4.825  4.088  3.823  3.254  2.551  2.193   ...     \n",
       "\n",
       "   att17   att18   att19   att20   att21   att22   att23    att24   att25  \\\n",
       "0  3.988  0.4957  0.1836 -2.2210  0.6453 -0.2923  1.2000 -0.09179  0.4674   \n",
       "1  7.098  1.2290  0.5971 -1.0670  0.9569 -1.8240  2.3130 -0.80890  0.5612   \n",
       "2  4.449  0.4773  1.6370 -1.0690  2.4160 -0.6299  1.4190 -0.81960  0.9151   \n",
       "3  2.785  1.9000 -1.1370  1.2750  1.7920 -2.1250  1.6090 -0.83230 -0.1998   \n",
       "4  4.536  2.1470  1.0200 -0.2656  2.8050  0.2762  0.2504  1.04900  0.3447   \n",
       "\n",
       "    att26  \n",
       "0  0.2158  \n",
       "1 -0.6225  \n",
       "2 -0.5948  \n",
       "3 -0.1218  \n",
       "4 -0.7689  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/submission-random.csv')\n",
    "test_y_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Blues, Country, Electronic, Folk, International, Jazz, Latin, New_Age, Pop_Rock, Rap, Reggae, RnB, Vocal]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "test_X = np.array(test_x)\n",
    "\n",
    "# Getting rid of the first and the last column: Id and msd_track_id\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)\n",
    "\n",
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Blues', 'Country', 'Vocal', 'Pop_Rock', 'International', 'New_Age', 'Reggae', 'RnB', 'Electronic', 'Folk', 'Jazz', 'Latin', 'Rap'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the freq of the keys in the training labels\n",
    "counted_labels = Counter(train_Y)\n",
    "labels_keys = counted_labels.keys()\n",
    "labels_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blues',\n",
       " 'Country',\n",
       " 'Electronic',\n",
       " 'Folk',\n",
       " 'International',\n",
       " 'Jazz',\n",
       " 'Latin',\n",
       " 'New_Age',\n",
       " 'Pop_Rock',\n",
       " 'Rap',\n",
       " 'Reggae',\n",
       " 'RnB',\n",
       " 'Vocal']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_keys_sorted = sorted(labels_keys)\n",
    "labels_keys_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Blues': 0,\n",
       " 'Country': 1,\n",
       " 'Electronic': 2,\n",
       " 'Folk': 3,\n",
       " 'International': 4,\n",
       " 'Jazz': 5,\n",
       " 'Latin': 6,\n",
       " 'New_Age': 7,\n",
       " 'Pop_Rock': 8,\n",
       " 'Rap': 9,\n",
       " 'Reggae': 10,\n",
       " 'RnB': 11,\n",
       " 'Vocal': 12}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This for loop for creating a dictionary/ vocab\n",
    "key_to_val = {key: val for val, key in enumerate(labels_keys_sorted)}\n",
    "key_to_val['Country']\n",
    "key_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Blues',\n",
       " 1: 'Country',\n",
       " 2: 'Electronic',\n",
       " 3: 'Folk',\n",
       " 4: 'International',\n",
       " 5: 'Jazz',\n",
       " 6: 'Latin',\n",
       " 7: 'New_Age',\n",
       " 8: 'Pop_Rock',\n",
       " 9: 'Rap',\n",
       " 10: 'Reggae',\n",
       " 11: 'RnB',\n",
       " 12: 'Vocal'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_to_key = {val: key for val, key in enumerate(labels_keys_sorted)}\n",
    "val_to_key[1]\n",
    "val_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_val[each])\n",
    "    Y_train_vec.append(key_to_val[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13000, 26), (10400, 26), dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std\n",
    "X_train_val.shape, X_test.shape, X_train_val.dtype, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 26), (1300, 26), (10400, 26), (1300,), (11700,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, \n",
    "X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape \n",
    "# X_train.dtype, X_val.dtype\n",
    "# Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, keep_prob):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'train_acc':[], 'valid':[], 'valid_acc':[]}\n",
    "        self.keep_prob = keep_prob # 1 - p_dropout\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.grads = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), \n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                     b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                 b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        dX = dout @ W.T # Backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.relu_forward(X=y)\n",
    "        y, nl_cache = l.selu_forward(X=y)\n",
    "        if train:\n",
    "            y, do_cache = l.selu_dropout_forward(h=y, q=self.keep_prob)\n",
    "            caches.append((fc_cache, nl_cache, do_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, do_caches = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.relu_forward(X=y)\n",
    "            y, nl_cache = l.selu_forward(X=y)\n",
    "            if train:\n",
    "                y, do_cache = l.selu_dropout_forward(h=y, q=self.keep_prob)\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "                do_caches.append(do_cache)\n",
    "            X = y.copy() # pass to next layer\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches, do_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        # Softmax is included in loss function\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # dSoftmax is included in loss function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches, do_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.selu_dropout_backward(cache=do_caches[layer], dout=dy)\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.relu_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dy = l.selu_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache, do_cache = caches[0]\n",
    "        dy = l.selu_dropout_backward(cache=do_cache, dout=dy)\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.relu_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dy = l.selu_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        _, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Momentums\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y, y_mini)\n",
    "            grad = self.train_backward(dy, caches) \n",
    "            \n",
    "            # Update the model\n",
    "            for key in grad[0].keys():\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer].keys():\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for key in grad[2].keys():\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "                \n",
    "            # Trained model info\n",
    "            y_pred, y_logit = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_logit, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validated model info\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter: {}, train loss: {:.4f}, train acc: {:.4f}, valid loss: {:.4f}, valid acc: {:.4f}'.format(\n",
    "                    iter, loss, acc, valid_loss, valid_acc))\n",
    "\n",
    "            #         # Test the final model\n",
    "            #         y_pred, y_logit = nn.test(X_test)\n",
    "            #         loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "            #         acc = np.mean(y_pred == y_test)\n",
    "            #         print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            #             acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700,), (11700, 26), (1300, 26), (1300,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100, train loss: 2.2348, train acc: 0.2656, valid loss: 2.1873, valid acc: 0.2577\n",
      "Iter: 200, train loss: 2.0841, train acc: 0.3125, valid loss: 2.1241, valid acc: 0.2823\n",
      "Iter: 300, train loss: 1.9221, train acc: 0.3125, valid loss: 2.1000, valid acc: 0.2946\n",
      "Iter: 400, train loss: 2.0342, train acc: 0.2188, valid loss: 2.0815, valid acc: 0.3062\n",
      "Iter: 500, train loss: 1.8491, train acc: 0.4375, valid loss: 2.0772, valid acc: 0.3046\n",
      "Iter: 600, train loss: 2.1459, train acc: 0.3906, valid loss: 2.0800, valid acc: 0.3177\n",
      "Iter: 700, train loss: 1.9785, train acc: 0.3594, valid loss: 2.0658, valid acc: 0.3162\n",
      "Iter: 800, train loss: 1.7177, train acc: 0.3594, valid loss: 2.0711, valid acc: 0.3154\n",
      "Iter: 900, train loss: 2.0029, train acc: 0.4219, valid loss: 2.0629, valid acc: 0.3192\n",
      "Iter: 1000, train loss: 1.9380, train acc: 0.3281, valid loss: 2.0641, valid acc: 0.3115\n",
      "Iter: 1100, train loss: 1.8061, train acc: 0.4531, valid loss: 2.0619, valid acc: 0.3215\n",
      "Iter: 1200, train loss: 2.0008, train acc: 0.4062, valid loss: 2.0583, valid acc: 0.3192\n",
      "Iter: 1300, train loss: 1.9126, train acc: 0.2969, valid loss: 2.0491, valid acc: 0.3323\n",
      "Iter: 1400, train loss: 1.7665, train acc: 0.4219, valid loss: 2.0531, valid acc: 0.3277\n",
      "Iter: 1500, train loss: 2.0207, train acc: 0.4375, valid loss: 2.0285, valid acc: 0.3377\n",
      "Iter: 1600, train loss: 1.8879, train acc: 0.3125, valid loss: 2.0482, valid acc: 0.3292\n",
      "Iter: 1700, train loss: 2.1184, train acc: 0.2812, valid loss: 2.0244, valid acc: 0.3215\n",
      "Iter: 1800, train loss: 1.7843, train acc: 0.4375, valid loss: 2.0256, valid acc: 0.3431\n",
      "Iter: 1900, train loss: 1.8499, train acc: 0.3281, valid loss: 2.0265, valid acc: 0.3323\n",
      "Iter: 2000, train loss: 1.9414, train acc: 0.3281, valid loss: 2.0347, valid acc: 0.3177\n",
      "Iter: 2100, train loss: 1.7620, train acc: 0.3906, valid loss: 2.0160, valid acc: 0.3315\n",
      "Iter: 2200, train loss: 1.8439, train acc: 0.4688, valid loss: 2.0225, valid acc: 0.3362\n",
      "Iter: 2300, train loss: 1.9461, train acc: 0.3281, valid loss: 2.0170, valid acc: 0.3400\n",
      "Iter: 2400, train loss: 2.0205, train acc: 0.3438, valid loss: 2.0159, valid acc: 0.3308\n",
      "Iter: 2500, train loss: 1.9881, train acc: 0.3125, valid loss: 2.0130, valid acc: 0.3331\n",
      "Iter: 2600, train loss: 2.0801, train acc: 0.2344, valid loss: 2.0379, valid acc: 0.3369\n",
      "Iter: 2700, train loss: 2.1279, train acc: 0.2500, valid loss: 2.0229, valid acc: 0.3315\n",
      "Iter: 2800, train loss: 1.8614, train acc: 0.4219, valid loss: 2.0246, valid acc: 0.3369\n",
      "Iter: 2900, train loss: 1.8106, train acc: 0.3906, valid loss: 2.0218, valid acc: 0.3369\n",
      "Iter: 3000, train loss: 1.9367, train acc: 0.3594, valid loss: 2.0197, valid acc: 0.3400\n",
      "Iter: 3100, train loss: 2.1050, train acc: 0.3281, valid loss: 2.0080, valid acc: 0.3477\n",
      "Iter: 3200, train loss: 1.7526, train acc: 0.4062, valid loss: 2.0282, valid acc: 0.3323\n",
      "Iter: 3300, train loss: 1.9041, train acc: 0.3281, valid loss: 2.0191, valid acc: 0.3508\n",
      "Iter: 3400, train loss: 1.5985, train acc: 0.4219, valid loss: 2.0174, valid acc: 0.3462\n",
      "Iter: 3500, train loss: 1.8554, train acc: 0.4375, valid loss: 2.0048, valid acc: 0.3423\n",
      "Iter: 3600, train loss: 1.7316, train acc: 0.4531, valid loss: 2.0009, valid acc: 0.3477\n",
      "Iter: 3700, train loss: 1.8266, train acc: 0.4062, valid loss: 2.0038, valid acc: 0.3546\n",
      "Iter: 3800, train loss: 1.8308, train acc: 0.3281, valid loss: 2.0102, valid acc: 0.3477\n",
      "Iter: 3900, train loss: 1.8038, train acc: 0.3906, valid loss: 1.9967, valid acc: 0.3569\n",
      "Iter: 4000, train loss: 2.1126, train acc: 0.2969, valid loss: 1.9962, valid acc: 0.3538\n",
      "Iter: 4100, train loss: 1.6975, train acc: 0.4688, valid loss: 2.0009, valid acc: 0.3531\n",
      "Iter: 4200, train loss: 1.6770, train acc: 0.3750, valid loss: 1.9988, valid acc: 0.3423\n",
      "Iter: 4300, train loss: 1.9951, train acc: 0.3750, valid loss: 2.0018, valid acc: 0.3515\n",
      "Iter: 4400, train loss: 1.6198, train acc: 0.4531, valid loss: 1.9988, valid acc: 0.3454\n",
      "Iter: 4500, train loss: 1.8349, train acc: 0.4219, valid loss: 2.0012, valid acc: 0.3538\n",
      "Iter: 4600, train loss: 2.0048, train acc: 0.3438, valid loss: 2.0103, valid acc: 0.3585\n",
      "Iter: 4700, train loss: 1.7214, train acc: 0.4688, valid loss: 2.0076, valid acc: 0.3492\n",
      "Iter: 4800, train loss: 1.7530, train acc: 0.4219, valid loss: 2.0091, valid acc: 0.3485\n",
      "Iter: 4900, train loss: 1.5864, train acc: 0.5000, valid loss: 1.9978, valid acc: 0.3500\n",
      "Iter: 5000, train loss: 1.8995, train acc: 0.3750, valid loss: 1.9900, valid acc: 0.3538\n",
      "Iter: 5100, train loss: 1.9091, train acc: 0.3438, valid loss: 1.9959, valid acc: 0.3577\n",
      "Iter: 5200, train loss: 1.8743, train acc: 0.3281, valid loss: 1.9741, valid acc: 0.3615\n",
      "Iter: 5300, train loss: 1.9279, train acc: 0.3594, valid loss: 1.9951, valid acc: 0.3608\n",
      "Iter: 5400, train loss: 1.6679, train acc: 0.5312, valid loss: 2.0040, valid acc: 0.3577\n",
      "Iter: 5500, train loss: 1.7708, train acc: 0.3438, valid loss: 1.9968, valid acc: 0.3523\n",
      "Iter: 5600, train loss: 1.6907, train acc: 0.4375, valid loss: 1.9886, valid acc: 0.3546\n",
      "Iter: 5700, train loss: 1.7456, train acc: 0.3750, valid loss: 1.9838, valid acc: 0.3585\n",
      "Iter: 5800, train loss: 1.6772, train acc: 0.3438, valid loss: 1.9965, valid acc: 0.3577\n",
      "Iter: 5900, train loss: 1.7497, train acc: 0.4375, valid loss: 1.9938, valid acc: 0.3623\n",
      "Iter: 6000, train loss: 2.0005, train acc: 0.3281, valid loss: 1.9888, valid acc: 0.3662\n",
      "Iter: 6100, train loss: 1.5656, train acc: 0.5000, valid loss: 1.9842, valid acc: 0.3646\n",
      "Iter: 6200, train loss: 1.8354, train acc: 0.4375, valid loss: 1.9825, valid acc: 0.3623\n",
      "Iter: 6300, train loss: 1.6265, train acc: 0.5000, valid loss: 1.9697, valid acc: 0.3654\n",
      "Iter: 6400, train loss: 1.5785, train acc: 0.5312, valid loss: 1.9728, valid acc: 0.3623\n",
      "Iter: 6500, train loss: 2.0364, train acc: 0.3125, valid loss: 1.9878, valid acc: 0.3638\n",
      "Iter: 6600, train loss: 2.2661, train acc: 0.2812, valid loss: 1.9718, valid acc: 0.3723\n",
      "Iter: 6700, train loss: 1.7646, train acc: 0.4219, valid loss: 1.9806, valid acc: 0.3623\n",
      "Iter: 6800, train loss: 1.8073, train acc: 0.3438, valid loss: 1.9892, valid acc: 0.3485\n",
      "Iter: 6900, train loss: 1.6416, train acc: 0.4531, valid loss: 1.9825, valid acc: 0.3631\n",
      "Iter: 7000, train loss: 1.5725, train acc: 0.5000, valid loss: 1.9865, valid acc: 0.3654\n",
      "Iter: 7100, train loss: 1.6457, train acc: 0.4375, valid loss: 2.0001, valid acc: 0.3608\n",
      "Iter: 7200, train loss: 2.0089, train acc: 0.3750, valid loss: 1.9805, valid acc: 0.3623\n",
      "Iter: 7300, train loss: 1.5301, train acc: 0.4844, valid loss: 2.0025, valid acc: 0.3562\n",
      "Iter: 7400, train loss: 1.9919, train acc: 0.3750, valid loss: 1.9906, valid acc: 0.3631\n",
      "Iter: 7500, train loss: 1.8606, train acc: 0.4219, valid loss: 1.9909, valid acc: 0.3638\n",
      "Iter: 7600, train loss: 1.7890, train acc: 0.4219, valid loss: 1.9775, valid acc: 0.3692\n",
      "Iter: 7700, train loss: 1.6928, train acc: 0.4531, valid loss: 1.9656, valid acc: 0.3577\n",
      "Iter: 7800, train loss: 1.9852, train acc: 0.3750, valid loss: 1.9752, valid acc: 0.3700\n",
      "Iter: 7900, train loss: 1.9238, train acc: 0.4688, valid loss: 1.9763, valid acc: 0.3577\n",
      "Iter: 8000, train loss: 1.6364, train acc: 0.5000, valid loss: 1.9762, valid acc: 0.3677\n",
      "Iter: 8100, train loss: 1.6788, train acc: 0.4219, valid loss: 1.9713, valid acc: 0.3685\n",
      "Iter: 8200, train loss: 1.8666, train acc: 0.3906, valid loss: 1.9894, valid acc: 0.3546\n",
      "Iter: 8300, train loss: 1.4534, train acc: 0.5156, valid loss: 1.9924, valid acc: 0.3577\n",
      "Iter: 8400, train loss: 1.5885, train acc: 0.5000, valid loss: 1.9831, valid acc: 0.3662\n",
      "Iter: 8500, train loss: 1.7176, train acc: 0.4062, valid loss: 1.9767, valid acc: 0.3585\n",
      "Iter: 8600, train loss: 1.3700, train acc: 0.5156, valid loss: 1.9702, valid acc: 0.3638\n",
      "Iter: 8700, train loss: 1.6264, train acc: 0.4531, valid loss: 1.9746, valid acc: 0.3669\n",
      "Iter: 8800, train loss: 1.9133, train acc: 0.4375, valid loss: 1.9827, valid acc: 0.3623\n",
      "Iter: 8900, train loss: 1.5797, train acc: 0.5000, valid loss: 1.9810, valid acc: 0.3608\n",
      "Iter: 9000, train loss: 1.6881, train acc: 0.4688, valid loss: 1.9736, valid acc: 0.3631\n",
      "Iter: 9100, train loss: 2.0014, train acc: 0.3906, valid loss: 1.9857, valid acc: 0.3654\n",
      "Iter: 9200, train loss: 1.5708, train acc: 0.4844, valid loss: 1.9784, valid acc: 0.3592\n",
      "Iter: 9300, train loss: 1.7261, train acc: 0.4062, valid loss: 1.9863, valid acc: 0.3662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 9400, train loss: 1.7615, train acc: 0.4062, valid loss: 1.9764, valid acc: 0.3646\n",
      "Iter: 9500, train loss: 1.3543, train acc: 0.5625, valid loss: 1.9752, valid acc: 0.3669\n",
      "Iter: 9600, train loss: 1.8790, train acc: 0.3906, valid loss: 1.9860, valid acc: 0.3623\n",
      "Iter: 9700, train loss: 1.8520, train acc: 0.3750, valid loss: 1.9743, valid acc: 0.3646\n",
      "Iter: 9800, train loss: 1.7997, train acc: 0.4531, valid loss: 1.9835, valid acc: 0.3654\n",
      "Iter: 9900, train loss: 2.0369, train acc: 0.2969, valid loss: 1.9916, valid acc: 0.3623\n",
      "Iter: 10000, train loss: 1.7792, train acc: 0.4062, valid loss: 1.9775, valid acc: 0.3608\n",
      "Iter: 10100, train loss: 1.7888, train acc: 0.4375, valid loss: 1.9787, valid acc: 0.3677\n",
      "Iter: 10200, train loss: 1.8597, train acc: 0.4219, valid loss: 1.9795, valid acc: 0.3662\n",
      "Iter: 10300, train loss: 1.7453, train acc: 0.4844, valid loss: 1.9870, valid acc: 0.3700\n",
      "Iter: 10400, train loss: 1.9795, train acc: 0.3594, valid loss: 1.9869, valid acc: 0.3708\n",
      "Iter: 10500, train loss: 1.7902, train acc: 0.3438, valid loss: 1.9865, valid acc: 0.3700\n",
      "Iter: 10600, train loss: 1.8258, train acc: 0.4375, valid loss: 1.9889, valid acc: 0.3692\n",
      "Iter: 10700, train loss: 1.5997, train acc: 0.5000, valid loss: 1.9850, valid acc: 0.3654\n",
      "Iter: 10800, train loss: 2.1123, train acc: 0.2500, valid loss: 1.9858, valid acc: 0.3638\n",
      "Iter: 10900, train loss: 1.7483, train acc: 0.4531, valid loss: 1.9855, valid acc: 0.3669\n",
      "Iter: 11000, train loss: 1.8668, train acc: 0.3594, valid loss: 1.9842, valid acc: 0.3708\n",
      "Iter: 11100, train loss: 1.8892, train acc: 0.3281, valid loss: 1.9996, valid acc: 0.3585\n",
      "Iter: 11200, train loss: 1.9268, train acc: 0.3438, valid loss: 1.9890, valid acc: 0.3662\n",
      "Iter: 11300, train loss: 1.6966, train acc: 0.4219, valid loss: 1.9784, valid acc: 0.3654\n",
      "Iter: 11400, train loss: 1.8189, train acc: 0.3750, valid loss: 1.9869, valid acc: 0.3754\n",
      "Iter: 11500, train loss: 1.6999, train acc: 0.5000, valid loss: 1.9973, valid acc: 0.3708\n",
      "Iter: 11600, train loss: 1.6724, train acc: 0.5000, valid loss: 1.9842, valid acc: 0.3731\n",
      "Iter: 11700, train loss: 1.5066, train acc: 0.5312, valid loss: 1.9856, valid acc: 0.3746\n",
      "Iter: 11800, train loss: 1.9294, train acc: 0.4375, valid loss: 1.9868, valid acc: 0.3685\n",
      "Iter: 11900, train loss: 1.5172, train acc: 0.4844, valid loss: 2.0036, valid acc: 0.3646\n",
      "Iter: 12000, train loss: 1.6742, train acc: 0.4688, valid loss: 1.9817, valid acc: 0.3662\n",
      "Iter: 12100, train loss: 1.7209, train acc: 0.4375, valid loss: 1.9800, valid acc: 0.3723\n",
      "Iter: 12200, train loss: 1.7463, train acc: 0.4062, valid loss: 1.9695, valid acc: 0.3708\n",
      "Iter: 12300, train loss: 1.6990, train acc: 0.4688, valid loss: 1.9775, valid acc: 0.3669\n",
      "Iter: 12400, train loss: 1.9473, train acc: 0.4062, valid loss: 2.0067, valid acc: 0.3731\n",
      "Iter: 12500, train loss: 1.7582, train acc: 0.3906, valid loss: 1.9855, valid acc: 0.3662\n",
      "Iter: 12600, train loss: 1.5430, train acc: 0.4688, valid loss: 1.9936, valid acc: 0.3708\n",
      "Iter: 12700, train loss: 2.0546, train acc: 0.2812, valid loss: 1.9827, valid acc: 0.3769\n",
      "Iter: 12800, train loss: 1.7581, train acc: 0.3906, valid loss: 1.9948, valid acc: 0.3723\n",
      "Iter: 12900, train loss: 1.7557, train acc: 0.3438, valid loss: 1.9846, valid acc: 0.3769\n",
      "Iter: 13000, train loss: 1.7743, train acc: 0.4062, valid loss: 1.9955, valid acc: 0.3723\n",
      "Iter: 13100, train loss: 1.7389, train acc: 0.4375, valid loss: 1.9816, valid acc: 0.3731\n",
      "Iter: 13200, train loss: 1.8181, train acc: 0.4844, valid loss: 1.9836, valid acc: 0.3669\n",
      "Iter: 13300, train loss: 1.5884, train acc: 0.5312, valid loss: 1.9974, valid acc: 0.3692\n",
      "Iter: 13400, train loss: 1.6453, train acc: 0.4219, valid loss: 1.9824, valid acc: 0.3638\n",
      "Iter: 13500, train loss: 1.6121, train acc: 0.4688, valid loss: 1.9901, valid acc: 0.3769\n",
      "Iter: 13600, train loss: 1.8197, train acc: 0.3594, valid loss: 1.9822, valid acc: 0.3738\n",
      "Iter: 13700, train loss: 1.9175, train acc: 0.3281, valid loss: 1.9909, valid acc: 0.3723\n",
      "Iter: 13800, train loss: 1.5670, train acc: 0.4531, valid loss: 2.0017, valid acc: 0.3785\n",
      "Iter: 13900, train loss: 1.5816, train acc: 0.4688, valid loss: 2.0039, valid acc: 0.3715\n",
      "Iter: 14000, train loss: 1.6962, train acc: 0.4062, valid loss: 2.0029, valid acc: 0.3685\n",
      "Iter: 14100, train loss: 1.6235, train acc: 0.4219, valid loss: 1.9906, valid acc: 0.3692\n",
      "Iter: 14200, train loss: 1.5513, train acc: 0.5156, valid loss: 1.9870, valid acc: 0.3738\n",
      "Iter: 14300, train loss: 1.4900, train acc: 0.5000, valid loss: 1.9740, valid acc: 0.3831\n",
      "Iter: 14400, train loss: 1.8352, train acc: 0.4219, valid loss: 1.9938, valid acc: 0.3654\n",
      "Iter: 14500, train loss: 1.6633, train acc: 0.4531, valid loss: 1.9928, valid acc: 0.3746\n",
      "Iter: 14600, train loss: 1.7090, train acc: 0.4531, valid loss: 1.9922, valid acc: 0.3708\n",
      "Iter: 14700, train loss: 1.4036, train acc: 0.5469, valid loss: 1.9923, valid acc: 0.3662\n",
      "Iter: 14800, train loss: 1.6814, train acc: 0.4844, valid loss: 1.9924, valid acc: 0.3738\n",
      "Iter: 14900, train loss: 1.6452, train acc: 0.4688, valid loss: 1.9892, valid acc: 0.3792\n",
      "Iter: 15000, train loss: 1.9671, train acc: 0.3125, valid loss: 1.9936, valid acc: 0.3792\n",
      "Iter: 15100, train loss: 1.4808, train acc: 0.5469, valid loss: 1.9877, valid acc: 0.3792\n",
      "Iter: 15200, train loss: 1.7383, train acc: 0.3906, valid loss: 1.9949, valid acc: 0.3669\n",
      "Iter: 15300, train loss: 1.7349, train acc: 0.4062, valid loss: 1.9956, valid acc: 0.3731\n",
      "Iter: 15400, train loss: 1.8304, train acc: 0.2812, valid loss: 1.9710, valid acc: 0.3646\n",
      "Iter: 15500, train loss: 1.8445, train acc: 0.4062, valid loss: 1.9869, valid acc: 0.3654\n",
      "Iter: 15600, train loss: 1.3459, train acc: 0.5312, valid loss: 1.9884, valid acc: 0.3738\n",
      "Iter: 15700, train loss: 1.5913, train acc: 0.4844, valid loss: 2.0011, valid acc: 0.3785\n",
      "Iter: 15800, train loss: 1.7897, train acc: 0.3750, valid loss: 1.9908, valid acc: 0.3700\n",
      "Iter: 15900, train loss: 1.5953, train acc: 0.5000, valid loss: 1.9989, valid acc: 0.3731\n",
      "Iter: 16000, train loss: 1.5814, train acc: 0.4844, valid loss: 2.0111, valid acc: 0.3762\n",
      "Iter: 16100, train loss: 1.7115, train acc: 0.4219, valid loss: 2.0046, valid acc: 0.3708\n",
      "Iter: 16200, train loss: 1.8515, train acc: 0.4531, valid loss: 1.9937, valid acc: 0.3692\n",
      "Iter: 16300, train loss: 1.5320, train acc: 0.4531, valid loss: 1.9984, valid acc: 0.3738\n",
      "Iter: 16400, train loss: 1.7893, train acc: 0.3750, valid loss: 1.9996, valid acc: 0.3692\n",
      "Iter: 16500, train loss: 1.7793, train acc: 0.4062, valid loss: 1.9935, valid acc: 0.3731\n",
      "Iter: 16600, train loss: 1.7379, train acc: 0.4219, valid loss: 2.0052, valid acc: 0.3738\n",
      "Iter: 16700, train loss: 1.7144, train acc: 0.4688, valid loss: 1.9950, valid acc: 0.3738\n",
      "Iter: 16800, train loss: 2.1061, train acc: 0.3281, valid loss: 2.0123, valid acc: 0.3731\n",
      "Iter: 16900, train loss: 1.7251, train acc: 0.4219, valid loss: 1.9942, valid acc: 0.3669\n",
      "Iter: 17000, train loss: 1.9369, train acc: 0.4219, valid loss: 2.0096, valid acc: 0.3700\n",
      "Iter: 17100, train loss: 1.4870, train acc: 0.4844, valid loss: 1.9908, valid acc: 0.3692\n",
      "Iter: 17200, train loss: 1.7867, train acc: 0.3438, valid loss: 1.9909, valid acc: 0.3738\n",
      "Iter: 17300, train loss: 2.0089, train acc: 0.3906, valid loss: 2.0015, valid acc: 0.3600\n",
      "Iter: 17400, train loss: 1.6006, train acc: 0.4062, valid loss: 1.9901, valid acc: 0.3746\n",
      "Iter: 17500, train loss: 1.7909, train acc: 0.4375, valid loss: 1.9829, valid acc: 0.3669\n",
      "Iter: 17600, train loss: 1.7015, train acc: 0.4531, valid loss: 1.9929, valid acc: 0.3692\n",
      "Iter: 17700, train loss: 1.8609, train acc: 0.4062, valid loss: 2.0006, valid acc: 0.3646\n",
      "Iter: 17800, train loss: 1.7500, train acc: 0.5000, valid loss: 1.9989, valid acc: 0.3669\n",
      "Iter: 17900, train loss: 1.7054, train acc: 0.4844, valid loss: 1.9867, valid acc: 0.3723\n",
      "Iter: 18000, train loss: 1.9013, train acc: 0.3438, valid loss: 1.9960, valid acc: 0.3708\n",
      "Iter: 18100, train loss: 1.8048, train acc: 0.4062, valid loss: 1.9899, valid acc: 0.3723\n",
      "Iter: 18200, train loss: 1.9031, train acc: 0.3750, valid loss: 1.9908, valid acc: 0.3623\n",
      "Iter: 18300, train loss: 1.7645, train acc: 0.4062, valid loss: 2.0012, valid acc: 0.3692\n",
      "Iter: 18400, train loss: 1.7921, train acc: 0.4219, valid loss: 1.9973, valid acc: 0.3692\n",
      "Iter: 18500, train loss: 1.8122, train acc: 0.3438, valid loss: 2.0044, valid acc: 0.3677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 18600, train loss: 1.7941, train acc: 0.4531, valid loss: 2.0117, valid acc: 0.3708\n",
      "Iter: 18700, train loss: 1.5311, train acc: 0.4688, valid loss: 1.9950, valid acc: 0.3808\n",
      "Iter: 18800, train loss: 1.7995, train acc: 0.4531, valid loss: 1.9971, valid acc: 0.3708\n",
      "Iter: 18900, train loss: 2.0592, train acc: 0.2500, valid loss: 1.9971, valid acc: 0.3815\n",
      "Iter: 19000, train loss: 1.5056, train acc: 0.4844, valid loss: 2.0036, valid acc: 0.3785\n",
      "Iter: 19100, train loss: 1.7119, train acc: 0.4688, valid loss: 2.0141, valid acc: 0.3700\n",
      "Iter: 19200, train loss: 1.7175, train acc: 0.3750, valid loss: 1.9973, valid acc: 0.3692\n",
      "Iter: 19300, train loss: 1.7742, train acc: 0.4062, valid loss: 2.0039, valid acc: 0.3685\n",
      "Iter: 19400, train loss: 1.6200, train acc: 0.4844, valid loss: 1.9996, valid acc: 0.3746\n",
      "Iter: 19500, train loss: 1.6497, train acc: 0.3750, valid loss: 1.9956, valid acc: 0.3685\n",
      "Iter: 19600, train loss: 1.7601, train acc: 0.4062, valid loss: 2.0143, valid acc: 0.3685\n",
      "Iter: 19700, train loss: 1.4577, train acc: 0.4531, valid loss: 2.0010, valid acc: 0.3785\n",
      "Iter: 19800, train loss: 1.5623, train acc: 0.5312, valid loss: 1.9991, valid acc: 0.3800\n",
      "Iter: 19900, train loss: 1.6333, train acc: 0.4688, valid loss: 2.0046, valid acc: 0.3685\n",
      "Iter: 20000, train loss: 1.6607, train acc: 0.4688, valid loss: 1.9958, valid acc: 0.3631\n",
      "Iter: 20100, train loss: 1.8237, train acc: 0.4062, valid loss: 2.0154, valid acc: 0.3715\n",
      "Iter: 20200, train loss: 1.8531, train acc: 0.3906, valid loss: 2.0147, valid acc: 0.3708\n",
      "Iter: 20300, train loss: 1.9813, train acc: 0.3125, valid loss: 2.0128, valid acc: 0.3662\n",
      "Iter: 20400, train loss: 1.2720, train acc: 0.6250, valid loss: 1.9956, valid acc: 0.3638\n",
      "Iter: 20500, train loss: 1.6250, train acc: 0.3750, valid loss: 2.0076, valid acc: 0.3638\n",
      "Iter: 20600, train loss: 1.8471, train acc: 0.4219, valid loss: 2.0258, valid acc: 0.3700\n",
      "Iter: 20700, train loss: 1.9136, train acc: 0.3594, valid loss: 2.0116, valid acc: 0.3631\n",
      "Iter: 20800, train loss: 1.6063, train acc: 0.5000, valid loss: 2.0072, valid acc: 0.3654\n",
      "Iter: 20900, train loss: 1.9097, train acc: 0.3906, valid loss: 1.9886, valid acc: 0.3762\n",
      "Iter: 21000, train loss: 1.6974, train acc: 0.4531, valid loss: 1.9959, valid acc: 0.3738\n",
      "Iter: 21100, train loss: 2.0786, train acc: 0.3654, valid loss: 2.0040, valid acc: 0.3731\n",
      "Iter: 21200, train loss: 1.5120, train acc: 0.4219, valid loss: 1.9940, valid acc: 0.3769\n",
      "Iter: 21300, train loss: 1.8082, train acc: 0.4375, valid loss: 1.9978, valid acc: 0.3746\n",
      "Iter: 21400, train loss: 1.6762, train acc: 0.5156, valid loss: 1.9959, valid acc: 0.3662\n",
      "Iter: 21500, train loss: 1.5580, train acc: 0.4688, valid loss: 2.0014, valid acc: 0.3762\n",
      "Iter: 21600, train loss: 1.6430, train acc: 0.5000, valid loss: 2.0062, valid acc: 0.3738\n",
      "Iter: 21700, train loss: 1.7923, train acc: 0.4219, valid loss: 2.0097, valid acc: 0.3746\n",
      "Iter: 21800, train loss: 1.6508, train acc: 0.4219, valid loss: 2.0115, valid acc: 0.3685\n",
      "Iter: 21900, train loss: 1.6666, train acc: 0.4844, valid loss: 2.0011, valid acc: 0.3669\n",
      "Iter: 22000, train loss: 1.7681, train acc: 0.4688, valid loss: 2.0003, valid acc: 0.3677\n",
      "Iter: 22100, train loss: 1.7481, train acc: 0.3906, valid loss: 2.0167, valid acc: 0.3623\n",
      "Iter: 22200, train loss: 1.7910, train acc: 0.3906, valid loss: 2.0102, valid acc: 0.3700\n",
      "Iter: 22300, train loss: 1.6797, train acc: 0.4219, valid loss: 2.0002, valid acc: 0.3631\n",
      "Iter: 22400, train loss: 1.8211, train acc: 0.3281, valid loss: 1.9938, valid acc: 0.3685\n",
      "Iter: 22500, train loss: 1.5687, train acc: 0.5156, valid loss: 1.9964, valid acc: 0.3723\n",
      "Iter: 22600, train loss: 1.8070, train acc: 0.3438, valid loss: 2.0157, valid acc: 0.3754\n",
      "Iter: 22700, train loss: 1.6515, train acc: 0.4844, valid loss: 2.0015, valid acc: 0.3777\n",
      "Iter: 22800, train loss: 1.6404, train acc: 0.4531, valid loss: 2.0026, valid acc: 0.3731\n",
      "Iter: 22900, train loss: 1.8336, train acc: 0.3281, valid loss: 1.9999, valid acc: 0.3692\n",
      "Iter: 23000, train loss: 1.6941, train acc: 0.4688, valid loss: 2.0021, valid acc: 0.3731\n",
      "Iter: 23100, train loss: 1.4947, train acc: 0.5000, valid loss: 2.0030, valid acc: 0.3738\n",
      "Iter: 23200, train loss: 1.7769, train acc: 0.4688, valid loss: 2.0126, valid acc: 0.3738\n",
      "Iter: 23300, train loss: 1.8834, train acc: 0.4219, valid loss: 2.0204, valid acc: 0.3731\n",
      "Iter: 23400, train loss: 1.6796, train acc: 0.4062, valid loss: 2.0120, valid acc: 0.3608\n",
      "Iter: 23500, train loss: 1.6495, train acc: 0.5000, valid loss: 2.0088, valid acc: 0.3662\n",
      "Iter: 23600, train loss: 1.2895, train acc: 0.5469, valid loss: 2.0224, valid acc: 0.3592\n",
      "Iter: 23700, train loss: 1.5111, train acc: 0.4531, valid loss: 2.0285, valid acc: 0.3608\n",
      "Iter: 23800, train loss: 1.7916, train acc: 0.4219, valid loss: 2.0257, valid acc: 0.3700\n",
      "Iter: 23900, train loss: 1.4648, train acc: 0.4531, valid loss: 2.0230, valid acc: 0.3631\n",
      "Iter: 24000, train loss: 1.7409, train acc: 0.4375, valid loss: 2.0198, valid acc: 0.3669\n",
      "Iter: 24100, train loss: 1.5879, train acc: 0.5000, valid loss: 2.0245, valid acc: 0.3546\n",
      "Iter: 24200, train loss: 1.7585, train acc: 0.4219, valid loss: 2.0196, valid acc: 0.3554\n",
      "Iter: 24300, train loss: 1.3525, train acc: 0.5156, valid loss: 2.0168, valid acc: 0.3577\n",
      "Iter: 24400, train loss: 1.5036, train acc: 0.5156, valid loss: 2.0369, valid acc: 0.3615\n",
      "Iter: 24500, train loss: 1.6192, train acc: 0.5000, valid loss: 2.0201, valid acc: 0.3623\n",
      "Iter: 24600, train loss: 1.5431, train acc: 0.5625, valid loss: 2.0126, valid acc: 0.3600\n",
      "Iter: 24700, train loss: 1.7423, train acc: 0.3750, valid loss: 2.0205, valid acc: 0.3677\n",
      "Iter: 24800, train loss: 1.5504, train acc: 0.5312, valid loss: 2.0136, valid acc: 0.3623\n",
      "Iter: 24900, train loss: 1.6931, train acc: 0.4531, valid loss: 2.0418, valid acc: 0.3731\n",
      "Iter: 25000, train loss: 1.6145, train acc: 0.4688, valid loss: 2.0319, valid acc: 0.3700\n",
      "Iter: 25100, train loss: 1.7381, train acc: 0.4531, valid loss: 2.0371, valid acc: 0.3685\n",
      "Iter: 25200, train loss: 1.6294, train acc: 0.4375, valid loss: 2.0171, valid acc: 0.3808\n",
      "Iter: 25300, train loss: 1.7987, train acc: 0.3594, valid loss: 2.0051, valid acc: 0.3708\n",
      "Iter: 25400, train loss: 1.7734, train acc: 0.3906, valid loss: 2.0174, valid acc: 0.3700\n",
      "Iter: 25500, train loss: 1.8058, train acc: 0.3906, valid loss: 2.0113, valid acc: 0.3754\n",
      "Iter: 25600, train loss: 1.4078, train acc: 0.5156, valid loss: 2.0222, valid acc: 0.3777\n",
      "Iter: 25700, train loss: 1.3845, train acc: 0.6094, valid loss: 2.0187, valid acc: 0.3715\n",
      "Iter: 25800, train loss: 1.8374, train acc: 0.4219, valid loss: 2.0229, valid acc: 0.3685\n",
      "Iter: 25900, train loss: 1.7977, train acc: 0.4219, valid loss: 2.0269, valid acc: 0.3700\n",
      "Iter: 26000, train loss: 1.4063, train acc: 0.5781, valid loss: 2.0079, valid acc: 0.3638\n",
      "Iter: 26100, train loss: 1.7728, train acc: 0.3750, valid loss: 2.0042, valid acc: 0.3646\n",
      "Iter: 26200, train loss: 1.6759, train acc: 0.4375, valid loss: 2.0210, valid acc: 0.3654\n",
      "Iter: 26300, train loss: 1.8593, train acc: 0.3654, valid loss: 2.0164, valid acc: 0.3738\n",
      "Iter: 26400, train loss: 1.5604, train acc: 0.4844, valid loss: 2.0224, valid acc: 0.3723\n",
      "Iter: 26500, train loss: 1.7476, train acc: 0.3594, valid loss: 2.0263, valid acc: 0.3762\n",
      "Iter: 26600, train loss: 1.6459, train acc: 0.4531, valid loss: 2.0271, valid acc: 0.3600\n",
      "Iter: 26700, train loss: 1.8209, train acc: 0.4062, valid loss: 2.0083, valid acc: 0.3800\n",
      "Iter: 26800, train loss: 1.8201, train acc: 0.4062, valid loss: 2.0172, valid acc: 0.3754\n",
      "Iter: 26900, train loss: 1.6519, train acc: 0.3594, valid loss: 2.0251, valid acc: 0.3754\n",
      "Iter: 27000, train loss: 1.6697, train acc: 0.4531, valid loss: 2.0219, valid acc: 0.3738\n",
      "Iter: 27100, train loss: 1.9310, train acc: 0.4219, valid loss: 2.0202, valid acc: 0.3700\n",
      "Iter: 27200, train loss: 1.5895, train acc: 0.4844, valid loss: 2.0364, valid acc: 0.3654\n",
      "Iter: 27300, train loss: 1.9172, train acc: 0.4219, valid loss: 2.0230, valid acc: 0.3754\n",
      "Iter: 27400, train loss: 1.5293, train acc: 0.5312, valid loss: 2.0334, valid acc: 0.3708\n",
      "Iter: 27500, train loss: 1.4943, train acc: 0.5577, valid loss: 2.0330, valid acc: 0.3708\n",
      "Iter: 27600, train loss: 1.3924, train acc: 0.5781, valid loss: 2.0207, valid acc: 0.3769\n",
      "Iter: 27700, train loss: 1.6255, train acc: 0.5156, valid loss: 2.0084, valid acc: 0.3762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 27800, train loss: 1.6126, train acc: 0.4531, valid loss: 2.0194, valid acc: 0.3754\n",
      "Iter: 27900, train loss: 1.6523, train acc: 0.4219, valid loss: 2.0274, valid acc: 0.3800\n",
      "Iter: 28000, train loss: 1.9316, train acc: 0.3281, valid loss: 2.0404, valid acc: 0.3685\n",
      "Iter: 28100, train loss: 1.7737, train acc: 0.4219, valid loss: 2.0158, valid acc: 0.3654\n",
      "Iter: 28200, train loss: 1.5911, train acc: 0.4844, valid loss: 2.0353, valid acc: 0.3715\n",
      "Iter: 28300, train loss: 1.7919, train acc: 0.3906, valid loss: 2.0303, valid acc: 0.3662\n",
      "Iter: 28400, train loss: 1.6420, train acc: 0.4688, valid loss: 2.0372, valid acc: 0.3715\n",
      "Iter: 28500, train loss: 1.6136, train acc: 0.5000, valid loss: 2.0266, valid acc: 0.3685\n",
      "Iter: 28600, train loss: 1.9329, train acc: 0.3594, valid loss: 2.0123, valid acc: 0.3762\n",
      "Iter: 28700, train loss: 2.0278, train acc: 0.3750, valid loss: 2.0299, valid acc: 0.3708\n",
      "Iter: 28800, train loss: 1.4843, train acc: 0.4844, valid loss: 2.0164, valid acc: 0.3738\n",
      "Iter: 28900, train loss: 1.5402, train acc: 0.4531, valid loss: 2.0122, valid acc: 0.3754\n",
      "Iter: 29000, train loss: 1.8958, train acc: 0.3906, valid loss: 2.0133, valid acc: 0.3731\n",
      "Iter: 29100, train loss: 1.5282, train acc: 0.4844, valid loss: 2.0331, valid acc: 0.3762\n",
      "Iter: 29200, train loss: 1.3948, train acc: 0.5156, valid loss: 2.0258, valid acc: 0.3692\n",
      "Iter: 29300, train loss: 1.5225, train acc: 0.5000, valid loss: 2.0281, valid acc: 0.3654\n",
      "Iter: 29400, train loss: 1.5420, train acc: 0.5156, valid loss: 2.0293, valid acc: 0.3823\n",
      "Iter: 29500, train loss: 1.6545, train acc: 0.4844, valid loss: 2.0283, valid acc: 0.3731\n",
      "Iter: 29600, train loss: 1.7402, train acc: 0.4219, valid loss: 2.0433, valid acc: 0.3762\n",
      "Iter: 29700, train loss: 1.7696, train acc: 0.3281, valid loss: 2.0294, valid acc: 0.3662\n",
      "Iter: 29800, train loss: 1.6769, train acc: 0.4375, valid loss: 2.0251, valid acc: 0.3685\n",
      "Iter: 29900, train loss: 1.7894, train acc: 0.4688, valid loss: 2.0269, valid acc: 0.3669\n",
      "Iter: 30000, train loss: 1.3406, train acc: 0.5156, valid loss: 2.0196, valid acc: 0.3669\n",
      "Iter: 30100, train loss: 1.6204, train acc: 0.4375, valid loss: 2.0308, valid acc: 0.3638\n",
      "Iter: 30200, train loss: 1.5877, train acc: 0.5000, valid loss: 2.0412, valid acc: 0.3692\n",
      "Iter: 30300, train loss: 1.7571, train acc: 0.4219, valid loss: 2.0498, valid acc: 0.3700\n",
      "Iter: 30400, train loss: 1.4712, train acc: 0.5000, valid loss: 2.0371, valid acc: 0.3654\n",
      "Iter: 30500, train loss: 1.7424, train acc: 0.3906, valid loss: 2.0341, valid acc: 0.3677\n",
      "Iter: 30600, train loss: 1.4715, train acc: 0.5000, valid loss: 2.0330, valid acc: 0.3692\n",
      "Iter: 30700, train loss: 2.1592, train acc: 0.3438, valid loss: 2.0217, valid acc: 0.3723\n",
      "Iter: 30800, train loss: 1.7862, train acc: 0.4219, valid loss: 2.0446, valid acc: 0.3738\n",
      "Iter: 30900, train loss: 1.7678, train acc: 0.3594, valid loss: 2.0354, valid acc: 0.3700\n",
      "Iter: 31000, train loss: 1.7286, train acc: 0.4375, valid loss: 2.0337, valid acc: 0.3669\n",
      "Iter: 31100, train loss: 1.6032, train acc: 0.4844, valid loss: 2.0196, valid acc: 0.3769\n",
      "Iter: 31200, train loss: 1.6876, train acc: 0.4531, valid loss: 2.0272, valid acc: 0.3708\n",
      "Iter: 31300, train loss: 1.3712, train acc: 0.5469, valid loss: 2.0544, valid acc: 0.3615\n",
      "Iter: 31400, train loss: 1.3552, train acc: 0.5312, valid loss: 2.0369, valid acc: 0.3715\n",
      "Iter: 31500, train loss: 1.6857, train acc: 0.4219, valid loss: 2.0362, valid acc: 0.3585\n",
      "Iter: 31600, train loss: 1.7092, train acc: 0.4531, valid loss: 2.0528, valid acc: 0.3662\n",
      "Iter: 31700, train loss: 1.4455, train acc: 0.5000, valid loss: 2.0263, valid acc: 0.3585\n",
      "Iter: 31800, train loss: 1.8296, train acc: 0.3906, valid loss: 2.0364, valid acc: 0.3692\n",
      "Iter: 31900, train loss: 1.8511, train acc: 0.4375, valid loss: 2.0396, valid acc: 0.3762\n",
      "Iter: 32000, train loss: 1.7707, train acc: 0.3750, valid loss: 2.0492, valid acc: 0.3677\n",
      "Iter: 32100, train loss: 1.7027, train acc: 0.4531, valid loss: 2.0384, valid acc: 0.3738\n",
      "Iter: 32200, train loss: 1.9505, train acc: 0.4688, valid loss: 2.0463, valid acc: 0.3677\n",
      "Iter: 32300, train loss: 1.4077, train acc: 0.4531, valid loss: 2.0456, valid acc: 0.3738\n",
      "Iter: 32400, train loss: 1.4647, train acc: 0.5469, valid loss: 2.0311, valid acc: 0.3777\n",
      "Iter: 32500, train loss: 1.6980, train acc: 0.4375, valid loss: 2.0362, valid acc: 0.3738\n",
      "Iter: 32600, train loss: 1.3799, train acc: 0.5156, valid loss: 2.0382, valid acc: 0.3800\n",
      "Iter: 32700, train loss: 1.6533, train acc: 0.4531, valid loss: 2.0333, valid acc: 0.3754\n",
      "Iter: 32800, train loss: 1.4731, train acc: 0.5156, valid loss: 2.0479, valid acc: 0.3738\n",
      "Iter: 32900, train loss: 1.8878, train acc: 0.3750, valid loss: 2.0456, valid acc: 0.3715\n",
      "Iter: 33000, train loss: 1.7610, train acc: 0.4062, valid loss: 2.0528, valid acc: 0.3762\n",
      "Iter: 33100, train loss: 1.8943, train acc: 0.3906, valid loss: 2.0475, valid acc: 0.3746\n",
      "Iter: 33200, train loss: 1.7730, train acc: 0.4062, valid loss: 2.0433, valid acc: 0.3769\n",
      "Iter: 33300, train loss: 1.6738, train acc: 0.4375, valid loss: 2.0574, valid acc: 0.3708\n",
      "Iter: 33400, train loss: 1.4858, train acc: 0.5312, valid loss: 2.0416, valid acc: 0.3715\n",
      "Iter: 33500, train loss: 1.5405, train acc: 0.4688, valid loss: 2.0445, valid acc: 0.3808\n",
      "Iter: 33600, train loss: 1.4300, train acc: 0.5000, valid loss: 2.0340, valid acc: 0.3762\n",
      "Iter: 33700, train loss: 1.8379, train acc: 0.3750, valid loss: 2.0211, valid acc: 0.3723\n",
      "Iter: 33800, train loss: 1.3932, train acc: 0.5312, valid loss: 2.0556, valid acc: 0.3646\n",
      "Iter: 33900, train loss: 1.8723, train acc: 0.3906, valid loss: 2.0294, valid acc: 0.3785\n",
      "Iter: 34000, train loss: 1.3123, train acc: 0.5312, valid loss: 2.0302, valid acc: 0.3731\n",
      "Iter: 34100, train loss: 1.7463, train acc: 0.3438, valid loss: 2.0341, valid acc: 0.3700\n",
      "Iter: 34200, train loss: 1.6688, train acc: 0.4688, valid loss: 2.0355, valid acc: 0.3654\n",
      "Iter: 34300, train loss: 1.5145, train acc: 0.5156, valid loss: 2.0462, valid acc: 0.3662\n",
      "Iter: 34400, train loss: 1.7058, train acc: 0.4844, valid loss: 2.0367, valid acc: 0.3715\n",
      "Iter: 34500, train loss: 1.7518, train acc: 0.4531, valid loss: 2.0271, valid acc: 0.3662\n",
      "Iter: 34600, train loss: 1.6470, train acc: 0.4375, valid loss: 2.0502, valid acc: 0.3669\n",
      "Iter: 34700, train loss: 1.6502, train acc: 0.4531, valid loss: 2.0155, valid acc: 0.3708\n",
      "Iter: 34800, train loss: 1.8347, train acc: 0.3125, valid loss: 2.0342, valid acc: 0.3685\n",
      "Iter: 34900, train loss: 1.4845, train acc: 0.5156, valid loss: 2.0411, valid acc: 0.3738\n",
      "Iter: 35000, train loss: 1.7896, train acc: 0.3906, valid loss: 2.0378, valid acc: 0.3700\n",
      "Iter: 35100, train loss: 1.4792, train acc: 0.5156, valid loss: 2.0320, valid acc: 0.3792\n",
      "Iter: 35200, train loss: 1.5820, train acc: 0.4844, valid loss: 2.0281, valid acc: 0.3808\n",
      "Iter: 35300, train loss: 1.5945, train acc: 0.5312, valid loss: 2.0206, valid acc: 0.3738\n",
      "Iter: 35400, train loss: 1.6805, train acc: 0.4219, valid loss: 2.0366, valid acc: 0.3792\n",
      "Iter: 35500, train loss: 1.6157, train acc: 0.4688, valid loss: 2.0290, valid acc: 0.3769\n",
      "Iter: 35600, train loss: 1.6905, train acc: 0.4844, valid loss: 2.0496, valid acc: 0.3738\n",
      "Iter: 35700, train loss: 1.2823, train acc: 0.6094, valid loss: 2.0414, valid acc: 0.3708\n",
      "Iter: 35800, train loss: 1.4050, train acc: 0.5000, valid loss: 2.0227, valid acc: 0.3762\n",
      "Iter: 35900, train loss: 1.6544, train acc: 0.4375, valid loss: 2.0296, valid acc: 0.3785\n",
      "Iter: 36000, train loss: 1.5596, train acc: 0.5000, valid loss: 2.0142, valid acc: 0.3754\n",
      "Iter: 36100, train loss: 1.7260, train acc: 0.3906, valid loss: 2.0256, valid acc: 0.3738\n",
      "Iter: 36200, train loss: 1.5626, train acc: 0.4844, valid loss: 2.0369, valid acc: 0.3692\n",
      "Iter: 36300, train loss: 1.5354, train acc: 0.5000, valid loss: 2.0265, valid acc: 0.3715\n",
      "Iter: 36400, train loss: 1.7962, train acc: 0.3438, valid loss: 2.0305, valid acc: 0.3677\n",
      "Iter: 36500, train loss: 1.3903, train acc: 0.4688, valid loss: 2.0328, valid acc: 0.3700\n",
      "Iter: 36600, train loss: 1.7521, train acc: 0.4219, valid loss: 2.0388, valid acc: 0.3785\n",
      "Iter: 36700, train loss: 1.4767, train acc: 0.4375, valid loss: 2.0291, valid acc: 0.3838\n",
      "Iter: 36800, train loss: 1.4801, train acc: 0.5625, valid loss: 2.0166, valid acc: 0.3831\n",
      "Iter: 36900, train loss: 1.5285, train acc: 0.4688, valid loss: 2.0507, valid acc: 0.3654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 37000, train loss: 1.6821, train acc: 0.4062, valid loss: 2.0390, valid acc: 0.3785\n",
      "Iter: 37100, train loss: 1.7442, train acc: 0.5312, valid loss: 2.0413, valid acc: 0.3715\n",
      "Iter: 37200, train loss: 1.5389, train acc: 0.4844, valid loss: 2.0276, valid acc: 0.3808\n",
      "Iter: 37300, train loss: 1.7964, train acc: 0.4062, valid loss: 2.0408, valid acc: 0.3685\n",
      "Iter: 37400, train loss: 1.4724, train acc: 0.5781, valid loss: 2.0482, valid acc: 0.3754\n",
      "Iter: 37500, train loss: 1.8634, train acc: 0.5000, valid loss: 2.0486, valid acc: 0.3762\n",
      "Iter: 37600, train loss: 1.7755, train acc: 0.4375, valid loss: 2.0447, valid acc: 0.3800\n",
      "Iter: 37700, train loss: 1.7068, train acc: 0.4062, valid loss: 2.0220, valid acc: 0.3823\n",
      "Iter: 37800, train loss: 1.6716, train acc: 0.4688, valid loss: 2.0424, valid acc: 0.3769\n",
      "Iter: 37900, train loss: 1.5158, train acc: 0.5156, valid loss: 2.0579, valid acc: 0.3623\n",
      "Iter: 38000, train loss: 1.8484, train acc: 0.3750, valid loss: 2.0436, valid acc: 0.3746\n",
      "Iter: 38100, train loss: 1.6548, train acc: 0.5000, valid loss: 2.0522, valid acc: 0.3677\n",
      "Iter: 38200, train loss: 1.5999, train acc: 0.4844, valid loss: 2.0467, valid acc: 0.3700\n",
      "Iter: 38300, train loss: 1.7338, train acc: 0.4688, valid loss: 2.0414, valid acc: 0.3692\n",
      "Iter: 38400, train loss: 1.7610, train acc: 0.4219, valid loss: 2.0444, valid acc: 0.3615\n",
      "Iter: 38500, train loss: 1.6018, train acc: 0.5000, valid loss: 2.0540, valid acc: 0.3638\n",
      "Iter: 38600, train loss: 1.5989, train acc: 0.5312, valid loss: 2.0257, valid acc: 0.3769\n",
      "Iter: 38700, train loss: 1.6805, train acc: 0.4531, valid loss: 2.0406, valid acc: 0.3646\n",
      "Iter: 38800, train loss: 1.6271, train acc: 0.4688, valid loss: 2.0405, valid acc: 0.3762\n",
      "Iter: 38900, train loss: 1.5065, train acc: 0.5469, valid loss: 2.0471, valid acc: 0.3777\n",
      "Iter: 39000, train loss: 1.6798, train acc: 0.5000, valid loss: 2.0453, valid acc: 0.3638\n",
      "Iter: 39100, train loss: 1.5756, train acc: 0.4844, valid loss: 2.0540, valid acc: 0.3615\n",
      "Iter: 39200, train loss: 1.3925, train acc: 0.5312, valid loss: 2.0539, valid acc: 0.3677\n",
      "Iter: 39300, train loss: 1.5941, train acc: 0.5312, valid loss: 2.0434, valid acc: 0.3700\n",
      "Iter: 39400, train loss: 1.5569, train acc: 0.4219, valid loss: 2.0511, valid acc: 0.3723\n",
      "Iter: 39500, train loss: 1.8041, train acc: 0.4219, valid loss: 2.0588, valid acc: 0.3715\n",
      "Iter: 39600, train loss: 1.7053, train acc: 0.4375, valid loss: 2.0389, valid acc: 0.3708\n",
      "Iter: 39700, train loss: 1.7825, train acc: 0.4375, valid loss: 2.0484, valid acc: 0.3769\n",
      "Iter: 39800, train loss: 1.3035, train acc: 0.5469, valid loss: 2.0589, valid acc: 0.3692\n",
      "Iter: 39900, train loss: 1.6715, train acc: 0.4375, valid loss: 2.0464, valid acc: 0.3623\n",
      "Iter: 40000, train loss: 1.5868, train acc: 0.4375, valid loss: 2.0586, valid acc: 0.3638\n",
      "Iter: 40100, train loss: 1.8486, train acc: 0.3906, valid loss: 2.0528, valid acc: 0.3638\n",
      "Iter: 40200, train loss: 1.5096, train acc: 0.5625, valid loss: 2.0325, valid acc: 0.3708\n",
      "Iter: 40300, train loss: 1.5928, train acc: 0.4844, valid loss: 2.0394, valid acc: 0.3631\n",
      "Iter: 40400, train loss: 1.7045, train acc: 0.4375, valid loss: 2.0524, valid acc: 0.3677\n",
      "Iter: 40500, train loss: 1.5289, train acc: 0.5000, valid loss: 2.0506, valid acc: 0.3646\n",
      "Iter: 40600, train loss: 1.6816, train acc: 0.4219, valid loss: 2.0349, valid acc: 0.3708\n",
      "Iter: 40700, train loss: 1.7191, train acc: 0.4531, valid loss: 2.0287, valid acc: 0.3731\n",
      "Iter: 40800, train loss: 1.6582, train acc: 0.3906, valid loss: 2.0501, valid acc: 0.3769\n",
      "Iter: 40900, train loss: 2.0418, train acc: 0.3125, valid loss: 2.0746, valid acc: 0.3700\n",
      "Iter: 41000, train loss: 1.5500, train acc: 0.4531, valid loss: 2.0506, valid acc: 0.3723\n",
      "Iter: 41100, train loss: 1.3809, train acc: 0.5000, valid loss: 2.0504, valid acc: 0.3677\n",
      "Iter: 41200, train loss: 1.7074, train acc: 0.4688, valid loss: 2.0546, valid acc: 0.3638\n",
      "Iter: 41300, train loss: 1.7465, train acc: 0.4375, valid loss: 2.0524, valid acc: 0.3585\n",
      "Iter: 41400, train loss: 1.7454, train acc: 0.3750, valid loss: 2.0630, valid acc: 0.3692\n",
      "Iter: 41500, train loss: 1.7402, train acc: 0.5156, valid loss: 2.0459, valid acc: 0.3585\n",
      "Iter: 41600, train loss: 1.9997, train acc: 0.3594, valid loss: 2.0656, valid acc: 0.3669\n",
      "Iter: 41700, train loss: 1.6814, train acc: 0.4062, valid loss: 2.0533, valid acc: 0.3677\n",
      "Iter: 41800, train loss: 1.8097, train acc: 0.4219, valid loss: 2.0651, valid acc: 0.3677\n",
      "Iter: 41900, train loss: 1.6059, train acc: 0.4531, valid loss: 2.0520, valid acc: 0.3708\n",
      "Iter: 42000, train loss: 1.3677, train acc: 0.5781, valid loss: 2.0382, valid acc: 0.3685\n",
      "Iter: 42100, train loss: 1.7378, train acc: 0.4062, valid loss: 2.0616, valid acc: 0.3685\n",
      "Iter: 42200, train loss: 1.6790, train acc: 0.4219, valid loss: 2.0642, valid acc: 0.3692\n",
      "Iter: 42300, train loss: 1.5824, train acc: 0.5469, valid loss: 2.0440, valid acc: 0.3685\n",
      "Iter: 42400, train loss: 1.6081, train acc: 0.4062, valid loss: 2.0475, valid acc: 0.3738\n",
      "Iter: 42500, train loss: 1.4981, train acc: 0.5312, valid loss: 2.0573, valid acc: 0.3769\n",
      "Iter: 42600, train loss: 1.5932, train acc: 0.5469, valid loss: 2.0644, valid acc: 0.3746\n",
      "Iter: 42700, train loss: 1.6822, train acc: 0.3750, valid loss: 2.0471, valid acc: 0.3746\n",
      "Iter: 42800, train loss: 1.5626, train acc: 0.4375, valid loss: 2.0521, valid acc: 0.3792\n",
      "Iter: 42900, train loss: 1.5551, train acc: 0.4375, valid loss: 2.0357, valid acc: 0.3685\n",
      "Iter: 43000, train loss: 1.7340, train acc: 0.3281, valid loss: 2.0545, valid acc: 0.3685\n",
      "Iter: 43100, train loss: 1.7877, train acc: 0.3594, valid loss: 2.0715, valid acc: 0.3708\n",
      "Iter: 43200, train loss: 1.2363, train acc: 0.6154, valid loss: 2.0712, valid acc: 0.3715\n",
      "Iter: 43300, train loss: 1.7113, train acc: 0.3906, valid loss: 2.0513, valid acc: 0.3838\n",
      "Iter: 43400, train loss: 1.6619, train acc: 0.4531, valid loss: 2.0514, valid acc: 0.3746\n",
      "Iter: 43500, train loss: 1.6611, train acc: 0.4531, valid loss: 2.0556, valid acc: 0.3700\n",
      "Iter: 43600, train loss: 1.6548, train acc: 0.4375, valid loss: 2.0694, valid acc: 0.3631\n",
      "Iter: 43700, train loss: 1.3715, train acc: 0.4844, valid loss: 2.0511, valid acc: 0.3692\n",
      "Iter: 43800, train loss: 1.5094, train acc: 0.4688, valid loss: 2.0598, valid acc: 0.3662\n",
      "Iter: 43900, train loss: 1.6974, train acc: 0.3906, valid loss: 2.0686, valid acc: 0.3685\n",
      "Iter: 44000, train loss: 1.4410, train acc: 0.5156, valid loss: 2.0616, valid acc: 0.3700\n",
      "Iter: 44100, train loss: 1.6517, train acc: 0.4531, valid loss: 2.0599, valid acc: 0.3608\n",
      "Iter: 44200, train loss: 1.4812, train acc: 0.5000, valid loss: 2.0783, valid acc: 0.3662\n",
      "Iter: 44300, train loss: 1.5475, train acc: 0.5000, valid loss: 2.0708, valid acc: 0.3723\n",
      "Iter: 44400, train loss: 1.4329, train acc: 0.5156, valid loss: 2.0767, valid acc: 0.3615\n",
      "Iter: 44500, train loss: 1.6440, train acc: 0.5156, valid loss: 2.0772, valid acc: 0.3577\n",
      "Iter: 44600, train loss: 1.6643, train acc: 0.4688, valid loss: 2.0709, valid acc: 0.3615\n",
      "Iter: 44700, train loss: 1.8044, train acc: 0.4531, valid loss: 2.0597, valid acc: 0.3738\n",
      "Iter: 44800, train loss: 1.5307, train acc: 0.5469, valid loss: 2.0726, valid acc: 0.3677\n",
      "Iter: 44900, train loss: 1.5688, train acc: 0.4375, valid loss: 2.0574, valid acc: 0.3708\n",
      "Iter: 45000, train loss: 1.5043, train acc: 0.5156, valid loss: 2.0673, valid acc: 0.3692\n",
      "Iter: 45100, train loss: 1.6975, train acc: 0.4531, valid loss: 2.0702, valid acc: 0.3762\n",
      "Iter: 45200, train loss: 1.7453, train acc: 0.3750, valid loss: 2.0656, valid acc: 0.3608\n",
      "Iter: 45300, train loss: 1.8414, train acc: 0.3125, valid loss: 2.0757, valid acc: 0.3646\n",
      "Iter: 45400, train loss: 1.6182, train acc: 0.4531, valid loss: 2.0691, valid acc: 0.3615\n",
      "Iter: 45500, train loss: 1.6287, train acc: 0.4531, valid loss: 2.0922, valid acc: 0.3669\n",
      "Iter: 45600, train loss: 1.4496, train acc: 0.4688, valid loss: 2.0907, valid acc: 0.3608\n",
      "Iter: 45700, train loss: 1.3451, train acc: 0.5781, valid loss: 2.0663, valid acc: 0.3700\n",
      "Iter: 45800, train loss: 1.5854, train acc: 0.4375, valid loss: 2.0524, valid acc: 0.3562\n",
      "Iter: 45900, train loss: 1.4891, train acc: 0.5781, valid loss: 2.0639, valid acc: 0.3608\n",
      "Iter: 46000, train loss: 1.6669, train acc: 0.4219, valid loss: 2.0667, valid acc: 0.3631\n",
      "Iter: 46100, train loss: 1.4768, train acc: 0.5156, valid loss: 2.0695, valid acc: 0.3585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 46200, train loss: 1.5307, train acc: 0.4531, valid loss: 2.0763, valid acc: 0.3585\n",
      "Iter: 46300, train loss: 1.6915, train acc: 0.4531, valid loss: 2.0832, valid acc: 0.3638\n",
      "Iter: 46400, train loss: 1.4956, train acc: 0.4062, valid loss: 2.0742, valid acc: 0.3677\n",
      "Iter: 46500, train loss: 1.6944, train acc: 0.4375, valid loss: 2.0754, valid acc: 0.3569\n",
      "Iter: 46600, train loss: 1.9042, train acc: 0.3594, valid loss: 2.0890, valid acc: 0.3592\n",
      "Iter: 46700, train loss: 1.6465, train acc: 0.4844, valid loss: 2.0995, valid acc: 0.3615\n",
      "Iter: 46800, train loss: 1.7711, train acc: 0.3594, valid loss: 2.0749, valid acc: 0.3677\n",
      "Iter: 46900, train loss: 1.8766, train acc: 0.4219, valid loss: 2.0728, valid acc: 0.3700\n",
      "Iter: 47000, train loss: 1.5832, train acc: 0.5000, valid loss: 2.0686, valid acc: 0.3615\n",
      "Iter: 47100, train loss: 1.4672, train acc: 0.4688, valid loss: 2.0613, valid acc: 0.3746\n",
      "Iter: 47200, train loss: 1.7326, train acc: 0.4844, valid loss: 2.0654, valid acc: 0.3615\n",
      "Iter: 47300, train loss: 1.4359, train acc: 0.5625, valid loss: 2.0649, valid acc: 0.3700\n",
      "Iter: 47400, train loss: 1.6140, train acc: 0.4688, valid loss: 2.0786, valid acc: 0.3631\n",
      "Iter: 47500, train loss: 1.6093, train acc: 0.4844, valid loss: 2.0815, valid acc: 0.3600\n",
      "Iter: 47600, train loss: 1.7837, train acc: 0.4219, valid loss: 2.0855, valid acc: 0.3638\n",
      "Iter: 47700, train loss: 1.6184, train acc: 0.5000, valid loss: 2.0621, valid acc: 0.3592\n",
      "Iter: 47800, train loss: 1.4870, train acc: 0.4688, valid loss: 2.0680, valid acc: 0.3662\n",
      "Iter: 47900, train loss: 1.5389, train acc: 0.6094, valid loss: 2.0773, valid acc: 0.3646\n",
      "Iter: 48000, train loss: 1.7169, train acc: 0.4844, valid loss: 2.0845, valid acc: 0.3569\n",
      "Iter: 48100, train loss: 1.5869, train acc: 0.4531, valid loss: 2.0969, valid acc: 0.3662\n",
      "Iter: 48200, train loss: 1.8728, train acc: 0.4062, valid loss: 2.0926, valid acc: 0.3600\n",
      "Iter: 48300, train loss: 1.5440, train acc: 0.5312, valid loss: 2.0777, valid acc: 0.3562\n",
      "Iter: 48400, train loss: 1.6454, train acc: 0.4375, valid loss: 2.0796, valid acc: 0.3669\n",
      "Iter: 48500, train loss: 1.9442, train acc: 0.3750, valid loss: 2.0934, valid acc: 0.3623\n",
      "Iter: 48600, train loss: 1.8825, train acc: 0.3750, valid loss: 2.0869, valid acc: 0.3600\n",
      "Iter: 48700, train loss: 1.6976, train acc: 0.3906, valid loss: 2.0729, valid acc: 0.3631\n",
      "Iter: 48800, train loss: 1.5119, train acc: 0.4375, valid loss: 2.0880, valid acc: 0.3623\n",
      "Iter: 48900, train loss: 2.0483, train acc: 0.3594, valid loss: 2.0904, valid acc: 0.3592\n",
      "Iter: 49000, train loss: 1.5165, train acc: 0.5156, valid loss: 2.0794, valid acc: 0.3562\n",
      "Iter: 49100, train loss: 1.8045, train acc: 0.4219, valid loss: 2.0886, valid acc: 0.3654\n",
      "Iter: 49200, train loss: 1.4847, train acc: 0.5469, valid loss: 2.0722, valid acc: 0.3631\n",
      "Iter: 49300, train loss: 1.7835, train acc: 0.4219, valid loss: 2.0923, valid acc: 0.3685\n",
      "Iter: 49400, train loss: 1.4968, train acc: 0.4844, valid loss: 2.0856, valid acc: 0.3600\n",
      "Iter: 49500, train loss: 1.8530, train acc: 0.4688, valid loss: 2.0703, valid acc: 0.3654\n",
      "Iter: 49600, train loss: 1.6594, train acc: 0.3438, valid loss: 2.0608, valid acc: 0.3623\n",
      "Iter: 49700, train loss: 1.4874, train acc: 0.4688, valid loss: 2.0900, valid acc: 0.3531\n",
      "Iter: 49800, train loss: 1.7056, train acc: 0.4375, valid loss: 2.0894, valid acc: 0.3600\n",
      "Iter: 49900, train loss: 1.6326, train acc: 0.5000, valid loss: 2.0804, valid acc: 0.3646\n",
      "Iter: 50000, train loss: 1.4443, train acc: 0.5312, valid loss: 2.1045, valid acc: 0.3677\n",
      "Iter: 50100, train loss: 1.7162, train acc: 0.4219, valid loss: 2.0884, valid acc: 0.3638\n",
      "Iter: 50200, train loss: 1.5327, train acc: 0.4844, valid loss: 2.0749, valid acc: 0.3615\n",
      "Iter: 50300, train loss: 1.6383, train acc: 0.5156, valid loss: 2.0941, valid acc: 0.3538\n",
      "Iter: 50400, train loss: 1.7395, train acc: 0.3906, valid loss: 2.0782, valid acc: 0.3585\n",
      "Iter: 50500, train loss: 1.6208, train acc: 0.4688, valid loss: 2.0860, valid acc: 0.3592\n",
      "Iter: 50600, train loss: 1.6609, train acc: 0.4375, valid loss: 2.0810, valid acc: 0.3608\n",
      "Iter: 50700, train loss: 1.5752, train acc: 0.4688, valid loss: 2.0882, valid acc: 0.3646\n",
      "Iter: 50800, train loss: 1.5148, train acc: 0.4375, valid loss: 2.0794, valid acc: 0.3623\n",
      "Iter: 50900, train loss: 1.7463, train acc: 0.4219, valid loss: 2.0867, valid acc: 0.3608\n",
      "Iter: 51000, train loss: 1.5481, train acc: 0.5312, valid loss: 2.0666, valid acc: 0.3646\n",
      "Iter: 51100, train loss: 1.4301, train acc: 0.5156, valid loss: 2.0758, valid acc: 0.3669\n",
      "Iter: 51200, train loss: 1.5324, train acc: 0.5156, valid loss: 2.0694, valid acc: 0.3646\n",
      "Iter: 51300, train loss: 1.3933, train acc: 0.5781, valid loss: 2.0993, valid acc: 0.3654\n",
      "Iter: 51400, train loss: 2.0485, train acc: 0.3750, valid loss: 2.1011, valid acc: 0.3654\n",
      "Iter: 51500, train loss: 1.5774, train acc: 0.5000, valid loss: 2.0890, valid acc: 0.3723\n",
      "Iter: 51600, train loss: 1.7405, train acc: 0.3438, valid loss: 2.0643, valid acc: 0.3654\n",
      "Iter: 51700, train loss: 1.5222, train acc: 0.5156, valid loss: 2.0828, valid acc: 0.3785\n",
      "Iter: 51800, train loss: 1.7422, train acc: 0.4375, valid loss: 2.0976, valid acc: 0.3631\n",
      "Iter: 51900, train loss: 1.5825, train acc: 0.4531, valid loss: 2.0779, valid acc: 0.3700\n",
      "Iter: 52000, train loss: 1.4667, train acc: 0.4375, valid loss: 2.0894, valid acc: 0.3654\n",
      "Iter: 52100, train loss: 1.8388, train acc: 0.4038, valid loss: 2.0784, valid acc: 0.3592\n",
      "Iter: 52200, train loss: 1.5790, train acc: 0.4531, valid loss: 2.0835, valid acc: 0.3623\n",
      "Iter: 52300, train loss: 1.7076, train acc: 0.4531, valid loss: 2.0894, valid acc: 0.3638\n",
      "Iter: 52400, train loss: 1.7059, train acc: 0.4062, valid loss: 2.0677, valid acc: 0.3654\n",
      "Iter: 52500, train loss: 1.7036, train acc: 0.3750, valid loss: 2.0854, valid acc: 0.3715\n",
      "Iter: 52600, train loss: 1.8609, train acc: 0.4808, valid loss: 2.0953, valid acc: 0.3615\n",
      "Iter: 52700, train loss: 1.4671, train acc: 0.5000, valid loss: 2.0792, valid acc: 0.3715\n",
      "Iter: 52800, train loss: 1.4809, train acc: 0.5312, valid loss: 2.0955, valid acc: 0.3692\n",
      "Iter: 52900, train loss: 1.6705, train acc: 0.4688, valid loss: 2.0816, valid acc: 0.3669\n",
      "Iter: 53000, train loss: 1.2851, train acc: 0.6250, valid loss: 2.0925, valid acc: 0.3631\n",
      "Iter: 53100, train loss: 1.7472, train acc: 0.3906, valid loss: 2.1045, valid acc: 0.3615\n",
      "Iter: 53200, train loss: 1.3984, train acc: 0.5781, valid loss: 2.0716, valid acc: 0.3654\n",
      "Iter: 53300, train loss: 1.6071, train acc: 0.4375, valid loss: 2.0760, valid acc: 0.3608\n",
      "Iter: 53400, train loss: 1.7403, train acc: 0.5000, valid loss: 2.0803, valid acc: 0.3646\n",
      "Iter: 53500, train loss: 1.6218, train acc: 0.4531, valid loss: 2.0976, valid acc: 0.3646\n",
      "Iter: 53600, train loss: 1.6605, train acc: 0.4688, valid loss: 2.0850, valid acc: 0.3708\n",
      "Iter: 53700, train loss: 1.4656, train acc: 0.5156, valid loss: 2.0864, valid acc: 0.3731\n",
      "Iter: 53800, train loss: 1.5308, train acc: 0.4375, valid loss: 2.0792, valid acc: 0.3692\n",
      "Iter: 53900, train loss: 1.5941, train acc: 0.4062, valid loss: 2.0890, valid acc: 0.3685\n",
      "Iter: 54000, train loss: 1.6860, train acc: 0.3906, valid loss: 2.0753, valid acc: 0.3669\n",
      "Iter: 54100, train loss: 1.6549, train acc: 0.4688, valid loss: 2.0975, valid acc: 0.3623\n",
      "Iter: 54200, train loss: 1.6114, train acc: 0.5000, valid loss: 2.1023, valid acc: 0.3654\n",
      "Iter: 54300, train loss: 1.7439, train acc: 0.3750, valid loss: 2.1089, valid acc: 0.3654\n",
      "Iter: 54400, train loss: 1.5405, train acc: 0.5000, valid loss: 2.0921, valid acc: 0.3731\n",
      "Iter: 54500, train loss: 1.6160, train acc: 0.3750, valid loss: 2.0982, valid acc: 0.3623\n",
      "Iter: 54600, train loss: 1.5544, train acc: 0.4688, valid loss: 2.0872, valid acc: 0.3638\n",
      "Iter: 54700, train loss: 1.4668, train acc: 0.4844, valid loss: 2.0773, valid acc: 0.3731\n",
      "Iter: 54800, train loss: 1.6626, train acc: 0.5000, valid loss: 2.0879, valid acc: 0.3631\n",
      "Iter: 54900, train loss: 1.5236, train acc: 0.5000, valid loss: 2.0839, valid acc: 0.3654\n",
      "Iter: 55000, train loss: 2.0347, train acc: 0.4038, valid loss: 2.0813, valid acc: 0.3646\n",
      "Iter: 55100, train loss: 1.7288, train acc: 0.4062, valid loss: 2.0922, valid acc: 0.3669\n",
      "Iter: 55200, train loss: 1.7556, train acc: 0.4062, valid loss: 2.0794, valid acc: 0.3685\n",
      "Iter: 55300, train loss: 1.5564, train acc: 0.5156, valid loss: 2.0815, valid acc: 0.3692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 55400, train loss: 1.5619, train acc: 0.5000, valid loss: 2.0853, valid acc: 0.3623\n",
      "Iter: 55500, train loss: 1.4395, train acc: 0.5000, valid loss: 2.0958, valid acc: 0.3592\n",
      "Iter: 55600, train loss: 1.3922, train acc: 0.5312, valid loss: 2.0870, valid acc: 0.3708\n",
      "Iter: 55700, train loss: 1.5410, train acc: 0.4688, valid loss: 2.0813, valid acc: 0.3708\n",
      "Iter: 55800, train loss: 1.7448, train acc: 0.3906, valid loss: 2.1101, valid acc: 0.3592\n",
      "Iter: 55900, train loss: 1.8070, train acc: 0.3906, valid loss: 2.0905, valid acc: 0.3646\n",
      "Iter: 56000, train loss: 1.4626, train acc: 0.5312, valid loss: 2.1040, valid acc: 0.3585\n",
      "Iter: 56100, train loss: 1.5152, train acc: 0.4844, valid loss: 2.0922, valid acc: 0.3677\n",
      "Iter: 56200, train loss: 1.5880, train acc: 0.4688, valid loss: 2.0877, valid acc: 0.3669\n",
      "Iter: 56300, train loss: 1.4628, train acc: 0.5000, valid loss: 2.0948, valid acc: 0.3569\n",
      "Iter: 56400, train loss: 1.5535, train acc: 0.4219, valid loss: 2.1086, valid acc: 0.3646\n",
      "Iter: 56500, train loss: 1.4880, train acc: 0.4844, valid loss: 2.0856, valid acc: 0.3608\n",
      "Iter: 56600, train loss: 1.7636, train acc: 0.3750, valid loss: 2.1188, valid acc: 0.3646\n",
      "Iter: 56700, train loss: 1.5131, train acc: 0.5469, valid loss: 2.1027, valid acc: 0.3592\n",
      "Iter: 56800, train loss: 1.6822, train acc: 0.4219, valid loss: 2.0888, valid acc: 0.3677\n",
      "Iter: 56900, train loss: 1.5121, train acc: 0.5000, valid loss: 2.0944, valid acc: 0.3631\n",
      "Iter: 57000, train loss: 1.4840, train acc: 0.5000, valid loss: 2.0924, valid acc: 0.3685\n",
      "Iter: 57100, train loss: 1.8135, train acc: 0.4062, valid loss: 2.0873, valid acc: 0.3692\n",
      "Iter: 57200, train loss: 1.6009, train acc: 0.4688, valid loss: 2.0882, valid acc: 0.3700\n",
      "Iter: 57300, train loss: 1.9628, train acc: 0.4219, valid loss: 2.1131, valid acc: 0.3554\n",
      "Iter: 57400, train loss: 1.6324, train acc: 0.5156, valid loss: 2.1023, valid acc: 0.3615\n",
      "Iter: 57500, train loss: 1.6409, train acc: 0.5469, valid loss: 2.0890, valid acc: 0.3608\n",
      "Iter: 57600, train loss: 1.7954, train acc: 0.5156, valid loss: 2.0934, valid acc: 0.3569\n",
      "Iter: 57700, train loss: 1.6761, train acc: 0.4531, valid loss: 2.1073, valid acc: 0.3677\n",
      "Iter: 57800, train loss: 1.5944, train acc: 0.4219, valid loss: 2.1020, valid acc: 0.3677\n",
      "Iter: 57900, train loss: 1.5090, train acc: 0.5156, valid loss: 2.1054, valid acc: 0.3600\n",
      "Iter: 58000, train loss: 1.6920, train acc: 0.4062, valid loss: 2.0999, valid acc: 0.3677\n",
      "Iter: 58100, train loss: 1.5147, train acc: 0.4844, valid loss: 2.1034, valid acc: 0.3608\n",
      "Iter: 58200, train loss: 1.7011, train acc: 0.4375, valid loss: 2.0946, valid acc: 0.3608\n",
      "Iter: 58300, train loss: 1.7955, train acc: 0.4531, valid loss: 2.0886, valid acc: 0.3677\n",
      "Iter: 58400, train loss: 1.6864, train acc: 0.4375, valid loss: 2.0878, valid acc: 0.3623\n",
      "Iter: 58500, train loss: 1.7384, train acc: 0.4219, valid loss: 2.0878, valid acc: 0.3754\n",
      "Iter: 58600, train loss: 1.5855, train acc: 0.3750, valid loss: 2.0876, valid acc: 0.3631\n",
      "Iter: 58700, train loss: 1.4641, train acc: 0.5469, valid loss: 2.1068, valid acc: 0.3592\n",
      "Iter: 58800, train loss: 1.7805, train acc: 0.4844, valid loss: 2.0935, valid acc: 0.3562\n",
      "Iter: 58900, train loss: 1.4666, train acc: 0.4219, valid loss: 2.0901, valid acc: 0.3577\n",
      "Iter: 59000, train loss: 1.4874, train acc: 0.4531, valid loss: 2.0954, valid acc: 0.3638\n",
      "Iter: 59100, train loss: 1.5193, train acc: 0.5625, valid loss: 2.0800, valid acc: 0.3631\n",
      "Iter: 59200, train loss: 1.4119, train acc: 0.5469, valid loss: 2.1024, valid acc: 0.3646\n",
      "Iter: 59300, train loss: 1.7128, train acc: 0.4844, valid loss: 2.0986, valid acc: 0.3600\n",
      "Iter: 59400, train loss: 1.8259, train acc: 0.4219, valid loss: 2.0792, valid acc: 0.3638\n",
      "Iter: 59500, train loss: 1.4868, train acc: 0.5000, valid loss: 2.1096, valid acc: 0.3662\n",
      "Iter: 59600, train loss: 1.5247, train acc: 0.5469, valid loss: 2.1014, valid acc: 0.3600\n",
      "Iter: 59700, train loss: 1.8677, train acc: 0.4062, valid loss: 2.1177, valid acc: 0.3631\n",
      "Iter: 59800, train loss: 1.5553, train acc: 0.5312, valid loss: 2.1080, valid acc: 0.3662\n",
      "Iter: 59900, train loss: 1.4951, train acc: 0.4844, valid loss: 2.1017, valid acc: 0.3608\n",
      "Iter: 60000, train loss: 1.7994, train acc: 0.4688, valid loss: 2.1043, valid acc: 0.3669\n",
      "Iter: 60100, train loss: 1.6552, train acc: 0.5000, valid loss: 2.0931, valid acc: 0.3554\n",
      "Iter: 60200, train loss: 1.3479, train acc: 0.5781, valid loss: 2.0933, valid acc: 0.3623\n",
      "Iter: 60300, train loss: 1.7374, train acc: 0.4219, valid loss: 2.1120, valid acc: 0.3592\n",
      "Iter: 60400, train loss: 1.6522, train acc: 0.4062, valid loss: 2.1066, valid acc: 0.3592\n",
      "Iter: 60500, train loss: 1.5059, train acc: 0.4688, valid loss: 2.1078, valid acc: 0.3508\n",
      "Iter: 60600, train loss: 1.5166, train acc: 0.4844, valid loss: 2.1120, valid acc: 0.3646\n",
      "Iter: 60700, train loss: 1.3172, train acc: 0.5000, valid loss: 2.1024, valid acc: 0.3638\n",
      "Iter: 60800, train loss: 1.6036, train acc: 0.5000, valid loss: 2.1192, valid acc: 0.3631\n",
      "Iter: 60900, train loss: 1.5542, train acc: 0.5156, valid loss: 2.0984, valid acc: 0.3638\n",
      "Iter: 61000, train loss: 1.5062, train acc: 0.5000, valid loss: 2.1130, valid acc: 0.3638\n",
      "Iter: 61100, train loss: 1.7578, train acc: 0.4219, valid loss: 2.1093, valid acc: 0.3608\n",
      "Iter: 61200, train loss: 1.5324, train acc: 0.5000, valid loss: 2.1048, valid acc: 0.3654\n",
      "Iter: 61300, train loss: 1.8090, train acc: 0.3438, valid loss: 2.1106, valid acc: 0.3608\n",
      "Iter: 61400, train loss: 1.5723, train acc: 0.4062, valid loss: 2.1026, valid acc: 0.3577\n",
      "Iter: 61500, train loss: 1.5979, train acc: 0.4531, valid loss: 2.1140, valid acc: 0.3592\n",
      "Iter: 61600, train loss: 1.6335, train acc: 0.4844, valid loss: 2.1251, valid acc: 0.3600\n",
      "Iter: 61700, train loss: 1.6162, train acc: 0.4844, valid loss: 2.0853, valid acc: 0.3677\n",
      "Iter: 61800, train loss: 1.5890, train acc: 0.3906, valid loss: 2.1128, valid acc: 0.3685\n",
      "Iter: 61900, train loss: 1.3057, train acc: 0.5781, valid loss: 2.1053, valid acc: 0.3577\n",
      "Iter: 62000, train loss: 1.7288, train acc: 0.3750, valid loss: 2.1018, valid acc: 0.3600\n",
      "Iter: 62100, train loss: 1.3899, train acc: 0.5312, valid loss: 2.1163, valid acc: 0.3654\n",
      "Iter: 62200, train loss: 1.3197, train acc: 0.5781, valid loss: 2.1153, valid acc: 0.3600\n",
      "Iter: 62300, train loss: 1.4947, train acc: 0.5156, valid loss: 2.1012, valid acc: 0.3608\n",
      "Iter: 62400, train loss: 1.6839, train acc: 0.4688, valid loss: 2.0955, valid acc: 0.3515\n",
      "Iter: 62500, train loss: 1.4249, train acc: 0.4844, valid loss: 2.1078, valid acc: 0.3615\n",
      "Iter: 62600, train loss: 1.3377, train acc: 0.5469, valid loss: 2.0961, valid acc: 0.3615\n",
      "Iter: 62700, train loss: 1.9950, train acc: 0.3281, valid loss: 2.1213, valid acc: 0.3492\n",
      "Iter: 62800, train loss: 1.6391, train acc: 0.4688, valid loss: 2.1165, valid acc: 0.3554\n",
      "Iter: 62900, train loss: 1.7033, train acc: 0.5000, valid loss: 2.1225, valid acc: 0.3585\n",
      "Iter: 63000, train loss: 1.4863, train acc: 0.4531, valid loss: 2.1065, valid acc: 0.3592\n",
      "Iter: 63100, train loss: 1.8124, train acc: 0.4062, valid loss: 2.1054, valid acc: 0.3515\n",
      "Iter: 63200, train loss: 1.6085, train acc: 0.4688, valid loss: 2.1109, valid acc: 0.3554\n",
      "Iter: 63300, train loss: 1.8117, train acc: 0.3750, valid loss: 2.1102, valid acc: 0.3654\n",
      "Iter: 63400, train loss: 1.2679, train acc: 0.5938, valid loss: 2.1153, valid acc: 0.3615\n",
      "Iter: 63500, train loss: 1.7390, train acc: 0.4375, valid loss: 2.1101, valid acc: 0.3554\n",
      "Iter: 63600, train loss: 1.3857, train acc: 0.5781, valid loss: 2.0884, valid acc: 0.3654\n",
      "Iter: 63700, train loss: 1.3743, train acc: 0.5781, valid loss: 2.1025, valid acc: 0.3631\n",
      "Iter: 63800, train loss: 1.4927, train acc: 0.4844, valid loss: 2.1110, valid acc: 0.3608\n",
      "Iter: 63900, train loss: 1.2957, train acc: 0.5469, valid loss: 2.1184, valid acc: 0.3638\n",
      "Iter: 64000, train loss: 1.7589, train acc: 0.3438, valid loss: 2.1161, valid acc: 0.3585\n",
      "Iter: 64100, train loss: 1.6624, train acc: 0.5000, valid loss: 2.1015, valid acc: 0.3523\n",
      "Iter: 64200, train loss: 1.3882, train acc: 0.5469, valid loss: 2.1098, valid acc: 0.3554\n",
      "Iter: 64300, train loss: 1.6133, train acc: 0.4844, valid loss: 2.1101, valid acc: 0.3546\n",
      "Iter: 64400, train loss: 1.5480, train acc: 0.4531, valid loss: 2.1259, valid acc: 0.3615\n",
      "Iter: 64500, train loss: 1.4248, train acc: 0.4688, valid loss: 2.1076, valid acc: 0.3608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 64600, train loss: 1.5064, train acc: 0.4844, valid loss: 2.1057, valid acc: 0.3623\n",
      "Iter: 64700, train loss: 1.6529, train acc: 0.4219, valid loss: 2.1173, valid acc: 0.3531\n",
      "Iter: 64800, train loss: 1.4945, train acc: 0.4531, valid loss: 2.1103, valid acc: 0.3569\n",
      "Iter: 64900, train loss: 1.6431, train acc: 0.5000, valid loss: 2.1177, valid acc: 0.3562\n",
      "Iter: 65000, train loss: 1.3383, train acc: 0.4844, valid loss: 2.1051, valid acc: 0.3623\n",
      "Iter: 65100, train loss: 1.4374, train acc: 0.5469, valid loss: 2.1226, valid acc: 0.3723\n",
      "Iter: 65200, train loss: 1.6495, train acc: 0.4531, valid loss: 2.1232, valid acc: 0.3646\n",
      "Iter: 65300, train loss: 1.5011, train acc: 0.4688, valid loss: 2.1232, valid acc: 0.3531\n",
      "Iter: 65400, train loss: 1.5925, train acc: 0.4062, valid loss: 2.1230, valid acc: 0.3562\n",
      "Iter: 65500, train loss: 1.5991, train acc: 0.4375, valid loss: 2.1145, valid acc: 0.3515\n",
      "Iter: 65600, train loss: 1.5215, train acc: 0.4844, valid loss: 2.1086, valid acc: 0.3646\n",
      "Iter: 65700, train loss: 1.3797, train acc: 0.5156, valid loss: 2.1149, valid acc: 0.3600\n",
      "Iter: 65800, train loss: 1.6419, train acc: 0.4375, valid loss: 2.1255, valid acc: 0.3592\n",
      "Iter: 65900, train loss: 1.2783, train acc: 0.6094, valid loss: 2.1227, valid acc: 0.3600\n",
      "Iter: 66000, train loss: 1.5824, train acc: 0.4844, valid loss: 2.1136, valid acc: 0.3531\n",
      "Iter: 66100, train loss: 1.3658, train acc: 0.5469, valid loss: 2.1203, valid acc: 0.3554\n",
      "Iter: 66200, train loss: 1.5280, train acc: 0.5469, valid loss: 2.1044, valid acc: 0.3638\n",
      "Iter: 66300, train loss: 1.5849, train acc: 0.4375, valid loss: 2.1161, valid acc: 0.3738\n",
      "Iter: 66400, train loss: 1.6829, train acc: 0.4688, valid loss: 2.1193, valid acc: 0.3631\n",
      "Iter: 66500, train loss: 1.6941, train acc: 0.4062, valid loss: 2.1054, valid acc: 0.3554\n",
      "Iter: 66600, train loss: 1.8682, train acc: 0.3906, valid loss: 2.1090, valid acc: 0.3638\n",
      "Iter: 66700, train loss: 1.4879, train acc: 0.4062, valid loss: 2.1013, valid acc: 0.3500\n",
      "Iter: 66800, train loss: 1.5152, train acc: 0.4219, valid loss: 2.1106, valid acc: 0.3515\n",
      "Iter: 66900, train loss: 1.7580, train acc: 0.4062, valid loss: 2.1129, valid acc: 0.3562\n",
      "Iter: 67000, train loss: 1.6877, train acc: 0.4375, valid loss: 2.1202, valid acc: 0.3546\n",
      "Iter: 67100, train loss: 1.7232, train acc: 0.4531, valid loss: 2.1081, valid acc: 0.3646\n",
      "Iter: 67200, train loss: 1.6297, train acc: 0.5156, valid loss: 2.1073, valid acc: 0.3538\n",
      "Iter: 67300, train loss: 1.6574, train acc: 0.4531, valid loss: 2.1145, valid acc: 0.3554\n",
      "Iter: 67400, train loss: 1.5258, train acc: 0.5000, valid loss: 2.0926, valid acc: 0.3562\n",
      "Iter: 67500, train loss: 1.3643, train acc: 0.5156, valid loss: 2.1124, valid acc: 0.3662\n",
      "Iter: 67600, train loss: 1.6765, train acc: 0.4531, valid loss: 2.0977, valid acc: 0.3631\n",
      "Iter: 67700, train loss: 1.6891, train acc: 0.4688, valid loss: 2.1064, valid acc: 0.3585\n",
      "Iter: 67800, train loss: 1.7174, train acc: 0.4688, valid loss: 2.1173, valid acc: 0.3569\n",
      "Iter: 67900, train loss: 1.4946, train acc: 0.4375, valid loss: 2.1152, valid acc: 0.3623\n",
      "Iter: 68000, train loss: 1.6349, train acc: 0.4844, valid loss: 2.1240, valid acc: 0.3546\n",
      "Iter: 68100, train loss: 1.8003, train acc: 0.4219, valid loss: 2.1260, valid acc: 0.3585\n",
      "Iter: 68200, train loss: 1.5869, train acc: 0.3906, valid loss: 2.1083, valid acc: 0.3654\n",
      "Iter: 68300, train loss: 1.5885, train acc: 0.4844, valid loss: 2.1268, valid acc: 0.3638\n",
      "Iter: 68400, train loss: 1.5458, train acc: 0.4844, valid loss: 2.1086, valid acc: 0.3654\n",
      "Iter: 68500, train loss: 1.2544, train acc: 0.6094, valid loss: 2.1024, valid acc: 0.3638\n",
      "Iter: 68600, train loss: 1.6517, train acc: 0.4062, valid loss: 2.1186, valid acc: 0.3569\n",
      "Iter: 68700, train loss: 1.5104, train acc: 0.5000, valid loss: 2.1266, valid acc: 0.3546\n",
      "Iter: 68800, train loss: 1.6047, train acc: 0.4062, valid loss: 2.1196, valid acc: 0.3654\n",
      "Iter: 68900, train loss: 1.5009, train acc: 0.5156, valid loss: 2.1192, valid acc: 0.3562\n",
      "Iter: 69000, train loss: 1.5312, train acc: 0.4375, valid loss: 2.1180, valid acc: 0.3669\n",
      "Iter: 69100, train loss: 1.9038, train acc: 0.3654, valid loss: 2.1251, valid acc: 0.3654\n",
      "Iter: 69200, train loss: 1.7196, train acc: 0.4844, valid loss: 2.1184, valid acc: 0.3646\n",
      "Iter: 69300, train loss: 1.4038, train acc: 0.5312, valid loss: 2.1324, valid acc: 0.3515\n",
      "Iter: 69400, train loss: 1.3646, train acc: 0.5312, valid loss: 2.1097, valid acc: 0.3569\n",
      "Iter: 69500, train loss: 1.5072, train acc: 0.4375, valid loss: 2.1165, valid acc: 0.3508\n",
      "Iter: 69600, train loss: 1.4281, train acc: 0.5156, valid loss: 2.1153, valid acc: 0.3562\n",
      "Iter: 69700, train loss: 1.5271, train acc: 0.4688, valid loss: 2.1200, valid acc: 0.3592\n",
      "Iter: 69800, train loss: 1.4453, train acc: 0.4688, valid loss: 2.1056, valid acc: 0.3615\n",
      "Iter: 69900, train loss: 1.5490, train acc: 0.4844, valid loss: 2.1594, valid acc: 0.3615\n",
      "Iter: 70000, train loss: 1.4521, train acc: 0.5938, valid loss: 2.1402, valid acc: 0.3569\n",
      "Iter: 70100, train loss: 1.4855, train acc: 0.5000, valid loss: 2.1125, valid acc: 0.3585\n",
      "Iter: 70200, train loss: 1.5058, train acc: 0.5469, valid loss: 2.1067, valid acc: 0.3592\n",
      "Iter: 70300, train loss: 1.2663, train acc: 0.5625, valid loss: 2.1072, valid acc: 0.3654\n",
      "Iter: 70400, train loss: 1.8931, train acc: 0.3906, valid loss: 2.1020, valid acc: 0.3662\n",
      "Iter: 70500, train loss: 1.7720, train acc: 0.4688, valid loss: 2.1019, valid acc: 0.3677\n",
      "Iter: 70600, train loss: 1.4896, train acc: 0.5000, valid loss: 2.1248, valid acc: 0.3546\n",
      "Iter: 70700, train loss: 1.8678, train acc: 0.3281, valid loss: 2.1090, valid acc: 0.3669\n",
      "Iter: 70800, train loss: 1.7811, train acc: 0.3125, valid loss: 2.1239, valid acc: 0.3669\n",
      "Iter: 70900, train loss: 1.3003, train acc: 0.5469, valid loss: 2.1293, valid acc: 0.3538\n",
      "Iter: 71000, train loss: 1.8127, train acc: 0.4375, valid loss: 2.1412, valid acc: 0.3500\n",
      "Iter: 71100, train loss: 1.5300, train acc: 0.5000, valid loss: 2.1236, valid acc: 0.3546\n",
      "Iter: 71200, train loss: 1.4832, train acc: 0.4531, valid loss: 2.1134, valid acc: 0.3562\n",
      "Iter: 71300, train loss: 1.2312, train acc: 0.6406, valid loss: 2.1192, valid acc: 0.3646\n",
      "Iter: 71400, train loss: 1.4710, train acc: 0.4844, valid loss: 2.1270, valid acc: 0.3577\n",
      "Iter: 71500, train loss: 1.7392, train acc: 0.4219, valid loss: 2.1238, valid acc: 0.3646\n",
      "Iter: 71600, train loss: 1.5688, train acc: 0.4688, valid loss: 2.1245, valid acc: 0.3662\n",
      "Iter: 71700, train loss: 1.5534, train acc: 0.5156, valid loss: 2.1221, valid acc: 0.3685\n",
      "Iter: 71800, train loss: 1.6038, train acc: 0.3906, valid loss: 2.1388, valid acc: 0.3615\n",
      "Iter: 71900, train loss: 1.6934, train acc: 0.3906, valid loss: 2.1167, valid acc: 0.3654\n",
      "Iter: 72000, train loss: 1.4890, train acc: 0.5156, valid loss: 2.1247, valid acc: 0.3700\n",
      "Iter: 72100, train loss: 1.5641, train acc: 0.4688, valid loss: 2.1272, valid acc: 0.3592\n",
      "Iter: 72200, train loss: 1.4189, train acc: 0.5156, valid loss: 2.1138, valid acc: 0.3600\n",
      "Iter: 72300, train loss: 1.9620, train acc: 0.3906, valid loss: 2.1167, valid acc: 0.3585\n",
      "Iter: 72400, train loss: 1.4874, train acc: 0.5312, valid loss: 2.1284, valid acc: 0.3562\n",
      "Iter: 72500, train loss: 1.8463, train acc: 0.3906, valid loss: 2.1366, valid acc: 0.3631\n",
      "Iter: 72600, train loss: 1.5165, train acc: 0.4844, valid loss: 2.1011, valid acc: 0.3608\n",
      "Iter: 72700, train loss: 1.6254, train acc: 0.5312, valid loss: 2.1261, valid acc: 0.3523\n",
      "Iter: 72800, train loss: 1.5560, train acc: 0.4844, valid loss: 2.1371, valid acc: 0.3523\n",
      "Iter: 72900, train loss: 1.6086, train acc: 0.5000, valid loss: 2.1156, valid acc: 0.3654\n",
      "Iter: 73000, train loss: 1.5137, train acc: 0.5156, valid loss: 2.1209, valid acc: 0.3685\n",
      "Iter: 73100, train loss: 1.9102, train acc: 0.3438, valid loss: 2.1271, valid acc: 0.3685\n",
      "Iter: 73200, train loss: 1.7862, train acc: 0.3438, valid loss: 2.1068, valid acc: 0.3677\n",
      "Iter: 73300, train loss: 1.5235, train acc: 0.4844, valid loss: 2.1165, valid acc: 0.3654\n",
      "Iter: 73400, train loss: 1.5704, train acc: 0.4531, valid loss: 2.1364, valid acc: 0.3592\n",
      "Iter: 73500, train loss: 1.6490, train acc: 0.4531, valid loss: 2.1282, valid acc: 0.3646\n",
      "Iter: 73600, train loss: 1.3649, train acc: 0.5312, valid loss: 2.1248, valid acc: 0.3669\n",
      "Iter: 73700, train loss: 1.5210, train acc: 0.4062, valid loss: 2.1275, valid acc: 0.3654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 73800, train loss: 1.5757, train acc: 0.4531, valid loss: 2.1289, valid acc: 0.3592\n",
      "Iter: 73900, train loss: 1.6849, train acc: 0.5000, valid loss: 2.1196, valid acc: 0.3646\n",
      "Iter: 74000, train loss: 1.8164, train acc: 0.4062, valid loss: 2.1137, valid acc: 0.3623\n",
      "Iter: 74100, train loss: 1.3983, train acc: 0.6250, valid loss: 2.1211, valid acc: 0.3677\n",
      "Iter: 74200, train loss: 1.8826, train acc: 0.4038, valid loss: 2.1335, valid acc: 0.3615\n",
      "Iter: 74300, train loss: 1.4870, train acc: 0.5469, valid loss: 2.1270, valid acc: 0.3600\n",
      "Iter: 74400, train loss: 1.4099, train acc: 0.4688, valid loss: 2.1285, valid acc: 0.3623\n",
      "Iter: 74500, train loss: 1.6641, train acc: 0.4688, valid loss: 2.1407, valid acc: 0.3585\n",
      "Iter: 74600, train loss: 1.7824, train acc: 0.4219, valid loss: 2.1226, valid acc: 0.3538\n",
      "Iter: 74700, train loss: 1.4376, train acc: 0.5312, valid loss: 2.1360, valid acc: 0.3600\n",
      "Iter: 74800, train loss: 1.5448, train acc: 0.4688, valid loss: 2.1344, valid acc: 0.3685\n",
      "Iter: 74900, train loss: 1.5572, train acc: 0.4844, valid loss: 2.1308, valid acc: 0.3638\n",
      "Iter: 75000, train loss: 1.5137, train acc: 0.4688, valid loss: 2.1201, valid acc: 0.3692\n",
      "Iter: 75100, train loss: 1.6495, train acc: 0.4531, valid loss: 2.1193, valid acc: 0.3715\n",
      "Iter: 75200, train loss: 1.6494, train acc: 0.4219, valid loss: 2.1569, valid acc: 0.3646\n",
      "Iter: 75300, train loss: 1.3349, train acc: 0.5469, valid loss: 2.1209, valid acc: 0.3646\n",
      "Iter: 75400, train loss: 1.4904, train acc: 0.4531, valid loss: 2.1275, valid acc: 0.3608\n",
      "Iter: 75500, train loss: 1.2387, train acc: 0.5312, valid loss: 2.1265, valid acc: 0.3585\n",
      "Iter: 75600, train loss: 1.5759, train acc: 0.5312, valid loss: 2.1151, valid acc: 0.3608\n",
      "Iter: 75700, train loss: 1.5983, train acc: 0.4375, valid loss: 2.1281, valid acc: 0.3631\n",
      "Iter: 75800, train loss: 1.4127, train acc: 0.5000, valid loss: 2.1190, valid acc: 0.3546\n",
      "Iter: 75900, train loss: 1.3987, train acc: 0.5156, valid loss: 2.1289, valid acc: 0.3615\n",
      "Iter: 76000, train loss: 1.5295, train acc: 0.4375, valid loss: 2.1278, valid acc: 0.3646\n",
      "Iter: 76100, train loss: 1.2608, train acc: 0.5938, valid loss: 2.1328, valid acc: 0.3538\n",
      "Iter: 76200, train loss: 1.4887, train acc: 0.4688, valid loss: 2.1325, valid acc: 0.3646\n",
      "Iter: 76300, train loss: 1.5449, train acc: 0.4688, valid loss: 2.1181, valid acc: 0.3577\n",
      "Iter: 76400, train loss: 1.3750, train acc: 0.5312, valid loss: 2.1392, valid acc: 0.3562\n",
      "Iter: 76500, train loss: 1.4135, train acc: 0.5000, valid loss: 2.1409, valid acc: 0.3631\n",
      "Iter: 76600, train loss: 1.4906, train acc: 0.4844, valid loss: 2.1482, valid acc: 0.3469\n",
      "Iter: 76700, train loss: 1.3380, train acc: 0.4844, valid loss: 2.1507, valid acc: 0.3577\n",
      "Iter: 76800, train loss: 1.7221, train acc: 0.3438, valid loss: 2.1390, valid acc: 0.3669\n",
      "Iter: 76900, train loss: 1.5313, train acc: 0.5469, valid loss: 2.1146, valid acc: 0.3608\n",
      "Iter: 77000, train loss: 1.5129, train acc: 0.4531, valid loss: 2.1167, valid acc: 0.3569\n",
      "Iter: 77100, train loss: 1.4529, train acc: 0.5000, valid loss: 2.1334, valid acc: 0.3600\n",
      "Iter: 77200, train loss: 1.3584, train acc: 0.5312, valid loss: 2.1352, valid acc: 0.3569\n",
      "Iter: 77300, train loss: 1.6165, train acc: 0.4531, valid loss: 2.1461, valid acc: 0.3585\n",
      "Iter: 77400, train loss: 1.5887, train acc: 0.4844, valid loss: 2.1287, valid acc: 0.3662\n",
      "Iter: 77500, train loss: 1.2969, train acc: 0.5938, valid loss: 2.1471, valid acc: 0.3554\n",
      "Iter: 77600, train loss: 1.5808, train acc: 0.4375, valid loss: 2.1359, valid acc: 0.3523\n",
      "Iter: 77700, train loss: 1.7554, train acc: 0.4844, valid loss: 2.1424, valid acc: 0.3585\n",
      "Iter: 77800, train loss: 1.7411, train acc: 0.4375, valid loss: 2.1486, valid acc: 0.3538\n",
      "Iter: 77900, train loss: 1.5092, train acc: 0.4844, valid loss: 2.1443, valid acc: 0.3569\n",
      "Iter: 78000, train loss: 1.5120, train acc: 0.5156, valid loss: 2.1465, valid acc: 0.3600\n",
      "Iter: 78100, train loss: 1.7829, train acc: 0.3750, valid loss: 2.1173, valid acc: 0.3638\n",
      "Iter: 78200, train loss: 1.4626, train acc: 0.5469, valid loss: 2.1344, valid acc: 0.3569\n",
      "Iter: 78300, train loss: 1.5941, train acc: 0.4688, valid loss: 2.1362, valid acc: 0.3638\n",
      "Iter: 78400, train loss: 1.5043, train acc: 0.4688, valid loss: 2.1360, valid acc: 0.3569\n",
      "Iter: 78500, train loss: 1.9172, train acc: 0.3594, valid loss: 2.1344, valid acc: 0.3669\n",
      "Iter: 78600, train loss: 1.4860, train acc: 0.5156, valid loss: 2.1221, valid acc: 0.3615\n",
      "Iter: 78700, train loss: 1.6185, train acc: 0.4219, valid loss: 2.1241, valid acc: 0.3646\n",
      "Iter: 78800, train loss: 1.7219, train acc: 0.4844, valid loss: 2.1360, valid acc: 0.3615\n",
      "Iter: 78900, train loss: 1.4903, train acc: 0.5156, valid loss: 2.1379, valid acc: 0.3546\n",
      "Iter: 79000, train loss: 1.7795, train acc: 0.4531, valid loss: 2.1543, valid acc: 0.3554\n",
      "Iter: 79100, train loss: 1.3380, train acc: 0.5781, valid loss: 2.1406, valid acc: 0.3600\n",
      "Iter: 79200, train loss: 1.4075, train acc: 0.5625, valid loss: 2.1294, valid acc: 0.3654\n",
      "Iter: 79300, train loss: 1.6738, train acc: 0.4219, valid loss: 2.1409, valid acc: 0.3623\n",
      "Iter: 79400, train loss: 1.4953, train acc: 0.5312, valid loss: 2.1295, valid acc: 0.3646\n",
      "Iter: 79500, train loss: 1.6590, train acc: 0.4219, valid loss: 2.1497, valid acc: 0.3538\n",
      "Iter: 79600, train loss: 1.5324, train acc: 0.4219, valid loss: 2.1509, valid acc: 0.3638\n",
      "Iter: 79700, train loss: 1.4013, train acc: 0.5000, valid loss: 2.1385, valid acc: 0.3585\n",
      "Iter: 79800, train loss: 1.7595, train acc: 0.4062, valid loss: 2.1313, valid acc: 0.3646\n",
      "Iter: 79900, train loss: 1.5765, train acc: 0.4375, valid loss: 2.1594, valid acc: 0.3585\n",
      "Iter: 80000, train loss: 1.3688, train acc: 0.5938, valid loss: 2.1581, valid acc: 0.3600\n",
      "Iter: 80100, train loss: 1.4749, train acc: 0.4688, valid loss: 2.1561, valid acc: 0.3569\n",
      "Iter: 80200, train loss: 1.5168, train acc: 0.4219, valid loss: 2.1289, valid acc: 0.3592\n",
      "Iter: 80300, train loss: 1.5521, train acc: 0.4375, valid loss: 2.1618, valid acc: 0.3608\n",
      "Iter: 80400, train loss: 1.4744, train acc: 0.5781, valid loss: 2.1433, valid acc: 0.3600\n",
      "Iter: 80500, train loss: 1.6762, train acc: 0.4844, valid loss: 2.1454, valid acc: 0.3531\n",
      "Iter: 80600, train loss: 1.6880, train acc: 0.5000, valid loss: 2.1505, valid acc: 0.3585\n",
      "Iter: 80700, train loss: 1.7260, train acc: 0.5156, valid loss: 2.1292, valid acc: 0.3554\n",
      "Iter: 80800, train loss: 1.4489, train acc: 0.5625, valid loss: 2.1416, valid acc: 0.3592\n",
      "Iter: 80900, train loss: 1.7007, train acc: 0.4688, valid loss: 2.1413, valid acc: 0.3585\n",
      "Iter: 81000, train loss: 1.6108, train acc: 0.4219, valid loss: 2.1481, valid acc: 0.3554\n",
      "Iter: 81100, train loss: 1.5498, train acc: 0.4688, valid loss: 2.1391, valid acc: 0.3669\n",
      "Iter: 81200, train loss: 1.5007, train acc: 0.4219, valid loss: 2.1496, valid acc: 0.3623\n",
      "Iter: 81300, train loss: 1.5977, train acc: 0.5000, valid loss: 2.1305, valid acc: 0.3592\n",
      "Iter: 81400, train loss: 1.1847, train acc: 0.5938, valid loss: 2.1388, valid acc: 0.3631\n",
      "Iter: 81500, train loss: 1.7706, train acc: 0.4062, valid loss: 2.1448, valid acc: 0.3554\n",
      "Iter: 81600, train loss: 1.5388, train acc: 0.5312, valid loss: 2.1149, valid acc: 0.3600\n",
      "Iter: 81700, train loss: 1.5100, train acc: 0.5000, valid loss: 2.1381, valid acc: 0.3608\n",
      "Iter: 81800, train loss: 1.5865, train acc: 0.5000, valid loss: 2.1431, valid acc: 0.3562\n",
      "Iter: 81900, train loss: 1.5321, train acc: 0.5000, valid loss: 2.1450, valid acc: 0.3577\n",
      "Iter: 82000, train loss: 1.7349, train acc: 0.4062, valid loss: 2.1554, valid acc: 0.3585\n",
      "Iter: 82100, train loss: 1.2534, train acc: 0.5781, valid loss: 2.1467, valid acc: 0.3615\n",
      "Iter: 82200, train loss: 1.6460, train acc: 0.4688, valid loss: 2.1468, valid acc: 0.3608\n",
      "Iter: 82300, train loss: 1.4402, train acc: 0.5781, valid loss: 2.1335, valid acc: 0.3585\n",
      "Iter: 82400, train loss: 1.7334, train acc: 0.4688, valid loss: 2.1351, valid acc: 0.3646\n",
      "Iter: 82500, train loss: 1.4472, train acc: 0.5156, valid loss: 2.1455, valid acc: 0.3515\n",
      "Iter: 82600, train loss: 1.6620, train acc: 0.4375, valid loss: 2.1264, valid acc: 0.3615\n",
      "Iter: 82700, train loss: 1.5383, train acc: 0.4844, valid loss: 2.1296, valid acc: 0.3646\n",
      "Iter: 82800, train loss: 1.6496, train acc: 0.5000, valid loss: 2.1330, valid acc: 0.3700\n",
      "Iter: 82900, train loss: 1.5021, train acc: 0.5312, valid loss: 2.1281, valid acc: 0.3638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 83000, train loss: 1.4160, train acc: 0.5156, valid loss: 2.1414, valid acc: 0.3662\n",
      "Iter: 83100, train loss: 1.5555, train acc: 0.4688, valid loss: 2.1356, valid acc: 0.3608\n",
      "Iter: 83200, train loss: 1.4448, train acc: 0.5000, valid loss: 2.1314, valid acc: 0.3638\n",
      "Iter: 83300, train loss: 1.7113, train acc: 0.4688, valid loss: 2.1406, valid acc: 0.3715\n",
      "Iter: 83400, train loss: 1.5962, train acc: 0.4531, valid loss: 2.1487, valid acc: 0.3631\n",
      "Iter: 83500, train loss: 1.2930, train acc: 0.6406, valid loss: 2.1165, valid acc: 0.3708\n",
      "Iter: 83600, train loss: 1.4032, train acc: 0.5469, valid loss: 2.1513, valid acc: 0.3638\n",
      "Iter: 83700, train loss: 1.6281, train acc: 0.4375, valid loss: 2.1241, valid acc: 0.3723\n",
      "Iter: 83800, train loss: 1.3968, train acc: 0.5469, valid loss: 2.1378, valid acc: 0.3662\n",
      "Iter: 83900, train loss: 1.3333, train acc: 0.6094, valid loss: 2.1425, valid acc: 0.3631\n",
      "Iter: 84000, train loss: 1.5566, train acc: 0.5625, valid loss: 2.1437, valid acc: 0.3515\n",
      "Iter: 84100, train loss: 1.5398, train acc: 0.4688, valid loss: 2.1403, valid acc: 0.3585\n",
      "Iter: 84200, train loss: 1.5111, train acc: 0.5000, valid loss: 2.1432, valid acc: 0.3623\n",
      "Iter: 84300, train loss: 1.4765, train acc: 0.5000, valid loss: 2.1532, valid acc: 0.3669\n",
      "Iter: 84400, train loss: 1.6190, train acc: 0.4219, valid loss: 2.1721, valid acc: 0.3608\n",
      "Iter: 84500, train loss: 1.5515, train acc: 0.5000, valid loss: 2.1549, valid acc: 0.3654\n",
      "Iter: 84600, train loss: 1.7958, train acc: 0.3906, valid loss: 2.1372, valid acc: 0.3631\n",
      "Iter: 84700, train loss: 1.2382, train acc: 0.6406, valid loss: 2.1552, valid acc: 0.3662\n",
      "Iter: 84800, train loss: 1.6220, train acc: 0.4688, valid loss: 2.1399, valid acc: 0.3585\n",
      "Iter: 84900, train loss: 1.5319, train acc: 0.4375, valid loss: 2.1335, valid acc: 0.3708\n",
      "Iter: 85000, train loss: 1.6080, train acc: 0.4844, valid loss: 2.1349, valid acc: 0.3654\n",
      "Iter: 85100, train loss: 1.6516, train acc: 0.4219, valid loss: 2.1375, valid acc: 0.3669\n",
      "Iter: 85200, train loss: 1.3979, train acc: 0.4844, valid loss: 2.1461, valid acc: 0.3592\n",
      "Iter: 85300, train loss: 1.6299, train acc: 0.3750, valid loss: 2.1380, valid acc: 0.3615\n",
      "Iter: 85400, train loss: 1.5604, train acc: 0.4688, valid loss: 2.1449, valid acc: 0.3638\n",
      "Iter: 85500, train loss: 1.6442, train acc: 0.5000, valid loss: 2.1349, valid acc: 0.3638\n",
      "Iter: 85600, train loss: 1.5311, train acc: 0.5000, valid loss: 2.1395, valid acc: 0.3638\n",
      "Iter: 85700, train loss: 1.5515, train acc: 0.3906, valid loss: 2.1272, valid acc: 0.3562\n",
      "Iter: 85800, train loss: 1.6726, train acc: 0.4219, valid loss: 2.1530, valid acc: 0.3546\n",
      "Iter: 85900, train loss: 1.4051, train acc: 0.5781, valid loss: 2.1275, valid acc: 0.3608\n",
      "Iter: 86000, train loss: 1.6351, train acc: 0.4688, valid loss: 2.1323, valid acc: 0.3600\n",
      "Iter: 86100, train loss: 1.6419, train acc: 0.4062, valid loss: 2.1385, valid acc: 0.3723\n",
      "Iter: 86200, train loss: 1.4821, train acc: 0.5625, valid loss: 2.1466, valid acc: 0.3538\n",
      "Iter: 86300, train loss: 1.4233, train acc: 0.5156, valid loss: 2.1146, valid acc: 0.3677\n",
      "Iter: 86400, train loss: 1.2868, train acc: 0.5938, valid loss: 2.1268, valid acc: 0.3546\n",
      "Iter: 86500, train loss: 1.6210, train acc: 0.4688, valid loss: 2.1446, valid acc: 0.3454\n",
      "Iter: 86600, train loss: 1.4839, train acc: 0.5000, valid loss: 2.1451, valid acc: 0.3515\n",
      "Iter: 86700, train loss: 1.4653, train acc: 0.5469, valid loss: 2.1228, valid acc: 0.3546\n",
      "Iter: 86800, train loss: 1.5717, train acc: 0.5312, valid loss: 2.1308, valid acc: 0.3569\n",
      "Iter: 86900, train loss: 1.3659, train acc: 0.5938, valid loss: 2.1627, valid acc: 0.3623\n",
      "Iter: 87000, train loss: 1.5130, train acc: 0.5000, valid loss: 2.1522, valid acc: 0.3569\n",
      "Iter: 87100, train loss: 1.6940, train acc: 0.4531, valid loss: 2.1508, valid acc: 0.3569\n",
      "Iter: 87200, train loss: 1.6071, train acc: 0.5312, valid loss: 2.1558, valid acc: 0.3577\n",
      "Iter: 87300, train loss: 1.5519, train acc: 0.5156, valid loss: 2.1460, valid acc: 0.3654\n",
      "Iter: 87400, train loss: 1.5487, train acc: 0.4062, valid loss: 2.1414, valid acc: 0.3600\n",
      "Iter: 87500, train loss: 1.7958, train acc: 0.4688, valid loss: 2.1543, valid acc: 0.3623\n",
      "Iter: 87600, train loss: 1.6450, train acc: 0.3906, valid loss: 2.1308, valid acc: 0.3600\n",
      "Iter: 87700, train loss: 1.6830, train acc: 0.3594, valid loss: 2.1495, valid acc: 0.3577\n",
      "Iter: 87800, train loss: 1.4360, train acc: 0.4688, valid loss: 2.1615, valid acc: 0.3569\n",
      "Iter: 87900, train loss: 1.5081, train acc: 0.5000, valid loss: 2.1360, valid acc: 0.3677\n",
      "Iter: 88000, train loss: 1.5775, train acc: 0.4531, valid loss: 2.1393, valid acc: 0.3623\n",
      "Iter: 88100, train loss: 1.4796, train acc: 0.5469, valid loss: 2.1301, valid acc: 0.3592\n",
      "Iter: 88200, train loss: 1.6473, train acc: 0.5156, valid loss: 2.1397, valid acc: 0.3577\n",
      "Iter: 88300, train loss: 1.3701, train acc: 0.5938, valid loss: 2.1512, valid acc: 0.3569\n",
      "Iter: 88400, train loss: 1.5455, train acc: 0.5000, valid loss: 2.1240, valid acc: 0.3515\n",
      "Iter: 88500, train loss: 1.5067, train acc: 0.5156, valid loss: 2.1526, valid acc: 0.3492\n",
      "Iter: 88600, train loss: 1.4522, train acc: 0.4844, valid loss: 2.1652, valid acc: 0.3585\n",
      "Iter: 88700, train loss: 1.3446, train acc: 0.5625, valid loss: 2.1649, valid acc: 0.3562\n",
      "Iter: 88800, train loss: 1.5158, train acc: 0.5000, valid loss: 2.1559, valid acc: 0.3631\n",
      "Iter: 88900, train loss: 1.7742, train acc: 0.4062, valid loss: 2.1462, valid acc: 0.3631\n",
      "Iter: 89000, train loss: 1.5977, train acc: 0.4375, valid loss: 2.1446, valid acc: 0.3592\n",
      "Iter: 89100, train loss: 1.6001, train acc: 0.4531, valid loss: 2.1374, valid acc: 0.3577\n",
      "Iter: 89200, train loss: 1.7903, train acc: 0.3906, valid loss: 2.1475, valid acc: 0.3577\n",
      "Iter: 89300, train loss: 1.5428, train acc: 0.4844, valid loss: 2.1414, valid acc: 0.3546\n",
      "Iter: 89400, train loss: 1.4273, train acc: 0.4844, valid loss: 2.1447, valid acc: 0.3538\n",
      "Iter: 89500, train loss: 1.5699, train acc: 0.5156, valid loss: 2.1533, valid acc: 0.3600\n",
      "Iter: 89600, train loss: 1.5172, train acc: 0.4844, valid loss: 2.1426, valid acc: 0.3515\n",
      "Iter: 89700, train loss: 1.6270, train acc: 0.4219, valid loss: 2.1351, valid acc: 0.3485\n",
      "Iter: 89800, train loss: 1.4915, train acc: 0.5469, valid loss: 2.1389, valid acc: 0.3615\n",
      "Iter: 89900, train loss: 1.5509, train acc: 0.4375, valid loss: 2.1387, valid acc: 0.3592\n",
      "Iter: 90000, train loss: 1.6397, train acc: 0.4062, valid loss: 2.1466, valid acc: 0.3608\n",
      "Iter: 90100, train loss: 1.5784, train acc: 0.5781, valid loss: 2.1542, valid acc: 0.3477\n",
      "Iter: 90200, train loss: 1.5770, train acc: 0.4688, valid loss: 2.1428, valid acc: 0.3600\n",
      "Iter: 90300, train loss: 1.6603, train acc: 0.3750, valid loss: 2.1687, valid acc: 0.3538\n",
      "Iter: 90400, train loss: 1.6151, train acc: 0.4375, valid loss: 2.1461, valid acc: 0.3585\n",
      "Iter: 90500, train loss: 1.5271, train acc: 0.4531, valid loss: 2.1638, valid acc: 0.3469\n",
      "Iter: 90600, train loss: 1.7077, train acc: 0.4688, valid loss: 2.1651, valid acc: 0.3562\n",
      "Iter: 90700, train loss: 1.6322, train acc: 0.4375, valid loss: 2.1473, valid acc: 0.3485\n",
      "Iter: 90800, train loss: 1.5965, train acc: 0.4531, valid loss: 2.1534, valid acc: 0.3515\n",
      "Iter: 90900, train loss: 1.6283, train acc: 0.5000, valid loss: 2.1675, valid acc: 0.3615\n",
      "Iter: 91000, train loss: 1.6565, train acc: 0.4531, valid loss: 2.1536, valid acc: 0.3662\n",
      "Iter: 91100, train loss: 1.6769, train acc: 0.4219, valid loss: 2.1470, valid acc: 0.3600\n",
      "Iter: 91200, train loss: 1.2847, train acc: 0.5469, valid loss: 2.1552, valid acc: 0.3608\n",
      "Iter: 91300, train loss: 1.6837, train acc: 0.4844, valid loss: 2.1411, valid acc: 0.3523\n",
      "Iter: 91400, train loss: 1.4566, train acc: 0.5000, valid loss: 2.1621, valid acc: 0.3569\n",
      "Iter: 91500, train loss: 1.4391, train acc: 0.5625, valid loss: 2.1341, valid acc: 0.3577\n",
      "Iter: 91600, train loss: 1.6294, train acc: 0.3906, valid loss: 2.1574, valid acc: 0.3600\n",
      "Iter: 91700, train loss: 1.4565, train acc: 0.5312, valid loss: 2.1421, valid acc: 0.3608\n",
      "Iter: 91800, train loss: 1.3986, train acc: 0.5312, valid loss: 2.1576, valid acc: 0.3585\n",
      "Iter: 91900, train loss: 1.5806, train acc: 0.4375, valid loss: 2.1632, valid acc: 0.3646\n",
      "Iter: 92000, train loss: 1.6227, train acc: 0.4062, valid loss: 2.1483, valid acc: 0.3623\n",
      "Iter: 92100, train loss: 2.0333, train acc: 0.3594, valid loss: 2.1599, valid acc: 0.3554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 92200, train loss: 1.8101, train acc: 0.4688, valid loss: 2.1564, valid acc: 0.3608\n",
      "Iter: 92300, train loss: 1.4741, train acc: 0.4219, valid loss: 2.1393, valid acc: 0.3615\n",
      "Iter: 92400, train loss: 1.5577, train acc: 0.5625, valid loss: 2.1483, valid acc: 0.3569\n",
      "Iter: 92500, train loss: 1.9222, train acc: 0.4375, valid loss: 2.1516, valid acc: 0.3531\n",
      "Iter: 92600, train loss: 1.6006, train acc: 0.4375, valid loss: 2.1393, valid acc: 0.3538\n",
      "Iter: 92700, train loss: 1.5986, train acc: 0.4219, valid loss: 2.1335, valid acc: 0.3662\n",
      "Iter: 92800, train loss: 1.3686, train acc: 0.5469, valid loss: 2.1516, valid acc: 0.3669\n",
      "Iter: 92900, train loss: 1.6092, train acc: 0.4531, valid loss: 2.1601, valid acc: 0.3638\n",
      "Iter: 93000, train loss: 1.5881, train acc: 0.4219, valid loss: 2.1634, valid acc: 0.3654\n",
      "Iter: 93100, train loss: 1.4061, train acc: 0.4531, valid loss: 2.1521, valid acc: 0.3685\n",
      "Iter: 93200, train loss: 1.5589, train acc: 0.4062, valid loss: 2.1316, valid acc: 0.3577\n",
      "Iter: 93300, train loss: 1.5340, train acc: 0.4844, valid loss: 2.1463, valid acc: 0.3600\n",
      "Iter: 93400, train loss: 1.5621, train acc: 0.5000, valid loss: 2.1744, valid acc: 0.3646\n",
      "Iter: 93500, train loss: 1.3845, train acc: 0.6094, valid loss: 2.1523, valid acc: 0.3646\n",
      "Iter: 93600, train loss: 1.4656, train acc: 0.4531, valid loss: 2.1471, valid acc: 0.3600\n",
      "Iter: 93700, train loss: 1.4874, train acc: 0.4375, valid loss: 2.1589, valid acc: 0.3662\n",
      "Iter: 93800, train loss: 1.7517, train acc: 0.3750, valid loss: 2.1548, valid acc: 0.3646\n",
      "Iter: 93900, train loss: 1.4572, train acc: 0.4688, valid loss: 2.1522, valid acc: 0.3638\n",
      "Iter: 94000, train loss: 1.7668, train acc: 0.3906, valid loss: 2.1529, valid acc: 0.3654\n",
      "Iter: 94100, train loss: 1.1170, train acc: 0.6250, valid loss: 2.1669, valid acc: 0.3554\n",
      "Iter: 94200, train loss: 1.6407, train acc: 0.4531, valid loss: 2.1203, valid acc: 0.3592\n",
      "Iter: 94300, train loss: 1.7638, train acc: 0.4688, valid loss: 2.1407, valid acc: 0.3615\n",
      "Iter: 94400, train loss: 1.3339, train acc: 0.5781, valid loss: 2.1471, valid acc: 0.3631\n",
      "Iter: 94500, train loss: 1.8078, train acc: 0.4375, valid loss: 2.1528, valid acc: 0.3638\n",
      "Iter: 94600, train loss: 1.3590, train acc: 0.4844, valid loss: 2.1603, valid acc: 0.3631\n",
      "Iter: 94700, train loss: 1.5980, train acc: 0.4688, valid loss: 2.1554, valid acc: 0.3546\n",
      "Iter: 94800, train loss: 1.5861, train acc: 0.4375, valid loss: 2.1557, valid acc: 0.3631\n",
      "Iter: 94900, train loss: 1.6374, train acc: 0.4844, valid loss: 2.1480, valid acc: 0.3608\n",
      "Iter: 95000, train loss: 1.4807, train acc: 0.5938, valid loss: 2.1453, valid acc: 0.3608\n",
      "Iter: 95100, train loss: 1.5075, train acc: 0.4844, valid loss: 2.1775, valid acc: 0.3577\n",
      "Iter: 95200, train loss: 1.5743, train acc: 0.4844, valid loss: 2.1630, valid acc: 0.3562\n",
      "Iter: 95300, train loss: 1.6182, train acc: 0.4531, valid loss: 2.1643, valid acc: 0.3569\n",
      "Iter: 95400, train loss: 1.4788, train acc: 0.6094, valid loss: 2.1639, valid acc: 0.3577\n",
      "Iter: 95500, train loss: 1.9371, train acc: 0.3750, valid loss: 2.1572, valid acc: 0.3631\n",
      "Iter: 95600, train loss: 1.6762, train acc: 0.4531, valid loss: 2.1364, valid acc: 0.3623\n",
      "Iter: 95700, train loss: 1.7167, train acc: 0.4375, valid loss: 2.1476, valid acc: 0.3592\n",
      "Iter: 95800, train loss: 1.8568, train acc: 0.3750, valid loss: 2.1387, valid acc: 0.3638\n",
      "Iter: 95900, train loss: 1.8968, train acc: 0.3750, valid loss: 2.1461, valid acc: 0.3608\n",
      "Iter: 96000, train loss: 1.9104, train acc: 0.3438, valid loss: 2.1439, valid acc: 0.3646\n",
      "Iter: 96100, train loss: 1.4974, train acc: 0.4219, valid loss: 2.1480, valid acc: 0.3608\n",
      "Iter: 96200, train loss: 1.7508, train acc: 0.4062, valid loss: 2.1490, valid acc: 0.3538\n",
      "Iter: 96300, train loss: 1.8678, train acc: 0.3750, valid loss: 2.1585, valid acc: 0.3577\n",
      "Iter: 96400, train loss: 1.6071, train acc: 0.4062, valid loss: 2.1469, valid acc: 0.3600\n",
      "Iter: 96500, train loss: 1.4760, train acc: 0.4531, valid loss: 2.1524, valid acc: 0.3492\n",
      "Iter: 96600, train loss: 1.7490, train acc: 0.4062, valid loss: 2.1374, valid acc: 0.3469\n",
      "Iter: 96700, train loss: 1.3979, train acc: 0.5625, valid loss: 2.1355, valid acc: 0.3554\n",
      "Iter: 96800, train loss: 1.5771, train acc: 0.5000, valid loss: 2.1623, valid acc: 0.3608\n",
      "Iter: 96900, train loss: 1.4081, train acc: 0.5156, valid loss: 2.1540, valid acc: 0.3531\n",
      "Iter: 97000, train loss: 1.4232, train acc: 0.5156, valid loss: 2.1551, valid acc: 0.3646\n",
      "Iter: 97100, train loss: 1.7635, train acc: 0.5000, valid loss: 2.1650, valid acc: 0.3508\n",
      "Iter: 97200, train loss: 1.4602, train acc: 0.5781, valid loss: 2.1533, valid acc: 0.3569\n",
      "Iter: 97300, train loss: 1.9641, train acc: 0.3438, valid loss: 2.1548, valid acc: 0.3554\n",
      "Iter: 97400, train loss: 1.3384, train acc: 0.5312, valid loss: 2.1705, valid acc: 0.3462\n",
      "Iter: 97500, train loss: 1.5590, train acc: 0.4531, valid loss: 2.1552, valid acc: 0.3592\n",
      "Iter: 97600, train loss: 1.7369, train acc: 0.3594, valid loss: 2.1571, valid acc: 0.3615\n",
      "Iter: 97700, train loss: 1.4970, train acc: 0.4688, valid loss: 2.1787, valid acc: 0.3554\n",
      "Iter: 97800, train loss: 1.4409, train acc: 0.5000, valid loss: 2.1624, valid acc: 0.3569\n",
      "Iter: 97900, train loss: 1.5961, train acc: 0.4062, valid loss: 2.1605, valid acc: 0.3585\n",
      "Iter: 98000, train loss: 1.7346, train acc: 0.4688, valid loss: 2.1616, valid acc: 0.3485\n",
      "Iter: 98100, train loss: 1.4824, train acc: 0.5156, valid loss: 2.1510, valid acc: 0.3569\n",
      "Iter: 98200, train loss: 1.6975, train acc: 0.4688, valid loss: 2.1651, valid acc: 0.3562\n",
      "Iter: 98300, train loss: 1.5661, train acc: 0.5156, valid loss: 2.1581, valid acc: 0.3554\n",
      "Iter: 98400, train loss: 1.4706, train acc: 0.4531, valid loss: 2.1519, valid acc: 0.3615\n",
      "Iter: 98500, train loss: 1.5012, train acc: 0.5156, valid loss: 2.1612, valid acc: 0.3554\n",
      "Iter: 98600, train loss: 1.3757, train acc: 0.5312, valid loss: 2.1948, valid acc: 0.3577\n",
      "Iter: 98700, train loss: 1.5863, train acc: 0.5312, valid loss: 2.1551, valid acc: 0.3592\n",
      "Iter: 98800, train loss: 1.6425, train acc: 0.4844, valid loss: 2.1809, valid acc: 0.3562\n",
      "Iter: 98900, train loss: 1.5541, train acc: 0.4531, valid loss: 2.1761, valid acc: 0.3577\n",
      "Iter: 99000, train loss: 1.4423, train acc: 0.4375, valid loss: 2.1532, valid acc: 0.3631\n",
      "Iter: 99100, train loss: 1.6976, train acc: 0.5156, valid loss: 2.1478, valid acc: 0.3554\n",
      "Iter: 99200, train loss: 1.3043, train acc: 0.5625, valid loss: 2.1617, valid acc: 0.3531\n",
      "Iter: 99300, train loss: 1.7257, train acc: 0.3906, valid loss: 2.1722, valid acc: 0.3562\n",
      "Iter: 99400, train loss: 1.3966, train acc: 0.6094, valid loss: 2.1601, valid acc: 0.3508\n",
      "Iter: 99500, train loss: 1.5638, train acc: 0.4531, valid loss: 2.1669, valid acc: 0.3569\n",
      "Iter: 99600, train loss: 1.4565, train acc: 0.5000, valid loss: 2.1839, valid acc: 0.3600\n",
      "Iter: 99700, train loss: 1.6517, train acc: 0.4688, valid loss: 2.1410, valid acc: 0.3631\n",
      "Iter: 99800, train loss: 1.3897, train acc: 0.4844, valid loss: 2.1398, valid acc: 0.3638\n",
      "Iter: 99900, train loss: 1.6105, train acc: 0.5000, valid loss: 2.1658, valid acc: 0.3523\n",
      "Iter: 100000, train loss: 1.7987, train acc: 0.4531, valid loss: 2.1616, valid acc: 0.3638\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "# num_output_units = Y_train.shape[1] # number of classes in this classification problem\n",
    "num_layers = 2 # depth\n",
    "keep_prob = 0.95 # SELU dropout\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers, keep_prob=keep_prob)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIQQA/w8Wkwf59zypuAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d54d88b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000,), (100000,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_train = np.array(nn.losses['train'], dtype=float)\n",
    "loss_valid = np.array(nn.losses['valid'], dtype=float)\n",
    "loss_train.shape, loss_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_train_norm = (loss_train - loss_train.mean(axis=0))/ loss_train.std(axis=0)\n",
    "loss_valid_norm = (loss_valid - loss_valid.mean(axis=0))/ loss_valid.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d54d88978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train_norm, label='Normalized train loss')\n",
    "plt.plot(loss_valid_norm, label='Normalized valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d06aeadd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'Blues',\n",
       " 'Country',\n",
       " 'Electronic',\n",
       " 'Folk',\n",
       " 'International',\n",
       " 'Jazz',\n",
       " 'Latin',\n",
       " 'New_Age',\n",
       " 'Pop_Rock',\n",
       " 'Rap',\n",
       " 'Reggae',\n",
       " 'RnB',\n",
       " 'Vocal']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = labels_keys_sorted.copy()\n",
    "heading.insert(0, 'Id')\n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10400, 13),\n",
       " (10400, 26),\n",
       " (10400, 13),\n",
       " (10400, 14),\n",
       "    Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       " 0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       " \n",
       "    New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       " 0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  )"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, y_logits = nn.test(X_test)\n",
    "y_prob = l.softmax(y_logits)\n",
    "y_prob.shape, X_test.shape, y_logits.shape, test_y_sample.shape, test_y_sample[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for Id, pred in enumerate(y_prob):\n",
    "#     print(Id+1, *pred)\n",
    "    pred_list.append([Id+1, *pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_file = open(file='prediction.csv', mode='w')\n",
    "pred_file.write('\\n') # because of the previous line        \n",
    "\n",
    "for idx in range(len(heading)):\n",
    "    if idx < len(heading) - 1:\n",
    "        pred_file.write(heading[idx] + ',')\n",
    "    else:\n",
    "        pred_file.write(heading[idx] + '\\n')        \n",
    "\n",
    "# len(test), test[0]\n",
    "# for key in test:\n",
    "for i in range(len(pred_list)): # rows\n",
    "    for j in range(len(pred_list[i])): # cols\n",
    "        if j < (len(pred_list[i]) - 1):\n",
    "            pred_file.write(str(pred_list[i][j]))\n",
    "            pred_file.write(',')\n",
    "        else: # last item before starting a new line\n",
    "            pred_file.write(str(pred_list[i][j]) + '\\n')        \n",
    "\n",
    "# pred_file.write(-',')\n",
    "pred_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.068574</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.013440</td>\n",
       "      <td>0.024538</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.667497</td>\n",
       "      <td>0.164285</td>\n",
       "      <td>0.016776</td>\n",
       "      <td>0.001828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.023286</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.012680</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.064853</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.134632</td>\n",
       "      <td>0.743432</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.003351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.018545</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.048679</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.058787</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.132043</td>\n",
       "      <td>0.632624</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.000734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>0.029615</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.088148</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.259878</td>\n",
       "      <td>0.564074</td>\n",
       "      <td>0.014082</td>\n",
       "      <td>0.005868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.003329</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.732700</td>\n",
       "      <td>0.250322</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id     Blues   Country  Electronic      Folk  International      Jazz  \\\n",
       "0   1  0.002392  0.004939    0.068574  0.002917       0.013440  0.024538   \n",
       "1   2  0.023286  0.004126    0.004385  0.002610       0.012680  0.000987   \n",
       "2   3  0.013049  0.002193    0.018545  0.003268       0.048679  0.003299   \n",
       "3   4  0.008601  0.010152    0.007329  0.004134       0.029615  0.001186   \n",
       "4   5  0.000309  0.000159    0.003329  0.000027       0.004999  0.000007   \n",
       "\n",
       "      Latin   New_Age  Pop_Rock       Rap    Reggae       RnB     Vocal  \n",
       "0  0.022177  0.008314  0.002323  0.667497  0.164285  0.016776  0.001828  \n",
       "1  0.064853  0.000094  0.000486  0.134632  0.743432  0.005079  0.003351  \n",
       "2  0.058787  0.001124  0.025355  0.132043  0.632624  0.060300  0.000734  \n",
       "3  0.088148  0.000552  0.006381  0.259878  0.564074  0.014082  0.005868  \n",
       "4  0.006768  0.000010  0.001107  0.732700  0.250322  0.000262  0.000001  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10400, 14), (10400, 14))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').shape, test_y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

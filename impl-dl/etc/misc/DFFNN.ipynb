{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.utils as utils\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "X_train, X_val, X_test = utils.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "\n",
    "# # if net_type == 'cnn':\n",
    "# img_shape = (1, 28, 28)\n",
    "# X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_val = X_val.reshape(-1, *img_shape)\n",
    "# X_test = X_test.reshape(-1, *img_shape)\n",
    "# X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.loss as loss_fun\n",
    "import impl.layer as l\n",
    "import impl.regularization as reg\n",
    "import impl.utils as util\n",
    "import impl.NN as nn\n",
    "\n",
    "class FFNN(nn.NN):\n",
    "\n",
    "    def __init__(self, D, C, H, lam=1e-3, p_dropout=.8, loss='cross_ent', nonlin='relu'):\n",
    "        super().__init__(D, C, H, lam, p_dropout, loss, nonlin)\n",
    "\n",
    "    def _init_model(self, D, C, H):\n",
    "        self.model = dict(\n",
    "            W1=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            W2=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            W3=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((1, H)),\n",
    "            b2=np.zeros((1, H)),\n",
    "            b3=np.zeros((1, C)),\n",
    "            gamma1=np.ones((1, H)),\n",
    "            gamma2=np.ones((1, H)),\n",
    "            beta1=np.zeros((1, H)),\n",
    "            beta2=np.zeros((1, H))\n",
    "        )\n",
    "\n",
    "        self.bn_caches = dict(\n",
    "            bn1_mean=np.zeros((1, H)),\n",
    "            bn2_mean=np.zeros((1, H)),\n",
    "            bn1_var=np.zeros((1, H)),\n",
    "            bn2_var=np.zeros((1, H))\n",
    "        )\n",
    "        \n",
    "    def forward(self, X, train=False):\n",
    "        gamma1, gamma2 = self.model['gamma1'], self.model['gamma2']\n",
    "        beta1, beta2 = self.model['beta1'], self.model['beta2']\n",
    "\n",
    "        u1, u2 = None, None\n",
    "        bn1_cache, bn2_cache = None, None\n",
    "\n",
    "        # First layer\n",
    "        h1, h1_cache = l.fc_forward(X, self.model['W1'], self.model['b1'])\n",
    "        bn1_cache = (self.bn_caches['bn1_mean'], self.bn_caches['bn1_var'])\n",
    "        h1, bn1_cache, run_mean, run_var = l.bn_forward(h1, gamma1, beta1, bn1_cache, train=train)\n",
    "        h1, nl_cache1 = self.forward_nonlin(h1)\n",
    "        self.bn_caches['bn1_mean'], self.bn_caches['bn1_var'] = run_mean, run_var\n",
    "        if train: h1, u1 = l.dropout_forward(h1, self.p_dropout)\n",
    "\n",
    "        # Second layer\n",
    "        h2, h2_cache = l.fc_forward(h1, self.model['W2'], self.model['b2'])\n",
    "        bn2_cache = (self.bn_caches['bn2_mean'], self.bn_caches['bn2_var'])\n",
    "        h2, bn2_cache, run_mean, run_var = l.bn_forward(h2, gamma2, beta2, bn2_cache, train=train)\n",
    "        h2, nl_cache2 = self.forward_nonlin(h2)\n",
    "        self.bn_caches['bn2_mean'], self.bn_caches['bn2_var'] = run_mean, run_var\n",
    "        if train: h2, u2 = l.dropout_forward(h2, self.p_dropout)\n",
    "\n",
    "        # Third layer\n",
    "        score, score_cache = l.fc_forward(h2, self.model['W3'], self.model['b3'])\n",
    "\n",
    "        cache = (X, h1_cache, h2_cache, score_cache, nl_cache1, nl_cache2, u1, u2, bn1_cache, bn2_cache)\n",
    "\n",
    "        return score, cache\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = util.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "\n",
    "        data_loss = np.sum(log_like) / m\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = util.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = self.cross_entropy(y, y_train)\n",
    "        dy = self.dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "\n",
    "    def backward(self, dy, cache):\n",
    "        X, h1_cache, h2_cache, score_cache, nl_cache1, nl_cache2, u1, u2, bn1_cache, bn2_cache = cache\n",
    "\n",
    "        # Third layer\n",
    "        dh2, dW3, db3 = l.fc_backward(dy, score_cache)\n",
    "        dW3 += reg.dl2_reg(self.model['W3'], self.lam)\n",
    "        dh2 = self.backward_nonlin(dh2, nl_cache2)\n",
    "        dh2 = l.dropout_backward(dh2, u2)\n",
    "        dh2, dgamma2, dbeta2 = l.bn_backward(dh2, bn2_cache)\n",
    "\n",
    "        # Second layer\n",
    "        dh1, dW2, db2 = l.fc_backward(dh2, h2_cache)\n",
    "        dW2 += reg.dl2_reg(self.model['W2'], self.lam)\n",
    "        dh1 = self.backward_nonlin(dh1, nl_cache1)\n",
    "        dh1 = l.dropout_backward(dh1, u1)\n",
    "        dh1, dgamma1, dbeta1 = l.bn_backward(dh1, bn1_cache)\n",
    "\n",
    "        # First layer\n",
    "        dX, dW1, db1 = l.fc_backward(dh1, h1_cache)\n",
    "        dW1 += reg.dl2_reg(self.model['W1'], self.lam)\n",
    "\n",
    "        grad = dict(\n",
    "            W1=dW1, W2=dW2, W3=dW3, b1=db1, b2=db2, b3=db3, gamma1=dgamma1,\n",
    "            gamma2=dgamma2, beta1=dbeta1, beta2=dbeta2\n",
    "        )\n",
    "\n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = util.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "# import numpy as np\n",
    "import impl.utils as util\n",
    "import impl.constant as c\n",
    "import copy\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam(nn, X_train, y_train, val_set=None, alpha=0.001, mb_size=256, n_iter=2000, print_after=100):\n",
    "    M = {k: np.zeros_like(v) for k, v in nn.model.items()}\n",
    "    R = {k: np.zeros_like(v) for k, v in nn.model.items()}\n",
    "    beta1 = .9\n",
    "    beta2 = .999\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size)\n",
    "\n",
    "    if val_set:\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        t = iter\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        #         grad, loss = nn.train_step(X_mini, y_mini)\n",
    "        #         def train_step(self, X_train, y_train):\n",
    "        #         \"\"\"\n",
    "        #         Single training step over minibatch: forward, loss, backprop\n",
    "        #         \"\"\"\n",
    "        y, cache = nn.forward(X_mini, train=True)\n",
    "        loss, dy = nn.loss_function(y, y_mini)\n",
    "        dX, grad = nn.backward(dy, cache)\n",
    "        #         return grad, loss\n",
    "\n",
    "        if iter % print_after == 0:\n",
    "            if val_set:\n",
    "                val_acc = util.accuracy(y_val, nn.test(X_val))\n",
    "                print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))\n",
    "\n",
    "        for k in grad:\n",
    "            M[k] = util.exp_running_avg(M[k], grad[k], beta1)\n",
    "            R[k] = util.exp_running_avg(R[k], grad[k]**2, beta2)\n",
    "\n",
    "            m_k_hat = M[k] / (1. - beta1**(t))\n",
    "            r_k_hat = R[k] / (1. - beta2**(t))\n",
    "\n",
    "            nn.model[k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + c.eps)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 20 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # width, timestep for sequential data or minibatch size\n",
    "# num_layers = 5 # depth \n",
    "print_after = 1 # print loss for train, valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 2.5914 validation accuracy: 0.100800\n",
      "Iter-2 training loss: 2.4456 validation accuracy: 0.107400\n",
      "Iter-3 training loss: 2.3638 validation accuracy: 0.112400\n",
      "Iter-4 training loss: 2.4001 validation accuracy: 0.119600\n",
      "Iter-5 training loss: 2.3254 validation accuracy: 0.125800\n",
      "Iter-6 training loss: 2.5086 validation accuracy: 0.134600\n",
      "Iter-7 training loss: 2.3193 validation accuracy: 0.141200\n",
      "Iter-8 training loss: 2.3538 validation accuracy: 0.150200\n",
      "Iter-9 training loss: 2.1430 validation accuracy: 0.159400\n",
      "Iter-10 training loss: 2.1316 validation accuracy: 0.170600\n",
      "Iter-11 training loss: 2.2585 validation accuracy: 0.177600\n",
      "Iter-12 training loss: 2.2326 validation accuracy: 0.185800\n",
      "Iter-13 training loss: 2.2405 validation accuracy: 0.191800\n",
      "Iter-14 training loss: 2.2273 validation accuracy: 0.197200\n",
      "Iter-15 training loss: 2.1351 validation accuracy: 0.207400\n",
      "Iter-16 training loss: 1.9702 validation accuracy: 0.217000\n",
      "Iter-17 training loss: 2.0959 validation accuracy: 0.226200\n",
      "Iter-18 training loss: 2.1727 validation accuracy: 0.234200\n",
      "Iter-19 training loss: 2.1247 validation accuracy: 0.245400\n",
      "Iter-20 training loss: 2.0909 validation accuracy: 0.255600\n",
      "\n",
      "Test Mean accuracy: 0.2647, std: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train, valid, and test\n",
    "net = FFNN(C=C, D=D, H=8, lam=1e-3, p_dropout=0.95)\n",
    "\n",
    "net = adam(nn=net, X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = net.predict(X_test)\n",
    "# accs[k] = np.mean(y_pred == y_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "\n",
    "print()\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

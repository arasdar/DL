{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "# with open('data/text_data/anna.txt', 'r') as f:\n",
    "\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Looking at the X, y\n",
    "# X.shape, y.shape, X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, K, N, char2idx, idx2char):\n",
    "        self.D = D # num_input_dim\n",
    "        self.H = H # num_hidden_units\n",
    "        self.L = L # num_hidden_layers\n",
    "        self.K = K # conv kernel size\n",
    "        self.N = N # number of conv units or neurons\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "                    \n",
    "        # Conv model parameters\n",
    "        # y_DxN = X_DxK @ W_KxN + b_1xN\n",
    "        m = dict(\n",
    "            W = np.random.randn(K, N) / np.sqrt(K / 2.),\n",
    "            b = np.random.randn(1, N)\n",
    "        )\n",
    "        self.model_conv = []\n",
    "        for _ in range(self.L):\n",
    "            self.model_conv.append(m)\n",
    "\n",
    "        # Recurrent Model params\n",
    "        Z = H + (D * N)\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.), \n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        # h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        h = h_in + (hz * (hh - h_in))\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        \n",
    "        dhh = hz * dh\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        # dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        dhz = (hh - h_in) * dh\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXh + dXz\n",
    "        dh_in2 = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "        \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches, caches_conv = [], [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            caches_conv.append([])\n",
    "        \n",
    "        # Embedding, Input layer, 1st layer\n",
    "        Xs = []\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            Xs.append(X)\n",
    "        \n",
    "        for layer in range(self.L):\n",
    "            ys = []\n",
    "            Xs = np.array(Xs).reshape(len(Xs), -1)\n",
    "            n = Xs.shape[1] # Xs_txn\n",
    "            pad = np.zeros((self.K//2, n))\n",
    "            Xs_pad = np.row_stack((pad, Xs, pad))\n",
    "\n",
    "            for i in range(0, len(Xs_pad) - kernel_size + 1, 1):\n",
    "                X = Xs_pad[i: i + kernel_size] # X_txn\n",
    "                # y_DxN = X_DxK @ W_KxN + b_1xN\n",
    "                X_conv = X.T # X_DxK\n",
    "                y, cache = l.fc_forward(X_conv, self.model_conv[layer]['W'], self.model_conv[layer]['b'])\n",
    "                caches_conv[layer].append(cache)\n",
    "                X = y.reshape(1, -1).copy() # X_1xD*N\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                ys.append(y)\n",
    "            Xs = ys.copy()\n",
    "\n",
    "        ys_caches = caches, caches_conv\n",
    "\n",
    "        return ys, ys_caches\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += cross_entropy(y_pred, y)\n",
    "            dy = dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, ys_caches):\n",
    "        dh, grad, grads, grads_conv = [], [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads_conv.append({key: np.zeros_like(val) for key, val in self.model_conv[layer].items()})\n",
    "        \n",
    "        caches, caches_conv = ys_caches\n",
    "        \n",
    "        for layer in reversed(range(self.L)):\n",
    "            # Convolution RNN forward\n",
    "            n = dys[0].reshape(1, -1).shape[1] # y_1xn\n",
    "            t = len(dys)\n",
    "            dXs = np.zeros((t, n))\n",
    "            pad = np.zeros((self.K//2, n))\n",
    "            dXs_pad = np.row_stack((pad, dXs, pad))\n",
    "\n",
    "            for t in reversed(range(len(dys))):\n",
    "                dy = dys[t].reshape(1, -1)\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                dy = dX.reshape((self.D, self.N)).copy() # DxN\n",
    "                dX_conv, dW, db = l.fc_backward(dy, caches_conv[layer][t])\n",
    "                grads_conv[layer]['W'] += dW\n",
    "                grads_conv[layer]['b'] += db\n",
    "                dX = dX_conv.T # X_DxK.T= X_KxD\n",
    "                for i in range(t, t + kernel_size, 1):\n",
    "                    np.add.at(dXs_pad, [i], dX[i-t])\n",
    "            dXs = dXs_pad[kernel_size// 2: -(kernel_size// 2)]\n",
    "            dys = dXs.copy()\n",
    "\n",
    "        grads_all = grads, grads_conv\n",
    "        \n",
    "        return grads_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "#     for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    M_conv, R_conv = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        M_conv.append({k: np.zeros_like(v) for k, v in nn.model_conv[layer].items()})\n",
    "        R_conv.append({k: np.zeros_like(v) for k, v in nn.model_conv[layer].items()})\n",
    "\n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            # Traing\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            grads_all = nn.train_backward(dys, caches)\n",
    "            grads, grads_conv = grads_all\n",
    "            \n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            # Update the weights & biases or model\n",
    "            for layer in range(nn.L):\n",
    "                # Recurrent model\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "                # ConvNet model\n",
    "                for k in grads_conv[layer].keys(): #key, value: items\n",
    "                    M_conv[layer][k] = l.exp_running_avg(M_conv[layer][k], grads_conv[layer][k], beta1)\n",
    "                    R_conv[layer][k] = l.exp_running_avg(R_conv[layer][k], grads_conv[layer][k]**2, beta2)\n",
    "                    m_k_hat = M_conv[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R_conv[layer][k] / (1. - (beta2**(iter)))\n",
    "                    nn.model_conv[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "                    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{}, train loss: {:.4f}'.format(iter, loss))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1, train loss: 74.3937\n",
      "Iter-2, train loss: 57.5399\n",
      "Iter-3, train loss: 46.5271\n",
      "Iter-4, train loss: 43.7953\n",
      "Iter-5, train loss: 30.0062\n",
      "Iter-6, train loss: 32.4532\n",
      "Iter-7, train loss: 27.3127\n",
      "Iter-8, train loss: 25.4037\n",
      "Iter-9, train loss: 21.4417\n",
      "Iter-10, train loss: 23.4881\n",
      "Iter-11, train loss: 26.1213\n",
      "Iter-12, train loss: 18.8867\n",
      "Iter-13, train loss: 16.4161\n",
      "Iter-14, train loss: 19.3961\n",
      "Iter-15, train loss: 19.1377\n",
      "Iter-16, train loss: 19.0623\n",
      "Iter-17, train loss: 15.3628\n",
      "Iter-18, train loss: 13.1310\n",
      "Iter-19, train loss: 12.8232\n",
      "Iter-20, train loss: 10.8488\n",
      "Iter-21, train loss: 10.4388\n",
      "Iter-22, train loss: 11.9792\n",
      "Iter-23, train loss: 10.7403\n",
      "Iter-24, train loss: 10.8400\n",
      "Iter-25, train loss: 9.2785\n",
      "Iter-26, train loss: 8.7134\n",
      "Iter-27, train loss: 6.8621\n",
      "Iter-28, train loss: 5.8310\n",
      "Iter-29, train loss: 5.4995\n",
      "Iter-30, train loss: 5.4457\n",
      "Iter-31, train loss: 5.1594\n",
      "Iter-32, train loss: 5.0942\n",
      "Iter-33, train loss: 4.8607\n",
      "Iter-34, train loss: 4.6971\n",
      "Iter-35, train loss: 4.5773\n",
      "Iter-36, train loss: 4.4426\n",
      "Iter-37, train loss: 4.2934\n",
      "Iter-38, train loss: 4.1639\n",
      "Iter-39, train loss: 4.0608\n",
      "Iter-40, train loss: 3.9454\n",
      "Iter-41, train loss: 3.8249\n",
      "Iter-42, train loss: 3.7171\n",
      "Iter-43, train loss: 3.6048\n",
      "Iter-44, train loss: 3.4846\n",
      "Iter-45, train loss: 3.3671\n",
      "Iter-46, train loss: 3.2545\n",
      "Iter-47, train loss: 3.1472\n",
      "Iter-48, train loss: 3.0360\n",
      "Iter-49, train loss: 2.9241\n",
      "Iter-50, train loss: 2.8074\n",
      "Iter-51, train loss: 2.6911\n",
      "Iter-52, train loss: 2.5792\n",
      "Iter-53, train loss: 2.4627\n",
      "Iter-54, train loss: 2.3601\n",
      "Iter-55, train loss: 2.2685\n",
      "Iter-56, train loss: 2.1769\n",
      "Iter-57, train loss: 2.0918\n",
      "Iter-58, train loss: 2.0071\n",
      "Iter-59, train loss: 1.9240\n",
      "Iter-60, train loss: 1.8453\n",
      "Iter-61, train loss: 1.7666\n",
      "Iter-62, train loss: 1.6936\n",
      "Iter-63, train loss: 1.6218\n",
      "Iter-64, train loss: 1.5537\n",
      "Iter-65, train loss: 1.4885\n",
      "Iter-66, train loss: 1.4260\n",
      "Iter-67, train loss: 1.3653\n",
      "Iter-68, train loss: 1.3090\n",
      "Iter-69, train loss: 1.2542\n",
      "Iter-70, train loss: 1.2026\n",
      "Iter-71, train loss: 1.1520\n",
      "Iter-72, train loss: 1.1065\n",
      "Iter-73, train loss: 1.0611\n",
      "Iter-74, train loss: 1.0194\n",
      "Iter-75, train loss: 0.9807\n",
      "Iter-76, train loss: 0.9432\n",
      "Iter-77, train loss: 0.9069\n",
      "Iter-78, train loss: 0.8738\n",
      "Iter-79, train loss: 0.8420\n",
      "Iter-80, train loss: 0.8092\n",
      "Iter-81, train loss: 0.7797\n",
      "Iter-82, train loss: 0.7456\n",
      "Iter-83, train loss: 0.6984\n",
      "Iter-84, train loss: 0.5877\n",
      "Iter-85, train loss: 0.4395\n",
      "Iter-86, train loss: 0.4035\n",
      "Iter-87, train loss: 0.3923\n",
      "Iter-88, train loss: 0.3846\n",
      "Iter-89, train loss: 0.3760\n",
      "Iter-90, train loss: 0.3704\n",
      "Iter-91, train loss: 0.3653\n",
      "Iter-92, train loss: 0.3585\n",
      "Iter-93, train loss: 0.3539\n",
      "Iter-94, train loss: 0.3493\n",
      "Iter-95, train loss: 0.3441\n",
      "Iter-96, train loss: 0.3379\n",
      "Iter-97, train loss: 0.3343\n",
      "Iter-98, train loss: 0.3285\n",
      "Iter-99, train loss: 0.3243\n",
      "Iter-100, train loss: 0.3198\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfE5BLyLicwp6ERGfU9CLiPjc/we7Mz4sOnlTuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ac88898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 3 # depth\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "kernel_size = 5\n",
    "num_kernels = 2\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, K=kernel_size, N=num_kernels, char2idx=char_to_idx, \n",
    "          idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

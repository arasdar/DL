{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "# with open('data/text_data/anna.txt', 'r') as f:\n",
    "\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[]}\n",
    "        \n",
    "        # Conv model parameters\n",
    "        # y_nx1 = X_nxt @ W_tx1 + b_1x1\n",
    "        # K: kernel size, min_size = 3, i.e. one before, one itself, one after\n",
    "        # N: number of kernels/filters/windows\n",
    "        # K, N = 3, 10\n",
    "        # For the seq or one sequence learning:\n",
    "        K = 2 # kernel \n",
    "        N = 1 # Neurons\n",
    "        self.N = N\n",
    "        # Stride/ scanning one by one: stride = 1\n",
    "        # Pad = K//2 half of the kernel size: pad = 1, zero padding\n",
    "        m = dict(\n",
    "            W = np.random.randn(K, N) / np.sqrt(K / 2.),\n",
    "            b = np.random.randn(1, N)\n",
    "        )\n",
    "        self.model_conv = []\n",
    "        for _ in range(self.L):\n",
    "            self.model_conv.append(m)\n",
    "            \n",
    "        # Recurrent Model params\n",
    "        Z = H + (D*N)\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X = np.column_stack((hr * h_in, X_in))\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz (hh - h_in)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        dhh = hz * dh\n",
    "        dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        # dhz = (hh - h_in) * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh = dXh[:, :self.H]\n",
    "        dX_in2 = dXh[:, self.H:]\n",
    "        dh_in2 = hr * dh\n",
    "\n",
    "        dhr = h_in * dh\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_in3 = dX[:, :self.H]\n",
    "        dX_in1 = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2 + dh_in3\n",
    "        dX = dX_in1 + dX_in2\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches, caches_conv, X_prev = [], [], [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        X_zero = np.zeros((1, self.D)) # 1xD\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            caches_conv.append([])\n",
    "            X_prev.append(X_zero)\n",
    "        \n",
    "        # Embedding, Input layer, 1st layer\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1) # X_1xn\n",
    "            for layer in range(self.L):\n",
    "                X_pad = X_prev[layer].copy() # X_1xD\n",
    "                X_prev[layer] = X.copy()\n",
    "                X_conv = np.row_stack((X_pad, X)).T # (1xD, 1XD).T = Dx2\n",
    "                # y_DxN = X_Dx2 @ W_2xN + b_1xN, D: dim, N: neurons/unit\n",
    "                y, cache = l.fc_forward(X_conv, self.model_conv[layer]['W'], self.model_conv[layer]['b'])\n",
    "                caches_conv[layer].append(cache)\n",
    "                #print('y.shape', y.shape)\n",
    "                X = y.reshape(1, -1).copy() # X_1xD*N\n",
    "                # print('X.shape', X.shape, X.dtype)\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy() # X_1xD \n",
    "            ys.append(y)\n",
    "\n",
    "        ys_caches = (caches, caches_conv)\n",
    "\n",
    "        return ys, ys_caches\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += cross_entropy(y_pred, y)\n",
    "            dy = dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, ys_caches):\n",
    "        dh, grad, grads, grads_conv, dX_prev = [], [], [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads_conv.append({key: np.zeros_like(val) for key, val in self.model_conv[layer].items()})\n",
    "            dX_prev.append(np.zeros((1, self.D))) # This will be dy for the previous layer\n",
    "        \n",
    "        caches, caches_conv = ys_caches\n",
    "        \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                dy = dX.reshape(self.D, self.N).copy()\n",
    "                dX_conv, dW, db = l.fc_backward(dy, caches_conv[layer][t])\n",
    "                grads_conv[layer]['W'] += dW\n",
    "                grads_conv[layer]['b'] += db\n",
    "                dX = dX_conv.T # dX_2xD\n",
    "                dy = dX[1].reshape(1, -1) + dX_prev[layer]\n",
    "                dX_prev[layer] = dX[0].reshape(1, -1)\n",
    "                \n",
    "        grads_all = grads, grads_conv\n",
    "        \n",
    "        return grads_all\n",
    "    \n",
    "#     def test(self, X_seed, h, size):\n",
    "#         chars = [self.idx2char[X_seed]]\n",
    "#         idx_list = list(range(self.vocab_size))\n",
    "#         X = X_seed\n",
    "        \n",
    "#         h_init = h.copy()\n",
    "#         h = []\n",
    "#         for _ in range(self.L):\n",
    "#             h.append(h_init.copy())\n",
    "\n",
    "#         ys = []\n",
    "#         for _ in range(size):\n",
    "#             X_one_hot = np.zeros(self.D)\n",
    "#             X_one_hot[X] = 1.\n",
    "#             X = X_one_hot.reshape(1, -1)\n",
    "#             for layer in range(self.L):\n",
    "#                 y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "#                 X = y.copy()\n",
    "                \n",
    "#             prob = l.softmax(y)\n",
    "#             idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "#             chars.append(self.idx2char[idx])\n",
    "#             X = idx\n",
    "#             #ys.append(prob) # entropy is the loss function\n",
    "\n",
    "#         return ''.join(chars) #, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "#     for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    M_conv, R_conv = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        M_conv.append({k: np.zeros_like(v) for k, v in nn.model_conv[layer].items()})\n",
    "        R_conv.append({k: np.zeros_like(v) for k, v in nn.model_conv[layer].items()})\n",
    "\n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            # Traing\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            grads_all = nn.train_backward(dys, caches)\n",
    "            grads, grads_conv = grads_all\n",
    "            \n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            # Update the weights & biases or model\n",
    "            for layer in range(nn.L):\n",
    "                # Recurrent model\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "                # ConvNet model\n",
    "                for k in grads_conv[layer].keys(): #key, value: items\n",
    "                    M_conv[layer][k] = l.exp_running_avg(M_conv[layer][k], grads_conv[layer][k], beta1)\n",
    "                    R_conv[layer][k] = l.exp_running_avg(R_conv[layer][k], grads_conv[layer][k]**2, beta2)\n",
    "                    m_k_hat = M_conv[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R_conv[layer][k] / (1. - (beta2**(iter)))\n",
    "                    nn.model_conv[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "                    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{}, train loss: {:.4f}'.format(iter, loss))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1, train loss: 47.0896\n",
      "Iter-2, train loss: 46.9066\n",
      "Iter-3, train loss: 44.2837\n",
      "Iter-4, train loss: 42.4659\n",
      "Iter-5, train loss: 41.2715\n",
      "Iter-6, train loss: 40.4115\n",
      "Iter-7, train loss: 39.7401\n",
      "Iter-8, train loss: 39.2772\n",
      "Iter-9, train loss: 38.9953\n",
      "Iter-10, train loss: 38.8389\n",
      "Iter-11, train loss: 38.7500\n",
      "Iter-12, train loss: 38.6994\n",
      "Iter-13, train loss: 38.6640\n",
      "Iter-14, train loss: 38.6366\n",
      "Iter-15, train loss: 38.6140\n",
      "Iter-16, train loss: 38.5946\n",
      "Iter-17, train loss: 38.5776\n",
      "Iter-18, train loss: 38.5625\n",
      "Iter-19, train loss: 38.5490\n",
      "Iter-20, train loss: 38.5368\n",
      "Iter-21, train loss: 38.5258\n",
      "Iter-22, train loss: 38.5157\n",
      "Iter-23, train loss: 38.5065\n",
      "Iter-24, train loss: 38.4981\n",
      "Iter-25, train loss: 38.4904\n",
      "Iter-26, train loss: 38.4832\n",
      "Iter-27, train loss: 38.4765\n",
      "Iter-28, train loss: 38.4703\n",
      "Iter-29, train loss: 38.4646\n",
      "Iter-30, train loss: 38.4592\n",
      "Iter-31, train loss: 38.4541\n",
      "Iter-32, train loss: 38.4494\n",
      "Iter-33, train loss: 38.4449\n",
      "Iter-34, train loss: 38.4407\n",
      "Iter-35, train loss: 38.4367\n",
      "Iter-36, train loss: 38.4330\n",
      "Iter-37, train loss: 38.4294\n",
      "Iter-38, train loss: 38.4265\n",
      "Iter-39, train loss: 38.7407\n",
      "Iter-40, train loss: 38.3781\n",
      "Iter-41, train loss: 38.4179\n",
      "Iter-42, train loss: 38.4128\n",
      "Iter-43, train loss: 38.4106\n",
      "Iter-44, train loss: 38.4084\n",
      "Iter-45, train loss: 38.4061\n",
      "Iter-46, train loss: 38.4040\n",
      "Iter-47, train loss: 38.4019\n",
      "Iter-48, train loss: 38.3998\n",
      "Iter-49, train loss: 38.3978\n",
      "Iter-50, train loss: 38.3959\n",
      "Iter-51, train loss: 38.3940\n",
      "Iter-52, train loss: 38.3923\n",
      "Iter-53, train loss: 38.3906\n",
      "Iter-54, train loss: 38.3889\n",
      "Iter-55, train loss: 38.3873\n",
      "Iter-56, train loss: 38.3858\n",
      "Iter-57, train loss: 38.3843\n",
      "Iter-58, train loss: 38.3829\n",
      "Iter-59, train loss: 38.3816\n",
      "Iter-60, train loss: 38.3803\n",
      "Iter-61, train loss: 38.3790\n",
      "Iter-62, train loss: 38.3778\n",
      "Iter-63, train loss: 38.3766\n",
      "Iter-64, train loss: 38.3755\n",
      "Iter-65, train loss: 38.3744\n",
      "Iter-66, train loss: 38.3733\n",
      "Iter-67, train loss: 38.3723\n",
      "Iter-68, train loss: 38.3713\n",
      "Iter-69, train loss: 38.3704\n",
      "Iter-70, train loss: 38.3695\n",
      "Iter-71, train loss: 38.3686\n",
      "Iter-72, train loss: 38.3678\n",
      "Iter-73, train loss: 38.3669\n",
      "Iter-74, train loss: 38.3662\n",
      "Iter-75, train loss: 38.3654\n",
      "Iter-76, train loss: 38.3647\n",
      "Iter-77, train loss: 38.3639\n",
      "Iter-78, train loss: 38.3633\n",
      "Iter-79, train loss: 38.3626\n",
      "Iter-80, train loss: 38.3620\n",
      "Iter-81, train loss: 38.3613\n",
      "Iter-82, train loss: 38.3607\n",
      "Iter-83, train loss: 38.3602\n",
      "Iter-84, train loss: 38.3596\n",
      "Iter-85, train loss: 38.3591\n",
      "Iter-86, train loss: 38.3586\n",
      "Iter-87, train loss: 38.3581\n",
      "Iter-88, train loss: 38.3576\n",
      "Iter-89, train loss: 38.3571\n",
      "Iter-90, train loss: 38.3567\n",
      "Iter-91, train loss: 38.3562\n",
      "Iter-92, train loss: 38.3558\n",
      "Iter-93, train loss: 38.3554\n",
      "Iter-94, train loss: 38.3550\n",
      "Iter-95, train loss: 38.3547\n",
      "Iter-96, train loss: 38.3543\n",
      "Iter-97, train loss: 38.3540\n",
      "Iter-98, train loss: 38.3536\n",
      "Iter-99, train loss: 38.3533\n",
      "Iter-100, train loss: 38.3530\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndexPosTi+/gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1123f5908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
